import arxiv
import requests
import feedparser
from habanero import Crossref
import bibtexparser
from collections import Counter
from difflib import SequenceMatcher

import re
import time
import itertools
import pickle

# ========== å·¥å…·å‡½æ•°ï¼šå¸ƒå°”è§£æ ==========
def parse_boolean_query(query):
    """
    è§£æå¸ƒå°”æŸ¥è¯¢ï¼Œè¿”å›æ‰€æœ‰å­æŸ¥è¯¢ç»„åˆ
    è¾“å…¥ç¤ºä¾‹ï¼š
      ("precision medicine" OR "digital health") AND ("interpretable machine learning" OR "explainable artificial intelligence")
    è¾“å‡ºï¼š
      ['"precision medicine" "interpretable machine learning"',
       '"precision medicine" "explainable artificial intelligence"',
       '"digital health" "interpretable machine learning"',
       '"digital health" "explainable artificial intelligence"']
    """
    # æŒ‰ AND æ‹†åˆ†å—
    blocks = [b.strip(" ()") for b in query.split("AND")]
    option_lists = []
    for b in blocks:
        parts = [p.strip(" ()\"") for p in b.split("OR")]
        option_lists.append(parts)

    # ç¬›å¡å°”ç§¯ç»„åˆ + ä¿ç•™åŒå¼•å·
    combos = list(itertools.product(*option_lists))
    return [" ".join([f"\"{term}\"" for term in c]) for c in combos]


# ========== æ•°æ®æºå‡½æ•° ==========
def fetch_arxiv(keyword, max_results=20):
    """
    ä» arXiv æœç´¢æ–‡ç« å¹¶å°†ç»“æœä¿å­˜ä¸º BibTeX æ–‡ä»¶ã€‚
    """
    client = arxiv.Client()
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    bib_entries = []
    try:
        for r in client.results(search):
            bib_entry = {
                "ENTRYTYPE": "article",
                "ID": r.get_short_id(),
                "title": r.title,
                "author": " and ".join([str(author) for author in r.authors]),
                "year": str(r.published.year),
                "journal": "arXiv preprint",
                "eprint": r.get_short_id(),
                "url": r.entry_id,
                "abstract": r.summary,
                "doi": r.doi if r.doi else "",
                "comment": r.comment if r.comment else "", 
				"citations": None,
				"source": "arXiv"
            }
            bib_entries.append(bib_entry)
    except arxiv.UnexpectedEmptyPageError:
        print(f"å…³é”®è¯ã€{query}ã€‘ç»“æœä¸è¶³ï¼Œå·²è·³è¿‡ç©ºé¡µã€‚")
    print(f"å…³é”®è¯ã€{query}ã€‘ï¼š")
    print(f"å…± {len(bib_entries)} æ¡è®°å½•ã€‚")
    return bib_entries

def fetch_crossref(keyword, max_results=20):
    """ä» Crossref æŠ“å–æ–‡çŒ®"""
    cr = Crossref(timeout=60)  # åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®è¶…æ—¶æ—¶é—´

    try:
        results = cr.works(query=keyword, limit=10)
        papers = []
        for item in results['message']['items']:
            papers.append({
                "title": item.get("title", [""])[0],
                "authors": [a.get("family", "") for a in item.get("author", []) if "family" in a],
                "year": item.get("issued", {}).get("date-parts", [[None]])[0][0],
                "doi": item.get("DOI", None),
                "url": item.get("URL", None),
                "citations": item.get("is-referenced-by-count", None),
                "source": "Crossref"
            })
    except requests.exceptions.Timeout:
        print("è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œã€‚")
        papers = []
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯ï¼š{e}")
        papers = []
    return papers

def fetch_semantic(doi=None, title=None):
    """ä» Semantic Scholar è·å–å¼•ç”¨æ•°"""
    base = "https://api.semanticscholar.org/graph/v1/paper/"
    if doi:
        url = base + f"DOI:{doi}?fields=citationCount"
    elif title:
        url = f"https://api.semanticscholar.org/graph/v1/paper/search?query={title}&limit=1&fields=citationCount"
    else:
        return None

    try:
        r = requests.get(url, timeout=10)
        if r.status_code == 200:
            data = r.json()
            if doi:
                return data.get("citationCount", None)
            elif "data" in data and data["data"]:
                return data["data"][0].get("citationCount", None)
    except Exception:
        return None
    return None

# ========== å»é‡ä¸è¡¥å…¨ ==========
def deduplicate_and_enrich(papers):
    """å»é‡ï¼šä¼˜å…ˆä¿ç•™ Crossref/DOI"""
    seen = {}
    for p in papers:
        key = p["doi"].lower() if p["doi"] else p["title"].lower()
        if key in seen:
            if p["source"] != "arXiv":  # ä¿ç•™æ­£å¼å‡ºç‰ˆ
                seen[key] = p
        else:
            seen[key] = p

    # è¡¥å…¨å¼•ç”¨æ•°
    for k, p in seen.items():
        if p["citations"] is None:
            p["citations"] = fetch_semantic(p["doi"], p["title"])
            time.sleep(0.2)
    return list(seen.values())

# ========== BibTeX & åˆ†æ ==========
def to_bibtex(papers, filename="output.bib"):
    """ä¿å­˜ä¸º BibTeX"""
    db = []
    for i, p in enumerate(papers):
        # entry = {
        #     "ENTRYTYPE": "article",
        #     "ID": p["ID"] if "ID" in p else f"paper{i+1}",
        #     "title": p["title"],
        #     "author": p["author"],
        #     "year": str(p["year"]),
        #     "url": p["url"],
        #     "note": f"Cited {p['citations']} times" if p["citations"] is not None else "Citations: NA"
        # }
        p["citations"] = str(p["citations"]) if p["citations"] is not None else ""
        db.append(p)

    bib_db = bibtexparser.bibdatabase.BibDatabase()
    bib_db.entries = db

    with open(filename, "w", encoding="utf-8") as bibfile:
        bibtexparser.dump(bib_db, bibfile)
        
    print(f"âœ… å·²ç”Ÿæˆ {filename}ï¼Œå…± {len(db)} æ¡æ–‡çŒ®")

def keyword_analysis(papers):
    text = " ".join(p["title"] for p in papers)
    words = re.findall(r"\w+", text.lower())
    counter = Counter(words)
    print("\nğŸ“Š Top 15 é«˜é¢‘è¯ï¼š")
    for word, freq in counter.most_common(15):
        print(f"{word}: {freq}")

def author_analysis(papers):
    authors = []
    for p in papers:
        authors.extend(p["author"])
    counter = Counter(authors)
    print("\nğŸ‘¥ Top 10 é«˜é¢‘ä½œè€…ï¼š")
    for author, freq in counter.most_common(10):
        print(f"{author}: {freq}")

# ========== æ–°å¢å‡½æ•° ==========
def find_doi_by_title(title, authors=None, topn=5):
    """
    ç”¨ Crossref æŸ¥æ‰¾æŸä¸ªæ ‡é¢˜çš„ DOIï¼ŒæŒ‰æ ‡é¢˜ç›¸ä¼¼åº¦ + ä½œè€…éªŒè¯
    """
    cr = Crossref(timeout=60)
    try:
        results = cr.works(query=title, limit=topn)
        time.sleep(1)
    except Exception as e:
        print(f"Crossref æŸ¥è¯¢å¤±è´¥ï¼š{e}")
        return None

    best_match = None
    best_score = 0
    for item in results['message']['items']:
        candidate_title = item.get("title", [""])[0]
        candidate_authors = [a.get("family", "").lower() for a in item.get("author", []) if "family" in a]
        candidate_doi = item.get("DOI", None)

        # æ ‡é¢˜ç›¸ä¼¼åº¦
        score = SequenceMatcher(None, title.lower(), candidate_title.lower()).ratio()

        # ä½œè€…éªŒè¯ï¼ˆæœ‰äº¤é›†åŠ åˆ†ï¼‰
        if authors:
            overlap = len(set(a.lower() for a in authors) & set(candidate_authors))
            if overlap > 0:
                score += 0.1

        if score > best_score:
            best_score = score
            best_match = candidate_doi

    return best_match if best_score > 0.75 else None


# ========== ä¸»ç¨‹åºä¿®æ”¹ ==========
if __name__ == "__main__":
    query = '("Artificial Intelligence" OR "Machine Learning" OR "Deep Learning") AND ("Explainable AI" OR "XAI" OR "Interpretability" OR "Transparency") AND ("Precision Medicine" OR "Personalized Medicine" OR "Healthcare" OR "Medical Diagnosis")'
    sub_queries = parse_boolean_query(query)
    print(f"ğŸ” å·²æ‹†è§£ä¸º {len(sub_queries)} ä¸ªå­æŸ¥è¯¢ï¼š")
    for sq in sub_queries:
        print("   ", sq)

    papers = []
    for sq in sub_queries:  # ä»…æµ‹è¯•ç¬¬ä¸€ä¸ªå­æŸ¥è¯¢
        # å…ˆæŠ“å– arXiv
        print(f"=== å¤„ç†å­æŸ¥è¯¢ï¼š{sq} ===")
        arxiv_entries = fetch_arxiv(sq, 1000)

        # å¯¹æ¯ä¸€æ¡ arXiv ç»“æœï¼Œå°è¯•æŸ¥æ‰¾ DOI
        entrys = []
        for entry in arxiv_entries:
            print(f"ğŸ” æŸ¥æ‰¾ DOI: {entry['title'][:60]}...")
            authors = entry["author"].split(" and ")
            doi_guess = find_doi_by_title(entry["title"], authors)
            if doi_guess:
                entry["doi"] = doi_guess
                print(f"âœ… åŒ¹é…åˆ° DOI: {doi_guess} | {entry['title'][:60]}...")

            entrys.append(entry)
        papers += entrys
    # æŒä¹…åŒ– papers ä¸º pickle æ ¼å¼
    with open("arxiv_raw.pkl", "wb") as f:
        pickle.dump(papers, f)
    print(f"\nğŸ“¥ å…±æŠ“å– {len(papers)} æ¡ arXivï¼Œå¹¶å·²ä¿å­˜ä¸º arxiv_raw.pkl")

    # è¯»å– pickle æ–‡ä»¶ç¤ºä¾‹
    with open("arxiv_raw.pkl", "rb") as f:
        papers = pickle.load(f)
    
    enriched = deduplicate_and_enrich(papers)

    to_bibtex(enriched, "arxiv1.bib")
    # keyword_analysis(enriched)
    # author_analysis(enriched)

