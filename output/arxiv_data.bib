@article{2406.03012v2,
 abstract = {Explainable AI (XAI) is widely used to analyze AI systems' decision-making, such as providing counterfactual explanations for recourse. When unexpected explanations occur, users may want to understand the training data properties shaping them. Under the umbrella of data valuation, first approaches have been proposed that estimate the influence of data samples on a given model. This process not only helps determine the data's value, but also offers insights into how individual, potentially noisy, or misleading examples affect a model, which is crucial for interpretable AI. In this work, we apply the concept of data valuation to the significant area of model evaluations, focusing on how individual training samples impact a model's internal reasoning rather than the predictive performance only. Hence, we introduce the novel problem of identifying training samples shaping a given explanation or related quantity, and investigate the particular case of the cost of computational recourse. We propose an algorithm to identify such influential samples and conduct extensive empirical evaluations in two case studies.},
 author = {André Artelt and Barbara Hammer},
 comment = {Extended version of the paper accepted at the "Workshop on Explainable Artificial Intelligence (XAI)" at IJCAI 2024},
 doi = {},
 eprint = {2406.03012v2},
 journal = {arXiv preprint},
 title = {Towards Understanding the Influence of Training Samples on Explanations},
 url = {http://arxiv.org/abs/2406.03012v2},
 year = {2024}
}

@article{2406.03594v1,
 abstract = {Explainable AI (XAI) algorithms aim to help users understand how a machine learning model makes predictions. To this end, many approaches explain which input features are most predictive of a target label. However, such explanations can still be puzzling to users (e.g., in product reviews, the word "problems" is predictive of positive sentiment). If left unexplained, puzzling explanations can have negative impacts. Explaining unintuitive associations between an input feature and a target label is an underexplored area in XAI research. We take an initial effort in this direction using unintuitive associations learned by sentiment classifiers as a case study. We propose approaches for (1) automatically detecting associations that can appear unintuitive to users and (2) generating explanations to help users understand why an unintuitive feature is predictive. Results from a crowdsourced study (N=300) found that our proposed approaches can effectively detect and explain predictive but unintuitive features in sentiment classification.},
 author = {Jiaming Qu and Jaime Arguello and Yue Wang},
 comment = {},
 doi = {10.1145/3630106.3658547},
 eprint = {2406.03594v1},
 journal = {arXiv preprint},
 title = {Why is "Problems" Predictive of Positive Sentiment? A Case Study of Explaining Unintuitive Features in Sentiment Classification},
 url = {http://arxiv.org/abs/2406.03594v1},
 year = {2024}
}

@article{2406.03921v1,
 abstract = {The advancement of science relies on the exchange of ideas across disciplines and the integration of diverse knowledge domains. However, tracking knowledge flows and interdisciplinary integration in rapidly evolving, multidisciplinary fields remains a significant challenge. This work introduces a novel network analysis framework to study the dynamics of knowledge transfer directly from citation data. By applying dynamic community detection to cumulative, time-evolving citation networks, we can identify research areas as groups of papers sharing knowledge sources and outputs. Our analysis characterises the life-cycles and knowledge transfer patterns of these dynamic communities over time. We demonstrate our approach through a case study of eXplainable Artificial Intelligence (XAI) research, an emerging interdisciplinary field at the intersection of machine learning, statistics, and psychology. Key findings include: (i) knowledge transfer between these important foundational topics and the contemporary topics in XAI research is limited, and the extent of knowledge transfer varies across different contemporary research topics; (ii) certain application domains exist as isolated "knowledge silos"; (iii) significant "knowledge gaps" are identified between related XAI research areas, suggesting opportunities for cross-pollination and improved knowledge integration. By mapping interdisciplinary integration and bridging knowledge gaps, this work can inform strategies to synthesise ideas from disparate sources and drive innovation. More broadly, our proposed framework enables new insights into the evolution of knowledge ecosystems directly from citation data, with applications spanning literature review, research planning, and science policy.},
 author = {Eoghan Cunningham and Derek Greene},
 comment = {},
 doi = {},
 eprint = {2406.03921v1},
 journal = {arXiv preprint},
 title = {Knowledge Transfer, Knowledge Gaps, and Knowledge Silos in Citation Networks},
 url = {http://arxiv.org/abs/2406.03921v1},
 year = {2024}
}

@article{2406.04280v3,
 abstract = {Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more general assumptions. We demonstrate how to obtain improved MIL explanations using layer-wise relevance propagation (LRP) and conduct extensive evaluation experiments on three toy settings and four real-world histopathology datasets. Our approach consistently outperforms previous explanation attempts with particularly improved faithfulness scores on challenging biomarker prediction tasks. Finally, we showcase how xMIL explanations enable pathologists to extract insights from MIL models, representing a significant advance for knowledge discovery and model debugging in digital histopathology. Codes are available at: https://github.com/bifold-pathomics/xMIL.},
 author = {Julius Hense and Mina Jamshidi Idaji and Oliver Eberle and Thomas Schnake and Jonas Dippel and Laure Ciernik and Oliver Buchstab and Andreas Mock and Frederick Klauschen and Klaus-Robert Müller},
 comment = {},
 doi = {},
 eprint = {2406.04280v3},
 journal = {arXiv preprint},
 title = {xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology},
 url = {http://arxiv.org/abs/2406.04280v3},
 year = {2024}
}

@article{2406.04610v2,
 abstract = {This paper presents a novel approach to Explainable AI (XAI) that combines contrastive explanations with differential privacy for clustering algorithms. Focusing on k-median and k-means problems, we calculate contrastive explanations as the utility difference between original clustering and clustering with a centroid fixed to a specific data point. This method provides personalized insights into centroid placement. Our key contribution is demonstrating that these differentially private explanations achieve essentially the same utility bounds as non-private explanations. Experiments across various datasets show that our approach offers meaningful, privacy-preserving, and individually relevant explanations without significantly compromising clustering utility. This work advances privacy-aware machine learning by balancing data protection, explanation quality, and personalization in clustering tasks.},
 author = {Dung Nguyen and Ariel Vetzler and Sarit Kraus and Anil Vullikanti},
 comment = {Accepted by AAMAS 2025: https://www.ifaamas.org/Proceedings/aamas2025/forms/contents.htm},
 doi = {},
 eprint = {2406.04610v2},
 journal = {arXiv preprint},
 title = {Contrastive Explainable Clustering with Differential Privacy},
 url = {http://arxiv.org/abs/2406.04610v2},
 year = {2024}
}

@article{2406.04612v2,
 abstract = {The self-attention mechanism has been adopted in various popular message passing neural networks (MPNNs), enabling the model to adaptively control the amount of information that flows along the edges of the underlying graph. Such attention-based MPNNs (Att-GNNs) have also been used as a baseline for multiple studies on explainable AI (XAI) since attention has steadily been seen as natural model interpretations, while being a viewpoint that has already been popularized in other domains (e.g., natural language processing and computer vision). However, existing studies often use naive calculations to derive attribution scores from attention, undermining the potential of attention as interpretations for Att-GNNs. In our study, we aim to fill the gap between the widespread usage of Att-GNNs and their potential explainability via attention. To this end, we propose GATT, edge attribution calculation method for self-attention MPNNs based on the computation tree, a rooted tree that reflects the computation process of the underlying model. Despite its simplicity, we empirically demonstrate the effectiveness of GATT in three aspects of model explanation: faithfulness, explanation accuracy, and case studies by using both synthetic and real-world benchmark datasets. In all cases, the results demonstrate that GATT greatly improves edge attribution scores, especially compared to the previous naive approach. Our code is available at https://github.com/jordan7186/GAtt.},
 author = {Yong-Min Shin and Siqing Li and Xin Cao and Won-Yong Shin},
 comment = {29 pages, 14 figures, 17 tables; an extended version of our paper to be presented at the 39th AAAI Conference on Artificial Intelligence (AAAI-25) (Please cite our conference version.)},
 doi = {},
 eprint = {2406.04612v2},
 journal = {arXiv preprint},
 title = {Faithful and Accurate Self-Attention Attribution for Message Passing Neural Networks via the Computation Tree Viewpoint},
 url = {http://arxiv.org/abs/2406.04612v2},
 year = {2024}
}

@article{2406.05984v1,
 abstract = {Mental health constitutes a complex and pervasive global challenge, affecting millions of lives and often leading to severe consequences. In this paper, we conduct a thorough survey to explore the intersection of data science, artificial intelligence, and mental healthcare, focusing on the recent developments of mental disorder detection through online social media (OSM). A significant portion of the population actively engages in OSM platforms, creating a vast repository of personal data that holds immense potential for mental health analytics. The paper navigates through traditional diagnostic methods, state-of-the-art data- and AI-driven research studies, and the emergence of explainable AI (XAI) models for mental healthcare. We review state-of-the-art machine learning methods, particularly those based on modern deep learning, while emphasising the need for explainability in healthcare AI models. The experimental design section provides insights into prevalent practices, including available datasets and evaluation approaches. We also identify key issues and challenges in the field and propose promising future research directions. As mental health decisions demand transparency, interpretability, and ethical considerations, this paper contributes to the ongoing discourse on advancing XAI in mental healthcare through social media. The comprehensive overview presented here aims to guide researchers, practitioners, and policymakers in developing the area of mental disorder detection.},
 author = {Yusif Ibrahimov and Tarique Anwar and Tommy Yuan},
 comment = {},
 doi = {},
 eprint = {2406.05984v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Mental Disorder Detection via Social Media: A survey and outlook},
 url = {http://arxiv.org/abs/2406.05984v1},
 year = {2024}
}

@article{2406.06596v1,
 abstract = {A Language Model is a term that encompasses various types of models designed to understand and generate human communication. Large Language Models (LLMs) have gained significant attention due to their ability to process text with human-like fluency and coherence, making them valuable for a wide range of data-related tasks fashioned as pipelines. The capabilities of LLMs in natural language understanding and generation, combined with their scalability, versatility, and state-of-the-art performance, enable innovative applications across various AI-related fields, including eXplainable Artificial Intelligence (XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG). Furthermore, we believe these models can extract valuable insights and make data-driven decisions at scale, a practice commonly referred to as Big Data Analytics (BDA). In this position paper, we provide some discussions in the direction of unlocking synergies among these technologies, which can lead to more powerful and intelligent AI solutions, driving improvements in data pipelines across a wide range of applications and domains integrating humans, computers, and knowledge.},
 author = {Sylvio Barbon Junior and Paolo Ceravolo and Sven Groppe and Mustafa Jarrar and Samira Maghool and Florence Sèdes and Soror Sahri and Maurice Van Keulen},
 comment = {},
 doi = {},
 eprint = {2406.06596v1},
 journal = {arXiv preprint},
 title = {Are Large Language Models the New Interface for Data Pipelines?},
 url = {http://arxiv.org/abs/2406.06596v1},
 year = {2024}
}

@article{2406.07369v1,
 abstract = {In this paper we present results from a qualitative field study on explainable AI (XAI) for lay users (n = 18) who were subjected to AI cyberattacks. The study was based on a custom-built smart heating application called Squid and was conducted over seven weeks in early 2023. Squid combined a smart radiator valve installed in participant homes with a web application that implemented an AI feature known as setpoint learning, which is commonly available in consumer smart thermostats. Development of Squid followed the XAI principle of interpretability-by-design where the AI feature was implemented using a simple glass-box machine learning model with the model subsequently exposed to users via the web interface (e.g. as interactive visualisations). AI attacks on users were simulated by injecting malicious training data and by manipulating data used for model predictions. Research data consisted of semi-structured interviews, researcher field notes, participant diaries, and application logs. In our analysis we reflect on the impact of XAI on user satisfaction and user comprehension as well as its use as a tool for diagnosing AI attacks. Our results show only limited engagement with XAI features and suggest that, for Squid users, common assumptions found in the XAI literature were not aligned to reality. On the positive side, users appear to have developed better mental models of the AI feature compared to previous work, and there is evidence that users did make some use of XAI as a diagnostic tool.},
 author = {Kevin McAreavey and Weiru Liu and Kim Bauters and Dennis Ivory and George Loukas and Manos Panaousis and Hsueh-Ju Chen and Rea Gill and Rachael Payler and Asimina Vasalou},
 comment = {},
 doi = {},
 eprint = {2406.07369v1},
 journal = {arXiv preprint},
 title = {A qualitative field study on explainable AI for lay users subjected to AI cyberattacks},
 url = {http://arxiv.org/abs/2406.07369v1},
 year = {2024}
}

@article{2406.07811v2,
 abstract = {Artificial intelligence methods are being increasingly applied across various domains, but their often opaque nature has raised concerns about accountability and trust. In response, the field of explainable AI (XAI) has emerged to address the need for human-understandable AI systems. Evolutionary computation (EC), a family of powerful optimization and learning algorithms, offers significant potential to contribute to XAI, and vice versa. This paper provides an introduction to XAI and reviews current techniques for explaining machine learning models. We then explore how EC can be leveraged in XAI and examine existing XAI approaches that incorporate EC techniques. Furthermore, we discuss the application of XAI principles within EC itself, investigating how these principles can illuminate the behavior and outcomes of EC algorithms, their (automatic) configuration, and the underlying problem landscapes they optimize. Finally, we discuss open challenges in XAI and highlight opportunities for future research at the intersection of XAI and EC. Our goal is to demonstrate EC's suitability for addressing current explainability challenges and to encourage further exploration of these methods, ultimately contributing to the development of more understandable and trustworthy ML models and EC algorithms.},
 author = {Ryan Zhou and Jaume Bacardit and Alexander Brownlee and Stefano Cagnoni and Martin Fyvie and Giovanni Iacca and John McCall and Niki van Stein and David Walker and Ting Hu},
 comment = {24 pages, 4 figures. arXiv admin note: substantial text overlap with arXiv:2306.14786},
 doi = {},
 eprint = {2406.07811v2},
 journal = {arXiv preprint},
 title = {Evolutionary Computation and Explainable AI: A Roadmap to Understandable Intelligent Systems},
 url = {http://arxiv.org/abs/2406.07811v2},
 year = {2024}
}

@article{2406.07820v1,
 abstract = {Explainable AI (XAI) has revolutionized the field of deep learning by empowering users to have more trust in neural network models. The field of XAI allows users to probe the inner workings of these algorithms to elucidate their decision-making processes. The rise in popularity of XAI has led to the advent of different strategies to produce explanations, all of which only occasionally agree. Thus several objective evaluation metrics have been devised to decide which of these modules give the best explanation for specific scenarios. The goal of the paper is twofold: (i) we employ the notions of necessity and sufficiency from causal literature to come up with a novel explanatory technique called SHifted Adversaries using Pixel Elimination(SHAPE) which satisfies all the theoretical and mathematical criteria of being a valid explanation, (ii) we show that SHAPE is, infact, an adversarial explanation that fools causal metrics that are employed to measure the robustness and reliability of popular importance based visual XAI methods. Our analysis shows that SHAPE outperforms popular explanatory techniques like GradCAM and GradCAM++ in these tests and is comparable to RISE, raising questions about the sanity of these metrics and the need for human involvement for an overall better evaluation.},
 author = {Prithwijit Chowdhury and Mohit Prabhushankar and Ghassan AlRegib and Mohamed Deriche},
 comment = {},
 doi = {},
 eprint = {2406.07820v1},
 journal = {arXiv preprint},
 title = {Are Objective Explanatory Evaluation metrics Trustworthy? An Adversarial Analysis},
 url = {http://arxiv.org/abs/2406.07820v1},
 year = {2024}
}

@article{2406.09335v2,
 abstract = {In recent years, explainable methods for artificial intelligence (XAI) have tried to reveal and describe models' decision mechanisms in the case of classification tasks. However, XAI for semantic segmentation and in particular for single instances has been little studied to date. Understanding the process underlying automatic segmentation of single instances is crucial to reveal what information was used to detect and segment a given object of interest. In this study, we proposed two instance-level explanation maps for semantic segmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated their relevance for the detection and segmentation of white matter lesions (WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS). 687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans were collected at the University Hospital of Basel, Switzerland. Data were randomly split into training, validation and test sets to train a 3D U-Net for MS lesion segmentation. We observed 3050 true positive (TP), 1818 false positive (FP), and 789 false negative (FN) cases. We generated instance-level explanation maps for semantic segmentation, by developing two XAI methods based on SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients in saliency maps with respect to both input MRI sequences; 2) the model's response in the case of synthetic lesions; 3) the amount of perilesional tissue needed by the model to segment a lesion. Saliency maps (based on SmoothGrad) in FLAIR showed positive values inside a lesion and negative in its neighborhood. Peak values of saliency maps generated for these four groups of volumes presented distributions that differ significantly from one another, suggesting a quantitative nature of the proposed saliency. Contextual information of 7mm around the lesion border was required for their segmentation.},
 author = {Federico Spagnolo and Nataliia Molchanova and Roger Schaer and Meritxell Bach Cuadra and Mario Ocampo Pineda and Lester Melie-Garcia and Cristina Granziera and Vincent Andrearczyk and Adrien Depeursinge},
 comment = {},
 doi = {},
 eprint = {2406.09335v2},
 journal = {arXiv preprint},
 title = {Instance-level quantitative saliency in multiple sclerosis lesion segmentation},
 url = {http://arxiv.org/abs/2406.09335v2},
 year = {2024}
}

@article{2406.09684v2,
 abstract = {Explainable Artificial Intelligence (XAI) has become a widely discussed topic, the related technologies facilitate better understanding of conventional black-box models like Random Forest, Neural Networks and etc. However, domain-specific applications of XAI are still insufficient. To fill this gap, this research analyzes various machine learning models to the tasks of binary and multi-class classification for intrusion detection from network traffic on the same dataset using occlusion sensitivity. The models evaluated include Linear Regression, Logistic Regression, Linear Support Vector Machine (SVM), K-Nearest Neighbors (KNN), Random Forest, Decision Trees, and Multi-Layer Perceptrons (MLP). We trained all models to the accuracy of 90\% on the UNSW-NB15 Dataset. We found that most classifiers leverage only less than three critical features to achieve such accuracies, indicating that effective feature engineering could actually be far more important for intrusion detection than applying complicated models. We also discover that Random Forest provides the best performance in terms of accuracy, time efficiency and robustness. Data and code available at https://github.com/pcwhy/XML-IntrusionDetection.git},
 author = {Pap M. Corea and Yongxin Liu and Jian Wang and Shuteng Niu and Houbing Song},
 comment = {Submitted to IEEE MeditCom 2024 - WS-05},
 doi = {},
 eprint = {2406.09684v2},
 journal = {arXiv preprint},
 title = {Explainable AI for Comparative Analysis of Intrusion Detection Models},
 url = {http://arxiv.org/abs/2406.09684v2},
 year = {2024}
}

@article{2406.11504v1,
 abstract = {As one of popular quantitative metrics to assess the quality of explanation of graph neural networks (GNNs), fidelity measures the output difference after removing unimportant parts of the input graph. Fidelity has been widely used due to its straightforward interpretation that the underlying model should produce similar predictions when features deemed unimportant from the explanation are removed. This raises a natural question: "Does fidelity induce a global (soft) mask for graph pruning?" To solve this, we aim to explore the potential of the fidelity measure to be used for graph pruning, eventually enhancing the GNN models for better efficiency. To this end, we propose Fidelity$^-$-inspired Pruning (FiP), an effective framework to construct global edge masks from local explanations. Our empirical observations using 7 edge attribution methods demonstrate that, surprisingly, general eXplainable AI methods outperform methods tailored to GNNs in terms of graph pruning performance.},
 author = {Yong-Min Shin and Won-Yong Shin},
 comment = {6 pages, 3 figures, 2 tables; IJCAI Workshop on Explainable AI (XAI 2024) (to appear) (Please cite our workshop version.)},
 doi = {},
 eprint = {2406.11504v1},
 journal = {arXiv preprint},
 title = {On the Feasibility of Fidelity$^-$ for Graph Pruning},
 url = {http://arxiv.org/abs/2406.11504v1},
 year = {2024}
}

@article{2406.11524v1,
 abstract = {Explainable Artificial Intelligence (XAI) methods help to understand the internal mechanism of machine learning models and how they reach a specific decision or made a specific action. The list of informative features is one of the most common output of XAI methods. Multicollinearity is one of the big issue that should be considered when XAI generates the explanation in terms of the most informative features in an AI system. No review has been dedicated to investigate the current approaches to handle such significant issue. In this paper, we provide a review of the current state-of-the-art approaches in relation to the XAI in the context of recent advances in dealing with the multicollinearity issue. To do so, we searched in three repositories that are: Web of Science, Scopus and IEEE Xplore to find pertinent published papers. After excluding irrelevant papers, seven papers were considered in the review. In addition, we discuss the current XAI methods and their limitations in dealing with the multicollinearity and suggest future directions.},
 author = {Ahmed M Salih},
 comment = {},
 doi = {},
 eprint = {2406.11524v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence and Multicollinearity : A Mini Review of Current Approaches},
 url = {http://arxiv.org/abs/2406.11524v1},
 year = {2024}
}

@article{2406.11547v1,
 abstract = {Large pre-trained language models have become popular for many applications and form an important backbone of many downstream tasks in natural language processing (NLP). Applying 'explainable artificial intelligence' (XAI) techniques to enrich such models' outputs is considered crucial for assuring their quality and shedding light on their inner workings. However, large language models are trained on a plethora of data containing a variety of biases, such as gender biases, affecting model weights and, potentially, behavior. Currently, it is unclear to what extent such biases also impact model explanations in possibly unfavorable ways. We create a gender-controlled text dataset, GECO, in which otherwise identical sentences appear in male and female forms. This gives rise to ground-truth 'world explanations' for gender classification tasks, enabling the objective evaluation of the correctness of XAI methods. We also provide GECOBench, a rigorous quantitative evaluation framework benchmarking popular XAI methods, applying them to pre-trained language models fine-tuned to different degrees. This allows us to investigate how pre-training induces undesirable bias in model explanations and to what extent fine-tuning can mitigate such explanation bias. We show a clear dependency between explanation performance and the number of fine-tuned layers, where XAI methods are observed to particularly benefit from fine-tuning or complete retraining of embedding layers. Remarkably, this relationship holds for models achieving similar classification performance on the same task. With that, we highlight the utility of the proposed gender-controlled dataset and novel benchmarking approach for research and development of novel XAI methods. All code including dataset generation, model training, evaluation and visualization is available at: https://github.com/braindatalab/gecobench},
 author = {Rick Wilming and Artur Dox and Hjalmar Schulz and Marta Oliveira and Benedict Clark and Stefan Haufe},
 comment = {Under review},
 doi = {},
 eprint = {2406.11547v1},
 journal = {arXiv preprint},
 title = {GECOBench: A Gender-Controlled Text Dataset and Benchmark for Quantifying Biases in Explanations},
 url = {http://arxiv.org/abs/2406.11547v1},
 year = {2024}
}

@article{2406.11873v1,
 abstract = {In recent years, the impact of machine learning (ML) and artificial intelligence (AI) in society has been absolutely remarkable. This impact is expected to continue in the foreseeable future. However,the adoption of AI/ML is also a cause of grave concern. The operation of the most advances AI/ML models is often beyond the grasp of human decision makers. As a result, decisions that impact humans may not be understood and may lack rigorous validation. Explainable AI (XAI) is concerned with providing human decision-makers with understandable explanations for the predictions made by ML models. As a result, XAI is a cornerstone of trustworthy AI. Despite its strategic importance, most work on XAI lacks rigor, and so its use in high-risk or safety-critical domains serves to foster distrust instead of contributing to build much-needed trust. Logic-based XAI has recently emerged as a rigorous alternative to those other non-rigorous methods of XAI. This paper provides a technical survey of logic-based XAI, its origins, the current topics of research, and emerging future topics of research. The paper also highlights the many myths that pervade non-rigorous approaches for XAI.},
 author = {Joao Marques-Silva},
 comment = {},
 doi = {},
 eprint = {2406.11873v1},
 journal = {arXiv preprint},
 title = {Logic-Based Explainability: Past, Present & Future},
 url = {http://arxiv.org/abs/2406.11873v1},
 year = {2024}
}

@article{2406.11882v1,
 abstract = {In recent years, artificial intelligence (AI) rapidly accelerated its influence and is expected to promote the development of Earth system science (ESS) if properly harnessed. In application of AI to ESS, a significant hurdle lies in the interpretability conundrum, an inherent problem of black-box nature arising from the complexity of AI algorithms. To address this, explainable AI (XAI) offers a set of powerful tools that make the models more transparent. The purpose of this review is twofold: First, to provide ESS scholars, especially newcomers, with a foundational understanding of XAI, serving as a primer to inspire future research advances; second, to encourage ESS professionals to embrace the benefits of AI, free from preconceived biases due to its lack of interpretability. We begin with elucidating the concept of XAI, along with typical methods. We then delve into a review of XAI applications in the ESS literature, highlighting the important role that XAI has played in facilitating communication with AI model decisions, improving model diagnosis, and uncovering scientific insights. We identify four significant challenges that XAI faces within the ESS, and propose solutions. Furthermore, we provide a comprehensive illustration of multifaceted perspectives. Given the unique challenges in ESS, an interpretable hybrid approach that seamlessly integrates AI with domain-specific knowledge appears to be a promising way to enhance the utility of AI in ESS. A visionary outlook for ESS envisions a harmonious blend where process-based models govern the known, AI models explore the unknown, and XAI bridges the gap by providing explanations.},
 author = {Feini Huang and Shijie Jiang and Lu Li and Yongkun Zhang and Ye Zhang and Ruqing Zhang and Qingliang Li and Danxi Li and Wei Shangguan and Yongjiu Dai},
 comment = {},
 doi = {},
 eprint = {2406.11882v1},
 journal = {arXiv preprint},
 title = {Applications of Explainable artificial intelligence in Earth system science},
 url = {http://arxiv.org/abs/2406.11882v1},
 year = {2024}
}

@article{2406.12660v1,
 abstract = {AI is becoming increasingly common across different domains. However, as sophisticated AI-based systems are often black-boxed, rendering the decision-making logic opaque, users find it challenging to comply with their recommendations. Although researchers are investigating Explainable AI (XAI) to increase the transparency of the underlying machine learning models, it is unclear what types of explanations are effective and what other factors increase compliance. To better understand the interplay of these factors, we conducted an experiment with 562 participants who were presented with the recommendations of an AI and two different types of XAI. We find that users' compliance increases with the introduction of XAI but is also affected by AI literacy. We also find that the relationships between AI literacy XAI and users' compliance are mediated by the users' mental model of AI. Our study has several implications for successfully designing AI-based systems utilizing XAI.},
 author = {Niklas Kühl and Christian Meske and Maximilian Nitsche and Jodie Lobana},
 comment = {},
 doi = {},
 eprint = {2406.12660v1},
 journal = {arXiv preprint},
 title = {Investigating the Role of Explainability and AI Literacy in User Compliance},
 url = {http://arxiv.org/abs/2406.12660v1},
 year = {2024}
}

@article{2406.12897v1,
 abstract = {It is imperative that breast cancer is detected precisely and timely to improve patient outcomes. Diagnostic methodologies have traditionally relied on unimodal approaches; however, medical data analytics is integrating diverse data sources beyond conventional imaging. Using multi-modal techniques, integrating both image and non-image data, marks a transformative advancement in breast cancer diagnosis. The purpose of this review is to explore the burgeoning field of multimodal techniques, particularly the fusion of histopathology images with non-image data. Further, Explainable AI (XAI) will be used to elucidate the decision-making processes of complex algorithms, emphasizing the necessity of explainability in diagnostic processes. This review utilizes multi-modal data and emphasizes explainability to enhance diagnostic accuracy, clinician confidence, and patient engagement, ultimately fostering more personalized treatment strategies for breast cancer, while also identifying research gaps in multi-modality and explainability, guiding future studies, and contributing to the strategic direction of the field.},
 author = {Faseela Abdullakutty and Younes Akbari and Somaya Al-Maadeed and Ahmed Bouridane and Rifat Hamoudi},
 comment = {31 pages including references},
 doi = {},
 eprint = {2406.12897v1},
 journal = {arXiv preprint},
 title = {Advancing Histopathology-Based Breast Cancer Diagnosis: Insights into Multi-Modality and Explainability},
 url = {http://arxiv.org/abs/2406.12897v1},
 year = {2024}
}

@article{2406.12916v3,
 abstract = {An important challenge in machine learning is to predict the initial conditions under which a given neural network will be trainable. We present a method for predicting the trainable regime in parameter space for deep feedforward neural networks (DNNs) based on reconstructing the input from subsequent activation layers via a cascade of single-layer auxiliary networks. We show that a single epoch of training of the shallow cascade networks is sufficient to predict the trainability of the deep feedforward network on a range of datasets (MNIST, CIFAR10, FashionMNIST, and white noise), thereby providing a significant reduction in overall training time. We achieve this by computing the relative entropy between reconstructed images and the original inputs, and show that this probe of information loss is sensitive to the phase behaviour of the network. We further demonstrate that this method generalizes to residual neural networks (ResNets) and convolutional neural networks (CNNs). Moreover, our method illustrates the network's decision making process by displaying the changes performed on the input data at each layer, which we demonstrate for both a DNN trained on MNIST and the vgg16 CNN trained on the ImageNet dataset. Our results provide a technique for significantly accelerating the training of large neural networks.},
 author = {Yanick Thurn and Ro Jefferson and Johanna Erdmenger},
 comment = {29 pages, 10 figures, 1 table, new examples for CNNs and ResNet added, Vgg16 example in xai section added},
 doi = {},
 eprint = {2406.12916v3},
 journal = {arXiv preprint},
 title = {Opening the Black Box: predicting the trainability of deep neural networks with reconstruction entropy},
 url = {http://arxiv.org/abs/2406.12916v3},
 year = {2024}
}

@article{2406.13257v1,
 abstract = {Challenges persist in providing interpretable explanations for neural network reasoning in explainable AI (xAI). Existing methods like Integrated Gradients produce noisy maps, and LIME, while intuitive, may deviate from the model's reasoning. We introduce a framework that uses hierarchical segmentation techniques for faithful and interpretable explanations of Convolutional Neural Networks (CNNs). Our method constructs model-based hierarchical segmentations that maintain the model's reasoning fidelity and allows both human-centric and model-centric segmentation. This approach offers multiscale explanations, aiding bias identification and enhancing understanding of neural network decision-making. Experiments show that our framework, xAiTrees, delivers highly interpretable and faithful model explanations, not only surpassing traditional xAI methods but shedding new light on a novel approach to enhancing xAI interpretability. Code at: https://github.com/CarolMazini/reasoning_with_trees .},
 author = {Caroline Mazini Rodrigues and Nicolas Boutry and Laurent Najman},
 comment = {},
 doi = {},
 eprint = {2406.13257v1},
 journal = {arXiv preprint},
 title = {Reasoning with trees: interpreting CNNs using hierarchies},
 url = {http://arxiv.org/abs/2406.13257v1},
 year = {2024}
}

@article{2406.13441v1,
 abstract = {This study focuses on analyzing dermoscopy images to determine the depth of melanomas, which is a critical factor in diagnosing and treating skin cancer. The Breslow depth, measured from the top of the granular layer to the deepest point of tumor invasion, serves as a crucial parameter for staging melanoma and guiding treatment decisions. This research aims to improve the prediction of the depth of melanoma through the use of machine learning models, specifically deep learning, while also providing an analysis of the possible existance of graduation in the images characteristics which correlates with the depth of the melanomas. Various datasets, including ISIC and private collections, were used, comprising a total of 1162 images. The datasets were combined and balanced to ensure robust model training. The study utilized pre-trained Convolutional Neural Networks (CNNs). Results indicated that the models achieved significant improvements over previous methods. Additionally, the study conducted a correlation analysis between model's predictions and actual melanoma thickness, revealing a moderate correlation that improves with higher thickness values. Explainability methods such as feature visualization through Principal Component Analysis (PCA) demonstrated the capability of deep features to distinguish between different depths of melanoma, providing insight into the data distribution and model behavior. In summary, this research presents a dual contribution: enhancing the state-of-the-art classification results through advanced training techniques and offering a detailed analysis of the data and model behavior to better understand the relationship between dermoscopy images and melanoma thickness.},
 author = {Miguel Nogales and Begoña Acha and Fernando Alarcón and José Pereyra and Carmen Serrano},
 comment = {},
 doi = {},
 eprint = {2406.13441v1},
 journal = {arXiv preprint},
 title = {Robust Melanoma Thickness Prediction via Deep Transfer Learning enhanced by XAI Techniques},
 url = {http://arxiv.org/abs/2406.13441v1},
 year = {2024}
}

@article{2406.14281v4,
 abstract = {We present FairX, an open-source Python-based benchmarking tool designed for the comprehensive analysis of models under the umbrella of fairness, utility, and eXplainability (XAI). FairX enables users to train benchmarking bias-mitigation models and evaluate their fairness using a wide array of fairness metrics, data utility metrics, and generate explanations for model predictions, all within a unified framework. Existing benchmarking tools do not have the way to evaluate synthetic data generated from fair generative models, also they do not have the support for training fair generative models either. In FairX, we add fair generative models in the collection of our fair-model library (pre-processing, in-processing, post-processing) and evaluation metrics for evaluating the quality of synthetic fair data. This version of FairX supports both tabular and image datasets. It also allows users to provide their own custom datasets. The open-source FairX benchmarking package is publicly available at \url{https://github.com/fahim-sikder/FairX}.},
 author = {Md Fahim Sikder and Resmi Ramachandranpillai and Daniel de Leng and Fredrik Heintz},
 comment = {},
 doi = {},
 eprint = {2406.14281v4},
 journal = {arXiv preprint},
 title = {FairX: A comprehensive benchmarking tool for model analysis using fairness, utility, and explainability},
 url = {http://arxiv.org/abs/2406.14281v4},
 year = {2024}
}

@article{2406.14349v2,
 abstract = {Recent legislative regulations have underlined the need for accountable and transparent artificial intelligence systems and have contributed to a growing interest in the Explainable Artificial Intelligence (XAI) field. Nonetheless, the lack of standardized criteria to validate explanation methodologies remains a major obstacle to developing trustworthy systems. We address a crucial yet often overlooked aspect of XAI, the robustness of explanations, which plays a central role in ensuring trust in both the system and the provided explanation. To this end, we propose a novel approach to analyse the robustness of neural network explanations to non-adversarial perturbations, leveraging the manifold hypothesis to produce new perturbed datapoints that resemble the observed data distribution. We additionally present an ensemble method to aggregate various explanations, showing how merging explanations can be beneficial for both understanding the model's decision and evaluating the robustness. The aim of our work is to provide practitioners with a framework for evaluating the trustworthiness of model explanations. Experimental results on feature importances derived from neural networks applied to tabular datasets highlight the importance of robust explanations in practical applications.},
 author = {Ilaria Vascotto and Alex Rodriguez and Alessandro Bonaita and Luca Bortolussi},
 comment = {Accepted at the 3rd World Conference on eXplainable Artificial Intelligence (to be held in July 2025)},
 doi = {10.1007/978-3-032-08327-2_11},
 eprint = {2406.14349v2},
 journal = {arXiv preprint},
 title = {When Can You Trust Your Explanations? A Robustness Analysis on Feature Importances},
 url = {http://arxiv.org/abs/2406.14349v2},
 year = {2024}
}

@article{2406.15789v1,
 abstract = {Machine learning (ML) models, demonstrably powerful, suffer from a lack of interpretability. The absence of transparency, often referred to as the black box nature of ML models, undermines trust and urges the need for efforts to enhance their explainability. Explainable AI (XAI) techniques address this challenge by providing frameworks and methods to explain the internal decision-making processes of these complex models. Techniques like Counterfactual Explanations (CF) and Feature Importance play a crucial role in achieving this goal. Furthermore, high-quality and diverse data remains the foundational element for robust and trustworthy ML applications. In many applications, the data used to train ML and XAI explainers contain sensitive information. In this context, numerous privacy-preserving techniques can be employed to safeguard sensitive information in the data, such as differential privacy. Subsequently, a conflict between XAI and privacy solutions emerges due to their opposing goals. Since XAI techniques provide reasoning for the model behavior, they reveal information relative to ML models, such as their decision boundaries, the values of features, or the gradients of deep learning models when explanations are exposed to a third entity. Attackers can initiate privacy breaching attacks using these explanations, to perform model extraction, inference, and membership attacks. This dilemma underscores the challenge of finding the right equilibrium between understanding ML decision-making and safeguarding privacy.},
 author = {Fatima Ezzeddine},
 comment = {},
 doi = {},
 eprint = {2406.15789v1},
 journal = {arXiv preprint},
 title = {Privacy Implications of Explainable AI in Data-Driven Systems},
 url = {http://arxiv.org/abs/2406.15789v1},
 year = {2024}
}

@article{2406.15839v2,
 abstract = {Recent work has investigated the vulnerability of local surrogate methods to adversarial perturbations on a machine learning (ML) model's inputs, where the explanation is manipulated while the meaning and structure of the original input remains similar under the complex model. Although weaknesses across many methods have been shown to exist, the reasons behind why remain little explored. Central to the concept of adversarial attacks on explainable AI (XAI) is the similarity measure used to calculate how one explanation differs from another. A poor choice of similarity measure can lead to erroneous conclusions on the efficacy of an XAI method. Too sensitive a measure results in exaggerated vulnerability, while too coarse understates its weakness. We investigate a variety of similarity measures designed for text-based ranked lists, including Kendall's Tau, Spearman's Footrule, and Rank-biased Overlap to determine how substantial changes in the type of measure or threshold of success affect the conclusions generated from common adversarial attack processes. Certain measures are found to be overly sensitive, resulting in erroneous estimates of stability.},
 author = {Christopher Burger and Charles Walter and Thai Le},
 comment = {11 pages, 8 Tables (Minor edits for clarity and grammar)},
 doi = {},
 eprint = {2406.15839v2},
 journal = {arXiv preprint},
 title = {The Effect of Similarity Measures on Accurate Stability Estimates for Local Surrogate Models in Text-based Explainable AI},
 url = {http://arxiv.org/abs/2406.15839v2},
 year = {2024}
}

@article{2406.16626v1,
 abstract = {In recent years, the number of new applications for highly complex AI systems has risen significantly. Algorithmic decision-making systems (ADMs) are one of such applications, where an AI system replaces the decision-making process of a human expert. As one approach to ensure fairness and transparency of such systems, explainable AI (XAI) has become more important. One variant to achieve explainability are surrogate models, i.e., the idea to train a new simpler machine learning model based on the input-output-relationship of a black box model. The simpler machine learning model could, for example, be a decision tree, which is thought to be intuitively understandable by humans. However, there is not much insight into how well the surrogate model approximates the black box.
  Our main assumption is that a good surrogate model approach should be able to bring such a discriminating behavior to the attention of humans; prior to our research we assumed that a surrogate decision tree would identify such a pattern on one of its first levels. However, in this article we show that even if the discriminated subgroup - while otherwise being the same in all categories - does not get a single positive decision from the black box ADM system, the corresponding question of group membership can be pushed down onto a level as low as wanted by the operator of the system.
  We then generalize this finding to pinpoint the exact level of the tree on which the discriminating question is asked and show that in a more realistic scenario, where discrimination only occurs to some fraction of the disadvantaged group, it is even more feasible to hide such discrimination.
  Our approach can be generalized easily to other surrogate models.},
 author = {Alexander Wilhelm and Katharina A. Zweig},
 comment = {24 pages, 7 figures},
 doi = {},
 eprint = {2406.16626v1},
 journal = {arXiv preprint},
 title = {Hacking a surrogate model approach to XAI},
 url = {http://arxiv.org/abs/2406.16626v1},
 year = {2024}
}

@article{2406.16985v2,
 abstract = {This paper proposes a framework combining Neural Ordinary Differential Equations (Neural ODEs) and robust control theory to enhance the interpretability and control of large language models (LLMs). By utilizing Neural ODEs to model the dynamic evolution of input-output relationships and introducing control mechanisms to optimize output quality, we demonstrate the effectiveness of this approach across multiple question-answer datasets. Experimental results show that the integration of Neural ODEs and control theory significantly improves output consistency and model interpretability, advancing the development of explainable AI technologies.},
 author = {Yukun Zhang and Qi Dong},
 comment = {},
 doi = {},
 eprint = {2406.16985v2},
 journal = {arXiv preprint},
 title = {Unveiling LLM Mechanisms Through Neural ODEs and Control Theory},
 url = {http://arxiv.org/abs/2406.16985v2},
 year = {2024}
}

@article{2406.17583v1,
 abstract = {Artificial intelligence (AI) is currently based largely on black-box machine learning models which lack interpretability. The field of eXplainable AI (XAI) strives to address this major concern, being critical in high-stakes areas such as the finance, legal and health sectors.
  We present an approach to defining AI models and their interpretability based on category theory. For this we employ the notion of a compositional model, which sees a model in terms of formal string diagrams which capture its abstract structure together with its concrete implementation. This comprehensive view incorporates deterministic, probabilistic and quantum models. We compare a wide range of AI models as compositional models, including linear and rule-based models, (recurrent) neural networks, transformers, VAEs, and causal and DisCoCirc models.
  Next we give a definition of interpretation of a model in terms of its compositional structure, demonstrating how to analyse the interpretability of a model, and using this to clarify common themes in XAI. We find that what makes the standard 'intrinsically interpretable' models so transparent is brought out most clearly diagrammatically. This leads us to the more general notion of compositionally-interpretable (CI) models, which additionally include, for instance, causal, conceptual space, and DisCoCirc models.
  We next demonstrate the explainability benefits of CI models. Firstly, their compositional structure may allow the computation of other quantities of interest, and may facilitate inference from the model to the modelled phenomenon by matching its structure. Secondly, they allow for diagrammatic explanations for their behaviour, based on influence constraints, diagram surgery and rewrite explanations. Finally, we discuss many future directions for the approach, raising the question of how to learn such meaningfully structured models in practice.},
 author = {Sean Tull and Robin Lorenz and Stephen Clark and Ilyas Khan and Bob Coecke},
 comment = {},
 doi = {},
 eprint = {2406.17583v1},
 journal = {arXiv preprint},
 title = {Towards Compositional Interpretability for XAI},
 url = {http://arxiv.org/abs/2406.17583v1},
 year = {2024}
}

@article{2406.17885v3,
 abstract = {In Explainable AI, rule extraction translates model knowledge into logical rules, such as IF-THEN statements, crucial for understanding patterns learned by black-box models. This could significantly aid in fields like disease diagnosis, disease progression estimation, or drug discovery. However, such application domains often contain imbalanced data, with the class of interest underrepresented. Existing methods inevitably compromise the performance of rules for the minor class to maximise the overall performance. As the first attempt in this field, we propose a model-agnostic approach for extracting rules from specific subgroups of data, featuring automatic rule generation for numerical features. This method enhances the regional explainability of machine learning models and offers wider applicability compared to existing methods. We additionally introduce a new method for selecting features to compose rules, reducing computational costs in high-dimensional spaces. Experiments across various datasets and models demonstrate the effectiveness of our methods.},
 author = {Yu Chen and Tianyu Cui and Alexander Capstick and Nan Fletcher-Loyd and Payam Barnaghi},
 comment = {},
 doi = {},
 eprint = {2406.17885v3},
 journal = {arXiv preprint},
 title = {Enabling Regional Explainability by Automatic and Model-agnostic Rule Extraction},
 url = {http://arxiv.org/abs/2406.17885v3},
 year = {2024}
}

@article{2407.00104v2,
 abstract = {An AI tool has been developed to provide interpretable support for the diagnosis of BCC via teledermatology, thus speeding up referrals and optimizing resource utilization. The interpretability is provided in two ways: on the one hand, the main BCC dermoscopic patterns are found in the image to justify the BCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM, a clinically inspired visual explanation is developed where the relevant features for diagnosis are located. Since there is no established ground truth for BCC dermoscopic features, a standard reference is inferred from the diagnosis of four dermatologists using an Expectation Maximization (EM) based algorithm. The results demonstrate significant improvements in classification accuracy and interpretability, positioning this approach as a valuable tool for early BCC detection and referral to dermatologists. The BCC/non-BCC classification achieved an accuracy rate of 90%. For Clinically-inspired XAI results, the detection of BCC patterns useful to clinicians reaches 99% accuracy. As for the Clinically-inspired Visual XAI results, the mean of the Grad-CAM normalized value within the manually segmented clinical features is 0.57, while outside this region it is 0.16. This indicates that the model struggles to accurately identify the regions of the BCC patterns. These results prove the ability of the AI tool to provide a useful explanation.},
 author = {Iván Matas and Carmen Serrano and Francisca Silva and Amalia Serrano and Tomás Toledo-Pastrana and Begoña Acha},
 comment = {8 pages, 4 figures, 4 tables, under review},
 doi = {},
 eprint = {2407.00104v2},
 journal = {arXiv preprint},
 title = {Clinically inspired enhance Explainability and Interpretability of an AI-Tool for BCC diagnosis based on expert annotation},
 url = {http://arxiv.org/abs/2407.00104v2},
 year = {2024}
}

@article{2407.00267v1,
 abstract = {Detecting and classifying lesions in breast ultrasound images is a promising application of artificial intelligence (AI) for reducing the burden of cancer in regions with limited access to mammography. Such AI systems are more likely to be useful in a clinical setting if their predictions can be explained to a radiologist. This work proposes an explainable AI model that provides interpretable predictions using a standard lexicon from the American College of Radiology's Breast Imaging and Reporting Data System (BI-RADS). The model is a deep neural network featuring a concept bottleneck layer in which known BI-RADS features are predicted before making a final cancer classification. This enables radiologists to easily review the predictions of the AI system and potentially fix errors in real time by modifying the concept predictions. In experiments, a model is developed on 8,854 images from 994 women with expert annotations and histological cancer labels. The model outperforms state-of-the-art lesion detection frameworks with 48.9 average precision on the held-out testing set, and for cancer classification, concept intervention is shown to increase performance from 0.876 to 0.885 area under the receiver operating characteristic curve. Training and evaluation code is available at https://github.com/hawaii-ai/bus-cbm.},
 author = {Arianna Bunnell and Yannik Glaser and Dustin Valdez and Thomas Wolfgruber and Aleen Altamirano and Carol Zamora González and Brenda Y. Hernandez and Peter Sadowski and John A. Shepherd},
 comment = {Submitted version of manuscript accepted at MICCAI 2024. This preprint has not undergone peer review or any post-submission improvements or corrections},
 doi = {},
 eprint = {2407.00267v1},
 journal = {arXiv preprint},
 title = {Learning a Clinically-Relevant Concept Bottleneck for Lesion Detection in Breast Ultrasound},
 url = {http://arxiv.org/abs/2407.00267v1},
 year = {2024}
}

@article{2407.00506v2,
 abstract = {With wide application of Artificial Intelligence (AI), it has become particularly important to make decisions of AI systems explainable and transparent. In this paper, we proposed a new Explainable Artificial Intelligence (XAI) method called ShapG (Explanations based on Shapley value for Graphs) for measuring feature importance. ShapG is a model-agnostic global explanation method. At the first stage, it defines an undirected graph based on the dataset, where nodes represent features and edges are added based on calculation of correlation coefficients between features. At the second stage, it calculates an approximated Shapley value by sampling the data taking into account this graph structure. The sampling approach of ShapG allows to calculate the importance of features efficiently, i.e. to reduce computational complexity. Comparison of ShapG with other existing XAI methods shows that it provides more accurate explanations for two examined datasets. We also compared other XAI methods developed based on cooperative game theory with ShapG in running time, and the results show that ShapG exhibits obvious advantages in its running time, which further proves efficiency of ShapG. In addition, extensive experiments demonstrate a wide range of applicability of the ShapG method for explaining complex models. We find ShapG an important tool in improving explainability and transparency of AI systems and believe it can be widely used in various fields.},
 author = {Chi Zhao and Jing Liu and Elena Parilina},
 comment = {This paper has been published in the journal "Engineering Applications of Artificial Intelligence"},
 doi = {10.1016/j.engappai.2025.110409},
 eprint = {2407.00506v2},
 journal = {arXiv preprint},
 title = {ShapG: new feature importance method based on the Shapley value},
 url = {http://arxiv.org/abs/2407.00506v2},
 year = {2024}
}

@article{2407.00849v1,
 abstract = {The interpretability of machine learning models has gained increasing attention, particularly in scientific domains where high precision and accountability are crucial. This research focuses on distinguishing between two critical data patterns -- sensitive patterns (model-related) and decisive patterns (task-related) -- which are commonly used as model interpretations but often lead to confusion. Specifically, this study compares the effectiveness of two main streams of interpretation methods: post-hoc methods and self-interpretable methods, in detecting these patterns. Recently, geometric deep learning (GDL) has shown superior predictive performance in various scientific applications, creating an urgent need for principled interpretation methods. Therefore, we conduct our study using several representative GDL applications as case studies. We evaluate thirteen interpretation methods applied to three major GDL backbone models, using four scientific datasets to assess how well these methods identify sensitive and decisive patterns. Our findings indicate that post-hoc methods tend to provide interpretations better aligned with sensitive patterns, whereas certain self-interpretable methods exhibit strong and stable performance in detecting decisive patterns. Additionally, our study offers valuable insights into improving the reliability of these interpretation methods. For example, ensembling post-hoc interpretations from multiple models trained on the same task can effectively uncover the task's decisive patterns.},
 author = {Jiajun Zhu and Siqi Miao and Rex Ying and Pan Li},
 comment = {},
 doi = {},
 eprint = {2407.00849v1},
 journal = {arXiv preprint},
 title = {Towards Understanding Sensitive and Decisive Patterns in Explainable AI: A Case Study of Model Interpretation in Geometric Deep Learning},
 url = {http://arxiv.org/abs/2407.00849v1},
 year = {2024}
}

@article{2407.02271v2,
 abstract = {We propose a prototype-based approach for improving explainability of softmax classifiers that provides an understandable prediction confidence, generated through stochastic sampling of prototypes, and demonstrates potential for out of distribution detection (OOD). By modifying the model architecture and training to make predictions using similarities to any set of class examples from the training dataset, we acquire the ability to sample for prototypical examples that contributed to the prediction, which provide an instance-based explanation for the model's decision. Furthermore, by learning relationships between images from the training dataset through relative distances within the model's latent space, we obtain a metric for uncertainty that is better able to detect out of distribution data than softmax confidence.},
 author = {Hilarie Sit and Brendan Keith and Karianne Bergen},
 comment = {8 pages, 8 figures, accepted at 2024 IJCAI-XAI workshop},
 doi = {},
 eprint = {2407.02271v2},
 journal = {arXiv preprint},
 title = {Improving Explainability of Softmax Classifiers Using a Prototype-Based Joint Embedding Method},
 url = {http://arxiv.org/abs/2407.02271v2},
 year = {2024}
}

@article{2407.02984v3,
 abstract = {Black box deep learning models trained on genomic sequences excel at predicting the outcomes of different gene regulatory mechanisms. Therefore, interpreting these models may provide novel insights into the underlying biology, supporting downstream biomedical applications. Due to their complexity, interpretable surrogate models can only be built for local explanations (e.g., a single instance). However, accomplishing this requires generating a dataset in the neighborhood of the input, which must maintain syntactic similarity to the original data while introducing semantic variability in the model's predictions. This task is challenging due to the complex sequence-to-function relationship of DNA.
  We propose using Genetic Programming to generate datasets by evolving perturbations in sequences that contribute to their semantic diversity. Our custom, domain-guided individual representation effectively constrains syntactic similarity, and we provide two alternative fitness functions that promote diversity with no computational effort. Applied to the RNA splicing domain, our approach quickly achieves good diversity and significantly outperforms a random baseline in exploring the search space, as shown by our proof-of-concept, short RNA sequence. Furthermore, we assess its generalizability and demonstrate scalability to larger sequences, resulting in a ~30% improvement over the baseline.},
 author = {Pedro Barbosa and Rosina Savisaar and Alcides Fonseca},
 comment = {},
 doi = {10.1145/3638529.3653990},
 eprint = {2407.02984v3},
 journal = {arXiv preprint},
 title = {Semantically Rich Local Dataset Generation for Explainable AI in Genomics},
 url = {http://arxiv.org/abs/2407.02984v3},
 year = {2024}
}

@article{2407.03108v1,
 abstract = {Black box models are increasingly being used in the daily lives of human beings living in society. Along with this increase, there has been the emergence of Explainable Artificial Intelligence (XAI) methods aimed at generating additional explanations regarding how the model makes certain predictions. In this sense, methods such as Dalex, Eli5, eXirt, Lofo and Shap emerged as different proposals and methodologies for generating explanations of black box models in an agnostic way. Along with the emergence of these methods, questions arise such as "How Reliable and Stable are XAI Methods?". With the aim of shedding light on this main question, this research creates a pipeline that performs experiments using the diabetes dataset and four different machine learning models (LGBM, MLP, DT and KNN), creating different levels of perturbations of the test data and finally generates explanations from the eXirt method regarding the confidence of the models and also feature relevances ranks from all XAI methods mentioned, in order to measure their stability in the face of perturbations. As a result, it was found that eXirt was able to identify the most reliable models among all those used. It was also found that current XAI methods are sensitive to perturbations, with the exception of one specific method.},
 author = {José Ribeiro and Lucas Cardoso and Vitor Santos and Eduardo Carvalho and Níkolas Carneiro and Ronnie Alves},
 comment = {15 pages, 6 figures, submitted to BRACIS 2024},
 doi = {},
 eprint = {2407.03108v1},
 journal = {arXiv preprint},
 title = {How Reliable and Stable are Explanations of XAI Methods?},
 url = {http://arxiv.org/abs/2407.03108v1},
 year = {2024}
}

@article{2407.04526v1,
 abstract = {Machine learned potentials are becoming a popular tool to define an effective energy model for complex systems, either incorporating electronic structure effects at the atomistic resolution, or effectively renormalizing part of the atomistic degrees of freedom at a coarse-grained resolution. One of the main criticisms to machine learned potentials is that the energy inferred by the network is not as interpretable as in more traditional approaches where a simpler functional form is used. Here we address this problem by extending tools recently proposed in the nascent field of Explainable Artificial Intelligence (XAI) to coarse-grained potentials based on graph neural networks (GNN). We demonstrate the approach on three different coarse-grained systems including two fluids (methane and water) and the protein NTL9. On these examples, we show that the neural network potentials can be in practice decomposed in relevance contributions to different orders, that can be directly interpreted and provide physical insights on the systems of interest.},
 author = {Klara Bonneau and Jonas Lederer and Clark Templeton and David Rosenberger and Klaus-Robert Müller and Cecilia Clementi},
 comment = {},
 doi = {},
 eprint = {2407.04526v1},
 journal = {arXiv preprint},
 title = {Peering inside the black box: Learning the relevance of many-body functions in Neural Network potentials},
 url = {http://arxiv.org/abs/2407.04526v1},
 year = {2024}
}

@article{2407.06206v1,
 abstract = {Point-of-Care Ultrasound (POCUS) is the practice of clinicians conducting and interpreting ultrasound scans right at the patient's bedside. However, the expertise needed to interpret these images is considerable and may not always be present in emergency situations. This reality makes algorithms such as machine learning classifiers extremely valuable to augment human decisions. POCUS devices are becoming available at a reasonable cost in the size of a mobile phone. The challenge of turning POCUS devices into life-saving tools is that interpretation of ultrasound images requires specialist training and experience. Unfortunately, the difficulty to obtain positive training images represents an important obstacle to building efficient and accurate classifiers. Hence, the problem we try to investigate is how to explore strategies to increase accuracy of classifiers trained with scarce data. We hypothesize that training with a few data instances may not suffice for classifiers to generalize causing them to overfit. Our approach uses an Explainable AI-Augmented approach to help the algorithm learn more from less and potentially help the classifier better generalize.},
 author = {Ximing Wen and Rosina O. Weber and Anik Sen and Darryl Hannan and Steven C. Nesbit and Vincent Chan and Alberto Goffi and Michael Morris and John C. Hunninghake and Nicholas E. Villalobos and Edward Kim and Christopher J. MacLellan},
 comment = {7 pages, 3 figures, accepted by XAI 2024 workshop @ IJCAI},
 doi = {},
 eprint = {2407.06206v1},
 journal = {arXiv preprint},
 title = {The Impact of an XAI-Augmented Approach on Binary Classification with Scarce Data},
 url = {http://arxiv.org/abs/2407.06206v1},
 year = {2024}
}

@article{2407.06658v3,
 abstract = {Geomagnetic storms, caused by solar wind energy transfer to Earth's magnetic field, can disrupt critical infrastructure like GPS, satellite communications, and power grids. The disturbance storm-time (Dst) index measures storm intensity. Despite advancements in empirical, physics-based, and machine-learning models using real-time solar wind data, accurately forecasting extreme geomagnetic events remains challenging due to noise and sensor failures. This research introduces TriQXNet, a novel hybrid classical-quantum neural network for Dst forecasting. Our model integrates classical and quantum computing, conformal prediction, and explainable AI (XAI) within a hybrid architecture. To ensure high-quality input data, we developed a comprehensive preprocessing pipeline that included feature selection, normalization, aggregation, and imputation. TriQXNet processes preprocessed solar wind data from NASA's ACE and NOAA's DSCOVR satellites, predicting the Dst index for the current hour and the next, providing vital advance notice to mitigate geomagnetic storm impacts. TriQXNet outperforms 13 state-of-the-art hybrid deep-learning models, achieving a root mean squared error of 9.27 nanoteslas (nT). Rigorous evaluation through 10-fold cross-validated paired t-tests confirmed its superior performance with 95% confidence. Conformal prediction techniques provide quantifiable uncertainty, which is essential for operational decisions, while XAI methods like ShapTime enhance interpretability. Comparative analysis shows TriQXNet's superior forecasting accuracy, setting a new level of expectations for geomagnetic storm prediction and highlighting the potential of classical-quantum hybrid models in space weather forecasting.},
 author = {Md Abrar Jahin and M. F. Mridha and Zeyar Aung and Nilanjan Dey and R. Simon Sherratt},
 comment = {},
 doi = {},
 eprint = {2407.06658v3},
 journal = {arXiv preprint},
 title = {TriQXNet: Forecasting Dst Index from Solar Wind Data Using an Interpretable Parallel Classical-Quantum Framework with Uncertainty Quantification},
 url = {http://arxiv.org/abs/2407.06658v3},
 year = {2024}
}

@article{2407.07009v2,
 abstract = {The support of artificial intelligence (AI) based decision-making is a key element in future 6G networks, where the concept of native AI will be introduced. Moreover, AI is widely employed in different critical applications such as autonomous driving and medical diagnosis. In such applications, using AI as black-box models is risky and challenging. Hence, it is crucial to understand and trust the decisions taken by these models. Tackling this issue can be achieved by developing explainable AI (XAI) schemes that aim to explain the logic behind the black-box model behavior, and thus, ensure its efficient and safe deployment. Recently, we proposed a novel perturbation-based XAI-CHEST framework that is oriented toward channel estimation in wireless communications. The core idea of the XAI-CHEST framework is to identify the relevant model inputs by inducing high noise on the irrelevant ones. This manuscript provides the detailed theoretical foundations of the XAI-CHEST framework. In particular, we derive the analytical expressions of the XAI-CHEST loss functions and the noise threshold fine-tuning optimization problem. Hence the designed XAI-CHEST delivers a smart input feature selection methodology that can further improve the overall performance while optimizing the architecture of the employed model. Simulation results show that the XAI-CHEST framework provides valid interpretations, where it offers an improved bit error rate performance while reducing the required computational complexity in comparison to the classical DL-based channel estimation.},
 author = {Abdul Karim Gizzini and Yahia Medjahdi and Ali J. Ghandour and Laurent Clavier},
 comment = {This paper has been submitted to the IEEE Transactions on Machine Learning in Communications and Networking on 19 March 2025},
 doi = {},
 eprint = {2407.07009v2},
 journal = {arXiv preprint},
 title = {Explainable AI for Enhancing Efficiency of DL-based Channel Estimation},
 url = {http://arxiv.org/abs/2407.07009v2},
 year = {2024}
}

@article{2407.07066v4,
 abstract = {Machine Learning (ML) models integrated with in-situ sensing offer transformative solutions for defect detection in Additive Manufacturing (AM), but this integration brings critical challenges in safeguarding sensitive data, such as part designs and material compositions. Differential Privacy (DP), which introduces mathematically controlled noise, provides a balance between data utility and privacy. However, black-box Artificial Intelligence (AI) models often obscure how this noise impacts model accuracy, complicating the optimization of privacy-accuracy trade-offs. This study introduces the Differential Privacy-Hyperdimensional Computing (DP-HD) framework, a novel approach combining Explainable AI (XAI) and vector symbolic paradigms to quantify and predict noise effects on accuracy using a Signal-to-Noise Ratio (SNR) metric. DP-HD enables precise tuning of DP noise levels, ensuring an optimal balance between privacy and performance. The framework has been validated using real-world AM data, demonstrating its applicability to industrial environments. Experimental results demonstrate DP-HD's capability to achieve state-of-the-art accuracy (94.43%) with robust privacy protections in anomaly detection for AM, even under significant noise conditions. Beyond AM, DP-HD holds substantial promise for broader applications in privacy-sensitive domains such as healthcare, financial services, and government data management, where securing sensitive data while maintaining high ML performance is paramount.},
 author = {Fardin Jalil Piran and Prathyush P. Poduval and Hamza Errahmouni Barkam and Mohsen Imani and Farhad Imani},
 comment = {30 pages, 14 figures},
 doi = {10.1016/j.engappai.2025.110282},
 eprint = {2407.07066v4},
 journal = {arXiv preprint},
 title = {Explainable Differential Privacy-Hyperdimensional Computing for Balancing Privacy and Transparency in Additive Manufacturing Monitoring},
 url = {http://arxiv.org/abs/2407.07066v4},
 year = {2024}
}

@article{2407.07521v1,
 abstract = {The trustworthiness of Machine Learning (ML) models can be difficult to assess, but is critical in high-risk or ethically sensitive applications. Many models are treated as a `black-box' where the reasoning or criteria for a final decision is opaque to the user. To address this, some existing Explainable AI (XAI) approaches approximate model behaviour using perturbed data. However, such methods have been criticised for ignoring feature dependencies, with explanations being based on potentially unrealistic data. We propose a novel framework, CHILLI, for incorporating data context into XAI by generating contextually aware perturbations, which are faithful to the training data of the base model being explained. This is shown to improve both the soundness and accuracy of the explanations.},
 author = {Saif Anwar and Nathan Griffiths and Abhir Bhalerao and Thomas Popham},
 comment = {},
 doi = {},
 eprint = {2407.07521v1},
 journal = {arXiv preprint},
 title = {CHILLI: A data context-aware perturbation method for XAI},
 url = {http://arxiv.org/abs/2407.07521v1},
 year = {2024}
}

@article{2407.08298v1,
 abstract = {Vegetation indices allow to efficiently monitor vegetation growth and agricultural activities. Previous generations of satellites were capturing a limited number of spectral bands, and a few expert-designed vegetation indices were sufficient to harness their potential. New generations of multi- and hyperspectral satellites can however capture additional bands, but are not yet efficiently exploited. In this work, we propose an explainable-AI-based method to select and design suitable vegetation indices. We first train a deep neural network using multispectral satellite data, then extract feature importance to identify the most influential bands. We subsequently select suitable existing vegetation indices or modify them to incorporate the identified bands and retrain our model. We validate our approach on a crop classification task. Our results indicate that models trained on individual indices achieve comparable results to the baseline model trained on all bands, while the combination of two indices surpasses the baseline in certain cases.},
 author = {Hiba Najjar and Francisco Mena and Marlon Nuske and Andreas Dengel},
 comment = {Accepted at IEEE International Geoscience and Remote Sensing Symposium 2024},
 doi = {10.1109/IGARSS53475.2024.10641150},
 eprint = {2407.08298v1},
 journal = {arXiv preprint},
 title = {XAI-Guided Enhancement of Vegetation Indices for Crop Mapping},
 url = {http://arxiv.org/abs/2407.08298v1},
 year = {2024}
}

@article{2407.09127v2,
 abstract = {eXplainable Artificial Intelligence (XAI) aims at providing understandable explanations of black box models. In this paper, we evaluate current XAI methods by scoring them based on ground truth simulations and sensitivity analysis. To this end, we used an Electric Arc Furnace (EAF) model to better understand the limits and robustness characteristics of XAI methods such as SHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic Explanations (LIME), as well as Averaged Local Effects (ALE) or Smooth Gradients (SG) in a highly topical setting. These XAI methods were applied to various types of black-box models and then scored based on their correctness compared to the ground-truth sensitivity of the data-generating processes using a novel scoring evaluation methodology over a range of simulated additive noise. The resulting evaluation shows that the capability of the Machine Learning (ML) models to capture the process accurately is, indeed, coupled with the correctness of the explainability of the underlying data-generating process. We furthermore show the differences between XAI methods in their ability to correctly predict the true sensitivity of the modeled industrial process.},
 author = {Benedikt Kantz and Clemens Staudinger and Christoph Feilmayr and Johannes Wachlmayr and Alexander Haberl and Stefan Schuster and Franz Pernkopf},
 comment = {11 pages, 3 figures, accepted at the ICML'24 Workshop ML4LMS; updated with improved results and fixed typos},
 doi = {},
 eprint = {2407.09127v2},
 journal = {arXiv preprint},
 title = {Robustness of Explainable Artificial Intelligence in Industrial Process Modelling},
 url = {http://arxiv.org/abs/2407.09127v2},
 year = {2024}
}

@article{2407.09556v1,
 abstract = {Image captioning is a technology that produces text-based descriptions for an image. Deep learning-based solutions built on top of feature recognition may very well serve the purpose. But as with any other machine learning solution, the user understanding in the process of caption generation is poor and the model does not provide any explanation for its predictions and hence the conventional methods are also referred to as Black-Box methods. Thus, an approach where the model's predictions are trusted by the user is needed to appreciate interoperability. Explainable AI is an approach where a conventional method is approached in a way that the model or the algorithm's predictions can be explainable and justifiable. Thus, this article tries to approach image captioning using Explainable AI such that the resulting captions generated by the model can be Explained and visualized. A newer architecture with a CNN decoder and hierarchical attention concept has been used to increase speed and accuracy of caption generation. Also, incorporating explainability to a model makes it more trustable when used in an application. The model is trained and evaluated using MSCOCO dataset and both quantitative and qualitative results are presented in this article.},
 author = {Rishi Kesav Mohan and Sanjay Sureshkumar and Vignesh Sivasubramaniam},
 comment = {23 pages,9 figures},
 doi = {},
 eprint = {2407.09556v1},
 journal = {arXiv preprint},
 title = {Explainable Image Captioning using CNN- CNN architecture and Hierarchical Attention},
 url = {http://arxiv.org/abs/2407.09556v1},
 year = {2024}
}

@article{2407.11771v2,
 abstract = {Recent advancements in deep learning have significantly improved visual quality inspection and predictive maintenance within industrial settings. However, deploying these technologies on low-resource edge devices poses substantial challenges due to their high computational demands and the inherent complexity of Explainable AI (XAI) methods. This paper addresses these challenges by introducing a novel XAI-integrated Visual Quality Inspection framework that optimizes the deployment of semantic segmentation models on low-resource edge devices. Our framework incorporates XAI and the Large Vision Language Model to deliver human-centered interpretability through visual and textual explanations to end-users. This is crucial for end-user trust and model interpretability. We outline a comprehensive methodology consisting of six fundamental modules: base model fine-tuning, XAI-based explanation generation, evaluation of XAI approaches, XAI-guided data augmentation, development of an edge-compatible model, and the generation of understandable visual and textual explanations. Through XAI-guided data augmentation, the enhanced model incorporating domain expert knowledge with visual and textual explanations is successfully deployed on mobile devices to support end-users in real-world scenarios. Experimental results showcase the effectiveness of the proposed framework, with the mobile model achieving competitive accuracy while significantly reducing model size. This approach paves the way for the broader adoption of reliable and interpretable AI tools in critical industrial applications, where decisions must be both rapid and justifiable. Our code for this work can be found at https://github.com/Analytics-Everywhere-Lab/vqixai.},
 author = {Truong Thanh Hung Nguyen and Phuc Truong Loc Nguyen and Hung Cao},
 comment = {29 pages, preprint submitted to Information Fusion journal},
 doi = {},
 eprint = {2407.11771v2},
 journal = {arXiv preprint},
 title = {XEdgeAI: A Human-centered Industrial Inspection Framework with Data-centric Explainable Edge AI Approach},
 url = {http://arxiv.org/abs/2407.11771v2},
 year = {2024}
}

@article{2407.12058v1,
 abstract = {With the advances in artificial intelligence (AI), data-driven algorithms are becoming increasingly popular in the medical domain. However, due to the nonlinear and complex behavior of many of these algorithms, decision-making by such algorithms is not trustworthy for clinicians and is considered a black-box process. Hence, the scientific community has introduced explainable artificial intelligence (XAI) to remedy the problem. This systematic scoping review investigates the application of XAI in breast cancer detection and risk prediction. We conducted a comprehensive search on Scopus, IEEE Explore, PubMed, and Google Scholar (first 50 citations) using a systematic search strategy. The search spanned from January 2017 to July 2023, focusing on peer-reviewed studies implementing XAI methods in breast cancer datasets. Thirty studies met our inclusion criteria and were included in the analysis. The results revealed that SHapley Additive exPlanations (SHAP) is the top model-agnostic XAI technique in breast cancer research in terms of usage, explaining the model prediction results, diagnosis and classification of biomarkers, and prognosis and survival analysis. Additionally, the SHAP model primarily explained tree-based ensemble machine learning models. The most common reason is that SHAP is model agnostic, which makes it both popular and useful for explaining any model prediction. Additionally, it is relatively easy to implement effectively and completely suits performant models, such as tree-based models. Explainable AI improves the transparency, interpretability, fairness, and trustworthiness of AI-enabled health systems and medical devices and, ultimately, the quality of care and outcomes.},
 author = {Amirehsan Ghasemi and Soheil Hashtarkhani and David L Schwartz and Arash Shaban-Nejad},
 comment = {22 Pages, 6 Figures, 13 Tables},
 doi = {10.1002/cai2.136},
 eprint = {2407.12058v1},
 journal = {arXiv preprint},
 title = {Explainable artificial intelligence in breast cancer detection and risk prediction: A systematic scoping review},
 url = {http://arxiv.org/abs/2407.12058v1},
 year = {2024}
}

@article{2407.12177v1,
 abstract = {Explainable artificial intelligence (XAI) is a set of tools and algorithms that applied or embedded to machine learning models to understand and interpret the models. They are recommended especially for complex or advanced models including deep neural network because they are not interpretable from human point of view. On the other hand, simple models including linear regression are easy to implement, has less computational complexity and easy to visualize the output. The common notion in the literature that simple models including linear regression are considered as "white box" because they are more interpretable and easier to understand. This is based on the idea that linear regression models have several favorable outcomes including the effect of the features in the model and whether they affect positively or negatively toward model output. Moreover, uncertainty of the model can be measured or estimated using the confidence interval. However, we argue that this perception is not accurate and linear regression models are not easy to interpret neither easy to understand considering common XAI metrics and possible challenges might face. This includes linearity, local explanation, multicollinearity, covariates, normalization, uncertainty, features contribution and fairness. Consequently, we recommend the so-called simple models should be treated equally to complex models when it comes to explainability and interpretability.},
 author = {Ahmed M Salih and Yuhe Wang},
 comment = {},
 doi = {},
 eprint = {2407.12177v1},
 journal = {arXiv preprint},
 title = {Are Linear Regression Models White Box and Interpretable?},
 url = {http://arxiv.org/abs/2407.12177v1},
 year = {2024}
}

@article{2407.12243v1,
 abstract = {Despite their impact on the society, deep neural networks are often regarded as black-box models due to their intricate structures and the absence of explanations for their decisions. This opacity poses a significant challenge to AI systems wider adoption and trustworthiness. This thesis addresses this issue by contributing to the field of eXplainable AI, focusing on enhancing the interpretability of deep neural networks. The core contributions lie in introducing novel techniques aimed at making these networks more interpretable by leveraging an analysis of their inner workings. Specifically, the contributions are threefold. Firstly, the thesis introduces designs for self-explanatory deep neural networks, such as the integration of external memory for interpretability purposes and the usage of prototype and constraint-based layers across several domains. Secondly, this research delves into novel investigations on neurons within trained deep neural networks, shedding light on overlooked phenomena related to their activation values. Lastly, the thesis conducts an analysis of the application of explanatory techniques in the field of visual analytics, exploring the maturity of their adoption and the potential of these systems to convey explanations to users effectively.},
 author = {Biagio La Rosa},
 comment = {PhD Thesis},
 doi = {},
 eprint = {2407.12243v1},
 journal = {arXiv preprint},
 title = {Explaining Deep Neural Networks by Leveraging Intrinsic Methods},
 url = {http://arxiv.org/abs/2407.12243v1},
 year = {2024}
}

@article{2407.12401v1,
 abstract = {Identifying the relevant input features that have a critical influence on the output results is indispensable for the development of explainable artificial intelligence (XAI). Remove-and-Retrain (ROAR) is a widely accepted approach for assessing the importance of individual pixels by measuring changes in accuracy following their removal and subsequent retraining of the modified dataset. However, we uncover notable limitations in pixel-perturbation strategies. When viewed from a geometric perspective, we discover that these metrics fail to discriminate between differences among feature attribution methods, thereby compromising the reliability of the evaluation. To address this challenge, we introduce an alternative feature-perturbation approach named Geometric Remove-and-Retrain (GOAR). Through a series of experiments with both synthetic and real datasets, we substantiate that GOAR transcends the limitations of pixel-centric metrics.},
 author = {Yong-Hyun Park and Junghoon Seo and Bomseok Park and Seongsu Lee and Junghyo Jo},
 comment = {Accepted in XAI in Action Workshop @ NeurIPS2023},
 doi = {},
 eprint = {2407.12401v1},
 journal = {arXiv preprint},
 title = {Geometric Remove-and-Retrain (GOAR): Coordinate-Invariant eXplainable AI Assessment},
 url = {http://arxiv.org/abs/2407.12401v1},
 year = {2024}
}

@article{2407.12950v2,
 abstract = {We introduce a novel metric for measuring semantic continuity in Explainable AI methods and machine learning models. We posit that for models to be truly interpretable and trustworthy, similar inputs should yield similar explanations, reflecting a consistent semantic understanding. By leveraging XAI techniques, we assess semantic continuity in the task of image recognition. We conduct experiments to observe how incremental changes in input affect the explanations provided by different XAI methods. Through this approach, we aim to evaluate the models' capability to generalize and abstract semantic concepts accurately and to evaluate different XAI methods in correctly capturing the model behaviour. This paper contributes to the broader discourse on AI interpretability by proposing a quantitative measure for semantic continuity for XAI methods, offering insights into the models' and explainers' internal reasoning processes, and promoting more reliable and transparent AI systems.},
 author = {Qi Huang and Emanuele Mezzi and Osman Mutlu and Miltiadis Kofinas and Vidya Prasad and Shadnan Azwad Khan and Elena Ranguelova and Niki van Stein},
 comment = {25 pages, accepted at the world conference of explainable AI, 2024, Malta},
 doi = {10.1007/978-3-031-63787-2_16},
 eprint = {2407.12950v2},
 journal = {arXiv preprint},
 title = {Beyond the Veil of Similarity: Quantifying Semantic Continuity in Explainable AI},
 url = {http://arxiv.org/abs/2407.12950v2},
 year = {2024}
}

@article{2407.13902v1,
 abstract = {The advancement of machine learning (ML) models has led to the development of ML-based approaches to improve numerous software engineering tasks in software maintenance and evolution. Nevertheless, research indicates that despite their potential successes, ML models may not be employed in real-world scenarios because they often remain a black box to practitioners, lacking explainability in their reasoning. Recently, various rule-based model-agnostic Explainable AI (XAI) techniques, such as PyExplainer and LIME, have been employed to explain the predictions of ML models in software analytics tasks. This paper assesses the ability of these techniques (e.g., PyExplainer and LIME) to generate reliable and consistent explanations for ML models across various software analytics tasks, including Just-in-Time (JIT) defect prediction, clone detection, and the classification of useful code review comments. Our manual investigations find inconsistencies and anomalies in the explanations generated by these techniques. Therefore, we design a novel framework: Evaluation of Explainable AI (EvaluateXAI), along with granular-level evaluation metrics, to automatically assess the effectiveness of rule-based XAI techniques in generating reliable and consistent explanations for ML models in software analytics tasks. After conducting in-depth experiments involving seven state-of-the-art ML models trained on five datasets and six evaluation metrics, we find that none of the evaluation metrics reached 100\%, indicating the unreliability of the explanations generated by XAI techniques. Additionally, PyExplainer and LIME failed to provide consistent explanations for 86.11% and 77.78% of the experimental combinations, respectively. Therefore, our experimental findings emphasize the necessity for further research in XAI to produce reliable and consistent explanations for ML models in software analytics tasks.},
 author = {Md Abdul Awal and Chanchal K. Roy},
 comment = {This manuscript was accepted in the Journal of Systems and Software (JSS)},
 doi = {},
 eprint = {2407.13902v1},
 journal = {arXiv preprint},
 title = {EvaluateXAI: A Framework to Evaluate the Reliability and Consistency of Rule-based XAI Techniques for Software Analytics Tasks},
 url = {http://arxiv.org/abs/2407.13902v1},
 year = {2024}
}

@article{2407.14430v1,
 abstract = {In this paper, we investigate the extrapolation capabilities of implicit deep learning models in handling unobserved data, where traditional deep neural networks may falter. Implicit models, distinguished by their adaptability in layer depth and incorporation of feedback within their computational graph, are put to the test across various extrapolation scenarios: out-of-distribution, geographical, and temporal shifts. Our experiments consistently demonstrate significant performance advantage with implicit models. Unlike their non-implicit counterparts, which often rely on meticulous architectural design for each task, implicit models demonstrate the ability to learn complex model structures without the need for task-specific design, highlighting their robustness in handling unseen data.},
 author = {Juliette Decugis and Alicia Y. Tsai and Max Emerling and Ashwin Ganesh and Laurent El Ghaoui},
 comment = {Accepted at the Workshop on Explainable Artificial Intelligence (XAI) at IJCAI 2024},
 doi = {},
 eprint = {2407.14430v1},
 journal = {arXiv preprint},
 title = {The Extrapolation Power of Implicit Models},
 url = {http://arxiv.org/abs/2407.14430v1},
 year = {2024}
}

@article{2407.14543v1,
 abstract = {Rule-based models offer a human-understandable representation, i.e. they are interpretable. For this reason, they are used to explain the decisions of non-interpretable complex models, referred to as black box models. The generation of such explanations involves the approximation of a black box model by a rule-based model. To date, however, it has not been investigated whether the rule-based model makes decisions in the same way as the black box model it approximates. Decision making in the same way is understood in this work as the consistency of decisions and the consistency of the most important attributes used for decision making. This study proposes a novel approach ensuring that the rule-based surrogate model mimics the performance of the black box model. The proposed solution performs an explanation fusion involving rule generation and taking into account the feature importance determined by the selected XAI methods for the black box model being explained. The result of the method can be both global and local rule-based explanations. The quality of the proposed solution was verified by extensive analysis on 30 tabular benchmark datasets representing classification problems. Evaluation included comparison with the reference method and an illustrative case study. In addition, the paper discusses the possible pathways for the application of the rule-based approach in XAI and how rule-based explanations, including the proposed method, meet the user perspective and requirements for both content and presentation. The software created and a detailed report containing the full experimental results are available on the GitHub repository (https://github.com/ruleminer/FI-rules4XAI ).},
 author = {Michał Kozielski and Marek Sikora and Łukasz Wawrowski},
 comment = {},
 doi = {10.1016/j.knosys.2025.113092},
 eprint = {2407.14543v1},
 journal = {arXiv preprint},
 title = {Towards consistency of rule-based explainer and black box model -- fusion of rule induction and XAI-based feature importance},
 url = {http://arxiv.org/abs/2407.14543v1},
 year = {2024}
}

@article{2407.15216v1,
 abstract = {The development of AI-driven generative audio mirrors broader AI trends, often prioritizing immediate accessibility at the expense of explainability. Consequently, integrating such tools into sustained artistic practice remains a significant challenge. In this paper, we explore several paths to improve explainability, drawing primarily from our research-creation practice in training and implementing generative audio models. As practical provisions for improved explainability, we highlight human agency over training materials, the viability of small-scale datasets, the facilitation of the iterative creative process, and the integration of interactive machine learning as a mapping tool. Importantly, these steps aim to enhance human agency over generative AI systems not only during model inference, but also when curating and preprocessing training data as well as during the training phase of models.},
 author = {Austin Tecks and Thomas Peschlow and Gabriel Vigliensoni},
 comment = {In Proceedings of Explainable AI for the Arts Workshop 2024 (XAIxArts 2024) arXiv:2406.14485},
 doi = {},
 eprint = {2407.15216v1},
 journal = {arXiv preprint},
 title = {Explainability Paths for Sustained Artistic Practice with AI},
 url = {http://arxiv.org/abs/2407.15216v1},
 year = {2024}
}

@article{2407.15780v1,
 abstract = {This paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning (ML) models. Contrary to the prevalent black-box perception, our study focuses on models with transparent internal mechanisms. We address two principal types of explanation problems: abductive and contrastive, both in their local and global variants. Our analysis encompasses diverse ML models, including Decision Trees, Decision Sets, Decision Lists, Ordered Binary Decision Diagrams, Random Forests, and Boolean Circuits, and ensembles thereof, each offering unique explanatory challenges. This research fills a significant gap in explainable AI (XAI) by providing a foundational understanding of the complexities of generating explanations for these models. This work provides insights vital for further research in the domain of XAI, contributing to the broader discourse on the necessity of transparency and accountability in AI systems.},
 author = {Sebastian Ordyniak and Giacomo Paesani and Mateusz Rychlicki and Stefan Szeider},
 comment = {A short version of the paper has been accepted at the 21st International Conference on Principles of Knowledge Representation and Reasoning (KR 2024)},
 doi = {},
 eprint = {2407.15780v1},
 journal = {arXiv preprint},
 title = {Explaining Decisions in ML Models: a Parameterized Complexity Analysis},
 url = {http://arxiv.org/abs/2407.15780v1},
 year = {2024}
}

@article{2407.15909v1,
 abstract = {Artificial Intelligence (AI) models have reached a very significant level of accuracy. While their superior performance offers considerable benefits, their inherent complexity often decreases human trust, which slows their application in high-risk decision-making domains, such as finance. The field of eXplainable AI (XAI) seeks to bridge this gap, aiming to make AI models more understandable. This survey, focusing on published work from the past five years, categorizes XAI approaches that predict financial time series. In this paper, explainability and interpretability are distinguished, emphasizing the need to treat these concepts separately as they are not applied the same way in practice. Through clear definitions, a rigorous taxonomy of XAI approaches, a complementary characterization, and examples of XAI's application in the finance industry, this paper provides a comprehensive view of XAI's current role in finance. It can also serve as a guide for selecting the most appropriate XAI approach for future applications.},
 author = {Pierre-Daniel Arsenault and Shengrui Wang and Jean-Marc Patenande},
 comment = {35 pages, This is the author's version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record will be published in a journal soon},
 doi = {10.1145/3729531},
 eprint = {2407.15909v1},
 journal = {arXiv preprint},
 title = {A Survey of Explainable Artificial Intelligence (XAI) in Financial Time Series Forecasting},
 url = {http://arxiv.org/abs/2407.15909v1},
 year = {2024}
}

@article{2407.16482v1,
 abstract = {Shapley Values are concepts established for eXplainable AI. They are used to explain black-box predictive models by quantifying the features' contributions to the model's outcomes. Since computing the exact Shapley Values is known to be computationally intractable on real-world datasets, neural estimators have emerged as alternative, more scalable approaches to get approximated Shapley Values estimates. However, experiments with neural estimators are currently hard to replicate as algorithm implementations, explainer evaluators, and results visualizations are neither standardized nor promptly usable. To bridge this gap, we present BONES, a new benchmark focused on neural estimation of Shapley Value. It provides researchers with a suite of state-of-the-art neural and traditional estimators, a set of commonly used benchmark datasets, ad hoc modules for training black-box models, as well as specific functions to easily compute the most popular evaluation metrics and visualize results. The purpose is to simplify XAI model usage, evaluation, and comparison. In this paper, we showcase BONES results and visualizations for XAI model benchmarking on both tabular and image data. The open-source library is available at the following link: https://github.com/DavideNapolitano/BONES.},
 author = {Davide Napolitano and Luca Cagliero},
 comment = {6 pages},
 doi = {},
 eprint = {2407.16482v1},
 journal = {arXiv preprint},
 title = {BONES: a Benchmark fOr Neural Estimation of Shapley values},
 url = {http://arxiv.org/abs/2407.16482v1},
 year = {2024}
}

@article{2407.17165v1,
 abstract = {Antimicrobial Resistance represents a significant challenge in the Intensive Care Unit (ICU), where patients are at heightened risk of Multidrug-Resistant (MDR) infections-pathogens resistant to multiple antimicrobial agents. This study introduces a novel methodology that integrates Gated Recurrent Units (GRUs) with advanced intrinsic and post-hoc interpretability techniques for detecting the onset of MDR in patients across time. Within interpretability methods, we propose Explainable Artificial Intelligence (XAI) approaches to handle irregular Multivariate Time Series (MTS), introducing Irregular Time Shapley Additive Explanations (IT-SHAP), a modification of Shapley Additive Explanations designed for irregular MTS with Recurrent Neural Networks focused on temporal outputs. Our methodology aims to identify specific risk factors associated with MDR in ICU patients. GRU with Hadamard's attention demonstrated high initial specificity and increasing sensitivity over time, correlating with increased nosocomial infection risks during prolonged ICU stays. XAI analysis, enhanced by Hadamard attention and IT-SHAP, identified critical factors such as previous non-resistant cultures, specific antibiotic usage patterns, and hospital environment dynamics. These insights suggest that early detection of at-risk patients can inform interventions such as preventive isolation and customized treatments, significantly improving clinical outcomes. The proposed GRU model for temporal classification achieved an average Receiver Operating Characteristic Area Under the Curve of 78.27 +- 1.26 over time, indicating strong predictive performance. In summary, this study highlights the clinical utility of our methodology, which combines predictive accuracy with interpretability, thereby facilitating more effective healthcare interventions by professionals.},
 author = {Óscar Escudero-Arnanz and Cristina Soguero-Ruiz and Joaquín Álvarez-Rodríguez and Antonio G. Marques},
 comment = {},
 doi = {},
 eprint = {2407.17165v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence Techniques for Irregular Temporal Classification of Multidrug Resistance Acquisition in Intensive Care Unit Patients},
 url = {http://arxiv.org/abs/2407.17165v1},
 year = {2024}
}

@article{2407.17454v3,
 abstract = {Automated reasoning is a key technology in the young but rapidly growing field of Explainable Artificial Intelligence (XAI). Explanability helps build trust in artificial intelligence systems beyond their mere predictive accuracy and robustness. In this paper, we propose a cycle of scientific discovery that combines machine learning with automated reasoning for the generation and the selection of explanations. We present a taxonomy of explanation selection problems that draws on insights from sociology and cognitive science. These selection criteria subsume existing notions and extend them with new properties.},
 author = {Markus Iser},
 comment = {Composite AI Workshop at ECAI 2024 (accepted for publication)},
 doi = {},
 eprint = {2407.17454v3},
 journal = {arXiv preprint},
 title = {Automated Explanation Selection for Scientific Discovery},
 url = {http://arxiv.org/abs/2407.17454v3},
 year = {2024}
}

@article{2407.18343v2,
 abstract = {Explainable Artificial Intelligence (XAI) is central to the debate on integrating Artificial Intelligence (AI) and Machine Learning (ML) algorithms into clinical practice. High-performing AI/ML models, such as ensemble learners and deep neural networks, often lack interpretability, hampering clinicians' trust in their predictions. To address this, XAI techniques are being developed to describe AI/ML predictions in human-understandable terms. One promising direction is the adaptation of sensitivity analysis (SA) and global sensitivity analysis (GSA), which inherently rank model inputs by their impact on predictions. Here, we introduce a novel delta-XAI method that provides local explanations of ML model predictions by extending the delta index, a GSA metric. The delta-XAI index assesses the impact of each feature's value on the predicted output for individual instances in both regression and classification problems. We formalize the delta-XAI index and provide code for its implementation. The delta-XAI method was evaluated on simulated scenarios using linear regression models, with Shapley values serving as a benchmark. Results showed that the delta-XAI index is generally consistent with Shapley values, with notable discrepancies in models with highly impactful or extreme feature values. The delta-XAI index demonstrated higher sensitivity in detecting dominant features and handling extreme feature values. Qualitatively, the delta-XAI provides intuitive explanations by leveraging probability density functions, making feature rankings clearer and more explainable for practitioners. Overall, the delta-XAI method appears promising for robustly obtaining local explanations of ML model predictions. Further investigations in real-world clinical settings will be conducted to evaluate its impact on AI-assisted clinical workflows.},
 author = {Alessandro De Carlo and Enea Parimbelli and Nicola Melillo and Giovanna Nicora},
 comment = {},
 doi = {},
 eprint = {2407.18343v2},
 journal = {arXiv preprint},
 title = {Introducing δ-XAI: a novel sensitivity-based method for local AI explanations},
 url = {http://arxiv.org/abs/2407.18343v2},
 year = {2024}
}

@article{2407.18482v1,
 abstract = {Different prediction models might perform equally well (Rashomon set) in the same task, but offer conflicting interpretations and conclusions about the data. The Rashomon effect in the context of Explainable AI (XAI) has been recognized as a critical factor. Although the Rashomon set has been introduced and studied in various contexts, its practical application is at its infancy stage and lacks adequate guidance and evaluation. We study the problem of the Rashomon set sampling from a practical viewpoint and identify two fundamental axioms - generalizability and implementation sparsity that exploring methods ought to satisfy in practical usage. These two axioms are not satisfied by most known attribution methods, which we consider to be a fundamental weakness. We use the norms to guide the design of an $ε$-subgradient-based sampling method. We apply this method to a fundamental mathematical problem as a proof of concept and to a set of practical datasets to demonstrate its ability compared with existing sampling methods.},
 author = {Sichao Li and Amanda S. Barnard and Quanling Deng},
 comment = {},
 doi = {},
 eprint = {2407.18482v1},
 journal = {arXiv preprint},
 title = {Practical Attribution Guidance for Rashomon Sets},
 url = {http://arxiv.org/abs/2407.18482v1},
 year = {2024}
}

@article{2407.18935v1,
 abstract = {The successful application of machine learning (ML) in catalyst design relies on high-quality and diverse data to ensure effective generalization to novel compositions, thereby aiding in catalyst discovery. However, due to complex interactions, catalyst design has long relied on trial-and-error, a costly and labor-intensive process leading to scarce data that is heavily biased towards undesired, low-yield catalysts. Despite the rise of ML in this field, most efforts have not focused on dealing with the challenges presented by such experimental data. To address these challenges, we introduce a robust machine learning and explainable AI (XAI) framework to accurately classify the catalytic yield of various compositions and identify the contributions of individual components. This framework combines a series of ML practices designed to handle the scarcity and imbalance of catalyst data. We apply the framework to classify the yield of various catalyst compositions in oxidative methane coupling, and use it to evaluate the performance of a range of ML models: tree-based models, logistic regression, support vector machines, and neural networks. These experiments demonstrate that the methods used in our framework lead to a significant improvement in the performance of all but one of the evaluated models. Additionally, the decision-making process of each ML model is analyzed by identifying the most important features for predicting catalyst performance using XAI methods. Our analysis found that XAI methods, providing class-aware explanations, such as Layer-wise Relevance Propagation, identified key components that contribute specifically to high-yield catalysts. These findings align with chemical intuition and existing literature, reinforcing their validity. We believe that such insights can assist chemists in the development and identification of novel catalysts with superior performance.},
 author = {Parastoo Semnani and Mihail Bogojeski and Florian Bley and Zizheng Zhang and Qiong Wu and Thomas Kneib and Jan Herrmann and Christoph Weisser and Florina Patcas and Klaus-Robert Müller},
 comment = {},
 doi = {},
 eprint = {2407.18935v1},
 journal = {arXiv preprint},
 title = {A Machine Learning and Explainable AI Framework Tailored for Unbalanced Experimental Catalyst Discovery},
 url = {http://arxiv.org/abs/2407.18935v1},
 year = {2024}
}

@article{2407.19153v1,
 abstract = {The problem of malicious software (malware) detection and classification is a complex task, and there is no perfect approach. There is still a lot of work to be done. Unlike most other research areas, standard benchmarks are difficult to find for malware detection. This paper aims to investigate recent advances in malware detection on MacOS, Windows, iOS, Android, and Linux using deep learning (DL) by investigating DL in text and image classification, the use of pre-trained and multi-task learning models for malware detection approaches to obtain high accuracy and which the best approach if we have a standard benchmark dataset. We discuss the issues and the challenges in malware detection using DL classifiers by reviewing the effectiveness of these DL classifiers and their inability to explain their decisions and actions to DL developers presenting the need to use Explainable Machine Learning (XAI) or Interpretable Machine Learning (IML) programs. Additionally, we discuss the impact of adversarial attacks on deep learning models, negatively affecting their generalization capabilities and resulting in poor performance on unseen data. We believe there is a need to train and test the effectiveness and efficiency of the current state-of-the-art deep learning models on different malware datasets. We examine eight popular DL approaches on various datasets. This survey will help researchers develop a general understanding of malware recognition using deep learning.},
 author = {Ahmed Bensaoud and Jugal Kalita and Mahmoud Bensaoud},
 comment = {},
 doi = {10.1016/j.mlwa.2024.100546},
 eprint = {2407.19153v1},
 journal = {arXiv preprint},
 title = {A Survey of Malware Detection Using Deep Learning},
 url = {http://arxiv.org/abs/2407.19153v1},
 year = {2024}
}

@article{2407.19683v1,
 abstract = {Post-hoc interpretability methods play a critical role in explainable artificial intelligence (XAI), as they pinpoint portions of data that a trained deep learning model deemed important to make a decision. However, different post-hoc interpretability methods often provide different results, casting doubts on their accuracy. For this reason, several evaluation strategies have been proposed to understand the accuracy of post-hoc interpretability. Many of these evaluation strategies provide a coarse-grained assessment -- i.e., they evaluate how the performance of the model degrades on average by corrupting different data points across multiple samples. While these strategies are effective in selecting the post-hoc interpretability method that is most reliable on average, they fail to provide a sample-level, also referred to as fine-grained, assessment. In other words, they do not measure the robustness of post-hoc interpretability methods. We propose an approach and two new metrics to provide a fine-grained assessment of post-hoc interpretability methods. We show that the robustness is generally linked to its coarse-grained performance.},
 author = {Jiawen Wei and Hugues Turbé and Gianmarco Mengaldo},
 comment = {},
 doi = {},
 eprint = {2407.19683v1},
 journal = {arXiv preprint},
 title = {Revisiting the robustness of post-hoc interpretability methods},
 url = {http://arxiv.org/abs/2407.19683v1},
 year = {2024}
}

@article{2407.19781v1,
 abstract = {This extended abstract accompanies an invited talk at CASC 2024, which surveys recent developments in Real Quantifier Elimination (QE) and Cylindrical Algebraic Decomposition (CAD). After introducing these concepts we will first consider adaptations of CAD inspired by computational logic, in particular the algorithms which underpin modern SAT solvers. CAD theory has found use in collaboration with these via the Satisfiability Modulo Theory (SMT) paradigm; while the ideas behind SAT/SMT have led to new algorithms for Real QE. Second we will consider the optimisation of CAD through the use of Machine Learning (ML). The choice of CAD variable ordering has become a key case study for the use of ML to tune algorithms in computer algebra. We will also consider how explainable AI techniques might give insight for improved computer algebra software without any reliance on ML in the final code.},
 author = {Matthew England},
 comment = {Extended Abstract to accompany Invited Talk at CASC 2024},
 doi = {10.1007/978-3-031-69070-9_1},
 eprint = {2407.19781v1},
 journal = {arXiv preprint},
 title = {Recent Developments in Real Quantifier Elimination and Cylindrical Algebraic Decomposition},
 url = {http://arxiv.org/abs/2407.19781v1},
 year = {2024}
}

@article{2407.19897v1,
 abstract = {Recent research in explainability has given rise to numerous post-hoc attribution methods aimed at enhancing our comprehension of the outputs of black-box machine learning models. However, evaluating the quality of explanations lacks a cohesive approach and a consensus on the methodology for deriving quantitative metrics that gauge the efficacy of explainability post-hoc attribution methods. Furthermore, with the development of increasingly complex deep learning models for diverse data applications, the need for a reliable way of measuring the quality and correctness of explanations is becoming critical. We address this by proposing BEExAI, a benchmark tool that allows large-scale comparison of different post-hoc XAI methods, employing a set of selected evaluation metrics.},
 author = {Samuel Sithakoul and Sara Meftah and Clément Feutry},
 comment = {},
 doi = {},
 eprint = {2407.19897v1},
 journal = {arXiv preprint},
 title = {BEExAI: Benchmark to Evaluate Explainable AI},
 url = {http://arxiv.org/abs/2407.19897v1},
 year = {2024}
}

@article{2407.19922v1,
 abstract = {Large language models (LLMs) play a vital role in almost every domain in today's organizations. In the context of this work, we highlight the use of LLMs for sentiment analysis (SA) and explainability. Specifically, we contribute a novel technique to leverage LLMs as a post-hoc model-independent tool for the explainability of SA. We applied our technique in the financial domain for currency-pair price predictions using open news feed data merged with market prices. Our application shows that the developed technique is not only a viable alternative to using conventional eXplainable AI but can also be fed back to enrich the input to the machine learning (ML) model to better predict future currency-pair values. We envision our results could be generalized to employing explainability as a conventional enrichment for ML input for better ML predictions in general.},
 author = {Lior Limonad and Fabiana Fournier and Juan Manuel Vera Díaz and Inna Skarbovsky and Shlomit Gur and Raquel Lazcano},
 comment = {7 pages, 3 figures, AIFin@ECAI 2024},
 doi = {},
 eprint = {2407.19922v1},
 journal = {arXiv preprint},
 title = {Monetizing Currency Pair Sentiments through LLM Explainability},
 url = {http://arxiv.org/abs/2407.19922v1},
 year = {2024}
}

@article{2407.19951v1,
 abstract = {Generative models based on variational autoencoders are a popular technique for detecting anomalies in images in a semi-supervised context. A common approach employs the anomaly score to detect the presence of anomalies, and it is known to reach high level of accuracy on benchmark datasets. However, since anomaly scores are computed from reconstruction disparities, they often obscure the detection of various spurious features, raising concerns regarding their actual efficacy. This case study explores the robustness of an anomaly detection system based on variational autoencoder generative models through the use of eXplainable AI methods. The goal is to get a different perspective on the real performances of anomaly detectors that use reconstruction differences. In our case study we discovered that, in many cases, samples are detected as anomalous for the wrong or misleading factors.},
 author = {Muhammad Rashid and Elvio Amparore and Enrico Ferrari and Damiano Verda},
 comment = {World Conference on eXplainable Artificial Intelligence},
 doi = {10.1007/978-3-031-63803-9_13},
 eprint = {2407.19951v1},
 journal = {arXiv preprint},
 title = {Can I trust my anomaly detection system? A case study based on explainable AI},
 url = {http://arxiv.org/abs/2407.19951v1},
 year = {2024}
}

@article{2407.20000v1,
 abstract = {We introduce CollisionPro, a pioneering framework designed to estimate cumulative collision probability distributions using temporal difference learning, specifically tailored to applications in robotics, with a particular emphasis on autonomous driving. This approach addresses the demand for explainable artificial intelligence (XAI) and seeks to overcome limitations imposed by model-based approaches and conservative constraints. We formulate our framework within the context of reinforcement learning to pave the way for safety-aware agents. Nevertheless, we assert that our approach could prove beneficial in various contexts, including a safety alert system or analytical purposes. A comprehensive examination of our framework is conducted using a realistic autonomous driving simulator, illustrating its high sample efficiency and reliable prediction capabilities for previously unseen collision events. The source code is publicly available.},
 author = {Thomas Steinecker and Thorsten Luettel and Mirko Maehlisch},
 comment = {Code: https://github.com/UniBwTAS/CollisionPro},
 doi = {},
 eprint = {2407.20000v1},
 journal = {arXiv preprint},
 title = {Collision Probability Distribution Estimation via Temporal Difference Learning},
 url = {http://arxiv.org/abs/2407.20000v1},
 year = {2024}
}

@article{2407.20067v2,
 abstract = {Graph Neural Networks (GNNs) have emerged as the predominant paradigm for learning from graph-structured data, offering a wide range of applications from social network analysis to bioinformatics. Despite their versatility, GNNs face challenges such as lack of generalization and poor interpretability, which hinder their wider adoption and reliability in critical applications. Dropping has emerged as an effective paradigm for improving the generalization capabilities of GNNs. However, existing approaches often rely on random or heuristic-based selection criteria, lacking a principled method to identify and exclude nodes that contribute to noise and over-complexity in the model. In this work, we argue that explainability should be a key indicator of a model's quality throughout its training phase. To this end, we introduce xAI-Drop, a novel topological-level dropping regularizer that leverages explainability to pinpoint noisy network elements to be excluded from the GNN propagation mechanism. An empirical evaluation on diverse real-world datasets demonstrates that our method outperforms current state-of-the-art dropping approaches in accuracy, and improves explanation quality.},
 author = {Vincenzo Marco De Luca and Antonio Longa and Andrea Passerini and Pietro Liò},
 comment = {},
 doi = {},
 eprint = {2407.20067v2},
 journal = {arXiv preprint},
 title = {xAI-Drop: Don't Use What You Cannot Explain},
 url = {http://arxiv.org/abs/2407.20067v2},
 year = {2024}
}

@article{2407.20274v1,
 abstract = {In this paper we investigate the explainability of transformer models and their plausibility for hate speech and counter speech detection. We compare representatives of four different explainability approaches, i.e., gradient-based, perturbation-based, attention-based, and prototype-based approaches, and analyze them quantitatively with an ablation study and qualitatively in a user study. Results show that perturbation-based explainability performs best, followed by gradient-based and attention-based explainability. Prototypebased experiments did not yield useful results. Overall, we observe that explainability strongly supports the users in better understanding the model predictions.},
 author = {Adrian Jaques Böck and Djordje Slijepčević and Matthias Zeppelzauer},
 comment = {conference, CBMI2024, 6 pages,},
 doi = {},
 eprint = {2407.20274v1},
 journal = {arXiv preprint},
 title = {Exploring the Plausibility of Hate and Counter Speech Detectors with Explainable AI},
 url = {http://arxiv.org/abs/2407.20274v1},
 year = {2024}
}

@article{2407.20284v1,
 abstract = {In modern healthcare, addressing the complexities of accurate disease prediction and personalized recommendations is both crucial and challenging. This research introduces MLtoGAI, which integrates Semantic Web technology with Machine Learning (ML) to enhance disease prediction and offer user-friendly explanations through ChatGPT. The system comprises three key components: a reusable disease ontology that incorporates detailed knowledge about various diseases, a diagnostic classification model that uses patient symptoms to detect specific diseases accurately, and the integration of Semantic Web Rule Language (SWRL) with ontology and ChatGPT to generate clear, personalized health advice. This approach significantly improves prediction accuracy and ensures results that are easy to understand, addressing the complexity of diseases and diverse symptoms. The MLtoGAI system demonstrates substantial advancements in accuracy and user satisfaction, contributing to developing more intelligent and accessible healthcare solutions. This innovative approach combines the strengths of ML algorithms with the ability to provide transparent, human-understandable explanations through ChatGPT, achieving significant improvements in prediction accuracy and user comprehension. By leveraging semantic technology and explainable AI, the system enhances the accuracy of disease prediction and ensures that the recommendations are relevant and easily understood by individual patients. Our research highlights the potential of integrating advanced technologies to overcome existing challenges in medical diagnostics, paving the way for future developments in intelligent healthcare systems. Additionally, the system is validated using 200 synthetic patient data records, ensuring robust performance and reliability.},
 author = {Shyam Dongre and Ritesh Chandra and Sonali Agarwal},
 comment = {},
 doi = {},
 eprint = {2407.20284v1},
 journal = {arXiv preprint},
 title = {MLtoGAI: Semantic Web based with Machine Learning for Enhanced Disease Prediction and Personalized Recommendations using Generative AI},
 url = {http://arxiv.org/abs/2407.20284v1},
 year = {2024}
}

@article{2407.21535v1,
 abstract = {A scoring system is a simple decision model that checks a set of features, adds a certain number of points to a total score for each feature that is satisfied, and finally makes a decision by comparing the total score to a threshold. Scoring systems have a long history of active use in safety-critical domains such as healthcare and justice, where they provide guidance for making objective and accurate decisions. Given their genuine interpretability, the idea of learning scoring systems from data is obviously appealing from the perspective of explainable AI. In this paper, we propose a practically motivated extension of scoring systems called probabilistic scoring lists (PSL), as well as a method for learning PSLs from data. Instead of making a deterministic decision, a PSL represents uncertainty in the form of probability distributions, or, more generally, probability intervals. Moreover, in the spirit of decision lists, a PSL evaluates features one by one and stops as soon as a decision can be made with enough confidence. To evaluate our approach, we conduct a case study in the medical domain.},
 author = {Jonas Hanselle and Stefan Heid and Johannes Fürnkranz and Eyke Hüllermeier},
 comment = {},
 doi = {},
 eprint = {2407.21535v1},
 journal = {arXiv preprint},
 title = {Probabilistic Scoring Lists for Interpretable Machine Learning},
 url = {http://arxiv.org/abs/2407.21535v1},
 year = {2024}
}

@article{2408.00493v1,
 abstract = {Modern Machine Learning (ML) has significantly advanced various research fields, but the opaque nature of ML models hinders their adoption in several domains. Explainable AI (XAI) addresses this challenge by providing additional information to help users understand the internal decision-making process of ML models. In the field of neuroscience, enriching a ML model for brain decoding with attribution-based XAI techniques means being able to highlight which brain areas correlate with the task at hand, thus offering valuable insights to domain experts. In this paper, we analyze human and Computer Vision (CV) systems in parallel, training and explaining two ML models based respectively on functional Magnetic Resonance Imaging (fMRI) and movie frames. We do so by leveraging the "StudyForrest" dataset, which includes functional Magnetic Resonance Imaging (fMRI) scans of subjects watching the "Forrest Gump" movie, emotion annotations, and eye-tracking data. For human vision the ML task is to link fMRI data with emotional annotations, and the explanations highlight the brain regions strongly correlated with the label. On the other hand, for computer vision, the input data is movie frames, and the explanations are pixel-level heatmaps. We cross-analyzed our results, linking human attention (obtained through eye-tracking) with XAI saliency on CV models and brain region activations. We show how a parallel analysis of human and computer vision can provide useful information for both the neuroscience community (allocation theory) and the ML community (biological plausibility of convolutional models).},
 author = {Alessio Borriero and Martina Milazzo and Matteo Diano and Davide Orsenigo and Maria Chiara Villa and Chiara Di Fazio and Marco Tamietto and Alan Perotti},
 comment = {This work has been accepted to be presented to The 2nd World Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19, 2024 - Malta},
 doi = {},
 eprint = {2408.00493v1},
 journal = {arXiv preprint},
 title = {Explainable Emotion Decoding for Human and Computer Vision},
 url = {http://arxiv.org/abs/2408.00493v1},
 year = {2024}
}

@article{2408.01408v1,
 abstract = {This paper provides a comprehensive and detailed derivation of the backpropagation algorithm for graph convolutional neural networks using matrix calculus. The derivation is extended to include arbitrary element-wise activation functions and an arbitrary number of layers. The study addresses two fundamental problems, namely node classification and link prediction. To validate our method, we compare it with reverse-mode automatic differentiation. The experimental results demonstrate that the median sum of squared errors of the updated weight matrices, when comparing our method to the approach using reverse-mode automatic differentiation, falls within the range of $10^{-18}$ to $10^{-14}$. These outcomes are obtained from conducting experiments on a five-layer graph convolutional network, applied to a node classification problem on Zachary's karate club social network and a link prediction problem on a drug-drug interaction network. Finally, we show how the derived closed-form solution can facilitate the development of explainable AI and sensitivity analysis.},
 author = {Yen-Che Hsiao and Rongting Yue and Abhishek Dutta},
 comment = {},
 doi = {},
 eprint = {2408.01408v1},
 journal = {arXiv preprint},
 title = {Derivation of Back-propagation for Graph Convolutional Networks using Matrix Calculus and its Application to Explainable Artificial Intelligence},
 url = {http://arxiv.org/abs/2408.01408v1},
 year = {2024}
}

@article{2408.02123v3,
 abstract = {Explainability in artificial intelligence (XAI) remains a crucial aspect for fostering trust and understanding in machine learning models. Current visual explanation techniques, such as gradient-based or class-activation-based methods, often exhibit a strong dependence on specific model architectures. Conversely, perturbation-based methods, despite being model-agnostic, are computationally expensive as they require evaluating models on a large number of forward passes. In this work, we introduce Foveation-based Explanations (FovEx), a novel XAI method inspired by human vision. FovEx seamlessly integrates biologically inspired perturbations by iteratively creating foveated renderings of the image and combines them with gradient-based visual explorations to determine locations of interest efficiently. These locations are selected to maximize the performance of the model to be explained with respect to the downstream task and then combined to generate an attribution map. We provide a thorough evaluation with qualitative and quantitative assessments on established benchmarks. Our method achieves state-of-the-art performance on both transformers (on 4 out of 5 metrics) and convolutional models (on 3 out of 5 metrics), demonstrating its versatility among various architectures. Furthermore, we show the alignment between the explanation map produced by FovEx and human gaze patterns (+14\% in NSS compared to RISE, +203\% in NSS compared to GradCAM). This comparison enhances our confidence in FovEx's ability to close the interpretation gap between humans and machines.},
 author = {Mahadev Prasad Panda and Matteo Tiezzi and Martina Vilas and Gemma Roig and Bjoern M. Eskofier and Dario Zanca},
 comment = {Accepted in the International Journal of Computer Vision (Springer Nature)},
 doi = {10.1007/s11263-025-02543-y},
 eprint = {2408.02123v3},
 journal = {arXiv preprint},
 title = {FovEx: Human-Inspired Explanations for Vision Transformers and Convolutional Neural Networks},
 url = {http://arxiv.org/abs/2408.02123v3},
 year = {2024}
}

@article{2408.02132v3,
 abstract = {Identifying reaction coordinates (RCs) is a key to understanding the mechanism of reactions in complex systems. Deep neural network (DNN) and machine learning approaches have become a powerful tool to find the RC. On the other hand, the hyperparameters that determine the DNN model structure can be highly flexible and are often selected intuitively and in a non-trivial and tedious manner. Furthermore, how the hyperparameter choice affects the RC quality remains obscure. Here, we explore the hyperparameter space by developing the hyperparameter tuning approach for the DNN model for RC and investigate how the parameter set affects the RC quality. The DNN model is built to predict the committor along the RC from various collective variables by minimizing the cross-entropy function; the hyperparameters are automatically determined using the Bayesian optimization method. The approach is applied to study the isomerization of alanine dipeptide in vacuum and in water, and the features that characterize the RC are extracted using the explainable AI (XAI) tools. The results show that the DNN models with diverse structures can describe the RC with similar accuracy, and furthermore, the features analyzed by XAI are highly similar. This indicates that the hyperparameter space is multimodal. The electrostatic potential from the solvent to the hydrogen H18 plays an important role in the RC in water. The current study shows that the structure of the DNN models can be rather flexible, while the suitably optimized models share the same features; therefore, a common mechanism from the RC can be extracted.},
 author = {Kyohei Kawashima and Takumi Sato and Kei-ichi Okazaki and Kang Kim and Nobuyuki Matubayasi and Toshifumi Mori},
 comment = {11 pages, 11 figures, 2 table for main text, 12 pages for supplementary material},
 doi = {10.1063/5.0252631},
 eprint = {2408.02132v3},
 journal = {arXiv preprint},
 title = {Investigating the hyperparameter space of deep neural network models for reaction coordinates},
 url = {http://arxiv.org/abs/2408.02132v3},
 year = {2024}
}

@article{2408.02379v1,
 abstract = {Developing and certifying safe - or so-called trustworthy - AI has become an increasingly salient issue, especially in light of upcoming regulation such as the EU AI Act. In this context, the black-box nature of machine learning models limits the use of conventional avenues of approach towards certifying complex technical systems. As a potential solution, methods to give insights into this black-box - devised in the field of eXplainable AI (XAI) - could be used. In this study, the potential and shortcomings of such methods for the purpose of safe AI development and certification are discussed in 15 qualitative interviews with experts out of the areas of (X)AI and certification. We find that XAI methods can be a helpful asset for safe AI development, as they can show biases and failures of ML-models, but since certification relies on comprehensive and correct information about technical systems, their impact is expected to be limited.},
 author = {Benjamin Fresz and Vincent Philipp Göbels and Safa Omri and Danilo Brajovic and Andreas Aichele and Janika Kutz and Jens Neuhüttler and Marco F. Huber},
 comment = {},
 doi = {},
 eprint = {2408.02379v1},
 journal = {arXiv preprint},
 title = {The Contribution of XAI for the Safe Development and Certification of AI: An Expert-Based Analysis},
 url = {http://arxiv.org/abs/2408.02379v1},
 year = {2024}
}

@article{2408.03636v2,
 abstract = {Despite the massive attention given to time-series explanations due to their extensive applications, a notable limitation in existing approaches is their primary reliance on the time-domain. This overlooks the inherent characteristic of time-series data containing both time and frequency features. In this work, we present Spectral eXplanation (SpectralX), an XAI framework that provides time-frequency explanations for time-series black-box classifiers. This easily adaptable framework enables users to "plug-in" various perturbation-based XAI methods for any pre-trained time-series classification models to assess their impact on the explanation quality without having to modify the framework architecture. Additionally, we introduce Feature Importance Approximations (FIA), a new perturbation-based XAI method. These methods consist of feature insertion, deletion, and combination techniques to enhance computational efficiency and class-specific explanations in time-series classification tasks. We conduct extensive experiments in the generated synthetic dataset and various UCR Time-Series datasets to first compare the explanation performance of FIA and other existing perturbation-based XAI methods in both time-domain and time-frequency domain, and then show the superiority of our FIA in the time-frequency domain with the SpectralX framework. Finally, we conduct a user study to confirm the practicality of our FIA in SpectralX framework for class-specific time-frequency based time-series explanations. The source code is available in https://github.com/gustmd0121/Time_is_not_Enough},
 author = {Hyunseung Chung and Sumin Jo and Yeonsu Kwon and Edward Choi},
 comment = {Accepted to CIKM 2024 (10 pages, 9 figures, 9 tables)},
 doi = {10.1145/3627673.3679844},
 eprint = {2408.03636v2},
 journal = {arXiv preprint},
 title = {Time is Not Enough: Time-Frequency based Explanation for Time-Series Black-Box Models},
 url = {http://arxiv.org/abs/2408.03636v2},
 year = {2024}
}

@article{2408.04482v1,
 abstract = {Most of the sophisticated AI models utilize huge amounts of annotated data and heavy training to achieve high-end performance. However, there are certain challenges that hinder the deployment of AI models "in-the-wild" scenarios, i.e., inefficient use of unlabeled data, lack of incorporation of human expertise, and lack of interpretation of the results. To mitigate these challenges, we propose a novel Explainable Active Learning (XAL) model, XAL-based semantic segmentation model "SegXAL", that can (i) effectively utilize the unlabeled data, (ii) facilitate the "Human-in-the-loop" paradigm, and (iii) augment the model decisions in an interpretable way. In particular, we investigate the application of the SegXAL model for semantic segmentation in driving scene scenarios. The SegXAL model proposes the image regions that require labeling assistance from Oracle by dint of explainable AI (XAI) and uncertainty measures in a weakly-supervised manner. Specifically, we propose a novel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty (EBU) module to get an Explainable Error Mask, which enables the machine teachers/human experts to provide intuitive reasoning behind the results and to solicit feedback to the AI system via an active learning strategy. Such a mechanism bridges the semantic gap between man and machine through collaborative intelligence, where humans and AI actively enhance each other's complementary strengths. A novel high-confidence sample selection technique based on the DICE similarity coefficient is also presented within the SegXAL framework. Extensive quantitative and qualitative analyses are carried out in the benchmarking Cityscape dataset. Results show the outperformance of our proposed SegXAL against other state-of-the-art models.},
 author = {Sriram Mandalika and Athira Nambiar},
 comment = {17 pages, 7 figures. To appear in the proceedings of the 27th International Conference on Pattern Recognition (ICPR), 01-05 December, 2024, Kolkata, India},
 doi = {},
 eprint = {2408.04482v1},
 journal = {arXiv preprint},
 title = {SegXAL: Explainable Active Learning for Semantic Segmentation in Driving Scene Scenarios},
 url = {http://arxiv.org/abs/2408.04482v1},
 year = {2024}
}

@article{2408.04778v1,
 abstract = {Tailoring XAI methods to individual needs is crucial for intuitive Human-AI interactions. While context and task goals are vital, factors like user personality traits could also influence method selection. Our study investigates using personality traits to predict user preferences among decision trees, texts, and factor graphs. We trained a Machine Learning model on responses to the Big Five personality test to predict preferences. Deploying these predicted preferences in a navigation game (n=6), we found users more receptive to personalized XAI recommendations, enhancing trust in the system. This underscores the significance of customization in XAI interfaces, impacting user engagement and confidence.},
 author = {Zhaoxin Li and Sophie Yang and Shijie Wang},
 comment = {},
 doi = {},
 eprint = {2408.04778v1},
 journal = {arXiv preprint},
 title = {Exploring Personality-Driven Personalization in XAI: Enhancing User Trust in Gameplay},
 url = {http://arxiv.org/abs/2408.04778v1},
 year = {2024}
}

@article{2408.05248v1,
 abstract = {In recent years, the threat facing airports from growing and increasingly sophisticated cyberattacks has become evident. Airports are considered a strategic national asset, so protecting them from attacks, specifically cyberattacks, is a crucial mission. One way to increase airports' security is by using Digital Twins (DTs). This paper shows and demonstrates how DTs can enhance the security mission. The integration of DTs with Generative AI (GenAI) algorithms can lead to synergy and new frontiers in fighting cyberattacks. The paper exemplifies ways to model cyberattack scenarios using simulations and generate synthetic data for testing defenses. It also discusses how DTs can be used as a crucial tool for vulnerability assessment by identifying weaknesses, prioritizing, and accelerating remediations in case of cyberattacks. Moreover, the paper demonstrates approaches for anomaly detection and threat hunting using Machine Learning (ML) and GenAI algorithms. Additionally, the paper provides impact prediction and recovery coordination methods that can be used by DT operators and stakeholders. It also introduces ways to harness the human factor by integrating training and simulation algorithms with Explainable AI (XAI) into the DT platforms. Lastly, the paper offers future applications and technologies that can be utilized in DT environments.},
 author = {Abraham Itzhak Weinberg},
 comment = {},
 doi = {},
 eprint = {2408.05248v1},
 journal = {arXiv preprint},
 title = {The Role and Applications of Airport Digital Twin in Cyberattack Protection during the Generative AI Era},
 url = {http://arxiv.org/abs/2408.05248v1},
 year = {2024}
}

@article{2408.06509v1,
 abstract = {Explainable AI~(XAI) methods such as SHAP can help discover feature attributions in black-box models. If the method reveals a significant attribution from a ``protected feature'' (e.g., gender, race) on the model output, the model is considered unfair. However, adversarial attacks can subvert the detection of XAI methods. Previous approaches to constructing such an adversarial model require access to underlying data distribution, which may not be possible in many practical scenarios. We relax this constraint and propose a novel family of attacks, called shuffling attacks, that are data-agnostic. The proposed attack strategies can adapt any trained machine learning model to fool Shapley value-based explanations. We prove that Shapley values cannot detect shuffling attacks. However, algorithms that estimate Shapley values, such as linear SHAP and SHAP, can detect these attacks with varying degrees of effectiveness. We demonstrate the efficacy of the attack strategies by comparing the performance of linear SHAP and SHAP using real-world datasets.},
 author = {Jun Yuan and Aritra Dasgupta},
 comment = {},
 doi = {},
 eprint = {2408.06509v1},
 journal = {arXiv preprint},
 title = {Fooling SHAP with Output Shuffling Attacks},
 url = {http://arxiv.org/abs/2408.06509v1},
 year = {2024}
}

@article{2408.06679v1,
 abstract = {The explainability of black-box machine learning algorithms, commonly known as Explainable Artificial Intelligence (XAI), has become crucial for financial and other regulated industrial applications due to regulatory requirements and the need for transparency in business practices. Among the various paradigms of XAI, Explainable Case-Based Reasoning (XCBR) stands out as a pragmatic approach that elucidates the output of a model by referencing actual examples from the data used to train or test the model. Despite its potential, XCBR has been relatively underexplored for many algorithms such as tree-based models until recently. We start by observing that most XCBR methods are defined based on the distance metric learned by the algorithm. By utilizing a recently proposed technique to extract the distance metric learned by Random Forests (RFs), which is both geometry- and accuracy-preserving, we investigate various XCBR methods. These methods amount to identify special points from the training datasets, such as prototypes, critics, counter-factuals, and semi-factuals, to explain the predictions for a given query of the RF. We evaluate these special points using various evaluation metrics to assess their explanatory power and effectiveness.},
 author = {Gregory Yampolsky and Dhruv Desai and Mingshu Li and Stefano Pasquali and Dhagash Mehta},
 comment = {8 pages, 2 figures, 5 tables},
 doi = {},
 eprint = {2408.06679v1},
 journal = {arXiv preprint},
 title = {Case-based Explainability for Random Forest: Prototypes, Critics, Counter-factuals and Semi-factuals},
 url = {http://arxiv.org/abs/2408.06679v1},
 year = {2024}
}

@article{2408.06960v2,
 abstract = {The field of eXplainable Artificial Intelligence (XAI) is increasingly recognizing the need to personalize and/or interactively adapt the explanation to better reflect users' explanation needs. While dialogue-based approaches to XAI have been proposed recently, the state-of-the-art in XAI is still characterized by what we call one-shot, non-personalized and one-way explanations. In contrast, dialogue-based systems that can adapt explanations through interaction with a user promise to be superior to GUI-based or dashboard explanations as they offer a more intuitive way of requesting information. In general, while interactive XAI systems are often evaluated in terms of user satisfaction, there are limited studies that access user's objective model understanding. This is in particular the case for dialogue-based XAI approaches. In this paper, we close this gap by carrying out controlled experiments within a dialogue framework in which we measure understanding of users in three phases by asking them to simulate the predictions of the model they are learning about. By this, we can quantify the level of (improved) understanding w.r.t. how the model works, comparing the state prior, and after the interaction. We further analyze the data to reveal patterns of how the interaction between groups with high vs. low understanding gain differ. Overall, our work thus contributes to our understanding about the effectiveness of XAI approaches.},
 author = {Dimitry Mindlin and Amelie Sophie Robrecht and Michael Morasch and Philipp Cimiano},
 comment = {Accepted at the ECAI 2024 main conference - final version and code coming soon. 9 pages, 5 figures},
 doi = {},
 eprint = {2408.06960v2},
 journal = {arXiv preprint},
 title = {Measuring User Understanding in Dialogue-based XAI Systems},
 url = {http://arxiv.org/abs/2408.06960v2},
 year = {2024}
}

@article{2408.08041v1,
 abstract = {Unsupervised learning has become an essential building block of AI systems. The representations it produces, e.g. in foundation models, are critical to a wide variety of downstream applications. It is therefore important to carefully examine unsupervised models to ensure not only that they produce accurate predictions, but also that these predictions are not "right for the wrong reasons", the so-called Clever Hans (CH) effect. Using specially developed Explainable AI techniques, we show for the first time that CH effects are widespread in unsupervised learning. Our empirical findings are enriched by theoretical insights, which interestingly point to inductive biases in the unsupervised learning machine as a primary source of CH effects. Overall, our work sheds light on unexplored risks associated with practical applications of unsupervised learning and suggests ways to make unsupervised learning more robust.},
 author = {Jacob Kauffmann and Jonas Dippel and Lukas Ruff and Wojciech Samek and Klaus-Robert Müller and Grégoire Montavon},
 comment = {12 pages + supplement},
 doi = {10.1038/s42256-025-01000-2},
 eprint = {2408.08041v1},
 journal = {arXiv preprint},
 title = {The Clever Hans Effect in Unsupervised Learning},
 url = {http://arxiv.org/abs/2408.08041v1},
 year = {2024}
}

@article{2408.08214v1,
 abstract = {Federated Learning (FL) is a privacy-enhancing technology for distributed ML. By training models locally and aggregating updates - a federation learns together, while bypassing centralised data collection. FL is increasingly popular in healthcare, finance and personal computing. However, it inherits fairness challenges from classical ML and introduces new ones, resulting from differences in data quality, client participation, communication constraints, aggregation methods and underlying hardware. Fairness remains an unresolved issue in FL and the community has identified an absence of succinct definitions and metrics to quantify fairness; to address this, we propose Federated Fairness Analytics - a methodology for measuring fairness. Our definition of fairness comprises four notions with novel, corresponding metrics. They are symptomatically defined and leverage techniques originating from XAI, cooperative game-theory and networking engineering. We tested a range of experimental settings, varying the FL approach, ML task and data settings. The results show that statistical heterogeneity and client participation affect fairness and fairness conscious approaches such as Ditto and q-FedAvg marginally improve fairness-performance trade-offs. Using our techniques, FL practitioners can uncover previously unobtainable insights into their system's fairness, at differing levels of granularity in order to address fairness challenges in FL. We have open-sourced our work at: https://github.com/oscardilley/federated-fairness.},
 author = {Oscar Dilley and Juan Marcelo Parra-Ullauri and Rasheed Hussain and Dimitra Simeonidou},
 comment = {},
 doi = {},
 eprint = {2408.08214v1},
 journal = {arXiv preprint},
 title = {Federated Fairness Analytics: Quantifying Fairness in Federated Learning},
 url = {http://arxiv.org/abs/2408.08214v1},
 year = {2024}
}

@article{2408.09484v5,
 abstract = {Within the family of explainable machine-learning, we present Fredholm neural networks (Fredholm NNs): deep neural networks (DNNs) architectures motivated by fixed-point iteration schemes for the solution of linear and nonlinear Fredholm integral equations (FIEs) of the second kind. We also show how the proposed framework can be used for the solution of inverse problems. Applications of FIEs include the solution of ordinary, as well as partial differential equations (ODEs, PDEs) and many more. We first prove that Fredholm NNs provide accurate solutions. We then provide insight into the values of the hyperparameters and trainable/explainable weights and biases of the DNN, by directly connecting their values to the underlying mathematical theory. For our illustrations, we use Fredholm NNs to solve both linear and nonlinear problems, including elliptic PDEs and boundary value problems. We show that the proposed scheme achieves significant numerical approximation accuracy across both the domain and boundary. The proposed methodology provides insight into the connection between neural networks and classical numerical methods, and we posit that it can have applications in fields such as Uncertainty Quantification (UQ) and explainable artificial intelligence (XAI). Thus, we believe that it will trigger further advances in the intersection between scientific machine learning and numerical analysis.},
 author = {Kyriakos Georgiou and Constantinos Siettos and Athanasios N. Yannacopoulos},
 comment = {The final version of this preprint with updated examples can be found in the journal publication},
 doi = {10.1137/24M1686991},
 eprint = {2408.09484v5},
 journal = {arXiv preprint},
 title = {Fredholm Neural Networks},
 url = {http://arxiv.org/abs/2408.09484v5},
 year = {2024}
}

@article{2408.10085v1,
 abstract = {Existing local Explainable AI (XAI) methods, such as LIME, select a region of the input space in the vicinity of a given input instance, for which they approximate the behaviour of a model using a simpler and more interpretable surrogate model. The size of this region is often controlled by a user-defined locality hyperparameter. In this paper, we demonstrate the difficulties associated with defining a suitable locality size to capture impactful model behaviour, as well as the inadequacy of using a single locality size to explain all predictions. We propose a novel method, MASALA, for generating explanations, which automatically determines the appropriate local region of impactful model behaviour for each individual instance being explained. MASALA approximates the local behaviour used by a complex model to make a prediction by fitting a linear surrogate model to a set of points which experience similar model behaviour. These points are found by clustering the input space into regions of linear behavioural trends exhibited by the model. We compare the fidelity and consistency of explanations generated by our method with existing local XAI methods, namely LIME and CHILLI. Experiments on the PHM08 and MIDAS datasets show that our method produces more faithful and consistent explanations than existing methods, without the need to define any sensitive locality hyperparameters.},
 author = {Saif Anwar and Nathan Griffiths and Abhir Bhalerao and Thomas Popham},
 comment = {},
 doi = {},
 eprint = {2408.10085v1},
 journal = {arXiv preprint},
 title = {MASALA: Model-Agnostic Surrogate Explanations by Locality Adaptation},
 url = {http://arxiv.org/abs/2408.10085v1},
 year = {2024}
}

@article{2408.10633v1,
 abstract = {We propose an interactive methodology for generating counterfactual explanations for univariate time series data in classification tasks by leveraging 2D projections and decision boundary maps to tackle interpretability challenges. Our approach aims to enhance the transparency and understanding of deep learning models' decision processes. The application simplifies the time series data analysis by enabling users to interactively manipulate projected data points, providing intuitive insights through inverse projection techniques. By abstracting user interactions with the projected data points rather than the raw time series data, our method facilitates an intuitive generation of counterfactual explanations. This approach allows for a more straightforward exploration of univariate time series data, enabling users to manipulate data points to comprehend potential outcomes of hypothetical scenarios. We validate this method using the ECG5000 benchmark dataset, demonstrating significant improvements in interpretability and user understanding of time series classification. The results indicate a promising direction for enhancing explainable AI, with potential applications in various domains requiring transparent and interpretable deep learning models. Future work will explore the scalability of this method to multivariate time series data and its integration with other interpretability techniques.},
 author = {Udo Schlegel and Julius Rauscher and Daniel A. Keim},
 comment = {14 pages, 4 figures, accepted at XKDD @ ECML-PKDD},
 doi = {},
 eprint = {2408.10633v1},
 journal = {arXiv preprint},
 title = {Interactive Counterfactual Generation for Univariate Time Series},
 url = {http://arxiv.org/abs/2408.10633v1},
 year = {2024}
}

@article{2408.11401v1,
 abstract = {Prototypical parts networks, such as ProtoPNet, became popular due to their potential to produce more genuine explanations than post-hoc methods. However, for a long time, this potential has been strictly theoretical, and no systematic studies have existed to support it. That changed recently with the introduction of the FunnyBirds benchmark, which includes metrics for evaluating different aspects of explanations.
  However, this benchmark employs attribution maps visualization for all explanation techniques except for the ProtoPNet, for which the bounding boxes are used. This choice significantly influences the metric scores and questions the conclusions stated in FunnyBirds publication.
  In this study, we comprehensively compare metric scores obtained for two types of ProtoPNet visualizations: bounding boxes and similarity maps. Our analysis indicates that employing similarity maps aligns better with the essence of ProtoPNet, as evidenced by different metric scores obtained from FunnyBirds. Therefore, we advocate using similarity maps as a visualization technique for prototypical parts networks in explainability evaluation benchmarks.},
 author = {Szymon Opłatek and Dawid Rymarczyk and Bartosz Zieliński},
 comment = {Published at 2nd XAI World Conference},
 doi = {},
 eprint = {2408.11401v1},
 journal = {arXiv preprint},
 title = {Revisiting FunnyBirds evaluation framework for prototypical parts networks},
 url = {http://arxiv.org/abs/2408.11401v1},
 year = {2024}
}

@article{2408.12304v1,
 abstract = {The growing interest in Explainable Artificial Intelligence (XAI) motivates promising studies of computing optimal Interpretable Machine Learning models, especially decision trees. Such models generally provide optimality in compact size or empirical accuracy. Recent works focus on improving efficiency due to the natural scalability issue. The application of such models to practical problems is quite limited. As an emerging problem in circuit design, Approximate Logic Synthesis (ALS) aims to reduce circuit complexity by sacrificing correctness. Recently, multiple heuristic machine learning methods have been applied in ALS, which learns approximated circuits from samples of input-output pairs.
  In this paper, we propose a new ALS methodology realizing the approximation via learning optimal decision trees in empirical accuracy. Compared to previous heuristic ALS methods, the guarantee of optimality achieves a more controllable trade-off between circuit complexity and accuracy. Experimental results show clear improvements in our methodology in the quality of approximated designs (circuit complexity and accuracy) compared to the state-of-the-art approaches.},
 author = {Hao Hu and Shaowei Cai},
 comment = {},
 doi = {},
 eprint = {2408.12304v1},
 journal = {arXiv preprint},
 title = {OPTDTALS: Approximate Logic Synthesis via Optimal Decision Trees Approach},
 url = {http://arxiv.org/abs/2408.12304v1},
 year = {2024}
}

@article{2408.12387v1,
 abstract = {Deep learning-based face recognition (FR) systems pose significant privacy risks by tracking users without their consent. While adversarial attacks can protect privacy, they often produce visible artifacts compromising user experience. To mitigate this issue, recent facial privacy protection approaches advocate embedding adversarial noise into the natural looking makeup styles. However, these methods require training on large-scale makeup datasets that are not always readily available. In addition, these approaches also suffer from dataset bias. For instance, training on makeup data that predominantly contains female faces could compromise protection efficacy for male faces. To handle these issues, we propose a test-time optimization approach that solely optimizes an untrained neural network to transfer makeup style from a reference to a source image in an adversarial manner. We introduce two key modules: a correspondence module that aligns regions between reference and source images in latent space, and a decoder with conditional makeup layers. The untrained decoder, optimized via carefully designed structural and makeup consistency losses, generates a protected image that resembles the source but incorporates adversarial makeup to deceive FR models. As our approach does not rely on training with makeup face datasets, it avoids potential male/female dataset biases while providing effective protection. We further extend the proposed approach to videos by leveraging on temporal correlations. Experiments on benchmark datasets demonstrate superior performance in face verification and identification tasks and effectiveness against commercial FR systems. Our code and models will be available at https://github.com/fahadshamshad/deep-facial-privacy-prior},
 author = {Fahad Shamshad and Muzammal Naseer and Karthik Nandakumar},
 comment = {Proceedings of ECCV Workshop on Explainable AI for Biometrics, 2024},
 doi = {},
 eprint = {2408.12387v1},
 journal = {arXiv preprint},
 title = {Makeup-Guided Facial Privacy Protection via Untrained Neural Network Priors},
 url = {http://arxiv.org/abs/2408.12387v1},
 year = {2024}
}

@article{2408.12420v1,
 abstract = {We often use "explainable" Artificial Intelligence (XAI)" and "interpretable AI (IAI)" interchangeably when we apply various XAI tools for a given dataset to explain the reasons that underpin machine learning (ML) outputs. However, these notions can sometimes be confusing because interpretation often has a subjective connotation, while explanations lean towards objective facts. We argue that XAI is a subset of IAI. The concept of IAI is beyond the sphere of a dataset. It includes the domain of a mindset. At the core of this ambiguity is the duality of reasons, in which we can reason either outwards or inwards. When directed outwards, we want the reasons to make sense through the laws of nature. When turned inwards, we want the reasons to be happy, guided by the laws of the heart. While XAI and IAI share reason as the common notion for the goal of transparency, clarity, fairness, reliability, and accountability in the context of ethical AI and trustworthy AI (TAI), their differences lie in that XAI emphasizes the post-hoc analysis of a dataset, and IAI requires a priori mindset of abstraction. This hypothesis can be proved by empirical experiments based on an open dataset and harnessed by High-Performance Computing (HPC). The demarcation of XAI and IAI is indispensable because it would be impossible to determine regulatory policies for many AI applications, especially in healthcare, human resources, banking, and finance. We aim to clarify these notions and lay the foundation of XAI, IAI, EAI, and TAI for many practitioners and policymakers in future AI applications and research.},
 author = {Caesar Wu and Rajkumar Buyya and Yuan Fang Li and Pascal Bouvry},
 comment = {},
 doi = {},
 eprint = {2408.12420v1},
 journal = {arXiv preprint},
 title = {Dataset | Mindset = Explainable AI | Interpretable AI},
 url = {http://arxiv.org/abs/2408.12420v1},
 year = {2024}
}

@article{2408.12426v1,
 abstract = {The increasing popularity of Artificial Intelligence in recent years has led to a surge in interest in image classification, especially in the agricultural sector. With the help of Computer Vision, Machine Learning, and Deep Learning, the sector has undergone a significant transformation, leading to the development of new techniques for crop classification in the field. Despite the extensive research on various image classification techniques, most have limitations such as low accuracy, limited use of data, and a lack of reporting model size and prediction. The most significant limitation of all is the need for model explainability. This research evaluates four different approaches for crop classification, namely traditional ML with handcrafted feature extraction methods like SIFT, ORB, and Color Histogram; Custom Designed CNN and established DL architecture like AlexNet; transfer learning on five models pre-trained using ImageNet such as EfficientNetV2, ResNet152V2, Xception, Inception-ResNetV2, MobileNetV3; and cutting-edge foundation models like YOLOv8 and DINOv2, a self-supervised Vision Transformer Model. All models performed well, but Xception outperformed all of them in terms of generalization, achieving 98% accuracy on the test data, with a model size of 80.03 MB and a prediction time of 0.0633 seconds. A key aspect of this research was the application of Explainable AI to provide the explainability of all the models. This journal presents the explainability of Xception model with LIME, SHAP, and GradCAM, ensuring transparency and trustworthiness in the models' predictions. This study highlights the importance of selecting the right model according to task-specific needs. It also underscores the important role of explainability in deploying AI in agriculture, providing insightful information to help enhance AI-driven crop management strategies.},
 author = {Sudi Murindanyi and Joyce Nakatumba-Nabende and Rahman Sanya and Rose Nakibuule and Andrew Katumba},
 comment = {},
 doi = {},
 eprint = {2408.12426v1},
 journal = {arXiv preprint},
 title = {Enhanced Infield Agriculture with Interpretable Machine Learning Approaches for Crop Classification},
 url = {http://arxiv.org/abs/2408.12426v1},
 year = {2024}
}

@article{2408.12568v2,
 abstract = {To solve ever more complex problems, Deep Neural Networks are scaled to billions of parameters, leading to huge computational costs. An effective approach to reduce computational requirements and increase efficiency is to prune unnecessary components of these often over-parameterized networks. Previous work has shown that attribution methods from the field of eXplainable AI serve as effective means to extract and prune the least relevant network components in a few-shot fashion. We extend the current state by proposing to explicitly optimize hyperparameters of attribution methods for the task of pruning, and further include transformer-based networks in our analysis. Our approach yields higher model compression rates of large transformer- and convolutional architectures (VGG, ResNet, ViT) compared to previous works, while still attaining high performance on ImageNet classification tasks. Here, our experiments indicate that transformers have a higher degree of over-parameterization compared to convolutional neural networks. Code is available at https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.},
 author = {Sayed Mohammad Vakilzadeh Hatefi and Maximilian Dreyer and Reduan Achtibat and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin},
 comment = {Accepted as a workshop paper at ECCV 2024, 26 pages (11 pages manuscript, 3 pages references, 12 pages appendix)},
 doi = {},
 eprint = {2408.12568v2},
 journal = {arXiv preprint},
 title = {Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers},
 url = {http://arxiv.org/abs/2408.12568v2},
 year = {2024}
}

@article{2408.12808v1,
 abstract = {Deep Neural Networks (DNNs) have revolutionized various fields by enabling task automation and reducing human error. However, their internal workings and decision-making processes remain obscure due to their black box nature. Consequently, the lack of interpretability limits the application of these models in high-risk scenarios. To address this issue, the emerging field of eXplainable Artificial Intelligence (XAI) aims to explain and interpret the inner workings of DNNs. Despite advancements, XAI faces challenges such as the semantic gap between machine and human understanding, the trade-off between interpretability and performance, and the need for context-specific explanations. To overcome these limitations, we propose a novel multimodal framework named VALE Visual and Language Explanation. VALE integrates explainable AI techniques with advanced language models to provide comprehensive explanations. This framework utilizes visual explanations from XAI tools, an advanced zero-shot image segmentation model, and a visual language model to generate corresponding textual explanations. By combining visual and textual explanations, VALE bridges the semantic gap between machine outputs and human interpretation, delivering results that are more comprehensible to users. In this paper, we conduct a pilot study of the VALE framework for image classification tasks. Specifically, Shapley Additive Explanations (SHAP) are used to identify the most influential regions in classified images. The object of interest is then extracted using the Segment Anything Model (SAM), and explanations are generated using state-of-the-art pre-trained Vision-Language Models (VLMs). Extensive experimental studies are performed on two datasets: the ImageNet dataset and a custom underwater SONAR image dataset, demonstrating VALEs real-world applicability in underwater image classification.},
 author = {Purushothaman Natarajan and Athira Nambiar},
 comment = {15 pages, 10 tables, 3 figures},
 doi = {},
 eprint = {2408.12808v1},
 journal = {arXiv preprint},
 title = {VALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models},
 url = {http://arxiv.org/abs/2408.12808v1},
 year = {2024}
}

@article{2408.12837v2,
 abstract = {Deep learning techniques have revolutionized image classification by mimicking human cognition and automating complex decision-making processes. However, the deployment of AI systems in the wild, especially in high-security domains such as defence, is curbed by the lack of explainability of the model. To this end, eXplainable AI (XAI) is an emerging area of research that is intended to explore the unexplained hidden black box nature of deep neural networks. This paper explores the application of the eXplainable Artificial Intelligence (XAI) tool to interpret the underwater image classification results, one of the first works in the domain to the best of our knowledge. Our study delves into the realm of SONAR image classification using a custom dataset derived from diverse sources, including the Seabed Objects KLSG dataset, the camera SONAR dataset, the mine SONAR images dataset, and the SCTD dataset. An extensive analysis of transfer learning techniques for image classification using benchmark Convolutional Neural Network (CNN) architectures such as VGG16, ResNet50, InceptionV3, DenseNet121, etc. is carried out. On top of this classification model, a post-hoc XAI technique, viz. Local Interpretable Model-Agnostic Explanations (LIME) are incorporated to provide transparent justifications for the model's decisions by perturbing input data locally to see how predictions change. Furthermore, Submodular Picks LIME (SP-LIME) a version of LIME particular to images, that perturbs the image based on the submodular picks is also extensively studied. To this end, two submodular optimization algorithms i.e. Quickshift and Simple Linear Iterative Clustering (SLIC) are leveraged towards submodular picks. The extensive analysis of XAI techniques highlights interpretability of the results in a more human-compliant way, thus boosting our confidence and reliability.},
 author = {Purushothaman Natarajan and Athira Nambiar},
 comment = {55 pages, 9 tables, 18 figures},
 doi = {},
 eprint = {2408.12837v2},
 journal = {arXiv preprint},
 title = {Underwater SONAR Image Classification and Analysis using LIME-based Explainable Artificial Intelligence},
 url = {http://arxiv.org/abs/2408.12837v2},
 year = {2024}
}

@article{2408.13397v1,
 abstract = {The inherent "black box" nature of deep neural networks (DNNs) compromises their transparency and reliability. Recently, explainable AI (XAI) has garnered increasing attention from researchers. Several perturbation-based interpretations have emerged. However, these methods often fail to adequately consider feature dependencies. To solve this problem, we introduce a perturbation-based interpretation guided by feature coalitions, which leverages deep information of network to extract correlated features. Then, we proposed a carefully-designed consistency loss to guide network interpretation. Both quantitative and qualitative experiments are conducted to validate the effectiveness of our proposed method. Code is available at github.com/Teriri1999/Perturebation-on-Feature-Coalition.},
 author = {Xuran Hu and Mingzhe Zhu and Zhenpeng Feng and Miloš Daković and Ljubiša Stanković},
 comment = {4 pages, 4 figures, 2 tables},
 doi = {},
 eprint = {2408.13397v1},
 journal = {arXiv preprint},
 title = {Perturbation on Feature Coalition: Towards Interpretable Deep Neural Networks},
 url = {http://arxiv.org/abs/2408.13397v1},
 year = {2024}
}

@article{2408.13438v3,
 abstract = {Understanding the inner representation of a neural network helps users improve models. Concept-based methods have become a popular choice for explaining deep neural networks post-hoc because, unlike most other explainable AI techniques, they can be used to test high-level visual "concepts" that are not directly related to feature attributes. For instance, the concept of "stripes" is important to classify an image as a zebra. Concept-based explanation methods, however, require practitioners to guess and manually collect multiple candidate concept image sets, making the process labor-intensive and prone to overlooking important concepts. Addressing this limitation, in this paper, we frame concept image set creation as an image generation problem. However, since naively using a standard generative model does not result in meaningful concepts, we devise a reinforcement learning-based preference optimization (RLPO) algorithm that fine-tunes a vision-language generative model from approximate textual descriptions of concepts. Through a series of experiments, we demonstrate our method's ability to efficiently and reliably articulate diverse concepts that are otherwise challenging to craft manually.},
 author = {Aditya Taparia and Som Sagar and Ransalu Senanayake},
 comment = {28 pages, 31 figures},
 doi = {},
 eprint = {2408.13438v3},
 journal = {arXiv preprint},
 title = {Explainable Concept Generation through Vision-Language Preference Learning for Understanding Neural Networks' Internal Representations},
 url = {http://arxiv.org/abs/2408.13438v3},
 year = {2024}
}

@article{2408.13626v1,
 abstract = {We explore deep generative models to generate case-based explanations in a medical federated learning setting. Explaining AI model decisions through case-based interpretability is paramount to increasing trust and allowing widespread adoption of AI in clinical practice. However, medical AI training paradigms are shifting towards federated learning settings in order to comply with data protection regulations. In a federated scenario, past data is inaccessible to the current user. Thus, we use a deep generative model to generate synthetic examples that protect privacy and explain decisions. Our proof-of-concept focuses on pleural effusion diagnosis and uses publicly available Chest X-ray data.},
 author = {Laura Latorre and Liliana Petrychenko and Regina Beets-Tan and Taisiya Kopytova and Wilson Silva},
 comment = {\c{opyright} 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works},
 doi = {},
 eprint = {2408.13626v1},
 journal = {arXiv preprint},
 title = {Towards Case-based Interpretability for Medical Federated Learning},
 url = {http://arxiv.org/abs/2408.13626v1},
 year = {2024}
}

@article{2408.14552v1,
 abstract = {AI prevails in financial fraud detection and decision making. Yet, due to concerns about biased automated decision making or profiling, regulations mandate that final decisions are made by humans. Financial fraud investigators face the challenge of manually synthesizing vast amounts of unstructured information, including AI alerts, transaction histories, social media insights, and governmental laws. Current Visual Analytics (VA) systems primarily support isolated aspects of this process, such as explaining binary AI alerts and visualizing transaction patterns, thus adding yet another layer of information to the overall complexity. In this work, we propose a framework where the VA system supports decision makers throughout all stages of financial fraud investigation, including data collection, information synthesis, and human criteria iteration. We illustrate how VA can claim a central role in AI-aided decision making, ensuring that human judgment remains in control while minimizing potential biases and labor-intensive tasks.},
 author = {Angelos Chatzimparmpas and Evanthia Dimara},
 comment = {Accepted poster at IEEE VIS '24, Florida, USA, 13-18 October, 2024},
 doi = {},
 eprint = {2408.14552v1},
 journal = {arXiv preprint},
 title = {Aiding Humans in Financial Fraud Decision Making: Toward an XAI-Visualization Framework},
 url = {http://arxiv.org/abs/2408.14552v1},
 year = {2024}
}

@article{2408.15073v1,
 abstract = {The field of Explainable Artificial Intelligence (XAI) for Deep Neural Network models has developed significantly, offering numerous techniques to extract explanations from models. However, evaluating explanations is often not trivial, and differences in applied metrics can be subtle, especially with non-intelligible data. Thus, there is a need for visualizations tailored to explore explanations for domains with such data, e.g., time series. We propose DAVOTS, an interactive visual analytics approach to explore raw time series data, activations of neural networks, and attributions in a dense-pixel visualization to gain insights into the data, models' decisions, and explanations. To further support users in exploring large datasets, we apply clustering approaches to the visualized data domains to highlight groups and present ordering strategies for individual and combined data exploration to facilitate finding patterns. We visualize a CNN trained on the FordA dataset to demonstrate the approach.},
 author = {Udo Schlegel and Daniel A. Keim},
 comment = {5 pages, 2 figures, accepted at MLVIS 2023},
 doi = {},
 eprint = {2408.15073v1},
 journal = {arXiv preprint},
 title = {Interactive dense pixel visualizations for time series and model attribution explanations},
 url = {http://arxiv.org/abs/2408.15073v1},
 year = {2024}
}

@article{2408.15133v1,
 abstract = {Causality is vital for understanding true cause-and-effect relationships between variables within predictive models, rather than relying on mere correlations, making it highly relevant in the field of Explainable AI. In an automated decision-making scenario, causal inference methods can analyze the underlying data-generation process, enabling explanations of a model's decision by manipulating features and creating counterfactual examples. These counterfactuals explore hypothetical scenarios where a minimal number of factors are altered, providing end-users with valuable information on how to change their situation. However, interpreting a set of multiple counterfactuals can be challenging for end-users who are not used to analyzing raw data records. In our work, we propose a novel multi-step pipeline that uses counterfactuals to generate natural language explanations of actions that will lead to a change in outcome in classifiers of tabular data using LLMs. This pipeline is designed to guide the LLM through smaller tasks that mimic human reasoning when explaining a decision based on counterfactual cases. We conducted various experiments using a public dataset and proposed a method of closed-loop evaluation to assess the coherence of the final explanation with the counterfactuals, as well as the quality of the content. Results are promising, although further experiments with other datasets and human evaluations should be carried out.},
 author = {Arturo Fredes and Jordi Vitria},
 comment = {Presented as a poster in the 2nd Workshop on Causal Inference and Machine Learning in Practice at KDD 2024},
 doi = {},
 eprint = {2408.15133v1},
 journal = {arXiv preprint},
 title = {Using LLMs for Explaining Sets of Counterfactual Examples to Final Users},
 url = {http://arxiv.org/abs/2408.15133v1},
 year = {2024}
}

@article{2408.17198v2,
 abstract = {Explainable Artificial Intelligence (XAI) plays a crucial role in fostering transparency and trust in AI systems, where traditional XAI approaches typically offer one level of abstraction for explanations, often in the form of heatmaps highlighting single or multiple input features. However, we ask whether abstract reasoning or problem-solving strategies of a model may also be relevant, as these align more closely with how humans approach solutions to problems. We propose a framework, called Symbolic XAI, that attributes relevance to symbolic queries expressing logical relationships between input features, thereby capturing the abstract reasoning behind a model's predictions. The methodology is built upon a simple yet general multi-order decomposition of model predictions. This decomposition can be specified using higher-order propagation-based relevance methods, such as GNN-LRP, or perturbation-based explanation methods commonly used in XAI. The effectiveness of our framework is demonstrated in the domains of natural language processing (NLP), vision, and quantum chemistry (QC), where abstract symbolic domain knowledge is abundant and of significant interest to users. The Symbolic XAI framework provides an understanding of the model's decision-making process that is both flexible for customization by the user and human-readable through logical formulas.},
 author = {Thomas Schnake and Farnoush Rezaei Jafari and Jonas Lederer and Ping Xiong and Shinichi Nakajima and Stefan Gugler and Grégoire Montavon and Klaus-Robert Müller},
 comment = {},
 doi = {},
 eprint = {2408.17198v2},
 journal = {arXiv preprint},
 title = {Towards Symbolic XAI -- Explanation Through Human Understandable Logical Relationships Between Features},
 url = {http://arxiv.org/abs/2408.17198v2},
 year = {2024}
}

@article{2408.17322v1,
 abstract = {The use of transformer-based models is growing rapidly throughout society. With this growth, it is important to understand how they work, and in particular, how the attention mechanisms represent concepts. Though there are many interpretability methods, many look at models through their neuronal activations, which are poorly understood. We describe different lenses through which to view neuron activations, and investigate the effectiveness in language models and vision transformers through various methods of neural ablation: zero ablation, mean ablation, activation resampling, and a novel approach we term 'peak ablation'. Through experimental analysis, we find that in different regimes and models, each method can offer the lowest degradation of model performance compared to other methods, with resampling usually causing the most significant performance deterioration. We make our code available at https://github.com/nickypro/investigating-ablation.},
 author = {Nicholas Pochinkov and Ben Pasero and Skylar Shibayama},
 comment = {9 pages, 2 figures, XAI World Conference 2024 Late-Breaking Work},
 doi = {},
 eprint = {2408.17322v1},
 journal = {arXiv preprint},
 title = {Investigating Neuron Ablation in Attention Heads: The Case for Peak Activation Centering},
 url = {http://arxiv.org/abs/2408.17322v1},
 year = {2024}
}

@article{2409.00001v1,
 abstract = {Early detection of Cerebral Palsy (CP) is crucial for effective intervention and monitoring. This paper tests the reliability and applicability of Explainable AI (XAI) methods using a deep learning method that predicts CP by analyzing skeletal data extracted from video recordings of infant movements. Specifically, we use XAI evaluation metrics -- namely faithfulness and stability -- to quantitatively assess the reliability of Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) in this specific medical application. We utilize a unique dataset of infant movements and apply skeleton data perturbations without distorting the original dynamics of the infant movements. Our CP prediction model utilizes an ensemble approach, so we evaluate the XAI metrics performances for both the overall ensemble and the individual models. Our findings indicate that both XAI methods effectively identify key body points influencing CP predictions and that the explanations are robust against minor data perturbations. Grad-CAM significantly outperforms CAM in the RISv metric, which measures stability in terms of velocity. In contrast, CAM performs better in the RISb metric, which relates to bone stability, and the RRS metric, which assesses internal representation robustness. Individual models within the ensemble show varied results, and neither CAM nor Grad-CAM consistently outperform the other, with the ensemble approach providing a representation of outcomes from its constituent models.},
 author = {Kimji N. Pellano and Inga Strümke and Daniel Groos and Lars Adde and Espen Alexander F. Ihlen},
 comment = {},
 doi = {},
 eprint = {2409.00001v1},
 journal = {arXiv preprint},
 title = {Evaluating Explainable AI Methods in Deep Learning Models for Early Detection of Cerebral Palsy},
 url = {http://arxiv.org/abs/2409.00001v1},
 year = {2024}
}

@article{2409.00265v2,
 abstract = {Artificial intelligence models encounter significant challenges due to their black-box nature, particularly in safety-critical domains such as healthcare, finance, and autonomous vehicles. Explainable Artificial Intelligence (XAI) addresses these challenges by providing explanations for how these models make decisions and predictions, ensuring transparency, accountability, and fairness. Existing studies have examined the fundamental concepts of XAI, its general principles, and the scope of XAI techniques. However, there remains a gap in the literature as there are no comprehensive reviews that delve into the detailed mathematical representations, design methodologies of XAI models, and other associated aspects. This paper provides a comprehensive literature review encompassing common terminologies and definitions, the need for XAI, beneficiaries of XAI, a taxonomy of XAI methods, and the application of XAI methods in different application areas. The survey is aimed at XAI researchers, XAI practitioners, AI model developers, and XAI beneficiaries who are interested in enhancing the trustworthiness, transparency, accountability, and fairness of their AI models.},
 author = {Melkamu Mersha and Khang Lam and Joseph Wood and Ali AlShami and Jugal Kalita},
 comment = {},
 doi = {10.1016/j.neucom.2024.128111},
 eprint = {2409.00265v2},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence: A Survey of Needs, Techniques, Applications, and Future Direction},
 url = {http://arxiv.org/abs/2409.00265v2},
 year = {2024}
}

@article{2409.00438v1,
 abstract = {In the fast-paced and volatile financial markets, accurately predicting stock movements based on financial news is critical for investors and analysts. Traditional models often struggle to capture the intricate and dynamic relationships between news events and market reactions, limiting their ability to provide actionable insights. This paper introduces a novel approach leveraging Explainable Artificial Intelligence (XAI) through the development of a Geometric Hypergraph Attention Network (GHAN) to analyze the impact of financial news on market behaviours. Geometric hypergraphs extend traditional graph structures by allowing edges to connect multiple nodes, effectively modelling high-order relationships and interactions among financial entities and news events. This unique capability enables the capture of complex dependencies, such as the simultaneous impact of a single news event on multiple stocks or sectors, which traditional models frequently overlook.
  By incorporating attention mechanisms within hypergraphs, GHAN enhances the model's ability to focus on the most relevant information, ensuring more accurate predictions and better interpretability. Additionally, we employ BERT-based embeddings to capture the semantic richness of financial news texts, providing a nuanced understanding of the content. Using a comprehensive financial news dataset, our GHAN model addresses key challenges in financial news impact analysis, including the complexity of high-order interactions, the necessity for model interpretability, and the dynamic nature of financial markets. Integrating attention mechanisms and SHAP values within GHAN ensures transparency, highlighting the most influential factors driving market predictions.
  Empirical validation demonstrates the superior effectiveness of our approach over traditional sentiment analysis and time-series models.},
 author = {Anoushka Harit and Zhongtian Sun and Jongmin Yu and Noura Al Moubayed},
 comment = {16 pages, conference},
 doi = {},
 eprint = {2409.00438v1},
 journal = {arXiv preprint},
 title = {Breaking Down Financial News Impact: A Novel AI Approach with Geometric Hypergraphs},
 url = {http://arxiv.org/abs/2409.00438v1},
 year = {2024}
}

@article{2409.01354v3,
 abstract = {Human understandable explanation of deep learning models is essential for various critical and sensitive applications. Unlike image or tabular data where the importance of each input feature (for the classifier's decision) can be directly projected into the input, time series distinguishable features (e.g. dominant frequency) are often hard to manifest in time domain for a user to easily understand. Additionally, most explanation methods require a baseline value as an indication of the absence of any feature. However, the notion of lack of feature, which is often defined as black pixels for vision tasks or zero/mean values for tabular data, is not well-defined in time series. Despite the adoption of explainable AI methods (XAI) from tabular and vision domain into time series domain, these differences limit the application of these XAI methods in practice. In this paper, we propose a simple yet effective method that allows a model originally trained on the time domain to be interpreted in other explanation spaces using existing methods. We suggest five explanation spaces, each of which can potentially alleviate these issues in certain types of time series. Our method can be easily integrated into existing platforms without any changes to trained models or XAI methods. The code will be released upon acceptance.},
 author = {Shahbaz Rezaei and Xin Liu},
 comment = {},
 doi = {},
 eprint = {2409.01354v3},
 journal = {arXiv preprint},
 title = {Explanation Space: A New Perspective into Time Series Interpretability},
 url = {http://arxiv.org/abs/2409.01354v3},
 year = {2024}
}

@article{2409.01713v2,
 abstract = {Outlier detection is a crucial analytical tool in various fields. In critical systems like manufacturing, malfunctioning outlier detection can be costly and safety-critical. Therefore, there is a significant need for explainable artificial intelligence (XAI) when deploying opaque models in such environments. This study focuses on manufacturing time series data from a German automotive supply industry. We utilize autoencoders to compress the entire time series and then apply anomaly detection techniques to its latent features. For outlier interpretation, we (i) adopt widely used XAI techniques to the autoencoder's encoder. Additionally, (ii) we propose AEE, Aggregated Explanatory Ensemble, a novel approach that fuses explanations of multiple XAI techniques into a single, more expressive interpretation. For evaluation of explanations, (iii) we propose a technique to measure the quality of encoder explanations quantitatively. Furthermore, we qualitatively assess the effectiveness of outlier explanations with domain expertise.},
 author = {Patrick Knab and Sascha Marton and Christian Bartelt and Robert Fuder},
 comment = {14 pages, 8 figures, accepted at TempXAI @ ECML-PKDD, published in CEUR Workshop Proceedings, Vol. 3761. https://ceur-ws.org/Vol-3761/paper3.pdf},
 doi = {},
 eprint = {2409.01713v2},
 journal = {arXiv preprint},
 title = {Interpreting Outliers in Time Series Data through Decoding Autoencoder},
 url = {http://arxiv.org/abs/2409.01713v2},
 year = {2024}
}

@article{2409.01731v3,
 abstract = {Mutagenicity is a concern due to its association with genetic mutations which can result in a variety of negative consequences, including the development of cancer. Earlier identification of mutagenic compounds in the drug development process is therefore crucial for preventing the progression of unsafe candidates and reducing development costs. While computational techniques, especially machine learning models have become increasingly prevalent for this endpoint, they rely on a single modality. In this work, we introduce a novel stacked ensemble based mutagenicity prediction model which incorporate multiple modalities such as simplified molecular input line entry system (SMILES) and molecular graph. These modalities capture diverse information about molecules such as substructural, physicochemical, geometrical and topological. To derive substructural, geometrical and physicochemical information, we use SMILES, while topological information is extracted through a graph attention network (GAT) via molecular graph. Our model uses a stacked ensemble of machine learning classifiers to make predictions using these multiple features. We employ the explainable artificial intelligence (XAI) technique SHAP (Shapley Additive Explanations) to determine the significance of each classifier and the most relevant features in the prediction. We demonstrate that our method surpasses SOTA methods on two standard datasets across various metrics. Notably, we achieve an area under the curve of 95.21\% on the Hansen benchmark dataset, affirming the efficacy of our method in predicting mutagenicity. We believe that this research will captivate the interest of both clinicians and computational biologists engaged in translational research.},
 author = {Tanya Liyaqat and Tanvir Ahmad and Mohammad Kashif and Chandni Saxena},
 comment = {Submitted to a journal},
 doi = {},
 eprint = {2409.01731v3},
 journal = {arXiv preprint},
 title = {Stacked ensemble\-based mutagenicity prediction model using multiple modalities with graph attention network},
 url = {http://arxiv.org/abs/2409.01731v3},
 year = {2024}
}

@article{2409.03148v2,
 abstract = {Cycling has gained global popularity for its health benefits and positive urban impacts. To effectively promote cycling, early studies have extensively investigated the relationship between cycling behaviors and environmental factors, especially cyclists' preferences when making route decisions. However, these studies often struggle to comprehensively describe detailed cycling procedures at a large scale due to data limitations, and they tend to overlook the complex nature of cyclists' preferences. To address these issues, we propose a novel framework aimed to quantify and interpret cyclists' complicated visual preferences by leveraging maximum entropy deep inverse reinforcement learning(MEDIRL)and explainable artificial intelligence(XAI). Implemented in Bantian Sub-district, Shenzhen, we adapt MEDIRL model for efficient estimation of cycling reward function by integrating dockless-bike-sharing(DBS) trajectory and street view images(SVIs), which serves as a representation of cyclists' preferences for street visual environments during routing. In addition, we demonstrate the feasibility and reliability of MEDIRL in discovering cyclists' visual preferences. We find that cyclists focus on specific street visual elements when making route decisions, which can be summarized as their attention to safety, street enclosure, and cycling comfort. Further analysis reveals the complex nonlinear effects of street visual elements on cyclists' preferences, offering a cost-effective perspective on streetscapes design. Our proposed framework advances the understanding of individual cycling behaviors and provides actionable insights for urban planners to design bicycle-friendly streetscapes that prioritize cyclists' preferences.},
 author = {Kezhou Ren and Meihan Jin and Huiming Liu and Yongxi Gong and Yu Liu},
 comment = {39 pages, 16 figures},
 doi = {},
 eprint = {2409.03148v2},
 journal = {arXiv preprint},
 title = {Discovering Cyclists' Visual Preferences Through Shared Bike Trajectories and Street View Images Using Inverse Reinforcement Learning},
 url = {http://arxiv.org/abs/2409.03148v2},
 year = {2024}
}

@article{2409.03772v1,
 abstract = {To date, several methods have been developed to explain deep learning algorithms for classification tasks. Recently, an adaptation of two of such methods has been proposed to generate instance-level explainable maps in a semantic segmentation scenario, such as multiple sclerosis (MS) lesion segmentation. In the mentioned work, a 3D U-Net was trained and tested for MS lesion segmentation, yielding an F1 score of 0.7006, and a positive predictive value (PPV) of 0.6265. The distribution of values in explainable maps exposed some differences between maps of true and false positive (TP/FP) examples. Inspired by those results, we explore in this paper the use of characteristics of lesion-specific saliency maps to refine segmentation and detection scores. We generate around 21000 maps from as many TP/FP lesions in a batch of 72 patients (training set) and 4868 from the 37 patients in the test set. 93 radiomic features extracted from the first set of maps were used to train a logistic regression model and classify TP versus FP. On the test set, F1 score and PPV were improved by a large margin when compared to the initial model, reaching 0.7450 and 0.7817, with 95% confidence intervals of [0.7358, 0.7547] and [0.7679, 0.7962], respectively. These results suggest that saliency maps can be used to refine prediction scores, boosting a model's performances.},
 author = {Federico Spagnolo and Nataliia Molchanova and Mario Ocampo Pineda and Lester Melie-Garcia and Meritxell Bach Cuadra and Cristina Granziera and Vincent Andrearczyk and Adrien Depeursinge},
 comment = {},
 doi = {},
 eprint = {2409.03772v1},
 journal = {arXiv preprint},
 title = {Exploiting XAI maps to improve MS lesion segmentation and detection in MRI},
 url = {http://arxiv.org/abs/2409.03772v1},
 year = {2024}
}

@article{2409.05918v1,
 abstract = {This study presents an explainable artificial intelligent (XAI) model for predicting pile driving vibrations in Bangkok's soft clay subsoil. A deep neural network was developed using a dataset of 1,018 real-world pile driving measurements, encompassing variations in pile dimensions, hammer characteristics, sensor locations, and vibration measurement axes. The model achieved a mean absolute error (MAE) of 0.276, outperforming traditional empirical methods and other machine learning approaches such as XGBoost and CatBoost. SHapley Additive exPlanations (SHAP) analysis was employed to interpret the model's predictions, revealing complex relationships between input features and peak particle velocity (PPV). Distance from the pile driving location emerged as the most influential factor, followed by hammer weight and pile size. Non-linear relationships and threshold effects were observed, providing new insights into vibration propagation in soft clay. A web-based application was developed to facilitate adoption by practicing engineers, bridging the gap between advanced machine learning techniques and practical engineering applications. This research contributes to the field of geotechnical engineering by offering a more accurate and nuanced approach to predicting pile driving vibrations, with implications for optimizing construction practices and mitigating environmental impacts in urban areas. The model and its source code are publicly available, promoting transparency and reproducibility in geotechnical research.},
 author = {Sompote Youwai and Anuwat Pamungmoon},
 comment = {},
 doi = {},
 eprint = {2409.05918v1},
 journal = {arXiv preprint},
 title = {Developing an Explainable Artificial Intelligent (XAI) Model for Predicting Pile Driving Vibrations in Bangkok's Subsoil},
 url = {http://arxiv.org/abs/2409.05918v1},
 year = {2024}
}

@article{2409.07347v3,
 abstract = {The complex nature of disease mechanisms and the variability of patient symptoms pose significant challenges in developing effective diagnostic tools. Although machine learning (ML) has made substantial advances in medical diagnosis, the decision-making processes of these models often lack transparency, potentially jeopardizing patient outcomes. This review aims to highlight the role of Explainable AI (XAI) in addressing the interpretability issues of ML models in healthcare, with a focus on chronic conditions such as Parkinson's, stroke, depression, cancer, heart disease, and Alzheimer's disease. A comprehensive literature search was conducted across multiple databases to identify studies that applied XAI techniques in healthcare. The search focused on XAI algorithms used in diagnosing and monitoring chronic diseases. The review identified the application of nine trending XAI algorithms, each evaluated for their advantages and limitations in various healthcare contexts. The findings underscore the importance of transparency in ML models, which is crucial for improving trust and outcomes in clinical practice. While XAI provides significant potential to bridge the gap between complex ML models and clinical practice, challenges such as scalability, validation, and clinician acceptance remain. The review also highlights areas requiring further research, particularly in integrating XAI into healthcare systems. The study concludes that XAI methods offer a promising path forward for enhancing human health monitoring and patient care, though significant challenges must be addressed to fully realize their potential in clinical settings.},
 author = {Abdullah Alharthi and Ahmed Alqurashi and Turki Alharbi and Mohammed Alammar and Nasser Aldosari and Houssem Bouchekara and Yusuf Shaaban and Mohammad Shoaib Shahriar and Abdulrahman Al Ayidh},
 comment = {},
 doi = {},
 eprint = {2409.07347v3},
 journal = {arXiv preprint},
 title = {The Role of Explainable AI in Revolutionizing Human Health Monitoring: A Review},
 url = {http://arxiv.org/abs/2409.07347v3},
 year = {2024}
}

@article{2409.08027v2,
 abstract = {Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students. In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation). We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses. Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation. We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields.},
 author = {Vinitra Swamy and Davide Romano and Bhargav Srinivasa Desikan and Oana-Maria Camburu and Tanja Käser},
 comment = {Accepted at AAAI 2025},
 doi = {},
 eprint = {2409.08027v2},
 journal = {arXiv preprint},
 title = {iLLuMinaTE: An LLM-XAI Framework Leveraging Social Science Explanation Theories Towards Actionable Student Performance Feedback},
 url = {http://arxiv.org/abs/2409.08027v2},
 year = {2024}
}

@article{2409.08919v1,
 abstract = {Despite its significant benefits in enhancing the transparency and trustworthiness of artificial intelligence (AI) systems, explainable AI (XAI) has yet to reach its full potential in real-world applications. One key challenge is that XAI can unintentionally provide adversaries with insights into black-box models, inevitably increasing their vulnerability to various attacks. In this paper, we develop a novel explanation-driven adversarial attack against black-box classifiers based on feature substitution, called XSub. The key idea of XSub is to strategically replace important features (identified via XAI) in the original sample with corresponding important features from a "golden sample" of a different label, thereby increasing the likelihood of the model misclassifying the perturbed sample. The degree of feature substitution is adjustable, allowing us to control how much of the original samples information is replaced. This flexibility effectively balances a trade-off between the attacks effectiveness and its stealthiness. XSub is also highly cost-effective in that the number of required queries to the prediction model and the explanation model in conducting the attack is in O(1). In addition, XSub can be easily extended to launch backdoor attacks in case the attacker has access to the models training data. Our evaluation demonstrates that XSub is not only effective and stealthy but also cost-effective, enabling its application across a wide range of AI models.},
 author = {Kiana Vu and Phung Lai and Truc Nguyen},
 comment = {},
 doi = {},
 eprint = {2409.08919v1},
 journal = {arXiv preprint},
 title = {XSub: Explanation-Driven Adversarial Attack against Blackbox Classifiers via Feature Substitution},
 url = {http://arxiv.org/abs/2409.08919v1},
 year = {2024}
}

@article{2409.08980v2,
 abstract = {Low trust remains a significant barrier to Autonomous Vehicle (AV) adoption. To design trustworthy AVs, we need to better understand the individual traits, attitudes, and experiences that impact people's trust judgements. We use machine learning to understand the most important factors that contribute to young adult trust based on a comprehensive set of personal factors gathered via survey (n = 1457). Factors ranged from psychosocial and cognitive attributes to driving style, experiences, and perceived AV risks and benefits. Using the explainable AI technique SHAP, we found that perceptions of AV risks and benefits, attitudes toward feasibility and usability, institutional trust, prior experience, and a person's mental model are the most important predictors. Surprisingly, psychosocial and many technology- and driving-specific factors were not strong predictors. Results highlight the importance of individual differences for designing trustworthy AVs for diverse groups and lead to key implications for future design and research.},
 author = {Robert Kaufman and Emi Lee and Manas Satish Bedmutha and David Kirsh and Nadir Weibel},
 comment = {24 pages (including references and appendix), Accepted to CHI Conference on Human Factors in Computing Systems (CHI 2025)},
 doi = {10.1145/3706598.3713188},
 eprint = {2409.08980v2},
 journal = {arXiv preprint},
 title = {Predicting Trust In Autonomous Vehicles: Modeling Young Adult Psychosocial Traits, Risk-Benefit Attitudes, And Driving Factors With Machine Learning},
 url = {http://arxiv.org/abs/2409.08980v2},
 year = {2024}
}

@article{2409.10046v1,
 abstract = {Wildfires pose a significant natural disaster risk to populations and contribute to accelerated climate change. As wildfires are also affected by climate change, extreme wildfires are becoming increasingly frequent. Although they occur less frequently globally than those sparked by human activities, lightning-ignited wildfires play a substantial role in carbon emissions and account for the majority of burned areas in certain regions. While existing computational models, especially those based on machine learning, aim to predict lightning-ignited wildfires, they are typically tailored to specific regions with unique characteristics, limiting their global applicability. In this study, we present machine learning models designed to characterize and predict lightning-ignited wildfires on a global scale. Our approach involves classifying lightning-ignited versus anthropogenic wildfires, and estimating with high accuracy the probability of lightning to ignite a fire based on a wide spectrum of factors such as meteorological conditions and vegetation. Utilizing these models, we analyze seasonal and spatial trends in lightning-ignited wildfires shedding light on the impact of climate change on this phenomenon. We analyze the influence of various features on the models using eXplainable Artificial Intelligence (XAI) frameworks. Our findings highlight significant global differences between anthropogenic and lightning-ignited wildfires. Moreover, we demonstrate that, even over a short time span of less than a decade, climate changes have steadily increased the global risk of lightning-ignited wildfires. This distinction underscores the imperative need for dedicated predictive models and fire weather indices tailored specifically to each type of wildfire.},
 author = {Assaf Shmuel and Teddy Lazebnik and Oren Glickman and Eyal Heifetz and Colin Price},
 comment = {},
 doi = {},
 eprint = {2409.10046v1},
 journal = {arXiv preprint},
 title = {Global Lightning-Ignited Wildfires Prediction and Climate Change Projections based on Explainable Machine Learning Models},
 url = {http://arxiv.org/abs/2409.10046v1},
 year = {2024}
}

@article{2409.10733v2,
 abstract = {Black box neural networks are an indispensable part of modern robots. Nevertheless, deploying such high-stakes systems in real-world scenarios poses significant challenges when the stakeholders, such as engineers and legislative bodies, lack insights into the neural networks' decision-making process. Presently, explainable AI is primarily tailored to natural language processing and computer vision, falling short in two critical aspects when applied in robots: grounding in decision-making tasks and the ability to assess trustworthiness of their explanations. In this paper, we introduce a trustworthy explainable robotics technique based on human-interpretable, high-level concepts that attribute to the decisions made by the neural network. Our proposed technique provides explanations with associated uncertainty scores for the explanation by matching neural network's activations with human-interpretable visualizations. To validate our approach, we conducted a series of experiments with various simulated and real-world robot decision-making models, demonstrating the effectiveness of the proposed approach as a post-hoc, human-friendly robot diagnostic tool.},
 author = {Som Sagar and Aditya Taparia and Harsh Mankodiya and Pranav Bidare and Yifan Zhou and Ransalu Senanayake},
 comment = {19 pages, 26 figures},
 doi = {},
 eprint = {2409.10733v2},
 journal = {arXiv preprint},
 title = {BaTCAVe: Trustworthy Explanations for Robot Behaviors},
 url = {http://arxiv.org/abs/2409.10733v2},
 year = {2024}
}

@article{2409.10898v3,
 abstract = {Ensuring safe water supplies requires effective water quality monitoring, especially in developing countries like Nepal, where contamination risks are high. This paper introduces various hybrid deep learning models to predict on the CCME dataset with multiple water quality parameters from Canada, China, the UK, the USA, and Ireland, with 2.82 million data records feature-engineered and evaluated using them. Models such as CatBoost, XGBoost, and Extra Trees, along with neural networks combining CNN and LSTM layers, are used to capture temporal and spatial patterns in the data. The model demonstrated notable accuracy improvements, aiding proactive water quality control. CatBoost, XGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values with an average RMSE of 1.2 and an R squared score of 0.99. Additionally, classifiers achieved 99% accuracy, cross-validated across models. SHAP analysis showed the importance of indicators like F.R.C. and orthophosphate levels in hybrid architectures' classification decisions. The practical application is demonstrated along with a chatbot application for water quality insights.},
 author = {Biplov Paneru and Bishwash Paneru},
 comment = {},
 doi = {},
 eprint = {2409.10898v3},
 journal = {arXiv preprint},
 title = {AI for Water Sustainability: Global Water Quality Assessment and Prediction with Explainable AI with LLM Chatbot for Insights},
 url = {http://arxiv.org/abs/2409.10898v3},
 year = {2024}
}

@article{2409.11078v2,
 abstract = {Artificial Neural Networks (ANNs) have significantly advanced various fields by effectively recognizing patterns and solving complex problems. Despite these advancements, their interpretability remains a critical challenge, especially in applications where transparency and accountability are essential. To address this, explainable AI (XAI) has made progress in demystifying ANNs, yet interpretability alone is often insufficient. In certain applications, model predictions must align with expert-imposed requirements, sometimes exemplified by partial monotonicity constraints. While monotonic approaches are found in the literature for traditional Multi-layer Perceptrons (MLPs), they still face difficulties in achieving both interpretability and certified partial monotonicity. Recently, the Kolmogorov-Arnold Network (KAN) architecture, based on learnable activation functions parametrized as splines, has been proposed as a more interpretable alternative to MLPs. Building on this, we introduce a novel ANN architecture called MonoKAN, which is based on the KAN architecture and achieves certified partial monotonicity while enhancing interpretability. To achieve this, we employ cubic Hermite splines, which guarantee monotonicity through a set of straightforward conditions. Additionally, by using positive weights in the linear combinations of these splines, we ensure that the network preserves the monotonic relationships between input and output. Our experiments demonstrate that MonoKAN not only enhances interpretability but also improves predictive performance across the majority of benchmarks, outperforming state-of-the-art monotonic MLP approaches.},
 author = {Alejandro Polo-Molina and David Alfaya and Jose Portela},
 comment = {18 pages, 8 figures},
 doi = {10.1016/j.neunet.2025.108278.},
 eprint = {2409.11078v2},
 journal = {arXiv preprint},
 title = {MonoKAN: Certified Monotonic Kolmogorov-Arnold Network},
 url = {http://arxiv.org/abs/2409.11078v2},
 year = {2024}
}

@article{2409.12087v3,
 abstract = {This study explores the potential of utilizing administrative claims data, combined with advanced machine learning and deep learning techniques, to predict the progression of Chronic Kidney Disease (CKD) to End-Stage Renal Disease (ESRD). We analyze a comprehensive, 10-year dataset provided by a major health insurance organization to develop prediction models for multiple observation windows using traditional machine learning methods such as Random Forest and XGBoost as well as deep learning approaches such as Long Short-Term Memory (LSTM) networks. Our findings demonstrate that the LSTM model, particularly with a 24-month observation window, exhibits superior performance in predicting ESRD progression, outperforming existing models in the literature. We further apply SHapley Additive exPlanations (SHAP) analysis to enhance interpretability, providing insights into the impact of individual features on predictions at the individual patient level. This study underscores the value of leveraging administrative claims data for CKD management and predicting ESRD progression.},
 author = {Yubo Li and Saba Al-Sayouri and Rema Padman},
 comment = {10pages, 4 figures, AMIA 2024},
 doi = {},
 eprint = {2409.12087v3},
 journal = {arXiv preprint},
 title = {Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques},
 url = {http://arxiv.org/abs/2409.12087v3},
 year = {2024}
}

@article{2409.13045v1,
 abstract = {The application of deep learning in medical imaging has significantly advanced diagnostic capabilities, enhancing both accuracy and efficiency. Despite these benefits, the lack of transparency in these AI models, often termed "black boxes," raises concerns about their reliability in clinical settings. Explainable AI (XAI) aims to mitigate these concerns by developing methods that make AI decisions understandable and trustworthy. In this study, we propose Tumor Aware Counterfactual Explanations (TACE), a framework designed to generate reliable counterfactual explanations for medical images. Unlike existing methods, TACE focuses on modifying tumor-specific features without altering the overall organ structure, ensuring the faithfulness of the counterfactuals. We achieve this by including an additional step in the generation process which allows to modify only the region of interest (ROI), thus yielding more reliable counterfactuals as the rest of the organ remains unchanged. We evaluate our method on mammography images and brain MRI. We find that our method far exceeds existing state-of-the-art techniques in quality, faithfulness, and generation speed of counterfactuals. Indeed, more faithful explanations lead to a significant improvement in classification success rates, with a 10.69% increase for breast cancer and a 98.02% increase for brain tumors. The code of our work is available at https://github.com/ispamm/TACE.},
 author = {Eleonora Beatrice Rossi and Eleonora Lopez and Danilo Comminiello},
 comment = {The paper has been accepted at Italian Workshop on Neural Networks (WIRN) 2024},
 doi = {},
 eprint = {2409.13045v1},
 journal = {arXiv preprint},
 title = {TACE: Tumor-Aware Counterfactual Explanations},
 url = {http://arxiv.org/abs/2409.13045v1},
 year = {2024}
}

@article{2409.13177v1,
 abstract = {The exponential growth of the Internet of Things (IoT) has significantly increased the complexity and volume of cybersecurity threats, necessitating the development of advanced, scalable, and interpretable security frameworks. This paper presents an innovative, comprehensive framework for real-time IoT attack detection and response that leverages Machine Learning (ML), Explainable AI (XAI), and Large Language Models (LLM). By integrating XAI techniques such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) with a model-independent architecture, we ensure our framework's adaptability across various ML algorithms. Additionally, the incorporation of LLMs enhances the interpretability and accessibility of detection decisions, providing system administrators with actionable, human-understandable explanations of detected threats. Our end-to-end framework not only facilitates a seamless transition from model development to deployment but also represents a real-world application capability that is often lacking in existing research. Based on our experiments with the CIC-IOT-2023 dataset \cite{neto2023ciciot2023}, Gemini and OPENAI LLMS demonstrate unique strengths in attack mitigation: Gemini offers precise, focused strategies, while OPENAI provides extensive, in-depth security measures. Incorporating SHAP and LIME algorithms within XAI provides comprehensive insights into attack detection, emphasizing opportunities for model improvement through detailed feature analysis, fine-tuning, and the adaptation of misclassifications to enhance accuracy.},
 author = {Sudipto Baral and Sajal Saha and Anwar Haque},
 comment = {6 pages, 1 figure, Accepted in 2024 IEEE WF-IoT Conference},
 doi = {},
 eprint = {2409.13177v1},
 journal = {arXiv preprint},
 title = {An Adaptive End-to-End IoT Security Framework Using Explainable AI and LLMs},
 url = {http://arxiv.org/abs/2409.13177v1},
 year = {2024}
}

@article{2409.13723v3,
 abstract = {Machine learning (ML) has rapidly advanced in recent years, revolutionizing fields such as finance, medicine, and cybersecurity. In malware detection, ML-based approaches have demonstrated high accuracy; however, their lack of transparency poses a significant challenge. Traditional black-box models often fail to provide interpretable justifications for their predictions, limiting their adoption in security-critical environments where understanding the reasoning behind a detection is essential for threat mitigation and response. Explainable AI (XAI) addresses this gap by enhancing model interpretability while maintaining strong detection capabilities. This survey presents a comprehensive review of state-of-the-art ML techniques for malware analysis, with a specific focus on explainability methods. We examine existing XAI frameworks, their application in malware classification and detection, and the challenges associated with making malware detection models more interpretable. Additionally, we explore recent advancements and highlight open research challenges in the field of explainable malware analysis. By providing a structured overview of XAI-driven malware detection approaches, this survey serves as a valuable resource for researchers and practitioners seeking to bridge the gap between ML performance and explainability in cybersecurity.},
 author = {Harikha Manthena and Shaghayegh Shajarian and Jeffrey Kimmell and Mahmoud Abdelsalam and Sajad Khorsandroo and Maanak Gupta},
 comment = {},
 doi = {10.1109/ACCESS.2025.3555926},
 eprint = {2409.13723v3},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence (XAI) for Malware Analysis: A Survey of Techniques, Applications, and Open Challenges},
 url = {http://arxiv.org/abs/2409.13723v3},
 year = {2024}
}

@article{2409.14590v3,
 abstract = {The use of machine learning (ML) in critical domains such as medicine poses risks and requires regulation. One requirement is that decisions of ML systems in high-risk applications should be human-understandable. The field of "explainable artificial intelligence" (XAI) seemingly addresses this need. However, in its current form, XAI is unfit to provide quality control for ML; it itself needs scrutiny. Popular XAI methods cannot reliably answer important questions about ML models, their training data, or a given test input. We recapitulate results demonstrating that popular XAI methods systematically attribute importance to input features that are independent of the prediction target. This limits their utility for purposes such as model and data (in)validation, model improvement, and scientific discovery. We argue that the fundamental reason for this limitation is that current XAI methods do not address well-defined problems and are not evaluated against objective criteria of explanation correctness. Researchers should formally define the problems they intend to solve first and then design methods accordingly. This will lead to notions of explanation correctness that can be theoretically verified and objective metrics of explanation performance that can be assessed using ground-truth data.},
 author = {Stefan Haufe and Rick Wilming and Benedict Clark and Rustam Zhumagambetov and Danny Panknin and Ahcène Boubekki},
 comment = {},
 doi = {},
 eprint = {2409.14590v3},
 journal = {arXiv preprint},
 title = {Explainable AI needs formal notions of explanation correctness},
 url = {http://arxiv.org/abs/2409.14590v3},
 year = {2024}
}

@article{2409.15374v2,
 abstract = {Early diagnosis and intervention for Autism Spectrum Disorder (ASD) has been shown to significantly improve the quality of life of autistic individuals. However, diagnostics methods for ASD rely on assessments based on clinical presentation that are prone to bias and can be challenging to arrive at an early diagnosis. There is a need for objective biomarkers of ASD which can help improve diagnostic accuracy. Deep learning (DL) has achieved outstanding performance in diagnosing diseases and conditions from medical imaging data. Extensive research has been conducted on creating models that classify ASD using resting-state functional Magnetic Resonance Imaging (fMRI) data. However, existing models lack interpretability. This research aims to improve the accuracy and interpretability of ASD diagnosis by creating a DL model that can not only accurately classify ASD but also provide explainable insights into its working. The dataset used is a preprocessed version of the Autism Brain Imaging Data Exchange (ABIDE) with 884 samples. Our findings show a model that can accurately classify ASD and highlight critical brain regions differing between ASD and typical controls, with potential implications for early diagnosis and understanding of the neural basis of ASD. These findings are validated by studies in the literature that use different datasets and modalities, confirming that the model actually learned characteristics of ASD and not just the dataset. This study advances the field of explainable AI in medical imaging by providing a robust and interpretable model, thereby contributing to a future with objective and reliable ASD diagnostics.},
 author = {Suryansh Vidya and Kush Gupta and Amir Aly and Andy Wills and Emmanuel Ifeachor and Rohit Shankar},
 comment = {This work has been submitted to the IEEE for possible publication},
 doi = {},
 eprint = {2409.15374v2},
 journal = {arXiv preprint},
 title = {Explainable AI for Autism Diagnosis: Identifying Critical Brain Regions Using fMRI Data},
 url = {http://arxiv.org/abs/2409.15374v2},
 year = {2024}
}

@article{2409.16089v2,
 abstract = {Face Recognition (FR) has advanced significantly with the development of deep learning, achieving high accuracy in several applications. However, the lack of interpretability of these systems raises concerns about their accountability, fairness, and reliability. In the present study, we propose an interactive framework to enhance the explainability of FR models by combining model-agnostic Explainable Artificial Intelligence (XAI) and Natural Language Processing (NLP) techniques. The proposed framework is able to accurately answer various questions of the user through an interactive chatbot. In particular, the explanations generated by our proposed method are in the form of natural language text and visual representations, which for example can describe how different facial regions contribute to the similarity measure between two faces. This is achieved through the automatic analysis of the output's saliency heatmaps of the face images and a BERT question-answering model, providing users with an interface that facilitates a comprehensive understanding of the FR decisions. The proposed approach is interactive, allowing the users to ask questions to get more precise information based on the user's background knowledge. More importantly, in contrast to previous studies, our solution does not decrease the face recognition performance. We demonstrate the effectiveness of the method through different experiments, highlighting its potential to make FR systems more interpretable and user-friendly, especially in sensitive applications where decision-making transparency is crucial.},
 author = {Ivan DeAndres-Tame and Muhammad Faisal and Ruben Tolosana and Rouqaiah Al-Refai and Ruben Vera-Rodriguez and Philipp Terhörst},
 comment = {},
 doi = {10.1007/978-3-031-87657-8_22},
 eprint = {2409.16089v2},
 journal = {arXiv preprint},
 title = {From Pixels to Words: Leveraging Explainability in Face Recognition through Interactive Natural Language Processing},
 url = {http://arxiv.org/abs/2409.16089v2},
 year = {2024}
}

@article{2409.16213v1,
 abstract = {Precision spraying evaluation requires automation primarily in post-spraying imagery. In this paper we propose an eXplainable Artificial Intelligence (XAI) computer vision pipeline to evaluate a precision spraying system post-spraying without the need for traditional agricultural methods. The developed system can semantically segment potential targets such as lettuce, chickweed, and meadowgrass and correctly identify if targets have been sprayed. Furthermore, this pipeline evaluates using a domain-specific Weakly Supervised Deposition Estimation task, allowing for class-specific quantification of spray deposit weights in μL. Estimation of coverage rates of spray deposition in a class-wise manner allows for further understanding of effectiveness of precision spraying systems. Our study evaluates different Class Activation Mapping techniques, namely AblationCAM and ScoreCAM, to determine which is more effective and interpretable for these tasks. In the pipeline, inference-only feature fusion is used to allow for further interpretability and to enable the automation of precision spraying evaluation post-spray. Our findings indicate that a Fully Convolutional Network with an EfficientNet-B0 backbone and inference-only feature fusion achieves an average absolute difference in deposition values of 156.8 μL across three classes in our test set. The dataset curated in this paper is publicly available at https://github.com/Harry-Rogers/PSIE},
 author = {Harry Rogers and Tahmina Zebin and Grzegorz Cielniak and Beatriz De La Iglesia and Ben Magri},
 comment = {},
 doi = {},
 eprint = {2409.16213v1},
 journal = {arXiv preprint},
 title = {Deep Learning for Precision Agriculture: Post-Spraying Evaluation and Deposition Estimation},
 url = {http://arxiv.org/abs/2409.16213v1},
 year = {2024}
}

@article{2409.16639v1,
 abstract = {Despite being the most popular privacy-enhancing network, Tor is increasingly adopted by cybercriminals to obfuscate malicious traffic, hindering the identification of malware-related communications between compromised devices and Command and Control (C&C) servers. This malicious traffic can induce congestion and reduce Tor's performance, while encouraging network administrators to block Tor traffic. Recent research, however, demonstrates the potential for accurately classifying captured Tor traffic as malicious or benign. While existing efforts have addressed malware class identification, their performance remains limited, with micro-average precision and recall values around 70%. Accurately classifying specific malware classes is crucial for effective attack prevention and mitigation. Furthermore, understanding the unique patterns and attack vectors employed by different malware classes helps the development of robust and adaptable defence mechanisms.
  We utilise a multi-label classification technique based on Message-Passing Neural Networks, demonstrating its superiority over previous approaches such as Binary Relevance, Classifier Chains, and Label Powerset, by achieving micro-average precision (MAP) and recall (MAR) exceeding 90%. Compared to previous work, we significantly improve performance by 19.98%, 10.15%, and 59.21% in MAP, MAR, and Hamming Loss, respectively. Next, we employ Explainable Artificial Intelligence (XAI) techniques to interpret the decision-making process within these models. Finally, we assess the robustness of all techniques by crafting adversarial perturbations capable of manipulating classifier predictions and generating false positives and negatives.},
 author = {Ishan Karunanayake and Mashael AlSabah and Nadeem Ahmed and Sanjay Jha},
 comment = {},
 doi = {},
 eprint = {2409.16639v1},
 journal = {arXiv preprint},
 title = {Examining the Rat in the Tunnel: Interpretable Multi-Label Classification of Tor-based Malware},
 url = {http://arxiv.org/abs/2409.16639v1},
 year = {2024}
}

@article{2409.16787v1,
 abstract = {Research in Explainable Artificial Intelligence (XAI) is increasing, aiming to make deep learning models more transparent. Most XAI methods focus on justifying the decisions made by Artificial Intelligence (AI) systems in security-relevant applications. However, relatively little attention has been given to using these methods to improve the performance and robustness of deep learning algorithms. Additionally, much of the existing XAI work primarily addresses classification problems. In this study, we investigate the potential of feature attribution methods to filter out uninformative features in input data for regression problems, thereby improving the accuracy and stability of predictions. We introduce a feature selection pipeline that combines Integrated Gradients with k-means clustering to select an optimal set of variables from the initial data space. To validate the effectiveness of this approach, we apply it to a real-world industrial problem - blade vibration analysis in the development process of turbo machinery.},
 author = {Alexander Hinterleitner and Thomas Bartz-Beielstein and Richard Schulz and Sebastian Spengler and Thomas Winter and Christoph Leitenmeier},
 comment = {},
 doi = {},
 eprint = {2409.16787v1},
 journal = {arXiv preprint},
 title = {Enhancing Feature Selection and Interpretability in AI Regression Tasks Through Feature Attribution},
 url = {http://arxiv.org/abs/2409.16787v1},
 year = {2024}
}

@article{2409.16978v2,
 abstract = {Explainable AI (XAI) aims to make AI systems more transparent, yet many practices emphasise mathematical rigour over practical user needs. We propose an alternative to this model-centric approach by following a design thinking process for the emerging XAI field of training data attribution (TDA), which risks repeating solutionist patterns seen in other subfields. However, because TDA is in its early stages, there is a valuable opportunity to shape its direction through user-centred practices. We engage directly with machine learning developers via a needfinding interview study (N=6) and a scenario-based interactive user study (N=31) to ground explanations in real workflows. Our exploration of the TDA design space reveals novel tasks for data-centric explanations useful to developers, such as grouping training samples behind specific model behaviours or identifying undersampled data. We invite the TDA, XAI, and HCI communities to engage with these tasks to strengthen their research's practical relevance and human impact.},
 author = {Elisa Nguyen and Johannes Bertram and Evgenii Kortukov and Jean Y. Song and Seong Joon Oh},
 comment = {},
 doi = {},
 eprint = {2409.16978v2},
 journal = {arXiv preprint},
 title = {Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI},
 url = {http://arxiv.org/abs/2409.16978v2},
 year = {2024}
}

@article{2409.17931v2,
 abstract = {Accurately estimating the Remaining Useful Life (RUL) of a battery is essential for determining its lifespan and recharge requirements. In this work, we develop machine learning-based models to predict and classify battery RUL. We introduce a two-level ensemble learning (TLE) framework and a CNN+MLP hybrid model for RUL prediction, comparing their performance against traditional, deep, and hybrid machine learning models. Our analysis evaluates various models for both prediction and classification while incorporating interpretability through SHAP. The proposed TLE model consistently outperforms baseline models in RMSE, MAE, and R squared error, demonstrating its superior predictive capabilities. Additionally, the XGBoost classifier achieves an impressive 99% classification accuracy, validated through cross-validation techniques. The models effectively predict relay-based charging triggers, enabling automated and energy-efficient charging processes. This automation reduces energy consumption and enhances battery performance by optimizing charging cycles. SHAP interpretability analysis highlights the cycle index and charging parameters as the most critical factors influencing RUL. To improve accessibility, we developed a Tkinter-based GUI that allows users to input new data and predict RUL in real time. This practical solution supports sustainable battery management by enabling data-driven decisions about battery usage and maintenance, contributing to energy-efficient and innovative battery life prediction.},
 author = {Biplov Paneru and Bipul Thapa and Durga Prasad Mainali and Bishwash Paneru and Krishna Bikram Shah},
 comment = {},
 doi = {},
 eprint = {2409.17931v2},
 journal = {arXiv preprint},
 title = {Remaining Useful Life Prediction for Batteries Utilizing an Explainable AI Approach with a Predictive Application for Decision-Making},
 url = {http://arxiv.org/abs/2409.17931v2},
 year = {2024}
}

@article{2409.18052v2,
 abstract = {Explanation is key to people having confidence in high-stakes AI systems. However, machine-learning-based systems -- which account for almost all current AI -- can't explain because they are usually black boxes. The explainable AI (XAI) movement hedges this problem by redefining "explanation". The human-centered explainable AI (HCXAI) movement identifies the explanation-oriented needs of users but can't fulfill them because of its commitment to machine learning. In order to achieve the kinds of explanations needed by real people operating in critical domains, we must rethink how to approach AI. We describe a hybrid approach to developing cognitive agents that uses a knowledge-based infrastructure supplemented by data obtained through machine learning when applicable. These agents will serve as assistants to humans who will bear ultimate responsibility for the decisions and actions of the human-robot team. We illustrate the explanatory potential of such agents using the under-the-hood panels of a demonstration system in which a team of simulated robots collaborate on a search task assigned by a human.},
 author = {Sergei Nirenburg and Marjorie McShane and Kenneth W. Goodman and Sanjay Oruganti},
 comment = {},
 doi = {},
 eprint = {2409.18052v2},
 journal = {arXiv preprint},
 title = {Explaining Explaining},
 url = {http://arxiv.org/abs/2409.18052v2},
 year = {2024}
}

@article{2409.18156v1,
 abstract = {The application of Shapley values to high-dimensional, time-series-like data is computationally challenging - and sometimes impossible. For $N$ inputs the problem is $2^N$ hard. In image processing, clusters of pixels, referred to as superpixels, are used to streamline computations. This research presents an efficient solution for time-seres-like data that adapts the idea of superpixels for Shapley value computation. Motivated by a forensic DNA classification example, the method is applied to multivariate time-series-like data whose features have been classified by a convolutional neural network (CNN). In DNA processing, it is important to identify alleles from the background noise created by DNA extraction and processing. A single DNA profile has $31,200$ scan points to classify, and the classification decisions must be defensible in a court of law. This means that classification is routinely performed by human readers - a monumental and time consuming process. The application of a CNN with fast computation of meaningful Shapley values provides a potential alternative to the classification. This research demonstrates the realistic, accurate and fast computation of Shapley values for this massive task},
 author = {Lauren Elborough and Duncan Taylor and Melissa Humphries},
 comment = {16 pages, 5 figures},
 doi = {},
 eprint = {2409.18156v1},
 journal = {arXiv preprint},
 title = {A novel application of Shapley values for large multidimensional time-series data: Applying explainable AI to a DNA profile classification neural network},
 url = {http://arxiv.org/abs/2409.18156v1},
 year = {2024}
}

@article{2409.20287v1,
 abstract = {Convolutional neural networks (CNNs) achieve prevailing results in segmentation tasks nowadays and represent the state-of-the-art for image-based analysis. However, the understanding of the accurate decision-making process of a CNN is rather unknown. The research area of explainable artificial intelligence (xAI) primarily revolves around understanding and interpreting this black-box behavior. One way of interpreting a CNN is the use of class activation maps (CAMs) that represent heatmaps to indicate the importance of image areas for the prediction of the CNN. For classification tasks, a variety of CAM algorithms exist. But for segmentation tasks, only one CAM algorithm for the interpretation of the output of a CNN exist. We propose a transfer between existing classification- and segmentation-based methods for more detailed, explainable, and consistent results which show salient pixels in semantic segmentation tasks. The resulting Seg-HiRes-Grad CAM is an extension of the segmentation-based Seg-Grad CAM with the transfer to the classification-based HiRes CAM. Our method improves the previously-mentioned existing segmentation-based method by adjusting it to recently published classification-based methods. Especially for medical image segmentation, this transfer solves existing explainability disadvantages.},
 author = {Tillmann Rheude and Andreas Wirtz and Arjan Kuijper and Stefan Wesarg},
 comment = {Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2024:023},
 doi = {10.59275/j.melba.2024-ebd3},
 eprint = {2409.20287v1},
 journal = {arXiv preprint},
 title = {Leveraging CAM Algorithms for Explaining Medical Semantic Segmentation},
 url = {http://arxiv.org/abs/2409.20287v1},
 year = {2024}
}

@article{2410.00366v1,
 abstract = {The rapid advancements in artificial intelligence (AI) have revolutionized smart healthcare, driving innovations in wearable technologies, continuous monitoring devices, and intelligent diagnostic systems. However, security, explainability, robustness, and performance optimization challenges remain critical barriers to widespread adoption in clinical environments. This research presents an innovative algorithmic method using the Adaptive Feature Evaluator (AFE) algorithm to improve feature selection in healthcare datasets and overcome problems. AFE integrating Genetic Algorithms (GA), Explainable Artificial Intelligence (XAI), and Permutation Combination Techniques (PCT), the algorithm optimizes Clinical Decision Support Systems (CDSS), thereby enhancing predictive accuracy and interpretability. The proposed method is validated across three diverse healthcare datasets using six distinct machine learning algorithms, demonstrating its robustness and superiority over conventional feature selection techniques. The results underscore the transformative potential of AFE in smart healthcare, enabling personalized and transparent patient care. Notably, the AFE algorithm, when combined with a Multi-layer Perceptron (MLP), achieved an accuracy of up to 98.5%, highlighting its capability to improve clinical decision-making processes in real-world healthcare applications.},
 author = {Prasenjit Maji and Amit Kumar Mondal and Hemanta Kumar Mondal and Saraju P. Mohanty},
 comment = {},
 doi = {},
 eprint = {2410.00366v1},
 journal = {arXiv preprint},
 title = {Easydiagnos: a framework for accurate feature selection for automatic diagnosis in smart healthcare},
 url = {http://arxiv.org/abs/2410.00366v1},
 year = {2024}
}

@article{2410.00984v2,
 abstract = {When performing predictions that use Machine Learning (ML), we are mainly interested in performance and interpretability. This generates a natural trade-off, where complex models generally have higher skills but are harder to explain and thus trust. Interpretability is particularly important in the climate community, where we aim at gaining a physical understanding of the underlying phenomena. Even more so when the prediction concerns extreme weather events with high impact on society. In this paper, we perform probabilistic forecasts of extreme heatwaves over France, using a hierarchy of increasingly complex ML models, which allows us to find the best compromise between accuracy and interpretability. More precisely, we use models that range from a global Gaussian Approximation (GA) to deep Convolutional Neural Networks (CNNs), with the intermediate steps of a simple Intrinsically Interpretable Neural Network (IINN) and a model using the Scattering Transform (ScatNet). Our findings reveal that CNNs provide higher accuracy, but their black-box nature severely limits interpretability, even when using state-of-the-art Explainable Artificial Intelligence (XAI) tools. In contrast, ScatNet achieves similar performance to CNNs while providing greater transparency, identifying key scales and patterns in the data that drive predictions. This study underscores the potential of interpretability in ML models for climate science, demonstrating that simpler models can rival the performance of their more complex counterparts, all the while being much easier to understand. This gained interpretability is crucial for building trust in model predictions and uncovering new scientific insights, ultimately advancing our understanding and management of extreme weather events.},
 author = {Alessandro Lovo and Amaury Lancelin and Corentin Herbert and Freddy Bouchet},
 comment = {Accepted for publication at Artificial Intelligence for the Earth Systems (AIES) (ISSN: 2769-7525). Authors Alessandro Lovo and Amaury Lancelin contributed equally as first authors},
 doi = {},
 eprint = {2410.00984v2},
 journal = {arXiv preprint},
 title = {Tackling the Accuracy-Interpretability Trade-off in a Hierarchy of Machine Learning Models for the Prediction of Extreme Heatwaves},
 url = {http://arxiv.org/abs/2410.00984v2},
 year = {2024}
}

@article{2410.01859v4,
 abstract = {Objective: To improve prediction of Chronic Kidney Disease (CKD) progression to End Stage Renal Disease (ESRD) using machine learning (ML) and deep learning (DL) models applied to an integrated clinical and claims dataset of varying observation windows, supported by explainable AI (XAI) to enhance interpretability and reduce bias.
  Materials and Methods: We utilized data about 10,326 CKD patients, combining their clinical and claims information from 2009 to 2018. Following data preprocessing, cohort identification, and feature engineering, we evaluated multiple statistical, ML and DL models using data extracted from five distinct observation windows. Feature importance and Shapley value analysis were employed to understand key predictors. Models were tested for robustness, clinical relevance, misclassification errors and bias issues.
  Results: Integrated data models outperformed those using single data sources, with the Long Short-Term Memory (LSTM) model achieving the highest AUC (0.93) and F1 score (0.65). A 24-month observation window was identified as optimal for balancing early detection and prediction accuracy. The 2021 eGFR equation improved prediction accuracy and reduced racial bias, notably for African American patients. Discussion: Improved ESRD prediction accuracy, results interpretability and bias mitigation strategies presented in this study have the potential to significantly enhance CKD and ESRD management, support targeted early interventions and reduce healthcare disparities.
  Conclusion: This study presents a robust framework for predicting ESRD outcomes in CKD patients, improving clinical decision-making and patient care through multi-sourced, integrated data and AI/ML methods. Future research will expand data integration and explore the application of this framework to other chronic diseases.},
 author = {Yubo Li and Rema Padman},
 comment = {},
 doi = {},
 eprint = {2410.01859v4},
 journal = {arXiv preprint},
 title = {Enhancing End Stage Renal Disease Outcome Prediction: A Multi-Sourced Data-Driven Approach},
 url = {http://arxiv.org/abs/2410.01859v4},
 year = {2024}
}

@article{2410.02609v1,
 abstract = {The proliferation of fake news has emerged as a significant threat to the integrity of information dissemination, particularly on social media platforms. Misinformation can spread quickly due to the ease of creating and disseminating content, affecting public opinion and sociopolitical events. Identifying false information is therefore essential to reducing its negative consequences and maintaining the reliability of online news sources. Traditional approaches to fake news detection often rely solely on content-based features, overlooking the crucial role of social context in shaping the perception and propagation of news articles. In this paper, we propose a comprehensive approach that integrates social context-based features with news content features to enhance the accuracy of fake news detection in under-resourced languages. We perform several experiments utilizing a variety of methodologies, including traditional machine learning, neural networks, ensemble learning, and transfer learning. Assessment of the outcomes of the experiments shows that the ensemble learning approach has the highest accuracy, achieving a 0.99 F1 score. Additionally, when compared with monolingual models, the fine-tuned model with the target language outperformed others, achieving a 0.94 F1 score. We analyze the functioning of the models, considering the important features that contribute to model performance, using explainable AI techniques.},
 author = {Mesay Gemeda Yigezu and Melkamu Abay Mersha and Girma Yohannis Bade and Jugal Kalita and Olga Kolesnikova and Alexander Gelbukh},
 comment = {},
 doi = {},
 eprint = {2410.02609v1},
 journal = {arXiv preprint},
 title = {Ethio-Fake: Cutting-Edge Approaches to Combat Fake News in Under-Resourced Languages Using Explainable AI},
 url = {http://arxiv.org/abs/2410.02609v1},
 year = {2024}
}

@article{2410.02970v2,
 abstract = {Recent research has developed a number of eXplainable AI (XAI) techniques, such as gradient-based approaches, input perturbation-base methods, and black-box explanation methods. While these XAI techniques can extract meaningful insights from deep learning models, how to properly evaluate them remains an open problem. The most widely used approach is to perturb or even remove what the XAI method considers to be the most important features in an input and observe the changes in the output prediction. This approach, although straightforward, suffers the Out-of-Distribution (OOD) problem as the perturbed samples may no longer follow the original data distribution. A recent method RemOve And Retrain (ROAR) solves the OOD issue by retraining the model with perturbed samples guided by explanations. However, using the model retrained based on XAI methods to evaluate these explainers may cause information leakage and thus lead to unfair comparisons. We propose Fine-tuned Fidelity (F-Fidelity), a robust evaluation framework for XAI, which utilizes i) an explanation-agnostic fine-tuning strategy, thus mitigating the information leakage issue, and ii) a random masking operation that ensures that the removal step does not generate an OOD input. We also design controlled experiments with state-of-the-art (SOTA) explainers and their degraded version to verify the correctness of our framework. We conduct experiments on multiple data modalities, such as images, time series, and natural language. The results demonstrate that F-Fidelity significantly improves upon prior evaluation metrics in recovering the ground-truth ranking of the explainers. Furthermore, we show both theoretically and empirically that, given a faithful explainer, F-Fidelity metric can be used to compute the sparsity of influential input components, i.e., to extract the true explanation size.},
 author = {Xu Zheng and Farhad Shirani and Zhuomin Chen and Chaohao Lin and Wei Cheng and Wenbo Guo and Dongsheng Luo},
 comment = {Accepted to International Conference on Learning Representations (ICLR 2025); 33 Pages, 5 figures, 26 Tables},
 doi = {},
 eprint = {2410.02970v2},
 journal = {arXiv preprint},
 title = {F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI},
 url = {http://arxiv.org/abs/2410.02970v2},
 year = {2024}
}

@article{2410.04823v3,
 abstract = {Despite the transformative impact of deep learning across multiple domains, the inherent opacity of these models has driven the development of Explainable Artificial Intelligence (XAI). Among these efforts, Concept Bottleneck Models (CBMs) have emerged as a key approach to improve interpretability by leveraging high-level semantic information. However, CBMs, like other machine learning models, are susceptible to security threats, particularly backdoor attacks, which can covertly manipulate model behaviors. Understanding that the community has not yet studied the concept level backdoor attack of CBM, because of "Better the devil you know than the devil you don't know.", we introduce CAT (Concept-level Backdoor ATtacks), a methodology that leverages the conceptual representations within CBMs to embed triggers during training, enabling controlled manipulation of model predictions at inference time. An enhanced attack pattern, CAT+, incorporates a correlation function to systematically select the most effective and stealthy concept triggers, thereby optimizing the attack's impact. Our comprehensive evaluation framework assesses both the attack success rate and stealthiness, demonstrating that CAT and CAT+ maintain high performance on clean data while achieving significant targeted effects on backdoored datasets. This work underscores the potential security risks associated with CBMs and provides a robust testing methodology for future security assessments.},
 author = {Songning Lai and Jiayu Yang and Yu Huang and Lijie Hu and Tianlang Xue and Zhangyi Hu and Jiaxu Li and Haicheng Liao and Yutao Yue},
 comment = {},
 doi = {},
 eprint = {2410.04823v3},
 journal = {arXiv preprint},
 title = {CAT: Concept-level backdoor ATtacks for Concept Bottleneck Models},
 url = {http://arxiv.org/abs/2410.04823v3},
 year = {2024}
}

@article{2410.04883v2,
 abstract = {In Explainable AI (XAI), Shapley values are a popular model-agnostic framework for explaining predictions made by complex machine learning models. The computation of Shapley values requires estimating non-trivial contribution functions representing predictions with only a subset of the features present. As the number of these terms grows exponentially with the number of features, computational costs escalate rapidly, creating a pressing need for efficient and accurate approximation methods. For tabular data, the KernelSHAP framework is considered the state-of-the-art model-agnostic approximation framework. KernelSHAP approximates the Shapley values using a weighted sample of the contribution functions for different feature subsets. We propose a novel modification of KernelSHAP which replaces the stochastic weights with deterministic ones to reduce the variance of the resulting Shapley value approximations. This may also be combined with our simple, yet effective modification to the KernelSHAP variant implemented in the popular Python library SHAP. Additionally, we provide an overview of established methods. Numerical experiments demonstrate that our methods can reduce the required number of contribution function evaluations by $5\%$ to $50\%$ while preserving the same accuracy of the approximated Shapley values -- essentially reducing the running time by up to $50\%$. These computational advancements push the boundaries of the feature dimensionality and number of predictions that can be accurately explained with Shapley values within a feasible runtime.},
 author = {Lars Henry Berge Olsen and Martin Jullum},
 comment = {This is the accepted, post peer-reviewed version of the manuscript, accepted for publication in the proceedings after the Third World Conference on eXplainable Artificial Intelligence, XAI-2025. A link to the version of record will be included here upon publication},
 doi = {},
 eprint = {2410.04883v2},
 journal = {arXiv preprint},
 title = {Improving the Weighting Strategy in KernelSHAP},
 url = {http://arxiv.org/abs/2410.04883v2},
 year = {2024}
}

@article{2410.05310v2,
 abstract = {Wireless communication has evolved significantly, with 6G offering groundbreaking capabilities, particularly for IoT. However, the integration of IoT into 6G presents new security challenges, expanding the attack surface due to vulnerabilities introduced by advanced technologies such as open RAN, terahertz (THz) communication, IRS, massive MIMO, and AI. Emerging threats like AI exploitation, virtualization risks, and evolving attacks, including data manipulation and signal interference, further complicate security efforts. As 6G standards are set to be finalized by 2030, work continues to align security measures with technological advances. However, substantial gaps remain in frameworks designed to secure integrated IoT and 6G systems. Our research addresses these challenges by utilizing tree-based machine learning algorithms to manage complex datasets and evaluate feature importance. We apply data balancing techniques to ensure fair attack representation and use SHAP and LIME to improve model transparency. By aligning feature importance with XAI methods and cross-validating for consistency, we boost model accuracy and enhance IoT security within the 6G ecosystem.},
 author = {Navneet Kaur and Lav Gupta},
 comment = {},
 doi = {},
 eprint = {2410.05310v2},
 journal = {arXiv preprint},
 title = {An Approach To Enhance IoT Security In 6G Networks Through Explainable AI},
 url = {http://arxiv.org/abs/2410.05310v2},
 year = {2024}
}

@article{2410.05479v1,
 abstract = {This paper addresses a significant gap in explainable AI: the necessity of interpreting epistemic uncertainty in model explanations. Although current methods mainly focus on explaining predictions, with some including uncertainty, they fail to provide guidance on how to reduce the inherent uncertainty in these predictions. To overcome this challenge, we introduce new types of explanations that specifically target epistemic uncertainty. These include ensured explanations, which highlight feature modifications that can reduce uncertainty, and categorisation of uncertain explanations counter-potential, semi-potential, and super-potential which explore alternative scenarios. Our work emphasises that epistemic uncertainty adds a crucial dimension to explanation quality, demanding evaluation based not only on prediction probability but also on uncertainty reduction. We introduce a new metric, ensured ranking, designed to help users identify the most reliable explanations by balancing trade-offs between uncertainty, probability, and competing alternative explanations. Furthermore, we extend the Calibrated Explanations method, incorporating tools that visualise how changes in feature values impact epistemic uncertainty. This enhancement provides deeper insights into model behaviour, promoting increased interpretability and appropriate trust in scenarios involving uncertain predictions.},
 author = {Helena Löfström and Tuwe Löfström and Johan Hallberg Szabadvary},
 comment = {35 pages, 11 figures, journal},
 doi = {},
 eprint = {2410.05479v1},
 journal = {arXiv preprint},
 title = {Ensured: Explanations for Decreasing the Epistemic Uncertainty in Predictions},
 url = {http://arxiv.org/abs/2410.05479v1},
 year = {2024}
}

@article{2410.05715v2,
 abstract = {Learning from Demonstration (LfD) is a powerful type of machine learning that can allow novices to teach and program robots to complete various tasks. However, the learning process for these systems may still be difficult for novices to interpret and understand, making effective teaching challenging. Explainable artificial intelligence (XAI) aims to address this challenge by explaining a system to the user. In this work, we investigate XAI within LfD by implementing an adaptive explanatory feedback system on an inverse reinforcement learning (IRL) algorithm. The feedback is implemented by demonstrating selected learnt trajectories to users. The system adapts to user teaching by categorizing and then selectively sampling trajectories shown to a user, to show a representative sample of both successful and unsuccessful trajectories. The system was evaluated through a user study with 26 participants teaching a robot a navigation task. The results of the user study demonstrated that the proposed explanatory feedback system can improve robot performance, teaching efficiency and user understanding of the robot.},
 author = {Morris Gu and Elizabeth Croft and Dana Kulic},
 comment = {8 Pages, 9 Figures, 2 Tables},
 doi = {},
 eprint = {2410.05715v2},
 journal = {arXiv preprint},
 title = {Demonstration Based Explainable AI for Learning from Demonstration Methods},
 url = {http://arxiv.org/abs/2410.05715v2},
 year = {2024}
}

@article{2410.06300v3,
 abstract = {SHAP (SHapley Additive exPlanations) values are a widely used method for local feature attribution in interpretable and explainable AI. We propose an efficient two-stage algorithm for computing SHAP values in both black-box setting and tree-based models. Motivated by spectral bias in real-world predictors, we first approximate models using compact Fourier representations, exactly for trees and approximately for black-box models. In the second stage, we introduce a closed-form formula for {\em exactly} computing SHAP values using the Fourier representation, that ``linearizes'' the computation into a simple summation and is amenable to parallelization. As the Fourier approximation is computed only once, our method enables amortized SHAP value computation, achieving significant speedups over existing methods and a tunable trade-off between efficiency and precision.},
 author = {Ali Gorji and Andisheh Amrollahi and Andreas Krause},
 comment = {Published in 39th Conference on Neural Information Processing Systems (NeurIPS 2025)},
 doi = {},
 eprint = {2410.06300v3},
 journal = {arXiv preprint},
 title = {SHAP values via sparse Fourier representation},
 url = {http://arxiv.org/abs/2410.06300v3},
 year = {2024}
}

@article{2410.07072v1,
 abstract = {Deep learning is making a profound impact in the physical layer of wireless communications. Despite exhibiting outstanding empirical performance in tasks such as MIMO receive processing, the reasons behind the demonstrated superior performance improvement remain largely unclear. In this work, we advance the field of Explainable AI (xAI) in the physical layer of wireless communications utilizing signal processing principles. Specifically, we focus on the task of MIMO-OFDM receive processing (e.g., symbol detection) using reservoir computing (RC), a framework within recurrent neural networks (RNNs), which outperforms both conventional and other learning-based MIMO detectors. Our analysis provides a signal processing-based, first-principles understanding of the corresponding operation of the RC. Building on this fundamental understanding, we are able to systematically incorporate the domain knowledge of wireless systems (e.g., channel statistics) into the design of the underlying RNN by directly configuring the untrained RNN weights for MIMO-OFDM symbol detection. The introduced RNN weight configuration has been validated through extensive simulations demonstrating significant performance improvements. This establishes a foundation for explainable RC-based architectures in MIMO-OFDM receive processing and provides a roadmap for incorporating domain knowledge into the design of neural networks for NextG systems.},
 author = {Shashank Jere and Lizhong Zheng and Karim Said and Lingjia Liu},
 comment = {},
 doi = {},
 eprint = {2410.07072v1},
 journal = {arXiv preprint},
 title = {Towards xAI: Configuring RNN Weights using Domain Knowledge for MIMO Receive Processing},
 url = {http://arxiv.org/abs/2410.07072v1},
 year = {2024}
}

@article{2410.07260v1,
 abstract = {Gene expression analysis is a critical method for cancer classification, enabling precise diagnoses through the identification of unique molecular signatures associated with various tumors. Identifying cancer-specific genes from gene expression values enables a more tailored and personalized treatment approach. However, the high dimensionality of mRNA gene expression data poses challenges for analysis and data extraction. This research presents a comprehensive pipeline designed to accurately identify 33 distinct cancer types and their corresponding gene sets. It incorporates a combination of normalization and feature selection techniques to reduce dataset dimensionality effectively while ensuring high performance. Notably, our pipeline successfully identifies a substantial number of cancer-specific genes using a reduced feature set of just 500, in contrast to using the full dataset comprising 19,238 features. By employing an ensemble approach that combines three top-performing classifiers, a classification accuracy of 96.61% was achieved. Furthermore, we leverage Explainable AI to elucidate the biological significance of the identified cancer-specific genes, employing Differential Gene Expression (DGE) analysis.},
 author = {Farzana Tabassum and Sabrina Islam and Siana Rizwan and Masrur Sobhan and Tasnim Ahmed and Sabbir Ahmed and Tareque Mohmud Chowdhury},
 comment = {37 pages, 2 figures, 8 tables, Submitted to Journal of Computational Science},
 doi = {},
 eprint = {2410.07260v1},
 journal = {arXiv preprint},
 title = {Precision Cancer Classification and Biomarker Identification from mRNA Gene Expression via Dimensionality Reduction and Explainable AI},
 url = {http://arxiv.org/abs/2410.07260v1},
 year = {2024}
}

@article{2410.08098v1,
 abstract = {Residential rooftop solar adoption is considered crucial for reducing carbon emissions. The lack of photovoltaic (PV) data at a finer resolution (e.g., household, hourly levels) poses a significant roadblock to informed decision-making. We discuss a novel methodology to generate a highly granular, residential-scale realistic dataset for rooftop solar adoption across the contiguous United States. The data-driven methodology consists of: (i) integrated machine learning models to identify PV adopters, (ii) methods to augment the data using explainable AI techniques to glean insights about key features and their interactions, and (iii) methods to generate household-level hourly solar energy output using an analytical model. The resulting synthetic datasets are validated using real-world data and can serve as a digital twin for modeling downstream tasks. Finally, a policy-based case study utilizing the digital twin for Virginia demonstrated increased rooftop solar adoption with the 30\% Federal Solar Investment Tax Credit, especially in Low-to-Moderate-Income communities.},
 author = {Aparna Kishore and Swapna Thorve and Madhav Marathe},
 comment = {41 pages including references and supplementary},
 doi = {},
 eprint = {2410.08098v1},
 journal = {arXiv preprint},
 title = {A Generative AI Technique for Synthesizing a Digital Twin for U.S. Residential Solar Adoption and Generation},
 url = {http://arxiv.org/abs/2410.08098v1},
 year = {2024}
}

@article{2410.08634v1,
 abstract = {Federated learning (FL) is a commonly distributed algorithm for mobile users (MUs) training artificial intelligence (AI) models, however, several challenges arise when applying FL to real-world scenarios, such as label scarcity, non-IID data, and unexplainability. As a result, we propose an explainable personalized FL framework, called XPFL. First, we introduce a generative AI (GAI) assisted personalized federated semi-supervised learning, called GFed. Particularly, in local training, we utilize a GAI model to learn from large unlabeled data and apply knowledge distillation-based semi-supervised learning to train the local FL model using the knowledge acquired from the GAI model. In global aggregation, we obtain the new local FL model by fusing the local and global FL models in specific proportions, allowing each local model to incorporate knowledge from others while preserving its personalized characteristics. Second, we propose an explainable AI mechanism for FL, named XFed. Specifically, in local training, we apply a decision tree to match the input and output of the local FL model. In global aggregation, we utilize t-distributed stochastic neighbor embedding (t-SNE) to visualize the local models before and after aggregation. Finally, simulation results validate the effectiveness of the proposed XPFL framework.},
 author = {Yubo Peng and Feibo Jiang and Li Dong and Kezhi Wang and Kun Yang},
 comment = {},
 doi = {},
 eprint = {2410.08634v1},
 journal = {arXiv preprint},
 title = {GAI-Enabled Explainable Personalized Federated Semi-Supervised Learning},
 url = {http://arxiv.org/abs/2410.08634v1},
 year = {2024}
}

@article{2410.09043v2,
 abstract = {In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle network (IVN) security is paramount. This paper introduces an advanced intrusion detection system (IDS) called KD-XVAE that uses a Variational Autoencoder (VAE)-based knowledge distillation approach to enhance both performance and efficiency. Our model significantly reduces complexity, operating with just 1669 parameters and achieving an inference time of 0.3 ms per batch, making it highly suitable for resource-constrained automotive environments. Evaluations in the HCRL Car-Hacking dataset demonstrate exceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score of 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing, Gear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset further underscores its superiority over traditional machine learning models, achieving perfect detection metrics. We furthermore integrate Explainable AI (XAI) techniques to ensure transparency in the model's decisions. The VAE compresses the original feature space into a latent space, on which the distilled model is trained. SHAP(SHapley Additive exPlanations) values provide insights into the importance of each latent dimension, mapped back to original features for intuitive understanding. Our paper advances the field by integrating state-of-the-art techniques, addressing critical challenges in the deployment of efficient, trustworthy, and reliable IDSes for autonomous vehicles, ensuring enhanced protection against emerging cyber threats.},
 author = {Muhammet Anil Yagiz and Pedram MohajerAnsari and Mert D. Pese and Polat Goktas},
 comment = {},
 doi = {},
 eprint = {2410.09043v2},
 journal = {arXiv preprint},
 title = {Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge Distillation Meets Explainable AI},
 url = {http://arxiv.org/abs/2410.09043v2},
 year = {2024}
}

@article{2410.09069v2,
 abstract = {The rapid expansion of e-commerce and the widespread use of credit cards in online purchases and financial transactions have significantly heightened the importance of promptly and accurately detecting credit card fraud (CCF). Not only do fraudulent activities in financial transactions lead to substantial monetary losses for banks and financial institutions, but they also undermine user trust in digital services. This study presents a new stacking-based approach for CCF detection by adding two extra layers to the usual classification process: an attention layer and a confidence-based combination layer. In the attention layer, we combine soft outputs from a convolutional neural network (CNN) and a recurrent neural network (RNN) using the dependent ordered weighted averaging (DOWA) operator, and from a graph neural network (GNN) and a long short-term memory (LSTM) network using the induced ordered weighted averaging (IOWA) operator. These weighted outputs capture different predictive signals, increasing the model's accuracy. Next, in the confidence-based layer, we select whichever aggregate (DOWA or IOWA) shows lower uncertainty to feed into a meta-learner. To make the model more explainable, we use shapley additive explanations (SHAP) to identify the top ten most important features for distinguishing between fraud and normal transactions. These features are then used in our attention-based model. Experiments on three datasets show that our method achieves high accuracy and robust generalization, making it effective for CCF detection.},
 author = {Mehdi Hosseini Chagahi and Niloufar Delfan and Saeed Mohammadi Dashtaki and Behzad Moshiri and Md. Jalil Piran},
 comment = {},
 doi = {},
 eprint = {2410.09069v2},
 journal = {arXiv preprint},
 title = {Explainable AI for Fraud Detection: An Attention-Based Ensemble of CNNs, GNNs, and A Confidence-Driven Gating Mechanism},
 url = {http://arxiv.org/abs/2410.09069v2},
 year = {2024}
}

@article{2410.09105v1,
 abstract = {Inherited retinal diseases (IRDs) are a diverse group of genetic disorders that lead to progressive vision loss and are a major cause of blindness in working-age adults. The complexity and heterogeneity of IRDs pose significant challenges in diagnosis, prognosis, and management. Recent advancements in artificial intelligence (AI) offer promising solutions to these challenges. However, the rapid development of AI techniques and their varied applications have led to fragmented knowledge in this field. This review consolidates existing studies, identifies gaps, and provides an overview of AI's potential in diagnosing and managing IRDs. It aims to structure pathways for advancing clinical applications by exploring AI techniques like machine learning and deep learning, particularly in disease detection, progression prediction, and personalized treatment planning. Special focus is placed on the effectiveness of convolutional neural networks in these areas. Additionally, the integration of explainable AI is discussed, emphasizing its importance in clinical settings to improve transparency and trust in AI-based systems. The review addresses the need to bridge existing gaps in focused studies on AI's role in IRDs, offering a structured analysis of current AI techniques and outlining future research directions. It concludes with an overview of the challenges and opportunities in deploying AI for IRDs, highlighting the need for interdisciplinary collaboration and the continuous development of robust, interpretable AI models to advance clinical applications.},
 author = {Han Trinh and Jordan Vice and Jason Charng and Zahra Tajbakhsh and Khyber Alam and Fred K. Chen and Ajmal Mian},
 comment = {},
 doi = {},
 eprint = {2410.09105v1},
 journal = {arXiv preprint},
 title = {Artificial intelligence techniques in inherited retinal diseases: A review},
 url = {http://arxiv.org/abs/2410.09105v1},
 year = {2024}
}

@article{2410.09295v2,
 abstract = {Explainable Artificial Intelligence (XAI) has emerged as a critical area of research to unravel the opaque inner logic of (deep) machine learning models. Among the various XAI techniques proposed in the literature, counterfactual explanations stand out as one of the most promising approaches. However, these "what-if" explanations are frequently complex and technical, making them difficult for non-experts to understand and, more broadly, challenging for humans to interpret. To bridge this gap, in this work, we exploit the power of open-source Large Language Models to generate natural language explanations when prompted with valid counterfactual instances produced by state-of-the-art explainers for graph-based models. Experiments across several graph datasets and counterfactual explainers show that our approach effectively produces accurate natural language representations of counterfactual instances, as demonstrated by key performance metrics.},
 author = {Flavio Giorgi and Cesare Campagnano and Fabrizio Silvestri and Gabriele Tolomei},
 comment = {},
 doi = {},
 eprint = {2410.09295v2},
 journal = {arXiv preprint},
 title = {Natural Language Counterfactual Explanations for Graphs Using Large Language Models},
 url = {http://arxiv.org/abs/2410.09295v2},
 year = {2024}
}

@article{2410.10463v1,
 abstract = {In the field of Explainable AI (XAI), counterfactual (CF) explanations are one prominent method to interpret a black-box model by suggesting changes to the input that would alter a prediction. In real-world applications, the input is predominantly in tabular form and comprised of mixed data types and complex feature interdependencies. These unique data characteristics are difficult to model, and we empirically show that they lead to bias towards specific feature types when generating CFs. To overcome this issue, we introduce TABCF, a CF explanation method that leverages a transformer-based Variational Autoencoder (VAE) tailored for modeling tabular data. Our approach uses transformers to learn a continuous latent space and a novel Gumbel-Softmax detokenizer that enables precise categorical reconstruction while preserving end-to-end differentiability. Extensive quantitative evaluation on five financial datasets demonstrates that TABCF does not exhibit bias toward specific feature types, and outperforms existing methods in producing effective CFs that align with common CF desiderata.},
 author = {Emmanouil Panagiotou and Manuel Heurich and Tim Landgraf and Eirini Ntoutsi},
 comment = {Paper accepted at ICAIF '24: 5th ACM International Conference on AI in Finance, Brooklyn, NY, USA, November 2024},
 doi = {10.1145/3677052.3698673},
 eprint = {2410.10463v1},
 journal = {arXiv preprint},
 title = {TABCF: Counterfactual Explanations for Tabular Data Using a Transformer-Based VAE},
 url = {http://arxiv.org/abs/2410.10463v1},
 year = {2024}
}

@article{2410.10907v1,
 abstract = {Thyroid carcinoma, a significant yet often controllable cancer, has seen a rise in cases, largely due to advancements in diagnostic methods. Differentiated thyroid cancer (DTC), which includes papillary and follicular varieties, is typically associated with a positive prognosis in academic circles. Nevertheless, there are still some individuals who may experience a recurrence. This study employs machine learning, particularly deep learning models, to predict the recurrence of DTC, with the goal of improving patient care through personalized treatment approaches. By analysing a dataset containing clinicopathological features of patients, the model achieved remarkable accuracy rates of 98% during training and 96% during testing. To improve the model's interpretability, we used techniques like LIME and Morris Sensitivity Analysis. These methods gave us valuable insights into how the model makes decisions. The results suggest that combining deep learning models with interpretability techniques can be extremely useful in quickly identifying the recurrence of thyroid cancer in patients. This can help in making informed therapeutic choices and customizing treatment approaches for individual patients.},
 author = {Mohammad Al-Sayed Ahmad and Jude Haddad},
 comment = {},
 doi = {10.1109/JIBEC63210.2024.10932125},
 eprint = {2410.10907v1},
 journal = {arXiv preprint},
 title = {An Explainable AI Model for Predicting the Recurrence of Differentiated Thyroid Cancer},
 url = {http://arxiv.org/abs/2410.10907v1},
 year = {2024}
}

@article{2410.11587v1,
 abstract = {Hydrological models often involve constitutive laws that may not be optimal in every application. We propose to replace such laws with the Kolmogorov-Arnold networks (KANs), a class of neural networks designed to identify symbolic expressions. We demonstrate KAN's potential on the problem of baseflow identification, a notoriously challenging task plagued by significant uncertainty. KAN-derived functional dependencies of the baseflow components on the aridity index outperform their original counterparts. On a test set, they increase the Nash-Sutcliffe Efficiency (NSE) by 67%, decrease the root mean squared error by 30%, and increase the Kling-Gupta efficiency by 24%. This superior performance is achieved while reducing the number of fitting parameters from three to two. Next, we use data from 378 catchments across the continental United States to refine the water-balance equation at the mean-annual scale. The KAN-derived equations based on the refined water balance outperform both the current aridity index model, with up to a 105% increase in NSE, and the KAN-derived equations based on the original water balance. While the performance of our model and tree-based machine learning methods is similar, KANs offer the advantage of simplicity and transparency and require no specific software or computational tools. This case study focuses on the aridity index formulation, but the approach is flexible and transferable to other hydrological processes.},
 author = {Chuyang Liu and Tirthankar Roy and Daniel M. Tartakovsky and Dipankar Dwivedi},
 comment = {},
 doi = {},
 eprint = {2410.11587v1},
 journal = {arXiv preprint},
 title = {Baseflow identification via explainable AI with Kolmogorov-Arnold networks},
 url = {http://arxiv.org/abs/2410.11587v1},
 year = {2024}
}

@article{2410.11896v1,
 abstract = {Explainable Artificial Intelligence (XAI) is essential for building advanced machine learning-powered applications, especially in critical domains such as medical diagnostics or autonomous driving. Legal, business, and ethical requirements motivate using effective XAI, but the increasing number of different methods makes it challenging to pick the right ones. Further, as explanations are highly context-dependent, measuring the effectiveness of XAI methods without users can only reveal a limited amount of information, excluding human factors such as the ability to understand it. We propose to evaluate XAI methods via the user's ability to successfully perform a proxy task, designed such that a good performance is an indicator for the explanation to provide helpful information. In other words, we address the helpfulness of XAI for human decision-making. Further, a user study on state-of-the-art methods was conducted, showing differences in their ability to generate trust and skepticism and the ability to judge the rightfulness of an AI decision correctly. Based on the results, we highly recommend using and extending this approach for more objective-based human-centered user studies to measure XAI performance in an end-to-end fashion.},
 author = {Tobias Labarta and Elizaveta Kulicheva and Ronja Froelian and Christian Geißler and Xenia Melman and Julian von Klitzing},
 comment = {World Conference on Explainable Artificial Intelligence},
 doi = {10.1007/978-3-031-63803-9_16},
 eprint = {2410.11896v1},
 journal = {arXiv preprint},
 title = {Study on the Helpfulness of Explainable Artificial Intelligence},
 url = {http://arxiv.org/abs/2410.11896v1},
 year = {2024}
}

@article{2410.11910v1,
 abstract = {Advancements in high-throughput technologies have led to a shift from traditional hypothesis-driven methodologies to data-driven approaches. Multi-omics refers to the integrative analysis of data derived from multiple 'omes', such as genomics, proteomics, transcriptomics, metabolomics, and microbiomics. This approach enables a comprehensive understanding of biological systems by capturing different layers of biological information. Deep learning methods are increasingly utilized to integrate multi-omics data, offering insights into molecular interactions and enhancing research into complex diseases. However, these models, with their numerous interconnected layers and nonlinear relationships, often function as black boxes, lacking transparency in decision-making processes. To overcome this challenge, explainable artificial intelligence (xAI) methods are crucial for creating transparent models that allow clinicians to interpret and work with complex data more effectively. This review explores how xAI can improve the interpretability of deep learning models in multi-omics research, highlighting its potential to provide clinicians with clear insights, thereby facilitating the effective application of such models in clinical settings.},
 author = {Ahmad Hussein and Mukesh Prasad and Ali Braytee},
 comment = {},
 doi = {},
 eprint = {2410.11910v1},
 journal = {arXiv preprint},
 title = {Explainable AI Methods for Multi-Omics Analysis: A Survey},
 url = {http://arxiv.org/abs/2410.11910v1},
 year = {2024}
}

@article{2410.12511v1,
 abstract = {The burgeoning field of Natural Language Processing (NLP) stands at a critical juncture where the integration of fairness within its frameworks has become an imperative. This PhD thesis addresses the need for equity and transparency in NLP systems, recognizing that fairness in NLP is not merely a technical challenge but a moral and ethical necessity, requiring a rigorous examination of how these technologies interact with and impact diverse human populations. Through this lens, this thesis undertakes a thorough investigation into the development of equitable NLP methodologies and the evaluation of biases that prevail in current systems.
  First, it introduces an innovative algorithm to mitigate biases in multi-class classifiers, tailored for high-risk NLP applications, surpassing traditional methods in both bias mitigation and prediction accuracy. Then, an analysis of the Bios dataset reveals the impact of dataset size on discriminatory biases and the limitations of standard fairness metrics. This awareness has led to explorations in the field of explainable AI, aiming for a more complete understanding of biases where traditional metrics are limited. Consequently, the thesis presents COCKATIEL, a model-agnostic explainability method that identifies and ranks concepts in Transformer models, outperforming previous approaches in sentiment analysis tasks. Finally, the thesis contributes to bridging the gap between fairness and explainability by introducing TaCo, a novel method to neutralize bias in Transformer model embeddings.
  In conclusion, this thesis constitutes a significant interdisciplinary endeavor that intertwines explicability and fairness to challenge and reshape current NLP paradigms. The methodologies and critiques presented contribute to the ongoing discourse on fairness in machine learning, offering actionable solutions for more equitable and responsible AI systems.},
 author = {Fanny Jourdan},
 comment = {PhD Thesis, Toulouse University},
 doi = {},
 eprint = {2410.12511v1},
 journal = {arXiv preprint},
 title = {Advancing Fairness in Natural Language Processing: From Traditional Methods to Explainability},
 url = {http://arxiv.org/abs/2410.12511v1},
 year = {2024}
}

@article{2410.12803v1,
 abstract = {Explainable Artificial Intelligence (XAI) techniques are used to provide transparency to complex, opaque predictive models. However, these techniques are often designed for image and text data, and it is unclear how fit-for-purpose they are when applied to tabular data. As XAI techniques are rarely evaluated in settings with tabular data, the applicability of existing evaluation criteria and methods are also unclear and needs (re-)examination. For example, some works suggest that evaluation methods may unduly influence the evaluation results when using tabular data. This lack of clarity on evaluation procedures can lead to reduced transparency and ineffective use of XAI techniques in real world settings. In this study, we examine literature on XAI evaluation to derive guidelines on functionally-grounded assessment of local, post hoc XAI techniques. We identify 20 evaluation criteria and associated evaluation methods, and derive guidelines on when and how each criterion should be evaluated. We also identify key research gaps to be addressed by future work. Our study contributes to the body of knowledge on XAI evaluation through in-depth examination of functionally-grounded XAI evaluation protocols, and has laid the groundwork for future research on XAI evaluation.},
 author = {Mythreyi Velmurugan and Chun Ouyang and Yue Xu and Renuka Sindhgatta and Bemali Wickramanayake and Catarina Moreira},
 comment = {},
 doi = {},
 eprint = {2410.12803v1},
 journal = {arXiv preprint},
 title = {Developing Guidelines for Functionally-Grounded Evaluation of Explainable Artificial Intelligence using Tabular Data},
 url = {http://arxiv.org/abs/2410.12803v1},
 year = {2024}
}

@article{2410.13190v2,
 abstract = {eXplainable Artificial Intelligence (XAI) has garnered significant attention for enhancing transparency and trust in machine learning models. However, the scopes of most existing explanation techniques focus either on offering a holistic view of the explainee model (global explanation) or on individual instances (local explanation), while the middle ground, i.e., cohort-based explanation, is less explored. Cohort explanations offer insights into the explainee's behavior on a specific group or cohort of instances, enabling a deeper understanding of model decisions within a defined context. In this paper, we discuss the unique challenges and opportunities associated with measuring cohort explanations, define their desired properties, and create a generalized framework for generating cohort explanations based on supervised clustering.},
 author = {Fanyu Meng and Xin Liu and Zhaodan Kong and Xin Chen},
 comment = {},
 doi = {},
 eprint = {2410.13190v2},
 journal = {arXiv preprint},
 title = {CohEx: A Generalized Framework for Cohort Explanation},
 url = {http://arxiv.org/abs/2410.13190v2},
 year = {2024}
}

@article{2410.13833v1,
 abstract = {While machine learning (ML) models have been able to achieve unprecedented accuracies across various prediction tasks in quantum chemistry, it is now apparent that accuracy on a test set alone is not a guarantee for robust chemical modeling such as stable molecular dynamics (MD). To go beyond accuracy, we use explainable artificial intelligence (XAI) techniques to develop a general analysis framework for atomic interactions and apply it to the SchNet and PaiNN neural network models. We compare these interactions with a set of fundamental chemical principles to understand how well the models have learned the underlying physicochemical concepts from the data. We focus on the strength of the interactions for different atomic species, how predictions for intensive and extensive quantum molecular properties are made, and analyze the decay and many-body nature of the interactions with interatomic distance. Models that deviate too far from known physical principles produce unstable MD trajectories, even when they have very high energy and force prediction accuracy. We also suggest further improvements to the ML architectures to better account for the polynomial decay of atomic interactions.},
 author = {Malte Esders and Thomas Schnake and Jonas Lederer and Adil Kabylda and Grégoire Montavon and Alexandre Tkatchenko and Klaus-Robert Müller},
 comment = {},
 doi = {},
 eprint = {2410.13833v1},
 journal = {arXiv preprint},
 title = {Analyzing Atomic Interactions in Molecules as Learned by Neural Networks},
 url = {http://arxiv.org/abs/2410.13833v1},
 year = {2024}
}

@article{2410.13883v1,
 abstract = {In recent years, interest in vision-language tasks has grown, especially those involving chart interactions. These tasks are inherently multimodal, requiring models to process chart images, accompanying text, underlying data tables, and often user queries. Traditionally, Chart Understanding (CU) relied on heuristics and rule-based systems. However, recent advancements that have integrated transformer architectures significantly improved performance. This paper reviews prominent research in CU, focusing on State-of-The-Art (SoTA) frameworks that employ transformers within End-to-End (E2E) solutions. Relevant benchmarking datasets and evaluation techniques are analyzed. Additionally, this article identifies key challenges and outlines promising future directions for advancing CU solutions. Following the PRISMA guidelines, a comprehensive literature search is conducted across Google Scholar, focusing on publications from Jan'20 to Jun'24. After rigorous screening and quality assessment, 32 studies are selected for in-depth analysis. The CU tasks are categorized into a three-layered paradigm based on the cognitive task required. Recent advancements in the frameworks addressing various CU tasks are also reviewed. Frameworks are categorized into single-task or multi-task based on the number of tasks solvable by the E2E solution. Within multi-task frameworks, pre-trained and prompt-engineering-based techniques are explored. This review overviews leading architectures, datasets, and pre-training tasks. Despite significant progress, challenges remain in OCR dependency, handling low-resolution images, and enhancing visual reasoning. Future directions include addressing these challenges, developing robust benchmarks, and optimizing model efficiency. Additionally, integrating explainable AI techniques and exploring the balance between real and synthetic data are crucial for advancing CU research.},
 author = {Mirna Al-Shetairy and Hanan Hindy and Dina Khattab and Mostafa M. Aref},
 comment = {},
 doi = {},
 eprint = {2410.13883v1},
 journal = {arXiv preprint},
 title = {Transformers Utilization in Chart Understanding: A Review of Recent Advances & Future Trends},
 url = {http://arxiv.org/abs/2410.13883v1},
 year = {2024}
}

@article{2410.14219v1,
 abstract = {Despite the practical success of Artificial Intelligence (AI), current neural AI algorithms face two significant issues. First, the decisions made by neural architectures are often prone to bias and brittleness. Second, when a chain of reasoning is required, neural systems often perform poorly. Neuro-symbolic artificial intelligence is a promising approach that tackles these (and other) weaknesses by combining the power of neural perception and symbolic reasoning. Meanwhile, the success of AI has made it critical to understand its behaviour, leading to the development of explainable artificial intelligence (XAI). While neuro-symbolic AI systems have important advantages over purely neural AI, we still need to explain their actions, which are obscured by the interactions of the neural and symbolic components. To address the issue, this paper proposes a formal approach to explaining the decisions of neuro-symbolic systems. The approach hinges on the use of formal abductive explanations and on solving the neuro-symbolic explainability problem hierarchically. Namely, it first computes a formal explanation for the symbolic component of the system, which serves to identify a subset of the individual parts of neural information that needs to be explained. This is followed by explaining only those individual neural inputs, independently of each other, which facilitates succinctness of hierarchical formal explanations and helps to increase the overall performance of the approach. Experimental results for a few complex reasoning tasks demonstrate practical efficiency of the proposed approach, in comparison to purely neural systems, from the perspective of explanation size, explanation time, training time, model sizes, and the quality of explanations reported.},
 author = {Sushmita Paul and Jinqiang Yu and Jip J. Dekker and Alexey Ignatiev and Peter J. Stuckey},
 comment = {},
 doi = {},
 eprint = {2410.14219v1},
 journal = {arXiv preprint},
 title = {Formal Explanations for Neuro-Symbolic AI},
 url = {http://arxiv.org/abs/2410.14219v1},
 year = {2024}
}

@article{2410.14760v2,
 abstract = {In an era increasingly focused on green computing and explainable AI, revisiting traditional approaches in theoretical and phenomenological particle physics is paramount. This project evaluates various machine learning (ML) algorithms-including Nearest Neighbors, Decision Trees, Random Forest, AdaBoost, Naive Bayes, Quadratic Discriminant Analysis (QDA), and XGBoost-alongside standard neural networks and a novel Physics-Informed Neural Network (PINN) for physics data analysis. We apply these techniques to a binary classification task that distinguishes the experimental viability of simulated scenarios based on Higgs observables and essential parameters. Through this comprehensive analysis, we aim to showcase the capabilities and computational efficiency of each model in binary classification tasks, thereby contributing to the ongoing discourse on integrating ML and Deep Neural Networks (DNNs) into physics research. In this study, XGBoost emerged as the preferred choice among the evaluated machine learning algorithms for its speed and effectiveness, especially in the initial stages of computation with limited datasets. However, while standard Neural Networks and Physics-Informed Neural Networks (PINNs) demonstrated superior performance in terms of accuracy and adherence to physical laws, they require more computational time. These findings underscore the trade-offs between computational efficiency and model sophistication.},
 author = {Vasileios Vatellis},
 comment = {7 page},
 doi = {},
 eprint = {2410.14760v2},
 journal = {arXiv preprint},
 title = {Advancing Physics Data Analysis through Machine Learning and Physics-Informed Neural Networks},
 url = {http://arxiv.org/abs/2410.14760v2},
 year = {2024}
}

@article{2410.15108v2,
 abstract = {The shape of the brain's white matter connections is relatively unexplored in diffusion MRI tractography analysis. While it is known that tract shape varies in populations and across the human lifespan, it is unknown if the variability in dMRI tractography-derived shape may relate to the brain's functional variability across individuals. This work explores the potential of leveraging tractography fiber cluster shape measures to predict subject-specific cognitive performance. We implement machine learning models to predict individual cognitive performance scores. We study a large-scale database from the HCP-YA study. We apply an atlas-based fiber cluster parcellation to the dMRI tractography of each individual. We compute 15 shape, microstructure, and connectivity features for each fiber cluster. Using these features as input, we train a total of 210 models to predict 7 different NIH Toolbox cognitive performance assessments. We apply an explainable AI technique, SHAP, to assess the importance of each fiber cluster for prediction. Our results demonstrate that shape measures are predictive of individual cognitive performance. The studied shape measures, such as irregularity, diameter, total surface area, volume, and branch volume, are as effective for prediction as microstructure and connectivity measures. The overall best-performing feature is a shape feature, irregularity, which describes how different a cluster's shape is from an idealized cylinder. Further interpretation using SHAP values suggest that fiber clusters with features highly predictive of cognitive ability are widespread throughout the brain, including fiber clusters from the superficial association, deep association, cerebellar, striatal, and projection pathways. This study demonstrates the strong potential of shape descriptors to enhance the study of the brain's white matter and its relationship to cognitive function.},
 author = {Yui Lo and Yuqian Chen and Dongnan Liu and Wan Liu and Leo Zekelman and Jarrett Rushmore and Fan Zhang and Yogesh Rathi and Nikos Makris and Alexandra J. Golby and Weidong Cai and Lauren J. O'Donnell},
 comment = {This work has been accepted by Human Brain Mapping for publication},
 doi = {10.1002/hbm.70166},
 eprint = {2410.15108v2},
 journal = {arXiv preprint},
 title = {The shape of the brain's connections is predictive of cognitive performance: an explainable machine learning study},
 url = {http://arxiv.org/abs/2410.15108v2},
 year = {2024}
}

@article{2410.15374v1,
 abstract = {In today's world, the significance of explainable AI (XAI) is growing in robotics and point cloud applications, as the lack of transparency in decision-making can pose considerable safety risks, particularly in autonomous systems. As these technologies are integrated into real-world environments, ensuring that model decisions are interpretable and trustworthy is vital for operational reliability and safety assurance. This study explores the implementation of SMILE, a novel explainability method originally designed for deep neural networks, on point cloud-based models. SMILE builds on LIME by incorporating Empirical Cumulative Distribution Function (ECDF) statistical distances, offering enhanced robustness and interpretability, particularly when the Anderson-Darling distance is used. The approach demonstrates superior performance in terms of fidelity loss, R2 scores, and robustness across various kernel widths, perturbation numbers, and clustering configurations. Moreover, this study introduces a stability analysis for point cloud data using the Jaccard index, establishing a new benchmark and baseline for model stability in this field. The study further identifies dataset biases in the classification of the 'person' category, emphasizing the necessity for more comprehensive datasets in safety-critical applications like autonomous driving and robotics. The results underscore the potential of advanced explainability models and highlight areas for future research, including the application of alternative surrogate models and explainability techniques in point cloud data.},
 author = {Seyed Mohammad Ahmadi and Koorosh Aslansefat and Ruben Valcarce-Dineiro and Joshua Barnfather},
 comment = {17 pages, 9 figures},
 doi = {},
 eprint = {2410.15374v1},
 journal = {arXiv preprint},
 title = {Explainability of Point Cloud Neural Networks Using SMILE: Statistical Model-Agnostic Interpretability with Local Explanations},
 url = {http://arxiv.org/abs/2410.15374v1},
 year = {2024}
}

@article{2410.15952v1,
 abstract = {This study is located in the Human-Centered Artificial Intelligence (HCAI) and focuses on the results of a user-centered assessment of commonly used eXplainable Artificial Intelligence (XAI) algorithms, specifically investigating how humans understand and interact with the explanations provided by these algorithms. To achieve this, we employed a multi-disciplinary approach that included state-of-the-art research methods from social sciences to measure the comprehensibility of explanations generated by a state-of-the-art lachine learning model, specifically the Gradient Boosting Classifier (XGBClassifier). We conducted an extensive empirical user study involving interviews with 39 participants from three different groups, each with varying expertise in data science, data visualization, and domain-specific knowledge related to the dataset used for training the machine learning model. Participants were asked a series of questions to assess their understanding of the model's explanations. To ensure replicability, we built the model using a publicly available dataset from the UC Irvine Machine Learning Repository, focusing on edible and non-edible mushrooms. Our findings reveal limitations in existing XAI methods and confirm the need for new design principles and evaluation techniques that address the specific information needs and user perspectives of different classes of AI stakeholders. We believe that the results of our research and the cross-disciplinary methodology we developed can be successfully adapted to various data types and user profiles, thus promoting dialogue and address opportunities in HCAI research. To support this, we are making the data resulting from our study publicly available.},
 author = {Szymon Bobek and Paloma Korycińska and Monika Krakowska and Maciej Mozolewski and Dorota Rak and Magdalena Zych and Magdalena Wójcik and Grzegorz J. Nalepa},
 comment = {},
 doi = {},
 eprint = {2410.15952v1},
 journal = {arXiv preprint},
 title = {User-centric evaluation of explainability of AI with and for humans: a comprehensive empirical study},
 url = {http://arxiv.org/abs/2410.15952v1},
 year = {2024}
}

@article{2410.16345v1,
 abstract = {While deep learning has been successfully applied to the data-driven classification of anomalous diffusion mechanisms, how the algorithm achieves the feat still remains a mystery. In this study, we use a well-known technique aimed at achieving explainable AI, namely the Gradient-weighted Class Activation Map (Grad-CAM), to investigate how deep learning (implemented by ResNets) recognizes the distinctive features of a particular anomalous diffusion model from the raw trajectory data. Our results show that Grad-CAM reveals the portions of the trajectory that hold crucial information about the underlying mechanism of anomalous diffusion, which can be utilized to enhance the robustness of the trained classifier against the measurement noise. Moreover, we observe that deep learning distills unique statistical characteristics of different diffusion mechanisms at various spatiotemporal scales, with larger-scale (smaller-scale) features identified at higher (lower) layers.},
 author = {Jaeyong Bae and Yongjoo Baek and Hawoong Jeong},
 comment = {14 pages, 12 figures},
 doi = {},
 eprint = {2410.16345v1},
 journal = {arXiv preprint},
 title = {Exploring how deep learning decodes anomalous diffusion via Grad-CAM},
 url = {http://arxiv.org/abs/2410.16345v1},
 year = {2024}
}

@article{2410.16537v1,
 abstract = {The impressive performance of deep learning models, particularly Convolutional Neural Networks (CNNs), is often hindered by their lack of interpretability, rendering them "black boxes." This opacity raises concerns in critical areas like healthcare, finance, and autonomous systems, where trust and accountability are crucial. This paper introduces the QIXAI Framework (Quantum-Inspired Explainable AI), a novel approach for enhancing neural network interpretability through quantum-inspired techniques. By utilizing principles from quantum mechanics, such as Hilbert spaces, superposition, entanglement, and eigenvalue decomposition, the QIXAI framework reveals how different layers of neural networks process and combine features to make decisions.
  We critically assess model-agnostic methods like SHAP and LIME, as well as techniques like Layer-wise Relevance Propagation (LRP), highlighting their limitations in providing a comprehensive view of neural network operations. The QIXAI framework overcomes these limitations by offering deeper insights into feature importance, inter-layer dependencies, and information propagation. A CNN for malaria parasite detection is used as a case study to demonstrate how quantum-inspired methods like Singular Value Decomposition (SVD), Principal Component Analysis (PCA), and Mutual Information (MI) provide interpretable explanations of model behavior. Additionally, we explore the extension of QIXAI to other architectures, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, Transformers, and Natural Language Processing (NLP) models, and its application to generative models and time-series analysis. The framework applies to both quantum and classical systems, demonstrating its potential to improve interpretability and transparency across a range of models, advancing the broader goal of developing trustworthy AI systems.},
 author = {John M. Willis},
 comment = {18 pages, 3 figures},
 doi = {},
 eprint = {2410.16537v1},
 journal = {arXiv preprint},
 title = {QIXAI: A Quantum-Inspired Framework for Enhancing Classical and Quantum Model Transparency and Understanding},
 url = {http://arxiv.org/abs/2410.16537v1},
 year = {2024}
}

@article{2410.18560v2,
 abstract = {Explainable Artificial Intelligence (XAI) methods in text summarization are essential for understanding the model behavior and fostering trust in model-generated summaries. Despite the effectiveness of XAI methods, recent studies have highlighted a key challenge in this area known as the "disagreement problem". This problem occurs when different XAI methods yield conflicting explanations for the same model outcome. Such discrepancies raise concerns about the consistency of explanations and reduce confidence in model interpretations, which is crucial for secure and accountable AI applications. This work is among the first to empirically investigate the disagreement problem in text summarization, demonstrating that such discrepancies are widespread in state-of-the-art summarization models. To address this gap, we propose Regional Explainable AI (RXAI) a novel segmentation-based approach, where each article is divided into smaller, coherent segments using sentence transformers and clustering. We use XAI methods on text segments to create localized explanations that help reduce disagreement between different XAI methods, thereby enhancing the trustworthiness of AI-generated summaries. Our results illustrate that the localized explanations are more consistent than full-text explanations. The proposed approach is validated using two benchmark summarization datasets, Extreme summarization (Xsum) and CNN/Daily Mail, indicating a substantial decrease in disagreement. Additionally, the interactive JavaScript visualization tool is developed to facilitate easy, color-coded exploration of attribution scores at the sentence level, enhancing user comprehension of model explanations.},
 author = {Seema Aswani and Sujala D. Shetty},
 comment = {This is a preprint version of the manuscript accepted for publication in the Machine Learning Journal (Springer Nature)},
 doi = {},
 eprint = {2410.18560v2},
 journal = {arXiv preprint},
 title = {"Let's Agree to Disagree": Investigating the Disagreement Problem in Explainable AI for Text Summarization},
 url = {http://arxiv.org/abs/2410.18560v2},
 year = {2024}
}

@article{2410.18725v2,
 abstract = {Artificial Intelligence is rapidly advancing and radically impacting everyday life, driven by the increasing availability of computing power. Despite this trend, the adoption of AI in real-world healthcare is still limited. One of the main reasons is the trustworthiness of AI models and the potential hesitation of domain experts with model predictions. Explainable Artificial Intelligence (XAI) techniques aim to address these issues. However, explainability can mean different things to people with different backgrounds, expertise, and goals. To address the target audience with diverse needs, we develop storytelling XAI. In this research, we have developed an approach that combines multi-task distillation with interpretability techniques to enable audience-centric explainability. Using multi-task distillation allows the model to exploit the relationships between tasks, potentially improving interpretability as each task supports the other leading to an enhanced interpretability from the perspective of a domain expert. The distillation process allows us to extend this research to large deep models that are highly complex. We focus on both model-agnostic and model-specific methods of interpretability, supported by textual justification of the results in healthcare through our use case. Our methods increase the trust of both the domain experts and the machine learning experts to enable a responsible AI.},
 author = {Akshat Dubey and Zewen Yang and Georges Hattab},
 comment = {Pre-print of the accepted manuscript in EXPLIMED - First Workshop on Explainable Artificial Intelligence for the Medical Domain, European Conference on Artificial Intelligence (ECAI) - 2024, Santiago de Compostela, Spain},
 doi = {},
 eprint = {2410.18725v2},
 journal = {arXiv preprint},
 title = {AI Readiness in Healthcare through Storytelling XAI},
 url = {http://arxiv.org/abs/2410.18725v2},
 year = {2024}
}

@article{2410.19739v1,
 abstract = {Introduction Schizophrenia is a severe mental disorder, and early diagnosis is key to improving outcomes. Its complexity makes predicting onset and progression challenging. EEG has emerged as a valuable tool for studying schizophrenia, with machine learning increasingly applied for diagnosis. This paper assesses the accuracy of ML models for predicting schizophrenia and examines the impact of stress during EEG recording on model performance. We integrate acute stress prediction into the analysis, showing that overlapping conditions like stress during recording can negatively affect model accuracy.
  Methods Four XGBoost models were built: one for stress prediction, two to classify schizophrenia (at rest and task), and a model to predict schizophrenia for both conditions. XAI techniques were applied to analyze results. Experiments tested the generalization of schizophrenia models using their datasets' healthy controls and independent health-screened controls. The stress model identified high-stress subjects, who were excluded from further analysis. A novel method was used to adjust EEG frequency band power to remove stress artifacts, improving predictive model performance.
  Results Our results show that acute stress vary across EEG sessions, affecting model performance and accuracy. Generalization improved once these varying stress levels were considered and compensated for during model training. Our findings highlight the importance of thorough health screening and management of the patient's condition during the process. Stress induced during or by the EEG recording can adversely affect model generalization. This may require further preprocessing of data by treating stress as an additional physiological artifact. Our proposed approach to compensate for stress artifacts in EEG data used for training models showed a significant improvement in predictive performance.},
 author = {Gideon Vos and Maryam Ebrahimpour and Liza van Eijk and Zoltan Sarnyai and Mostafa Rahimi Azghadi},
 comment = {20 pages, 7 figures},
 doi = {},
 eprint = {2410.19739v1},
 journal = {arXiv preprint},
 title = {The Effect of Acute Stress on the Interpretability and Generalization of Schizophrenia Predictive Machine Learning Models},
 url = {http://arxiv.org/abs/2410.19739v1},
 year = {2024}
}

@article{2410.19856v1,
 abstract = {Prototype-based methods are intrinsically interpretable XAI methods that produce predictions and explanations by comparing input data with a set of learned prototypical examples that are representative of the training data. In this work, we discuss a series of developments in the field of prototype-based XAI that show potential for scientific learning tasks, with a focus on the geosciences. We organize the prototype-based XAI literature into three themes: the development and visualization of prototypes, types of prototypes, and the use of prototypes in various learning tasks. We discuss how the authors use prototype-based methods, their novel contributions, and any limitations or challenges that may arise when adapting these methods for geoscientific learning tasks. We highlight differences between geoscientific data sets and the standard benchmarks used to develop XAI methods, and discuss how specific geoscientific applications may benefit from using or modifying existing prototype-based XAI techniques.},
 author = {Anushka Narayanan and Karianne J. Bergen},
 comment = {Accepted at AI for Science Workshop-Oral (Attention Track), Proceedings of 41st International Conference on Machine Learning (ICML) 2024},
 doi = {},
 eprint = {2410.19856v1},
 journal = {arXiv preprint},
 title = {Prototype-Based Methods in Explainable AI and Emerging Opportunities in the Geosciences},
 url = {http://arxiv.org/abs/2410.19856v1},
 year = {2024}
}

@article{2410.20539v1,
 abstract = {As the demand for interpretable machine learning approaches continues to grow, there is an increasing necessity for human involvement in providing informative explanations for model decisions. This is necessary for building trust and transparency in AI-based systems, leading to the emergence of the Explainable Artificial Intelligence (XAI) field. Recently, a novel counterfactual explanation model, CELS, has been introduced. CELS learns a saliency map for the interest of an instance and generates a counterfactual explanation guided by the learned saliency map. While CELS represents the first attempt to exploit learned saliency maps not only to provide intuitive explanations for the reason behind the decision made by the time series classifier but also to explore post hoc counterfactual explanations, it exhibits limitations in terms of high validity for the sake of ensuring high proximity and sparsity. In this paper, we present an enhanced approach that builds upon CELS. While the original model achieved promising results in terms of sparsity and proximity, it faced limitations in validity. Our proposed method addresses this limitation by removing mask normalization to provide more informative and valid counterfactual explanations. Through extensive experimentation on datasets from various domains, we demonstrate that our approach outperforms the CELS model, achieving higher validity and producing more informative explanations.},
 author = {Peiyu Li and Omar Bahri and Pouya Hosseinzadeh and Soukaïna Filali Boubrahimi and Shah Muhammad Hamdi},
 comment = {},
 doi = {},
 eprint = {2410.20539v1},
 journal = {arXiv preprint},
 title = {Info-CELS: Informative Saliency Map Guided Counterfactual Explanation},
 url = {http://arxiv.org/abs/2410.20539v1},
 year = {2024}
}

@article{2410.21131v3,
 abstract = {As machine learning models evolve, maintaining transparency demands more human-centric explainable AI techniques. Counterfactual explanations, with roots in human reasoning, identify the minimal input changes needed to obtain a given output and, hence, are crucial for supporting decision-making. Despite their importance, the evaluation of these explanations often lacks grounding in user studies and remains fragmented, with existing metrics not fully capturing human perspectives. To address this challenge, we developed a diverse set of 30 counterfactual scenarios and collected ratings across 8 evaluation metrics from 206 respondents. Subsequently, we fine-tuned different Large Language Models (LLMs) to predict average or individual human judgment across these metrics. Our methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot evaluations and 85% (over a 3-classes prediction) with fine-tuning across all metrics. The fine-tuned models predicting human ratings offer better comparability and scalability in evaluating different counterfactual explanation frameworks.},
 author = {Marharyta Domnich and Julius Välja and Rasmus Moorits Veski and Giacomo Magnifico and Kadi Tulver and Eduard Barbu and Raul Vicente},
 comment = {This paper extends the AAAI-2025 version by including the Appendix},
 doi = {10.1609/aaai.v39i15.33791},
 eprint = {2410.21131v3},
 journal = {arXiv preprint},
 title = {Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments},
 url = {http://arxiv.org/abs/2410.21131v3},
 year = {2024}
}

@article{2410.21298v1,
 abstract = {This study employs explainable artificial intelligence (XAI) techniques to analyze the behavior of asphalt concrete with varying aggregate gradations, focusing on resilience modulus (MR) and dynamic stability (DS) as measured by wheel track tests. The research utilizes a deep learning model with a multi-layer perceptron architecture to predict MR and DS based on aggregate gradation parameters derived from Bailey's Method, including coarse aggregate ratio (CA), fine aggregate coarse ratio (FAc), and other mix design variables. The model's performance was validated using k-fold cross-validation, demonstrating superior accuracy compared to alternative machine learning approaches. SHAP (SHapley Additive exPlanations) values were applied to interpret the model's predictions, providing insights into the relative importance and impact of different gradation characteristics on asphalt concrete performance. Key findings include the identification of critical aggregate size thresholds, particularly the 0.6 mm sieve size, which significantly influences both MR and DS. The study revealed size-dependent performance of aggregates, with coarse aggregates primarily affecting rutting resistance and medium-fine aggregates influencing stiffness. The research also highlighted the importance of aggregate lithology in determining rutting resistance. To facilitate practical application, web-based interfaces were developed for predicting MR and DS, incorporating explainable features to enhance transparency and interpretation of results. This research contributes a data-driven approach to understanding the complex relationships between aggregate gradation and asphalt concrete performance, potentially informing more efficient and performance-oriented mix design processes in the future.},
 author = {Warat Kongkitkul and Sompote Youwai and Siwipa Khamsoy and Manaswee Feungfung},
 comment = {The link to web app https://huggingface.co/spaces/Sompote/MRpredict https://huggingface.co/spaces/Sompote/Dynamic.stability},
 doi = {},
 eprint = {2410.21298v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligent (XAI) for Predicting Asphalt Concrete Stiffness and Rutting Resistance: Integrating Bailey's Aggregate Gradation Method},
 url = {http://arxiv.org/abs/2410.21298v1},
 year = {2024}
}

@article{2410.21815v2,
 abstract = {The debate between self-interpretable models and post-hoc explanations for black-box models is central to Explainable AI (XAI). Self-interpretable models, such as concept-based networks, offer insights by connecting decisions to human-understandable concepts but often struggle with performance and scalability. Conversely, post-hoc methods like Shapley values, while theoretically robust, are computationally expensive and resource-intensive. To bridge the gap between these two lines of research, we propose a novel method that combines their strengths, providing theoretically guaranteed self-interpretability for black-box models without compromising prediction accuracy. Specifically, we introduce a parameter-efficient pipeline, AutoGnothi, which integrates a small side network into the black-box model, allowing it to generate Shapley value explanations without changing the original network parameters. This side-tuning approach significantly reduces memory, training, and inference costs, outperforming traditional parameter-efficient methods, where full fine-tuning serves as the optimal baseline. AutoGnothi enables the black-box model to predict and explain its predictions with minimal overhead. Extensive experiments show that AutoGnothi offers accurate explanations for both vision and language tasks, delivering superior computational efficiency with comparable interpretability.},
 author = {Shaobo Wang and Hongxuan Tang and Mingyang Wang and Hongrui Zhang and Xuyang Liu and Weiya Li and Xuming Hu and Linfeng Zhang},
 comment = {Accepted by ICLR 2025, 29 pages, 13 figures},
 doi = {},
 eprint = {2410.21815v2},
 journal = {arXiv preprint},
 title = {Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers},
 url = {http://arxiv.org/abs/2410.21815v2},
 year = {2024}
}

@article{2410.21928v1,
 abstract = {Current trends in Machine Learning prefer explainability even when it comes at the cost of performance. Therefore, explainable AI methods are particularly important in the field of Fraud Detection. This work investigates the applicability of Differentiable Inductive Logic Programming (DILP) as an explainable AI approach to Fraud Detection. Although the scalability of DILP is a well-known issue, we show that with some data curation such as cleaning and adjusting the tabular and numerical data to the expected format of background facts statements, it becomes much more applicable. While in processing it does not provide any significant advantage on rather more traditional methods such as Decision Trees, or more recent ones like Deep Symbolic Classification, it still gives comparable results. We showcase its limitations and points to improve, as well as potential use cases where it can be much more useful compared to traditional methods, such as recursive rule learning.},
 author = {Boris Wolfson and Erman Acar},
 comment = {},
 doi = {},
 eprint = {2410.21928v1},
 journal = {arXiv preprint},
 title = {Differentiable Inductive Logic Programming for Fraud Detection},
 url = {http://arxiv.org/abs/2410.21928v1},
 year = {2024}
}

@article{2410.22390v1,
 abstract = {The widespread and diverse online media platforms and other internet-driven communication technologies have presented significant challenges in defining the boundaries of freedom of expression. Consequently, the internet has been transformed into a potential cyber weapon. Within this evolving landscape, two particularly hazardous phenomena have emerged: fake news and doxxing. Although these threats have been subjects of extensive scholarly analysis, the crossroads where they intersect remain unexplored. This research addresses this convergence by introducing a novel system. The Fake News and Doxxing Detection with Explainable Artificial Intelligence (FNDEX) system leverages the capabilities of three distinct transformer models to achieve high-performance detection for both fake news and doxxing. To enhance data security, a rigorous three-step anonymization process is employed, rooted in a pattern-based approach for anonymizing personally identifiable information. Finally, this research emphasizes the importance of generating coherent explanations for the outcomes produced by both detection models. Our experiments on realistic datasets demonstrate that our system significantly outperforms the existing baselines},
 author = {Dorsaf Sallami and Esma Aïmeur},
 comment = {},
 doi = {},
 eprint = {2410.22390v1},
 journal = {arXiv preprint},
 title = {FNDEX: Fake News and Doxxing Detection with Explainable AI},
 url = {http://arxiv.org/abs/2410.22390v1},
 year = {2024}
}

@article{2410.23101v2,
 abstract = {Procedurally generated levels created by machine learning models can be unsolvable without further editing. Various methods have been developed to automatically repair these levels by enforcing hard constraints during the post-processing step. However, as levels increase in size, these constraint-based repairs become increasingly slow. This paper proposes using explainability methods to identify specific regions of a level that contribute to its unsolvability. By assigning higher weights to these regions, constraint-based solvers can prioritize these problematic areas, enabling more efficient repairs. Our results, tested across three games, demonstrate that this approach can help to repair procedurally generated levels faster.},
 author = {Mahsa Bazzaz and Seth Cooper},
 comment = {},
 doi = {},
 eprint = {2410.23101v2},
 journal = {arXiv preprint},
 title = {Guided Game Level Repair via Explainable AI},
 url = {http://arxiv.org/abs/2410.23101v2},
 year = {2024}
}

@article{2411.00254v1,
 abstract = {Clinical diagnosis of breast malignancy (BM) is a challenging problem in the recent era. In particular, Deep learning (DL) models have continued to offer important solutions for early BM diagnosis but their performance experiences overfitting due to the limited volume of breast ultrasound (BUS) image data. Further, large BUS datasets are difficult to manage due to privacy and legal concerns. Hence, image augmentation is a necessary and challenging step to improve the performance of the DL models. However, the current DL-based augmentation models are inadequate and operate as a black box resulting lack of information and justifications about their suitability and efficacy. Additionally, pre and post-augmentation need high-performance computational resources and time to produce the augmented image and evaluate the model performance. Thus, this study aims to develop a novel efficient augmentation approach for BUS images with advanced neural style transfer (NST) and Explainable AI (XAI) harnessing GPU-based parallel infrastructure. We scale and distribute the training of the augmentation model across 8 GPUs using the Horovod framework on a DGX cluster, achieving a 5.09 speedup while maintaining the model's accuracy. The proposed model is evaluated on 800 (348 benign and 452 malignant) BUS images and its performance is analyzed with other progressive techniques, using different quantitative analyses. The result indicates that the proposed approach can successfully augment the BUS images with 92.47% accuracy.},
 author = {Lipismita Panigrahi and Prianka Rani Saha and Jurdana Masuma Iqrah and Sushil Prasad},
 comment = {},
 doi = {},
 eprint = {2411.00254v1},
 journal = {arXiv preprint},
 title = {A Novel Breast Ultrasound Image Augmentation Method Using Advanced Neural Style Transfer: An Efficient and Explainable Approach},
 url = {http://arxiv.org/abs/2411.00254v1},
 year = {2024}
}

@article{2411.00426v1,
 abstract = {Accurate prediction of Global Warming Potential (GWP) is essential for assessing the environmental impact of chemical processes and materials. Traditional GWP prediction models rely predominantly on molecular structure, overlooking critical process-related information. In this study, we present an integrative GWP prediction model that combines molecular descriptors (MACCS keys and Mordred descriptors) with process information (process title, description, and location) to improve predictive accuracy and interpretability. Using a deep neural network (DNN) model, we achieved an R-squared of 86% on test data with Mordred descriptors, process location, and description information, representing a 25% improvement over the previous benchmark of 61%; XAI analysis further highlighted the significant role of process title embeddings in enhancing model predictions. To enhance interpretability, we employed a Kolmogorov-Arnold Network (KAN) to derive a symbolic formula for GWP prediction, capturing key molecular and process features and providing a transparent, interpretable alternative to black-box models, enabling users to gain insights into the molecular and process factors influencing GWP. Error analysis showed that the model performs reliably in densely populated data ranges, with increased uncertainty for higher GWP values. This analysis allows users to manage prediction uncertainty effectively, supporting data-driven decision-making in chemical and process design. Our results suggest that integrating both molecular and process-level information in GWP prediction models yields substantial gains in accuracy and interpretability, offering a valuable tool for sustainability assessments. Future work may extend this approach to additional environmental impact categories and refine the model to further enhance its predictive reliability.},
 author = {Jaewook Lee and Xinyang Sun and Ethan Errington and Miao Guo},
 comment = {},
 doi = {},
 eprint = {2411.00426v1},
 journal = {arXiv preprint},
 title = {A KAN-based Interpretable Framework for Process-Informed Prediction of Global Warming Potential},
 url = {http://arxiv.org/abs/2411.00426v1},
 year = {2024}
}

@article{2411.00653v1,
 abstract = {Understanding node representations in graph-based models is crucial for uncovering biases ,diagnosing errors, and building trust in model decisions. However, previous work on explainable AI for node representations has primarily emphasized explanations (reasons for model predictions) rather than interpretations (mapping representations to understandable concepts). Furthermore, the limited research that focuses on interpretation lacks validation, and thus the reliability of such methods is unclear. We address this gap by proposing a novel interpretation method-Node Coherence Rate for Representation Interpretation (NCI)-which quantifies how well different node relations are captured in node representations. We also propose a novel method (IME) to evaluate the accuracy of different interpretation methods. Our experimental results demonstrate that NCI reduces the error of the previous best approach by an average of 39%. We then apply NCI to derive insights about the node representations produced by several graph-based methods and assess their quality in unsupervised settings.},
 author = {Ying-Chun Lin and Jennifer Neville and Cassiano Becker and Purvanshi Metha and Nabiha Asghar and Vipul Agarwal},
 comment = {},
 doi = {},
 eprint = {2411.00653v1},
 journal = {arXiv preprint},
 title = {Rethinking Node Representation Interpretation through Relation Coherence},
 url = {http://arxiv.org/abs/2411.00653v1},
 year = {2024}
}

@article{2411.00684v1,
 abstract = {Deep Learning methods are notorious for relying on extensive labeled datasets to train and assess their performance. This can cause difficulties in practical situations where models should be trained for new applications for which very little data is available. While few-shot learning algorithms can address the first problem, they still lack sufficient explanations for the results. This research presents a workflow that tackles both challenges by proposing an explainable few-shot learning workflow for detecting invasive and exotic tree species in the Atlantic Forest of Brazil using Unmanned Aerial Vehicle (UAV) images. By integrating a Siamese network with explainable AI (XAI), the workflow enables the classification of tree species with minimal labeled data while providing visual, case-based explanations for the predictions. Results demonstrate the effectiveness of the proposed workflow in identifying new tree species, even in data-scarce conditions. With a lightweight backbone, e.g., MobileNet, it achieves a F1-score of 0.86 in 3-shot learning, outperforming a shallow CNN. A set of explanation metrics, i.e., correctness, continuity, and contrastivity, accompanied by visual cases, provide further insights about the prediction results. This approach opens new avenues for using AI and UAVs in forest management and biodiversity conservation, particularly concerning rare or under-studied species.},
 author = {Caroline M. Gevaert and Alexandra Aguiar Pedro and Ou Ku and Hao Cheng and Pranav Chandramouli and Farzaneh Dadrass Javan and Francesco Nattino and Sonja Georgievska},
 comment = {},
 doi = {},
 eprint = {2411.00684v1},
 journal = {arXiv preprint},
 title = {Explainable few-shot learning workflow for detecting invasive and exotic tree species},
 url = {http://arxiv.org/abs/2411.00684v1},
 year = {2024}
}

@article{2411.00846v1,
 abstract = {Explainable Artificial Intelligence (XAI) emerged to reveal the internal mechanism of machine learning models and how the features affect the prediction outcome. Collinearity is one of the big issues that XAI methods face when identifying the most informative features in the model. Current XAI approaches assume the features in the models are independent and calculate the effect of each feature toward model prediction independently from the rest of the features. However, such assumption is not realistic in real life applications. We propose an Additive Effects of Collinearity (AEC) as a novel XAI method that aim to considers the collinearity issue when it models the effect of each feature in the model on the outcome. AEC is based on the idea of dividing multivariate models into several univariate models in order to examine their impact on each other and consequently on the outcome. The proposed method is implemented using simulated and real data to validate its efficiency comparing with the a state of arts XAI method. The results indicate that AEC is more robust and stable against the impact of collinearity when it explains AI models compared with the state of arts XAI method.},
 author = {Ahmed M Salih},
 comment = {Accepted for publication at 8th International Conference on Advances in Artificial Intelligence (ICAAI 2024)},
 doi = {},
 eprint = {2411.00846v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence for Dependent Features: Additive Effects of Collinearity},
 url = {http://arxiv.org/abs/2411.00846v1},
 year = {2024}
}

@article{2411.01140v3,
 abstract = {Federated Learning (FL) has become a key method for preserving data privacy in Internet of Things (IoT) environments, as it trains Machine Learning (ML) models locally while transmitting only model updates. Despite this design, FL remains susceptible to threats such as model inversion and membership inference attacks, which can reveal private training data. Differential Privacy (DP) techniques are often introduced to mitigate these risks, but simply injecting DP noise into black-box ML models can compromise accuracy, particularly in dynamic IoT contexts, where continuous, lifelong learning leads to excessive noise accumulation. To address this challenge, we propose Federated HyperDimensional computing with Privacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI) framework that integrates neuro-symbolic computing and DP. Unlike conventional approaches, FedHDPrivacy actively monitors the cumulative noise across learning rounds and adds only the additional noise required to satisfy privacy constraints. In a real-world application for monitoring manufacturing machining processes, FedHDPrivacy maintains high performance while surpassing standard FL frameworks - Federated Averaging (FedAvg), Federated Proximal (FedProx), Federated Normalized Averaging (FedNova), and Federated Optimization (FedOpt) - by up to 37%. Looking ahead, FedHDPrivacy offers a promising avenue for further enhancements, such as incorporating multimodal data fusion.},
 author = {Fardin Jalil Piran and Zhiling Chen and Mohsen Imani and Farhad Imani},
 comment = {31 Pages, 14 Figures},
 doi = {10.1016/j.compeleceng.2025.110261},
 eprint = {2411.01140v3},
 journal = {arXiv preprint},
 title = {Privacy-Preserving Federated Learning with Differentially Private Hyperdimensional Computing},
 url = {http://arxiv.org/abs/2411.01140v3},
 year = {2024}
}

@article{2411.01332v4,
 abstract = {Despite significant advancements in XAI, scholars continue to note a persistent lack of robust conceptual foundations and integration with broader discourse on scientific explanation. In response, emerging XAI research increasingly draws on explanatory strategies from various scientific disciplines and the philosophy of science to address these gaps. This paper outlines a mechanistic strategy for explaining the functional organization of deep learning systems, situating recent developments in AI explainability within a broader philosophical context. According to the mechanistic approach, explaining opaque AI systems involves identifying the mechanisms underlying decision-making processes. For deep neural networks, this means discerning functionally relevant components - such as neurons, layers, circuits, or activation patterns - and understanding their roles through decomposition, localization, and recomposition. Proof-of-principle case studies from image recognition and language modeling align this theoretical framework with recent research from OpenAI and Anthropic. The findings suggest that pursuing mechanistic explanations can uncover elements that traditional explainability techniques may overlook, ultimately contributing to more thoroughly explainable AI.},
 author = {Marcin Rabiza},
 comment = {Forthcoming in Müller, V. C., Dung, L., Löhr, G., & Rumana, A. (Eds.). Philosophy of Artificial Intelligence: The State of the Art, Synthese Library, Springer Nature. Please cite the published version},
 doi = {},
 eprint = {2411.01332v4},
 journal = {arXiv preprint},
 title = {A Mechanistic Explanatory Strategy for XAI},
 url = {http://arxiv.org/abs/2411.01332v4},
 year = {2024}
}

@article{2411.02168v2,
 abstract = {Deep neural networks (DNNs) achieve state-of-the-art performance on many tasks, but this often requires increasingly larger model sizes, which in turn leads to more complex internal representations. Explainability techniques (XAI) have made remarkable progress in the interpretability of ML models. However, the non-relational nature of Graph neural networks (GNNs) make it difficult to reuse already existing XAI methods. While other works have focused on instance-based explanation methods for GNNs, very few have investigated model-based methods and, to our knowledge, none have tried to probe the embedding of the GNNs for well-known structural graph properties. In this paper we present a model agnostic explainability pipeline for GNNs employing diagnostic classifiers. This pipeline aims to probe and interpret the learned representations in GNNs across various architectures and datasets, refining our understanding and trust in these models.},
 author = {Tom Pelletreau-Duris and Ruud van Bakel and Michael Cochez},
 comment = {10 pages, 22 figures, conference},
 doi = {},
 eprint = {2411.02168v2},
 journal = {arXiv preprint},
 title = {Do graph neural network states contain graph properties?},
 url = {http://arxiv.org/abs/2411.02168v2},
 year = {2024}
}

@article{2411.02419v2,
 abstract = {This paper introduces a dataset that is the result of a user study on the comprehensibility of explainable artificial intelligence (XAI) algorithms. The study participants were recruited from 149 candidates to form three groups representing experts in the domain of mycology (DE), students with a data science and visualization background (IT) and students from social sciences and humanities (SSH). The main part of the dataset contains 39 transcripts of interviews during which participants were asked to complete a series of tasks and questions related to the interpretation of explanations of decisions of a machine learning model trained to distinguish between edible and inedible mushrooms. The transcripts were complemented with additional data that includes visualizations of explanations presented to the user, results from thematic analysis, recommendations of improvements of explanations provided by the participants, and the initial survey results that allow to determine the domain knowledge of the participant and data analysis literacy. The transcripts were manually tagged to allow for automatic matching between the text and other data related to particular fragments. In the advent of the area of rapid development of XAI techniques, the need for a multidisciplinary qualitative evaluation of explainability is one of the emerging topics in the community. Our dataset allows not only to reproduce the study we conducted, but also to open a wide range of possibilities for the analysis of the material we gathered.},
 author = {Szymon Bobek and Paloma Korycińska and Monika Krakowska and Maciej Mozolewski and Dorota Rak and Magdalena Zych and Magdalena Wójcik and Grzegorz J. Nalepa},
 comment = {},
 doi = {10.1038/s41597-025-05167-6},
 eprint = {2411.02419v2},
 journal = {arXiv preprint},
 title = {Dataset resulting from the user study on comprehensibility of explainable AI algorithms},
 url = {http://arxiv.org/abs/2411.02419v2},
 year = {2024}
}

@article{2411.02540v3,
 abstract = {Graph Neural Networks (GNNs) are a powerful technique for machine learning on graph-structured data, yet they pose challenges in interpretability. Existing GNN explanation methods usually yield technical outputs, such as subgraphs and feature importance scores, that are difficult for non-data scientists to understand and thereby violate the purpose of explanations. Motivated by recent Explainable AI (XAI) research, we propose GraphXAIN, a method that generates natural language narratives explaining GNN predictions. GraphXAIN is a model- and explainer-agnostic method that uses Large Language Models (LLMs) to translate explanatory subgraphs and feature importance scores into coherent, story-like explanations of GNN decision-making processes. Evaluations on real-world datasets demonstrate GraphXAIN's ability to improve graph explanations. A survey of machine learning researchers and practitioners reveals that GraphXAIN enhances four explainability dimensions: understandability, satisfaction, convincingness, and suitability for communicating model predictions. When combined with another graph explainer method, GraphXAIN further improves trustworthiness, insightfulness, confidence, and usability. Notably, 95% of participants found GraphXAIN to be a valuable addition to the GNN explanation method. By incorporating natural language narratives, our approach serves both graph practitioners and non-expert users by providing clearer and more effective explanations.},
 author = {Mateusz Cedro and David Martens},
 comment = {19 pages, 9 figures, 2 tables},
 doi = {},
 eprint = {2411.02540v3},
 journal = {arXiv preprint},
 title = {GraphXAIN: Narratives to Explain Graph Neural Networks},
 url = {http://arxiv.org/abs/2411.02540v3},
 year = {2024}
}

@article{2411.02670v1,
 abstract = {Intrusion detection has been a commonly adopted detective security measures to safeguard systems and networks from various threats. A robust intrusion detection system (IDS) can essentially mitigate threats by providing alerts. In networks based IDS, typically we deal with cyber threats like distributed denial of service (DDoS), spoofing, reconnaissance, brute-force, botnets, and so on. In order to detect these threats various machine learning (ML) and deep learning (DL) models have been proposed. However, one of the key challenges with these predictive approaches is the presence of false positive (FP) and false negative (FN) instances. This FPs and FNs within any black-box intrusion detection system (IDS) make the decision-making task of an analyst further complicated. In this paper, we propose an explainable artificial intelligence (XAI) based visual analysis approach using overlapping SHAP plots that presents the feature explanation to identify potential false positive and false negatives in IDS. Our approach can further provide guidance to security analysts for effective decision-making. We present case study with multiple publicly available network traffic datasets to showcase the efficacy of our approach for identifying false positive and false negative instances. Our use-case scenarios provide clear guidance for analysts on how to use the visual analysis approach for reliable course-of-actions against such threats.},
 author = {Maraz Mia and Mir Mehedi A. Pritom and Tariqul Islam and Kamrul Hasan},
 comment = {10 pages, 14 figures, accepted in the MLC Workshop of the International Conference on Data Mining Conference (ICDM 2024)},
 doi = {},
 eprint = {2411.02670v1},
 journal = {arXiv preprint},
 title = {Visually Analyze SHAP Plots to Diagnose Misclassifications in ML-based Intrusion Detection},
 url = {http://arxiv.org/abs/2411.02670v1},
 year = {2024}
}

@article{2411.02746v1,
 abstract = {Most methods in explainable AI (XAI) focus on providing reasons for the prediction of a given set of features. However, we solve an inverse explanation problem, i.e., given the deviation of a label, find the reasons of this deviation. We use a Bayesian framework to recover the ``true'' features, conditioned on the observed label value. We efficiently explain the deviation of a label value from the mode, by identifying and ranking the influential features using the ``distances'' in the ANOVA functional decomposition. We show that the new method is more human-intuitive and robust than methods based on mean values, e.g., SHapley Additive exPlanations (SHAP values). The extra costs of solving a Bayesian inverse problem are dimension-independent.},
 author = {Quan Long},
 comment = {},
 doi = {},
 eprint = {2411.02746v1},
 journal = {arXiv preprint},
 title = {A Bayesian explanation of machine learning models based on modes and functional ANOVA},
 url = {http://arxiv.org/abs/2411.02746v1},
 year = {2024}
}

@article{2411.04316v1,
 abstract = {South Africa and the Democratic Republic of Congo (DRC) present a complex linguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French, English, and Tshiluba (Ciluba), which creates unique challenges for AI-driven translation and sentiment analysis systems due to a lack of accurately labeled data. This study seeks to address these challenges by developing a multilingual lexicon designed for French and Tshiluba, now expanded to include translations in English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural relevance in sentiment classification by integrating language-specific sentiment scores. A comprehensive testing corpus is created to support translation and sentiment analysis tasks, with machine learning models such as Random Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive Bayes (GNB) trained to predict sentiment across low resource languages (LRLs). Among them, the Random Forest model performed particularly well, capturing sentiment polarity and handling language-specific nuances effectively. Furthermore, Bidirectional Encoder Representations from Transformers (BERT), a Large Language Model (LLM), is applied to predict context-based sentiment with high accuracy, achieving 99% accuracy and 98% precision, outperforming other models. The BERT predictions were clarified using Explainable AI (XAI), improving transparency and fostering confidence in sentiment classification. Overall, findings demonstrate that the proposed lexicon and machine learning models significantly enhance translation and sentiment analysis for LRLs in South Africa and the DRC, laying a foundation for future AI models that support underrepresented languages, with applications across education, governance, and business in multilingual contexts.},
 author = {Melusi Malinga and Isaac Lupanda and Mike Wa Nkongolo and Phil van Deventer},
 comment = {This work is part of a PhD proposal in Information Technology at the University of Pretoria, supervised by Dr. Mike Wa Nkongolo and co-supervised by Dr. Phil van Deventer, under the Low-Resource Language Processing Lab in the Department of Informatics},
 doi = {},
 eprint = {2411.04316v1},
 journal = {arXiv preprint},
 title = {A Multilingual Sentiment Lexicon for Low-Resource Language Translation using Large Languages Models and Explainable AI},
 url = {http://arxiv.org/abs/2411.04316v1},
 year = {2024}
}

@article{2411.04855v3,
 abstract = {Explainable AI (XAI) holds the promise of advancing the implementation and adoption of AI-based tools in practice, especially in high-stakes environments like healthcare. However, most of the current research lacks input from end users, and therefore their practical value is limited. To address this, we conducted semi-structured interviews with clinicians to discuss their thoughts, hopes, and concerns. Clinicians from our sample generally think positively about developing AI-based tools for clinical practice, but they have concerns about how these will fit into their workflow and how it will impact clinician-patient relations. We further identify training of clinicians on AI as a crucial factor for the success of AI in healthcare and highlight aspects clinicians are looking for in (X)AI-based tools. In contrast to other studies, we take on a holistic and exploratory perspective to identify general requirements for (X)AI products for healthcare before moving on to testing specific tools.},
 author = {T. E. Röber and R. Goedhart and S. İ. Birbil},
 comment = {},
 doi = {},
 eprint = {2411.04855v3},
 journal = {arXiv preprint},
 title = {Clinicians' Voice: Fundamental Considerations for XAI in Healthcare},
 url = {http://arxiv.org/abs/2411.04855v3},
 year = {2024}
}

@article{2411.05196v2,
 abstract = {In democratic societies, electoral systems play a crucial role in translating public preferences into political representation. Among these, the D'Hondt method is widely used to ensure proportional representation, balancing fair representation with governmental stability. Recently, there has been a growing interest in applying similar principles of proportional representation to enhance interpretability in machine learning, specifically in Explainable AI (XAI). This study investigates the integration of D'Hondt-based voting principles in the DhondtXAI method, which leverages resource allocation concepts to interpret feature importance within AI models. Through a comparison of SHAP (Shapley Additive Explanations) and DhondtXAI, we evaluate their effectiveness in feature attribution within CatBoost and XGBoost models for breast cancer and diabetes prediction, respectively. The DhondtXAI approach allows for alliance formation and thresholding to enhance interpretability, representing feature importance as seats in a parliamentary view. Statistical correlation analyses between SHAP values and DhondtXAI allocations support the consistency of interpretations, demonstrating DhondtXAI's potential as a complementary tool for understanding feature importance in AI models. The results highlight that integrating electoral principles, such as proportional representation and alliances, into AI explainability can improve user understanding, especially in high-stakes fields like healthcare.},
 author = {Turker Berk Donmez},
 comment = {},
 doi = {},
 eprint = {2411.05196v2},
 journal = {arXiv preprint},
 title = {Explainable AI through a Democratic Lens: DhondtXAI for Proportional Feature Importance Using the D'Hondt Method},
 url = {http://arxiv.org/abs/2411.05196v2},
 year = {2024}
}

@article{2411.05874v2,
 abstract = {The joint implementation of federated learning (FL) and explainable artificial intelligence (XAI) could allow training models from distributed data and explaining their inner workings while preserving essential aspects of privacy. Toward establishing the benefits and tensions associated with their interplay, this scoping review maps the publications that jointly deal with FL and XAI, focusing on publications that reported an interplay between FL and model interpretability or post-hoc explanations. Out of the 37 studies meeting our criteria, only one explicitly and quantitatively analyzed the influence of FL on model explanations, revealing a significant research gap. The aggregation of interpretability metrics across FL nodes created generalized global insights at the expense of node-specific patterns being diluted. Several studies proposed FL algorithms incorporating explanation methods to safeguard the learning process against defaulting or malicious nodes. Studies using established FL libraries or following reporting guidelines are a minority. More quantitative research and structured, transparent practices are needed to fully understand their mutual impact and under which conditions it happens.},
 author = {Luis M. Lopez-Ramos and Florian Leiser and Aditya Rastogi and Steven Hicks and Inga Strümke and Vince I. Madai and Tobias Budig and Ali Sunyaev and Adam Hilbert},
 comment = {16 pages, 10 figures, submitted in IEEE Access},
 doi = {},
 eprint = {2411.05874v2},
 journal = {arXiv preprint},
 title = {Interplay between Federated Learning and Explainable Artificial Intelligence: a Scoping Review},
 url = {http://arxiv.org/abs/2411.05874v2},
 year = {2024}
}

@article{2411.06367v1,
 abstract = {Neural additive model (NAM) is a recently proposed explainable artificial intelligence (XAI) method that utilizes neural network-based architectures. Given the advantages of neural networks, NAMs provide intuitive explanations for their predictions with high model performance. In this paper, we analyze a critical yet overlooked phenomenon: NAMs often produce inconsistent explanations, even when using the same architecture and dataset. Traditionally, such inconsistencies have been viewed as issues to be resolved. However, we argue instead that these inconsistencies can provide valuable explanations within the given data model. Through a simple theoretical framework, we demonstrate that these inconsistencies are not mere artifacts but emerge naturally in datasets with multiple important features. To effectively leverage this information, we introduce a novel framework, Bayesian Neural Additive Model (BayesNAM), which integrates Bayesian neural networks and feature dropout, with theoretical proof demonstrating that feature dropout effectively captures model inconsistencies. Our experiments demonstrate that BayesNAM effectively reveals potential problems such as insufficient data or structural limitations of the model, providing more reliable explanations and potential remedies.},
 author = {Hoki Kim and Jinseong Park and Yujin Choi and Seungyun Lee and Jaewook Lee},
 comment = {Under Review},
 doi = {},
 eprint = {2411.06367v1},
 journal = {arXiv preprint},
 title = {BayesNAM: Leveraging Inconsistency for Reliable Explanations},
 url = {http://arxiv.org/abs/2411.06367v1},
 year = {2024}
}

@article{2411.06860v1,
 abstract = {Phishing attacks remain a persistent threat to online security, demanding robust detection methods. This study investigates the use of machine learning to identify phishing URLs, emphasizing the crucial role of feature selection and model interpretability for improved performance. Employing Recursive Feature Elimination, the research pinpointed key features like "length_url," "time_domain_activation" and "Page_rank" as strong indicators of phishing attempts. The study evaluated various algorithms, including CatBoost, XGBoost, and Explainable Boosting Machine, assessing their robustness and scalability. XGBoost emerged as highly efficient in terms of runtime, making it well-suited for large datasets. CatBoost, on the other hand, demonstrated resilience by maintaining high accuracy even with reduced features. To enhance transparency and trustworthiness, Explainable AI techniques, such as SHAP, were employed to provide insights into feature importance. The study's findings highlight that effective feature selection and model interpretability can significantly bolster phishing detection systems, paving the way for more efficient and adaptable defenses against evolving cyber threats},
 author = {Abdullah Fajar and Setiadi Yazid and Indra Budi},
 comment = {},
 doi = {},
 eprint = {2411.06860v1},
 journal = {arXiv preprint},
 title = {Enhancing Phishing Detection through Feature Importance Analysis and Explainable AI: A Comparative Study of CatBoost, XGBoost, and EBM Models},
 url = {http://arxiv.org/abs/2411.06860v1},
 year = {2024}
}

@article{2411.07597v1,
 abstract = {Code Language Models (CLMs) have achieved tremendous progress in source code understanding and generation, leading to a significant increase in research interests focused on applying CLMs to real-world software engineering tasks in recent years. However, in realistic scenarios, CLMs are exposed to potential malicious adversaries, bringing risks to the confidentiality, integrity, and availability of CLM systems. Despite these risks, a comprehensive analysis of the security vulnerabilities of CLMs in the extremely adversarial environment has been lacking. To close this research gap, we categorize existing attack techniques into three types based on the CIA triad: poisoning attacks (integrity \& availability infringement), evasion attacks (integrity infringement), and privacy attacks (confidentiality infringement). We have collected so far the most comprehensive (79) papers related to adversarial machine learning for CLM from the research fields of artificial intelligence, computer security, and software engineering. Our analysis covers each type of risk, examining threat model categorization, attack techniques, and countermeasures, while also introducing novel perspectives on eXplainable AI (XAI) and exploring the interconnections between different risks. Finally, we identify current challenges and future research opportunities. This study aims to provide a comprehensive roadmap for both researchers and practitioners and pave the way towards more reliable CLMs for practical applications.},
 author = {Yulong Yang and Haoran Fan and Chenhao Lin and Qian Li and Zhengyu Zhao and Chao Shen and Xiaohong Guan},
 comment = {Under a reviewing process since Sep. 3, 2024},
 doi = {},
 eprint = {2411.07597v1},
 journal = {arXiv preprint},
 title = {A Survey on Adversarial Machine Learning for Code Data: Realistic Threats, Countermeasures, and Interpretations},
 url = {http://arxiv.org/abs/2411.07597v1},
 year = {2024}
}

@article{2411.08195v1,
 abstract = {Objectives: Age and gender estimation is crucial for various applications, including forensic investigations and anthropological studies. This research aims to develop a predictive system for age and gender estimation in living individuals, leveraging dental measurements such as Coronal Height (CH), Coronal Pulp Cavity Height (CPCH), and Tooth Coronal Index (TCI). Methods: Machine learning models were employed in our study, including Cat Boost Classifier (Catboost), Gradient Boosting Machine (GBM), Ada Boost Classifier (AdaBoost), Random Forest (RF), eXtreme Gradient Boosting (XGB), Light Gradient Boosting Machine (LGB), and Extra Trees Classifier (ETC), to analyze dental data from 862 living individuals (459 males and 403 females). Specifically, periapical radiographs from six teeth per individual were utilized, including premolars and molars from both maxillary and mandibular. A novel ensemble learning technique was developed, which uses multiple models each tailored to distinct dental metrics, to estimate age and gender accurately. Furthermore, an explainable AI model has been created utilizing SHAP, enabling dental experts to make judicious decisions based on comprehensible insight. Results: The RF and XGB models were particularly effective, yielding the highest F1 score for age and gender estimation. Notably, the XGB model showed a slightly better performance in age estimation, achieving an F1 score of 73.26%. A similar trend for the RF model was also observed in gender estimation, achieving a F1 score of 77.53%. Conclusions: This study marks a significant advancement in dental forensic methods, showcasing the potential of machine learning to automate age and gender estimation processes with improved accuracy.},
 author = {Mohsin Ali and Haider Raza and John Q Gan and Ariel Pokhojaev and Matanel Katz and Esra Kosan and Dian Agustin Wahjuningrum and Omnina Saleh and Rachel Sarig and Akhilanada Chaurasia},
 comment = {},
 doi = {},
 eprint = {2411.08195v1},
 journal = {arXiv preprint},
 title = {An Explainable Machine Learning Approach for Age and Gender Estimation in Living Individuals Using Dental Biometrics},
 url = {http://arxiv.org/abs/2411.08195v1},
 year = {2024}
}

@article{2411.08640v1,
 abstract = {The evolution of wireless communication systems will be fundamentally impacted by an open radio access network (O-RAN), a new concept defining an intelligent architecture with enhanced flexibility, openness, and the ability to slice services more efficiently. For all its promises, and like any technological advancement, O-RAN is not without risks that need to be carefully assessed and properly addressed to accelerate its wide adoption in future mobile networks. In this paper, we present an in-depth security analysis of the O-RAN architecture, discussing the potential threats that may arise in the different O-RAN architecture layers and their impact on the Confidentiality, Integrity, and Availability (CIA) triad. We also promote the potential of zero trust, Moving Target Defense (MTD), blockchain, and large language models(LLM) technologies in fortifying O-RAN's security posture. Furthermore, we numerically demonstrate the effectiveness of MTD in empowering robust deep reinforcement learning methods for dynamic network slice admission control in the O-RAN architecture. Moreover, we examine the effect of explainable AI (XAI) based on LLMs in securing the system.},
 author = {Mojdeh Karbalaee Motalleb and Chafika Benzaid and Tarik Taleb and Marcos Katz and Vahid Shah-Mansouri and JaeSeung Song},
 comment = {10 pages},
 doi = {},
 eprint = {2411.08640v1},
 journal = {arXiv preprint},
 title = {Towards Secure Intelligent O-RAN Architecture: Vulnerabilities, Threats and Promising Technical Solutions using LLMs},
 url = {http://arxiv.org/abs/2411.08640v1},
 year = {2024}
}

@article{2411.09788v1,
 abstract = {Artificial Intelligence (AI) techniques, particularly machine learning techniques, are rapidly transforming tactical operations by augmenting human decision-making capabilities. This paper explores AI-driven Human-Autonomy Teaming (HAT) as a transformative approach, focusing on how it empowers human decision-making in complex environments. While trust and explainability continue to pose significant challenges, our exploration focuses on the potential of AI-driven HAT to transform tactical operations. By improving situational awareness and supporting more informed decision-making, AI-driven HAT can enhance the effectiveness and safety of such operations. To this end, we propose a comprehensive framework that addresses the key components of AI-driven HAT, including trust and transparency, optimal function allocation between humans and AI, situational awareness, and ethical considerations. The proposed framework can serve as a foundation for future research and development in the field. By identifying and discussing critical research challenges and knowledge gaps in this framework, our work aims to guide the advancement of AI-driven HAT for optimizing tactical operations. We emphasize the importance of developing scalable and ethical AI-driven HAT systems that ensure seamless human-machine collaboration, prioritize ethical considerations, enhance model transparency through Explainable AI (XAI) techniques, and effectively manage the cognitive load of human operators.},
 author = {Desta Haileselassie Hagos and Hassan El Alami and Danda B. Rawat},
 comment = {Submitted for review to the Proceedings of the IEEE},
 doi = {},
 eprint = {2411.09788v1},
 journal = {arXiv preprint},
 title = {AI-Driven Human-Autonomy Teaming in Tactical Operations: Proposed Framework, Challenges, and Future Directions},
 url = {http://arxiv.org/abs/2411.09788v1},
 year = {2024}
}

@article{2411.09813v2,
 abstract = {Phishing has been a prevalent cyber threat that manipulates users into revealing sensitive private information through deceptive tactics, designed to masquerade as trustworthy entities. Over the years, proactively detection of phishing URLs (or websites) has been established as an widely-accepted defense approach. In literature, we often find supervised Machine Learning (ML) models with highly competitive performance for detecting phishing websites based on the extracted features from both phishing and benign (i.e., legitimate) websites. However, it is still unclear if these features or indicators are dependent on a particular dataset or they are generalized for overall phishing detection. In this paper, we delve deeper into this issue by analyzing two publicly available phishing URL datasets, where each dataset has its own set of unique and overlapping features related to URL string and website contents. We want to investigate if overlapping features are similar in nature across datasets and how does the model perform when trained on one dataset and tested on the other. We conduct practical experiments and leverage explainable AI (XAI) methods such as SHAP plots to provide insights into different features' contributions in case of phishing detection to answer our primary question, "Can features for phishing URL detection be trusted across diverse dataset?". Our case study experiment results show that features for phishing URL detection can often be dataset-dependent and thus may not be trusted across different datasets even though they share same set of feature behaviors.},
 author = {Maraz Mia and Darius Derakhshan and Mir Mehedi A. Pritom},
 comment = {9 pages, 9 figures, 11th International Conference on Networking, Systems, and Security (NSysS 2024), 2024, Khulna, Bangladesh},
 doi = {10.1145/3704522.3704532},
 eprint = {2411.09813v2},
 journal = {arXiv preprint},
 title = {Can Features for Phishing URL Detection Be Trusted Across Diverse Datasets? A Case Study with Explainable AI},
 url = {http://arxiv.org/abs/2411.09813v2},
 year = {2024}
}

@article{2411.10273v1,
 abstract = {Model interpretability is a key challenge that has yet to align with the advancements observed in contemporary state-of-the-art deep learning models. In particular, deep learning aided vision tasks require interpretability, in order for their adoption in more specialized domains such as medical imaging. Although the field of explainable AI (XAI) developed methods for interpreting vision models along with early convolutional neural networks, recent XAI research has mainly focused on assigning attributes via saliency maps. As such, these methods are restricted to providing explanations at a sample level, and many explainability methods suffer from low adaptability across a wide range of vision models. In our work, we re-think vision-model explainability from a novel perspective, to probe the general input structure that a model has learnt during its training. To this end, we ask the question: "How would a vision model fill-in a masked-image". Experiments on standard vision datasets and pre-trained models reveal consistent patterns, and could be intergrated as an additional model-agnostic explainability tool in modern machine-learning platforms. The code will be available at \url{https://github.com/BoTZ-TND/FillingTheBlanks.git}},
 author = {Pathirage N. Deelaka and Tharindu Wickremasinghe and Devin Y. De Silva and Lisara N. Gajaweera},
 comment = {},
 doi = {},
 eprint = {2411.10273v1},
 journal = {arXiv preprint},
 title = {Fill in the blanks: Rethinking Interpretability in vision},
 url = {http://arxiv.org/abs/2411.10273v1},
 year = {2024}
}

@article{2411.13053v1,
 abstract = {Explaining the decision-making processes of Artificial Intelligence (AI) models is crucial for addressing their "black box" nature, particularly in tasks like image classification. Traditional eXplainable AI (XAI) methods typically rely on unimodal explanations, either visual or textual, each with inherent limitations. Visual explanations highlight key regions but often lack rationale, while textual explanations provide context without spatial grounding. Further, both explanation types can be inconsistent or incomplete, limiting their reliability. To address these challenges, we propose a novel Multimodal Explanation-Guided Learning (MEGL) framework that leverages both visual and textual explanations to enhance model interpretability and improve classification performance. Our Saliency-Driven Textual Grounding (SDTG) approach integrates spatial information from visual explanations into textual rationales, providing spatially grounded and contextually rich explanations. Additionally, we introduce Textual Supervision on Visual Explanations to align visual explanations with textual rationales, even in cases where ground truth visual annotations are missing. A Visual Explanation Distribution Consistency loss further reinforces visual coherence by aligning the generated visual explanations with dataset-level patterns, enabling the model to effectively learn from incomplete multimodal supervision. We validate MEGL on two new datasets, Object-ME and Action-ME, for image classification with multimodal explanations. Experimental results demonstrate that MEGL outperforms previous approaches in prediction accuracy and explanation quality across both visual and textual domains. Our code will be made available upon the acceptance of the paper.},
 author = {Yifei Zhang and Tianxu Jiang and Bo Pan and Jingyu Wang and Guangji Bai and Liang Zhao},
 comment = {},
 doi = {},
 eprint = {2411.13053v1},
 journal = {arXiv preprint},
 title = {MEGL: Multimodal Explanation-Guided Learning},
 url = {http://arxiv.org/abs/2411.13053v1},
 year = {2024}
}

@article{2411.13332v1,
 abstract = {We investigate the effectiveness of Explainable AI (XAI) in verifying Machine Unlearning (MU) within the context of harbor front monitoring, focusing on data privacy and regulatory compliance. With the increasing need to adhere to privacy legislation such as the General Data Protection Regulation (GDPR), traditional methods of retraining ML models for data deletions prove impractical due to their complexity and resource demands. MU offers a solution by enabling models to selectively forget specific learned patterns without full retraining. We explore various removal techniques, including data relabeling, and model perturbation. Then, we leverage attribution-based XAI to discuss the effects of unlearning on model performance. Our proof-of-concept introduces feature importance as an innovative verification step for MU, expanding beyond traditional metrics and demonstrating techniques' ability to reduce reliance on undesired patterns. Additionally, we propose two novel XAI-based metrics, Heatmap Coverage (HC) and Attention Shift (AS), to evaluate the effectiveness of these methods. This approach not only highlights how XAI can complement MU by providing effective verification, but also sets the stage for future research to enhance their joint integration.},
 author = {Àlex Pujol Vidal and Anders S. Johansen and Mohammad N. S. Jahromi and Sergio Escalera and Kamal Nasrollahi and Thomas B. Moeslund},
 comment = {ICPRW2024},
 doi = {},
 eprint = {2411.13332v1},
 journal = {arXiv preprint},
 title = {Verifying Machine Unlearning with Explainable AI},
 url = {http://arxiv.org/abs/2411.13332v1},
 year = {2024}
}

@article{2411.14254v1,
 abstract = {Course Outcome (CO) and Program Outcome (PO)/Program-Specific Outcome (PSO) alignment is a crucial task for ensuring curriculum coherence and assessing educational effectiveness. The construction of a Course Articulation Matrix (CAM), which quantifies the relationship between COs and POs/PSOs, typically involves assigning numerical values (0, 1, 2, 3) to represent the degree of alignment. In this study, We experiment with four models from the BERT family: BERT Base, DistilBERT, ALBERT, and RoBERTa, and use multiclass classification to assess the alignment between CO and PO/PSO pairs. We first evaluate traditional machine learning classifiers, such as Decision Tree, Random Forest, and XGBoost, and then apply transfer learning to evaluate the performance of the pretrained BERT models. To enhance model interpretability, we apply Explainable AI technique, specifically Local Interpretable Model-agnostic Explanations (LIME), to provide transparency into the decision-making process. Our system achieves accuracy, precision, recall, and F1-score values of 98.66%, 98.67%, 98.66%, and 98.66%, respectively. This work demonstrates the potential of utilizing transfer learning with BERT-based models for the automated generation of CAMs, offering high performance and interpretability in educational outcome assessment.},
 author = {Natenaile Asmamaw Shiferaw and Simpenzwe Honore Leandre and Aman Sinha and Dillip Rout},
 comment = {26 pages, 9 figures},
 doi = {},
 eprint = {2411.14254v1},
 journal = {arXiv preprint},
 title = {BERT-Based Approach for Automating Course Articulation Matrix Construction with Explainable AI},
 url = {http://arxiv.org/abs/2411.14254v1},
 year = {2024}
}

@article{2411.14471v1,
 abstract = {Diabetes, particularly Type 2 diabetes (T2D), poses a substantial global health burden, compounded by its associated complications such as cardiovascular diseases, kidney failure, and vision impairment. Early detection of T2D is critical for improving healthcare outcomes and optimizing resource allocation. In this study, we address the gap in early T2D detection by leveraging machine learning (ML) techniques on gene expression data obtained from T2D patients. Our primary objective was to enhance the accuracy of early T2D detection through advanced ML methodologies and increase the model's trustworthiness using the explainable artificial intelligence (XAI) technique. Analyzing the biological mechanisms underlying T2D through gene expression datasets represents a novel research frontier, relatively less explored in previous studies. While numerous investigations have focused on utilizing clinical and demographic data for T2D prediction, the integration of molecular insights from gene expression datasets offers a unique and promising avenue for understanding the pathophysiology of the disease. By employing six ML classifiers on data sourced from NCBI's Gene Expression Omnibus (GEO), we observed promising performance across all models. Notably, the XGBoost classifier exhibited the highest accuracy, achieving 97%. Our study addresses a notable gap in early T2D detection methodologies, emphasizing the importance of leveraging gene expression data and advanced ML techniques.},
 author = {Aurora Lithe Roy and Md Kamrul Siam and Nuzhat Noor Islam Prova and Sumaiya Jahan and Abdullah Al Maruf},
 comment = {8 pages},
 doi = {},
 eprint = {2411.14471v1},
 journal = {arXiv preprint},
 title = {Leveraging Gene Expression Data and Explainable Machine Learning for Enhanced Early Detection of Type 2 Diabetes},
 url = {http://arxiv.org/abs/2411.14471v1},
 year = {2024}
}

@article{2411.15265v2,
 abstract = {Gradient-based methods are a prototypical family of explainability techniques, especially for image-based models. Nonetheless, they have several shortcomings in that they (1) require white-box access to models, (2) are vulnerable to adversarial attacks, and (3) produce attributions that lie off the image manifold, leading to explanations that are not actually faithful to the model and do not align well with human perception. To overcome these challenges, we introduce Derivative-Free Diffusion Manifold-Constrainted Gradients (FreeMCG), a novel method that serves as an improved basis for explainability of a given neural network than the traditional gradient. Specifically, by leveraging ensemble Kalman filters and diffusion models, we derive a derivative-free approximation of the model's gradient projected onto the data manifold, requiring access only to the model's outputs. We demonstrate the effectiveness of FreeMCG by applying it to both counterfactual generation and feature attribution, which have traditionally been treated as distinct tasks. Through comprehensive evaluation on both tasks, counterfactual explanation and feature attribution, we show that our method yields state-of-the-art results while preserving the essential properties expected of XAI tools.},
 author = {Won Jun Kim and Hyungjin Chung and Jaemin Kim and Sangmin Lee and Byeongsu Sim and Jong Chul Ye},
 comment = {CVPR 2025 (poster), 19 pages, 5 figures},
 doi = {},
 eprint = {2411.15265v2},
 journal = {arXiv preprint},
 title = {Derivative-Free Diffusion Manifold-Constrained Gradient for Unified XAI},
 url = {http://arxiv.org/abs/2411.15265v2},
 year = {2024}
}

@article{2411.16120v2,
 abstract = {Reinforcement learning (RL) has demonstrated remarkable success in solving complex decision-making problems, yet its adoption in critical domains is hindered by the lack of interpretability in its decision-making processes. Existing explainable AI (xAI) approaches often fail to provide meaningful explanations for RL agents, particularly because they overlook the contrastive nature of human reasoning--answering "why this action instead of that one?". To address this gap, we propose a novel framework of contrastive learning to explain RL selected actions, named $\textbf{VisionMask}$. VisionMask is trained to generate explanations by explicitly contrasting the agent's chosen action with alternative actions in a given state using a self-supervised manner. We demonstrate the efficacy of our method through experiments across diverse RL environments, evaluating it in terms of faithfulness, robustness, and complexity. Our results show that VisionMask significantly improves human understanding of agent behavior while maintaining accuracy and fidelity. Furthermore, we present examples illustrating how VisionMask can be used for counterfactual analysis. This work bridges the gap between RL and xAI, paving the way for safer and more interpretable RL systems.},
 author = {Rui Zuo and Simon Khan and Zifan Wang and Garrett Ethan Katz and Qinru Qiu},
 comment = {},
 doi = {},
 eprint = {2411.16120v2},
 journal = {arXiv preprint},
 title = {Why the Agent Made that Decision: Contrastive Explanation Learning for Reinforcement Learning},
 url = {http://arxiv.org/abs/2411.16120v2},
 year = {2024}
}

@article{2411.16587v3,
 abstract = {In the field of autonomous surface vehicles (ASVs), devising decision-making and obstacle avoidance solutions that address maritime COLREGs (Collision Regulations), primarily defined for human operators, has long been a pressing challenge. Recent advancements in explainable Artificial Intelligence (AI) and machine learning have shown promise in enabling human-like decision-making. Notably, significant developments have occurred in the application of Large Language Models (LLMs) to the decision-making of complex systems, such as self-driving cars. The textual and somewhat ambiguous nature of COLREGs (from an algorithmic perspective), however, poses challenges that align well with the capabilities of LLMs, suggesting that LLMs may become increasingly suitable for this application soon. This paper presents and demonstrates the first application of LLM-based decision-making and control for ASVs. The proposed method establishes a high-level decision-maker that uses online collision risk indices and key measurements to make decisions for safe manoeuvres. A tailored design and runtime structure is developed to support training and real-time action generation on a realistic ASV model. Local planning and control algorithms are integrated to execute the commands for waypoint following and collision avoidance at a lower level. To the authors' knowledge, this study represents the first attempt to apply explainable AI to the dynamic control problem of maritime systems recognising the COLREGs rules, opening new avenues for research in this challenging area. Results obtained across multiple test scenarios demonstrate the system's ability to maintain online COLREGs compliance, accurate waypoint tracking, and feasible control, while providing human-interpretable reasoning for each decision.},
 author = {Klinsmann Agyei and Pouria Sarhadi and Wasif Naeem},
 comment = {This work has been accepted for publication at European Control Conference 2025, \c{opyright} IEEE 2025. Please cite the published version when available},
 doi = {},
 eprint = {2411.16587v3},
 journal = {arXiv preprint},
 title = {Large Language Model-based Decision-making for COLREGs and the Control of Autonomous Surface Vehicles},
 url = {http://arxiv.org/abs/2411.16587v3},
 year = {2024}
}

@article{2411.16817v1,
 abstract = {Android malware detection based on machine learning (ML) and deep learning (DL) models is widely used for mobile device security. Such models offer benefits in terms of detection accuracy and efficiency, but it is often difficult to understand how such learning models make decisions. As a result, these popular malware detection strategies are generally treated as black boxes, which can result in a lack of trust in the decisions made, as well as making adversarial attacks more difficult to detect. The field of eXplainable Artificial Intelligence (XAI) attempts to shed light on such black box models. In this paper, we apply XAI techniques to ML and DL models that have been trained on a challenging Android malware classification problem. Specifically, the classic ML models considered are Support Vector Machines (SVM), Random Forest, and $k$-Nearest Neighbors ($k$-NN), while the DL models we consider are Multi-Layer Perceptrons (MLP) and Convolutional Neural Networks (CNN). The state-of-the-art XAI techniques that we apply to these trained models are Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations (SHAP), PDP plots, ELI5, and Class Activation Mapping (CAM). We obtain global and local explanation results, and we discuss the utility of XAI techniques in this problem domain. We also provide a literature review of XAI work related to Android malware.},
 author = {Maithili Kulkarni and Mark Stamp},
 comment = {},
 doi = {},
 eprint = {2411.16817v1},
 journal = {arXiv preprint},
 title = {XAI and Android Malware Models},
 url = {http://arxiv.org/abs/2411.16817v1},
 year = {2024}
}

@article{2411.16895v1,
 abstract = {This paper introduces a novel XAI approach based on near-misses analysis (NMA). This approach reveals a hierarchy of logical 'concepts' inferred from the latent decision-making process of a Neural Network (NN) without delving into its explicit structure. We examined our proposed XAI approach on different network architectures that vary in size and shape (e.g., ResNet, VGG, EfficientNet, MobileNet) on several datasets (ImageNet and CIFAR100). The results demonstrate its usability to reflect NNs latent process of concepts generation. We generated a new metric for explainability. Moreover, our experiments suggest that efficient architectures, which achieve a similar accuracy level with much less neurons may still pay the price of explainability and robustness in terms of concepts generation. We, thus, pave a promising new path for XAI research to follow.},
 author = {Eran Kaufman and Avivit levy},
 comment = {},
 doi = {},
 eprint = {2411.16895v1},
 journal = {arXiv preprint},
 title = {Explainable AI Approach using Near Misses Analysis},
 url = {http://arxiv.org/abs/2411.16895v1},
 year = {2024}
}

@article{2411.17645v3,
 abstract = {The use of machine learning and AI on electronic health records (EHRs) holds substantial potential for clinical insight. However, this approach faces challenges due to data heterogeneity, sparsity, temporal misalignment, and limited labeled outcomes. In this context, we leverage a linked EHR dataset of approximately one million de-identified individuals from Bristol, North Somerset, and South Gloucestershire, UK, to characterize urinary tract infections (UTIs). We implemented a data pre-processing and curation pipeline that transforms the raw EHR data into a structured format suitable for developing predictive models focused on data fairness, accountability and transparency. Given the limited availability and biases of ground truth UTI outcomes, we introduce a UTI risk estimation framework informed by clinical expertise to estimate UTI risk across individual patient timelines. Pairwise XGBoost models are trained using this framework to differentiate UTI risk categories with explainable AI techniques applied to identify key predictors and support interpretability. Our findings reveal differences in clinical and demographic predictors across risk groups. While this study highlights the potential of AI-driven insights to support UTI clinical decision-making, further investigation of patient sub-strata and extensive validation are needed to ensure robustness and applicability in clinical practice.},
 author = {Yujie Dai and Brian Sullivan and Axel Montout and Amy Dillon and Chris Waller and Peter Acs and Rachel Denholm and Philip Williams and Alastair D Hay and Raul Santos-Rodriguez and Andrew Dowsey},
 comment = {},
 doi = {},
 eprint = {2411.17645v3},
 journal = {arXiv preprint},
 title = {Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset},
 url = {http://arxiv.org/abs/2411.17645v3},
 year = {2024}
}

@article{2412.00530v1,
 abstract = {Creativity is a fundamental skill of human cognition. We use textual forma mentis networks (TFMN) to extract network (semantic/syntactic associations) and emotional features from approximately one thousand human- and GPT3.5-generated stories. Using Explainable Artificial Intelligence (XAI), we test whether features relative to Mednick's associative theory of creativity can explain creativity ratings assigned by humans and GPT-3.5. Using XGBoost, we examine three scenarios: (i) human ratings of human stories, (ii) GPT-3.5 ratings of human stories, and (iii) GPT-3.5 ratings of GPT-generated stories. Our findings reveal that GPT-3.5 ratings differ significantly from human ratings not only in terms of correlations but also because of feature patterns identified with XAI methods. GPT-3.5 favours 'its own' stories and rates human stories differently from humans. Feature importance analysis with SHAP scores shows that: (i) network features are more predictive for human creativity ratings but also for GPT-3.5's ratings of human stories; (ii) emotional features played a greater role than semantic/syntactic network structure in GPT-3.5 rating its own stories. These quantitative results underscore key limitations in GPT-3.5's ability to align with human assessments of creativity. We emphasise the need for caution when using GPT-3.5 to assess and generate creative content, as it does not yet capture the nuanced complexity that characterises human creativity.},
 author = {Edith Haim and Natalie Fischer and Salvatore Citraro and Giulio Rossetti and Massimo Stella},
 comment = {},
 doi = {},
 eprint = {2412.00530v1},
 journal = {arXiv preprint},
 title = {Forma mentis networks predict creativity ratings of short texts via interpretable artificial intelligence in human and GPT-simulated raters},
 url = {http://arxiv.org/abs/2412.00530v1},
 year = {2024}
}

@article{2412.00800v2,
 abstract = {Explainable Artificial Intelligence (XAI) addresses the growing need for transparency and interpretability in AI systems, enabling trust and accountability in decision-making processes. This book offers a comprehensive guide to XAI, bridging foundational concepts with advanced methodologies. It explores interpretability in traditional models such as Decision Trees, Linear Regression, and Support Vector Machines, alongside the challenges of explaining deep learning architectures like CNNs, RNNs, and Large Language Models (LLMs), including BERT, GPT, and T5. The book presents practical techniques such as SHAP, LIME, Grad-CAM, counterfactual explanations, and causal inference, supported by Python code examples for real-world applications.
  Case studies illustrate XAI's role in healthcare, finance, and policymaking, demonstrating its impact on fairness and decision support. The book also covers evaluation metrics for explanation quality, an overview of cutting-edge XAI tools and frameworks, and emerging research directions, such as interpretability in federated learning and ethical AI considerations. Designed for a broad audience, this resource equips readers with the theoretical insights and practical skills needed to master XAI. Hands-on examples and additional resources are available at the companion GitHub repository: https://github.com/Echoslayer/XAI_From_Classical_Models_to_LLMs.},
 author = {Weiche Hsieh and Ziqian Bi and Chuanqi Jiang and Junyu Liu and Benji Peng and Sen Zhang and Xuanhe Pan and Jiawei Xu and Jinlang Wang and Keyu Chen and Pohsun Feng and Yizhu Wen and Xinyuan Song and Tianyang Wang and Ming Liu and Junjie Yang and Ming Li and Bowen Jing and Jintao Ren and Junhao Song and Hong-Ming Tseng and Yichao Zhang and Lawrence K. Q. Yan and Qian Niu and Silin Chen and Yunze Wang and Chia Xin Liang},
 comment = {},
 doi = {},
 eprint = {2412.00800v2},
 journal = {arXiv preprint},
 title = {A Comprehensive Guide to Explainable AI: From Classical Models to LLMs},
 url = {http://arxiv.org/abs/2412.00800v2},
 year = {2024}
}

@article{2412.00943v1,
 abstract = {Calibration is a frequently invoked concept when useful label probability estimates are required on top of classification accuracy. A calibrated model is a function whose values correctly reflect underlying label probabilities. Calibration in itself however does not imply classification accuracy, nor human interpretable estimates, nor is it straightforward to verify calibration from finite data. There is a plethora of evaluation metrics (and loss functions) that each assess a specific aspect of a calibration model. In this work, we initiate an axiomatic study of the notion of calibration. We catalogue desirable properties of calibrated models as well as corresponding evaluation metrics and analyze their feasibility and correspondences. We complement this analysis with an empirical evaluation, comparing common calibration methods to employing a simple, interpretable decision tree.},
 author = {Alireza Torabian and Ruth Urner},
 comment = {Published in XAI 2024},
 doi = {10.1007/978-3-031-63800-8_11},
 eprint = {2412.00943v1},
 journal = {arXiv preprint},
 title = {Calibration through the Lens of Interpretability},
 url = {http://arxiv.org/abs/2412.00943v1},
 year = {2024}
}

@article{2412.01829v1,
 abstract = {The continuous development of artificial intelligence (AI) theory has propelled this field to unprecedented heights, owing to the relentless efforts of scholars and researchers. In the medical realm, AI takes a pivotal role, leveraging robust machine learning (ML) algorithms. AI technology in medical imaging aids physicians in X-ray, computed tomography (CT) scans, and magnetic resonance imaging (MRI) diagnoses, conducts pattern recognition and disease prediction based on acoustic data, delivers prognoses on disease types and developmental trends for patients, and employs intelligent health management wearable devices with human-computer interaction technology to name but a few. While these well-established applications have significantly assisted in medical field diagnoses, clinical decision-making, and management, collaboration between the medical and AI sectors faces an urgent challenge: How to substantiate the reliability of decision-making? The underlying issue stems from the conflict between the demand for accountability and result transparency in medical scenarios and the black-box model traits of AI. This article reviews recent research grounded in explainable artificial intelligence (XAI), with an emphasis on medical practices within the visual, audio, and multimodal perspectives. We endeavour to categorise and synthesise these practices, aiming to provide support and guidance for future researchers and healthcare professionals.},
 author = {Qiyang Sun and Alican Akman and Björn W. Schuller},
 comment = {},
 doi = {},
 eprint = {2412.01829v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence for Medical Applications: A Review},
 url = {http://arxiv.org/abs/2412.01829v1},
 year = {2024}
}

@article{2412.02399v1,
 abstract = {Deep Learning (DL) models are often black boxes, making their decision-making processes difficult to interpret. This lack of transparency has driven advancements in eXplainable Artificial Intelligence (XAI), a field dedicated to clarifying the reasoning behind DL model predictions. Among these, attribution-based methods such as LRP and GradCAM are widely used, though they rely on approximations that can be imprecise.
  To address these limitations, we introduce One Matrix to Explain Neural Networks (OMENN), a novel post-hoc method that represents a neural network as a single, interpretable matrix for each specific input. This matrix is constructed through a series of linear transformations that represent the processing of the input by each successive layer in the neural network. As a result, OMENN provides locally precise, attribution-based explanations of the input across various modern models, including ViTs and CNNs. We present a theoretical analysis of OMENN based on dynamic linearity property and validate its effectiveness with extensive tests on two XAI benchmarks, demonstrating that OMENN is competitive with state-of-the-art methods.},
 author = {Adam Wróbel and Mikołaj Janusz and Bartosz Zieliński and Dawid Rymarczyk},
 comment = {Under review, code will be released after acceptance},
 doi = {},
 eprint = {2412.02399v1},
 journal = {arXiv preprint},
 title = {OMENN: One Matrix to Explain Neural Networks},
 url = {http://arxiv.org/abs/2412.02399v1},
 year = {2024}
}

@article{2412.03620v1,
 abstract = {Sustainability development goals (SDGs) are regarded as a universal call to action with the overall objectives of planet protection, ending of poverty, and ensuring peace and prosperity for all people. In order to achieve these objectives, different AI technologies play a major role. Specifically, recommender systems can provide support for organizations and individuals to achieve the defined goals. Recommender systems integrate AI technologies such as machine learning, explainable AI (XAI), case-based reasoning, and constraint solving in order to find and explain user-relevant alternatives from a potentially large set of options. In this article, we summarize the state of the art in applying recommender systems to support the achievement of sustainability development goals. In this context, we discuss open issues for future research.},
 author = {Alexander Felfernig and Manfred Wundara and Thi Ngoc Trang Tran and Seda Polat-Erdeniz and Sebastian Lubos and Merfat El-Mansi and Damian Garber and Viet-Man Le},
 comment = {},
 doi = {10.3389/fdata.2023.1284511},
 eprint = {2412.03620v1},
 journal = {arXiv preprint},
 title = {Recommender Systems for Sustainability: Overview and Research Issues},
 url = {http://arxiv.org/abs/2412.03620v1},
 year = {2024}
}

@article{2412.04183v1,
 abstract = {The development of computing has made credit scoring approaches possible, with various machine learning (ML) and deep learning (DL) techniques becoming more and more valuable. While complex models yield more accurate predictions, their interpretability is often weakened, which is a concern for credit scoring that places importance on decision fairness. As features of the dataset are a crucial factor for the credit scoring system, we implement Linear Discriminant Analysis (LDA) as a feature reduction technique, which reduces the burden of the models complexity. We compared 6 different machine learning models, 1 deep learning model, and a hybrid model with and without using LDA. From the result, we have found our hybrid model, XG-DNN, outperformed other models with the highest accuracy of 99.45% and a 99% F1 score with LDA. Lastly, to interpret model decisions, we have applied 2 different explainable AI techniques named LIME (local) and Morris Sensitivity Analysis (global). Through this research, we showed how feature reduction techniques can be used without affecting the performance and explainability of the model, which can be very useful in resource-constrained settings to optimize the computational workload.},
 author = {Md Shihab Reza and Monirul Islam Mahmud and Ifti Azad Abeer and Nova Ahmed},
 comment = {Accepted on International Conference on Computer and Information Technology (ICCIT) 2024},
 doi = {},
 eprint = {2412.04183v1},
 journal = {arXiv preprint},
 title = {Linear Discriminant Analysis in Credit Scoring: A Transparent Hybrid Model Approach},
 url = {http://arxiv.org/abs/2412.04183v1},
 year = {2024}
}

@article{2412.05145v1,
 abstract = {Explanations of machine learning (ML) model predictions generated by Explainable AI (XAI) techniques such as SHAP are essential for people using ML outputs for decision-making. We explore the potential of Large Language Models (LLMs) to transform these explanations into human-readable, narrative formats that align with natural communication. We address two key research questions: (1) Can LLMs reliably transform traditional explanations into high-quality narratives? and (2) How can we effectively evaluate the quality of narrative explanations? To answer these questions, we introduce Explingo, which consists of two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML explanations and transforms them into natural-language descriptions. The Grader scores these narratives on a set of metrics including accuracy, completeness, fluency, and conciseness.
  Our experiments demonstrate that LLMs can generate high-quality narratives that achieve high scores across all metrics, particularly when guided by a small number of human-labeled and bootstrapped examples. We also identified areas that remain challenging, in particular for effectively scoring narratives in complex domains. The findings from this work have been integrated into an open-source tool that makes narrative explanations available for further applications.},
 author = {Alexandra Zytek and Sara Pido and Sarah Alnegheimish and Laure Berti-Equille and Kalyan Veeramachaneni},
 comment = {To be presented in the 2024 IEEE International Conference on Big Data (IEEE BigData)},
 doi = {},
 eprint = {2412.05145v1},
 journal = {arXiv preprint},
 title = {Explingo: Explaining AI Predictions using Large Language Models},
 url = {http://arxiv.org/abs/2412.05145v1},
 year = {2024}
}

@article{2412.05686v1,
 abstract = {Interpreting complex neural networks is crucial for understanding their decision-making processes, particularly in applications where transparency and accountability are essential. This proposed method addresses this need by focusing on layer-wise Relevance Propagation (LRP), a technique used in explainable artificial intelligence (XAI) to attribute neural network outputs to input features through backpropagated relevance scores. Existing LRP methods often struggle with precision in evaluating individual neuron contributions. To overcome this limitation, we present a novel approach that improves the parsing of selected neurons during LRP backward propagation, using the Visual Geometry Group 16 (VGG16) architecture as a case study. Our method creates neural network graphs to highlight critical paths and visualizes these paths with heatmaps, optimizing neuron selection through accuracy metrics like Mean Squared Error (MSE) and Symmetric Mean Absolute Percentage Error (SMAPE). Additionally, we utilize a deconvolutional visualization technique to reconstruct feature maps, offering a comprehensive view of the network's inner workings. Extensive experiments demonstrate that our approach enhances interpretability and supports the development of more transparent artificial intelligence (AI) systems for computer vision applications. This advancement has the potential to improve the trustworthiness of AI models in real-world machine vision applications, thereby increasing their reliability and effectiveness.},
 author = {Deepshikha Bhati and Fnu Neha and Md Amiruzzaman and Angela Guercio and Deepak Kumar Shukla and Ben Ward},
 comment = {},
 doi = {},
 eprint = {2412.05686v1},
 journal = {arXiv preprint},
 title = {Neural network interpretability with layer-wise relevance propagation: novel techniques for neuron selection and visualization},
 url = {http://arxiv.org/abs/2412.05686v1},
 year = {2024}
}

@article{2412.07783v3,
 abstract = {Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.},
 author = {Patrick Styll and Dowon Kim and Jiook Cha},
 comment = {fMRI Transformer, Developing Human Connectome Project, Bayley Scales of Infant Development, Personalized Therapy, XAI},
 doi = {},
 eprint = {2412.07783v3},
 journal = {arXiv preprint},
 title = {Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI},
 url = {http://arxiv.org/abs/2412.07783v3},
 year = {2024}
}

@article{2412.08263v1,
 abstract = {Explainable artificial intelligence (XAI) aims to make machine learning models more transparent. While many approaches focus on generating explanations post-hoc, interpretable approaches, which generate the explanations intrinsically alongside the predictions, are relatively rare. In this work, we integrate different discrete subset sampling methods into a graph-based visual question answering system to compare their effectiveness in generating interpretable explanatory subgraphs intrinsically. We evaluate the methods on the GQA dataset and show that the integrated methods effectively mitigate the performance trade-off between interpretability and answer accuracy, while also achieving strong co-occurrences between answer and question tokens. Furthermore, we conduct a human evaluation to assess the interpretability of the generated subgraphs using a comparative setting with the extended Bradley-Terry model, showing that the answer and question token co-occurrence metrics strongly correlate with human preferences. Our source code is publicly available.},
 author = {Pascal Tilli and Ngoc Thang Vu},
 comment = {Accepted at COLING 2025},
 doi = {},
 eprint = {2412.08263v1},
 journal = {arXiv preprint},
 title = {Discrete Subgraph Sampling for Interpretable Graph based Visual Question Answering},
 url = {http://arxiv.org/abs/2412.08263v1},
 year = {2024}
}

@article{2412.08513v1,
 abstract = {Incorporating uncertainty is crucial to provide trustworthy explanations of deep learning models. Recent works have demonstrated how uncertainty modeling can be particularly important in the unsupervised field of representation learning explainable artificial intelligence (R-XAI). Current R-XAI methods provide uncertainty by measuring variability in the importance score. However, they fail to provide meaningful estimates of whether a pixel is certainly important or not. In this work, we propose a new R-XAI method called REPEAT that addresses the key question of whether or not a pixel is \textit{certainly} important. REPEAT leverages the stochasticity of current R-XAI methods to produce multiple estimates of importance, thus considering each pixel in an image as a Bernoulli random variable that is either important or unimportant. From these Bernoulli random variables we can directly estimate the importance of a pixel and its associated certainty, thus enabling users to determine certainty in pixel importance. Our extensive evaluation shows that REPEAT gives certainty estimates that are more intuitive, better at detecting out-of-distribution data, and more concise.},
 author = {Kristoffer K. Wickstrøm and Thea Brüsch and Michael C. Kampffmeyer and Robert Jenssen},
 comment = {Accepted at AAAI 2025. Code available at: https://github.com/Wickstrom/REPEAT},
 doi = {},
 eprint = {2412.08513v1},
 journal = {arXiv preprint},
 title = {REPEAT: Improving Uncertainty Estimation in Representation Learning Explainability},
 url = {http://arxiv.org/abs/2412.08513v1},
 year = {2024}
}

@article{2412.09472v1,
 abstract = {Chronic Kidney Disease (CKD) represents a significant global health challenge, characterized by the progressive decline in renal function, leading to the accumulation of waste products and disruptions in fluid balance within the body. Given its pervasive impact on public health, there is a pressing need for effective diagnostic tools to enable timely intervention. Our study delves into the application of cutting-edge transfer learning models for the early detection of CKD. Leveraging a comprehensive and publicly available dataset, we meticulously evaluate the performance of several state-of-the-art models, including EfficientNetV2, InceptionNetV2, MobileNetV2, and the Vision Transformer (ViT) technique. Remarkably, our analysis demonstrates superior accuracy rates, surpassing the 90% threshold with MobileNetV2 and achieving 91.5% accuracy with ViT. Moreover, to enhance predictive capabilities further, we integrate these individual methodologies through ensemble modeling, resulting in our ensemble model exhibiting a remarkable 96% accuracy in the early detection of CKD. This significant advancement holds immense promise for improving clinical outcomes and underscores the critical role of machine learning in addressing complex medical challenges.},
 author = {Md. Arifuzzaman and Iftekhar Ahmed and Md. Jalal Uddin Chowdhury and Shadman Sakib and Mohammad Shoaib Rahman and Md. Ebrahim Hossain and Shakib Absar},
 comment = {},
 doi = {},
 eprint = {2412.09472v1},
 journal = {arXiv preprint},
 title = {A Novel Ensemble-Based Deep Learning Model with Explainable AI for Accurate Kidney Disease Diagnosis},
 url = {http://arxiv.org/abs/2412.09472v1},
 year = {2024}
}

@article{2412.10161v2,
 abstract = {Resting-state fMRI captures spontaneous neural activity characterized by complex spatiotemporal dynamics. Various metrics, such as local and global brain connectivity and low-frequency amplitude fluctuations, quantify distinct aspects of these dynamics. However, these measures are typically analyzed independently, overlooking their interrelations and potentially limiting analytical sensitivity. Here, we introduce the Fusion Searchlight (FuSL) framework, which integrates complementary information from multiple resting-state fMRI metrics. We demonstrate that combining these metrics enhances the accuracy of pharmacological treatment prediction from rs-fMRI data, enabling the identification of additional brain regions affected by sedation with alprazolam. Furthermore, we leverage explainable AI to delineate the differential contributions of each metric, which additionally improves spatial specificity of the searchlight analysis. Moreover, this framework can be adapted to combine information across imaging modalities or experimental conditions, providing a versatile and interpretable tool for data fusion in neuroimaging.},
 author = {Simon Wein and Marco Riebel and Lisa-Marie Brunner and Caroline Nothdurfter and Rainer Rupprecht and Jens V. Schwarzbach},
 comment = {},
 doi = {},
 eprint = {2412.10161v2},
 journal = {arXiv preprint},
 title = {Data Integration with Fusion Searchlight: Classifying Brain States from Resting-state fMRI},
 url = {http://arxiv.org/abs/2412.10161v2},
 year = {2024}
}

@article{2412.10513v2,
 abstract = {Decision trees are a popular machine learning method, known for their inherent explainability. In Explainable AI, decision trees can be used as surrogate models for complex black box AI models or as approximations of parts of such models. A key challenge of this approach is determining how accurately the extracted decision tree represents the original model and to what extent it can be trusted as an approximation of their behavior. In this work, we investigate the use of the Probably Approximately Correct (PAC) framework to provide a theoretical guarantee of fidelity for decision trees extracted from AI models. Based on theoretical results from the PAC framework, we adapt a decision tree algorithm to ensure a PAC guarantee under certain conditions. We focus on binary classification and conduct experiments where we extract decision trees from BERT-based language models with PAC guarantees. Our results indicate occupational gender bias in these models.},
 author = {Ana Ozaki and Roberto Confalonieri and Ricardo Guimarães and Anders Imenes},
 comment = {This is a revision of the version published at AAAI 2025. We fixed an issue in Theorem 8 and run again all the experiments. We also fixed small grammar mistakes found while producing this revised version},
 doi = {},
 eprint = {2412.10513v2},
 journal = {arXiv preprint},
 title = {Extracting PAC Decision Trees from Black Box Binary Classifiers: The Gender Bias Case Study on BERT-based Language Models},
 url = {http://arxiv.org/abs/2412.10513v2},
 year = {2024}
}

@article{2412.10950v1,
 abstract = {The advancement of AI technologies has greatly increased the complexity of AI pipelines as they include many stages such as data collection, pre-processing, training, evaluation and visualisation. To provide effective and accessible AI solutions, it is important to design pipelines for different user groups such as experts, professionals from different fields and laypeople. Ease of use and trust play a central role in the acceptance of AI systems.
  The presented system, ALPACA (Adaptive Learning Pipeline for Advanced Comprehensive AI Analysis), offers a comprehensive AI pipeline that addresses the needs of diverse user groups. ALPACA integrates visual and code-based development and facilitates all key phases of the AI pipeline. Its architecture is based on Celery (with Redis backend) for efficient task management, MongoDB for seamless data storage and Kubernetes for cloud-based scalability and resource utilisation.
  Future versions of ALPACA will support modern techniques such as federated and continuous learning as well as explainable AI methods to further improve security, usability and trustworthiness. The application is demonstrated by an Android app for similarity recognition, which emphasises ALPACA's potential for use in everyday life.},
 author = {Simon Torka and Sahin Albayrak},
 comment = {},
 doi = {},
 eprint = {2412.10950v1},
 journal = {arXiv preprint},
 title = {ALPACA -- Adaptive Learning Pipeline for Comprehensive AI},
 url = {http://arxiv.org/abs/2412.10950v1},
 year = {2024}
}

@article{2412.11205v1,
 abstract = {Explainable AI (XAI) methods typically focus on identifying essential input features or more abstract concepts for tasks like image or text classification. However, for algorithmic tasks like combinatorial optimization, these concepts may depend not only on the input but also on the current state of the network, like in the graph neural networks (GNN) case. This work studies concept learning for an existing GNN model trained to solve Boolean satisfiability (SAT). \textcolor{black}{Our analysis reveals that the model learns key concepts matching those guiding human-designed SAT heuristics, particularly the notion of 'support.' We demonstrate that these concepts are encoded in the top principal components (PCs) of the embedding's covariance matrix, allowing for unsupervised discovery. Using sparse PCA, we establish the minimality of these concepts and show their teachability through a simplified GNN. Two direct applications of our framework are (a) We improve the convergence time of the classical WalkSAT algorithm and (b) We use the discovered concepts to "reverse-engineer" the black-box GNN and rewrite it as a white-box textbook algorithm. Our results highlight the potential of concept learning in understanding and enhancing algorithmic neural networks for combinatorial optimization tasks.},
 author = {Elad Shoham and Hadar Cohen and Khalil Wattad and Havana Rika and Dan Vilenchik},
 comment = {},
 doi = {},
 eprint = {2412.11205v1},
 journal = {arXiv preprint},
 title = {Concept Learning in the Wild: Towards Algorithmic Understanding of Neural Networks},
 url = {http://arxiv.org/abs/2412.11205v1},
 year = {2024}
}

@article{2412.11308v1,
 abstract = {Predictive models often face performance degradation due to evolving data distributions, a phenomenon known as data drift. Among its forms, concept drift, where the relationship between explanatory variables and the response variable changes, is particularly challenging to detect and adapt to. Traditional drift detection methods often rely on metrics such as accuracy or variable distributions, which may fail to capture subtle but significant conceptual changes. This paper introduces drifter, an R package designed to detect concept drift, and proposes a novel method called Profile Drift Detection (PDD) that enables both drift detection and an enhanced understanding of the cause behind the drift by leveraging an explainable AI tool - Partial Dependence Profiles (PDPs). The PDD method, central to the package, quantifies changes in PDPs through novel metrics, ensuring sensitivity to shifts in the data stream without excessive computational costs. This approach aligns with MLOps practices, emphasizing model monitoring and adaptive retraining in dynamic environments. The experiments across synthetic and real-world datasets demonstrate that PDD outperforms existing methods by maintaining high accuracy while effectively balancing sensitivity and stability. The results highlight its capability to adaptively retrain models in dynamic environments, making it a robust tool for real-time applications. The paper concludes by discussing the advantages, limitations, and future extensions of the package for broader use cases.},
 author = {Ugur Dar and Mustafa Cavus},
 comment = {37 pages, 6 figures},
 doi = {},
 eprint = {2412.11308v1},
 journal = {arXiv preprint},
 title = {datadriftR: An R Package for Concept Drift Detection in Predictive Models},
 url = {http://arxiv.org/abs/2412.11308v1},
 year = {2024}
}

@article{2412.12183v1,
 abstract = {In rapidly urbanizing regions, designing climate-responsive urban forms is crucial for sustainable development, especially in dry arid-climates where urban morphology has a significant impact on energy consumption and environmental performance. This study advances urban morphology evaluation by combining Urban Building Energy Modeling (UBEM) with machine learning methods (ML) and Explainable AI techniques, specifically Shapley Additive Explanations (SHAP). Using Tehran's dense urban landscape as a case study, this research assesses and ranks the impact of 30 morphology parameters at the urban block level on key energy metrics (cooling, heating, and lighting demand) and environmental performance (sunlight exposure, photovoltaic generation, and Sky View Factor). Among seven ML algorithms evaluated, the XGBoost model was the most effective predictor, achieving high accuracy (R2: 0.92) and a training time of 3.64 seconds. Findings reveal that building shape, window-to-wall ratio, and commercial ratio are the most critical parameters affecting energy efficiency, while the heights and distances of neighboring buildings strongly influence cooling demand and solar access. By evaluating urban blocks with varied densities and configurations, this study offers generalizable insights applicable to other dry-arid regions. Moreover, the integration of UBEM and Explainable AI offers a scalable, data-driven framework for developing climate-responsive urban designs adaptable to high-density environments worldwide.},
 author = {Pegah Eshraghi and Riccardo Talami and Arman Nikkhah Dehnavi and Maedeh Mirdamadi and Zahra-Sadat Zomorodian},
 comment = {},
 doi = {},
 eprint = {2412.12183v1},
 journal = {arXiv preprint},
 title = {Adopting Explainable-AI to investigate the impact of urban morphology design on energy and environmental performance in dry-arid climates},
 url = {http://arxiv.org/abs/2412.12183v1},
 year = {2024}
}

@article{2412.13421v1,
 abstract = {Machine-generated music (MGM) has become a groundbreaking innovation with wide-ranging applications, such as music therapy, personalised editing, and creative inspiration within the music industry. However, the unregulated proliferation of MGM presents considerable challenges to the entertainment, education, and arts sectors by potentially undermining the value of high-quality human compositions. Consequently, MGM detection (MGMD) is crucial for preserving the integrity of these fields. Despite its significance, MGMD domain lacks comprehensive benchmark results necessary to drive meaningful progress. To address this gap, we conduct experiments on existing large-scale datasets using a range of foundational models for audio processing, establishing benchmark results tailored to the MGMD task. Our selection includes traditional machine learning models, deep neural networks, Transformer-based architectures, and State Space Models (SSM). Recognising the inherently multimodal nature of music, which integrates both melody and lyrics, we also explore fundamental multimodal models in our experiments. Beyond providing basic binary classification outcomes, we delve deeper into model behaviour using multiple explainable Aritificial Intelligence (XAI) tools, offering insights into their decision-making processes. Our analysis reveals that ResNet18 performs the best according to in-domain and out-of-domain tests. By providing a comprehensive comparison of benchmark results and their interpretability, we propose several directions to inspire future research to develop more robust and effective detection methods for MGM.},
 author = {Yupei Li and Qiyang Sun and Hanqian Li and Lucia Specia and Björn W. Schuller},
 comment = {},
 doi = {},
 eprint = {2412.13421v1},
 journal = {arXiv preprint},
 title = {Detecting Machine-Generated Music with Explainability -- A Challenge and Early Benchmarks},
 url = {http://arxiv.org/abs/2412.13421v1},
 year = {2024}
}

@article{2412.13866v1,
 abstract = {The ubiquitous use of Shapley values in eXplainable AI (XAI) has been triggered by the tool SHAP, and as a result are commonly referred to as SHAP scores. Recent work devised examples of machine learning (ML) classifiers for which the computed SHAP scores are thoroughly unsatisfactory, by allowing human decision-makers to be misled. Nevertheless, such examples could be perceived as somewhat artificial, since the selected classes must be interpreted as numeric. Furthermore, it was unclear how general were the issues identified with SHAP scores. This paper answers these criticisms. First, the paper shows that for Boolean classifiers there are arbitrarily many examples for which the SHAP scores must be deemed unsatisfactory. Second, the paper shows that the issues with SHAP scores are also observed in the case of regression models. In addition, the paper studies the class of regression models that respect Lipschitz continuity, a measure of a function's rate of change that finds important recent uses in ML, including model robustness. Concretely, the paper shows that the issues with SHAP scores occur even for regression models that respect Lipschitz continuity. Finally, the paper shows that the same issues are guaranteed to exist for arbitrarily differentiable regression models.},
 author = {Olivier Letoffe and Xuanxiang Huang and Joao Marques-Silva},
 comment = {arXiv admin note: text overlap with arXiv:2405.00076},
 doi = {},
 eprint = {2412.13866v1},
 journal = {arXiv preprint},
 title = {SHAP scores fail pervasively even when Lipschitz succeeds},
 url = {http://arxiv.org/abs/2412.13866v1},
 year = {2024}
}

@article{2412.14056v1,
 abstract = {Artificial intelligence (AI) has rapidly developed through advancements in computational power and the growth of massive datasets. However, this progress has also heightened challenges in interpreting the "black-box" nature of AI models. To address these concerns, eXplainable AI (XAI) has emerged with a focus on transparency and interpretability to enhance human understanding and trust in AI decision-making processes. In the context of multimodal data fusion and complex reasoning scenarios, the proposal of Multimodal eXplainable AI (MXAI) integrates multiple modalities for prediction and explanation tasks. Meanwhile, the advent of Large Language Models (LLMs) has led to remarkable breakthroughs in natural language processing, yet their complexity has further exacerbated the issue of MXAI. To gain key insights into the development of MXAI methods and provide crucial guidance for building more transparent, fair, and trustworthy AI systems, we review the MXAI methods from a historical perspective and categorize them across four eras: traditional machine learning, deep learning, discriminative foundation models, and generative LLMs. We also review evaluation metrics and datasets used in MXAI research, concluding with a discussion of future challenges and directions. A project related to this review has been created at https://github.com/ShilinSun/mxai_review.},
 author = {Shilin Sun and Wenbin An and Feng Tian and Fang Nan and Qidong Liu and Jun Liu and Nazaraf Shah and Ping Chen},
 comment = {This work has been submitted to the IEEE for possible publication},
 doi = {},
 eprint = {2412.14056v1},
 journal = {arXiv preprint},
 title = {A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future},
 url = {http://arxiv.org/abs/2412.14056v1},
 year = {2024}
}

@article{2412.14869v1,
 abstract = {Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood within the skull, which occurs due to the rupture of blood vessels in or around the brain. If this condition is not diagnosed in a timely manner and appropriately treated, it can lead to serious complications such as decreased consciousness, permanent neurological disabilities, or even death.The primary aim of this study is to detect the occurrence or non-occurrence of ICH, followed by determining the type of subdural hemorrhage (SDH). These tasks are framed as two separate binary classification problems. By adding two layers to the co-scale convolutional attention (CCA) classifier architecture, we introduce a novel approach for ICH detection. In the first layer, after extracting features from different slices of computed tomography (CT) scan images, we combine these features and select the 50 components that capture the highest variance in the data, considering them as informative features. We then assess the discriminative power of these features using the bootstrap forest algorithm, discarding those that lack sufficient discriminative ability between different classes. This algorithm explicitly determines the contribution of each feature to the final prediction, assisting us in developing an explainable AI model. The features feed into a boosting neural network as a latent feature space. In the second layer, we introduce a novel uncertainty-based fuzzy integral operator to fuse information from different CT scan slices. This operator, by accounting for the dependencies between consecutive slices, significantly improves detection accuracy.},
 author = {Mehdi Hosseini Chagahi and Md. Jalil Piran and Niloufar Delfan and Behzad Moshiri and Jaber Hatam Parikhan},
 comment = {},
 doi = {},
 eprint = {2412.14869v1},
 journal = {arXiv preprint},
 title = {AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature Screening},
 url = {http://arxiv.org/abs/2412.14869v1},
 year = {2024}
}

@article{2412.15748v2,
 abstract = {Background: Despite the current ubiquity of Large Language Models (LLMs) across the medical domain, there is a surprising lack of studies which address their reasoning behaviour. We emphasise the importance of understanding reasoning behaviour as opposed to high-level prediction accuracies, since it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical LLMs used in the clinical domain will have a significant impact across the healthcare sector. Results: Therefore, in this work, we adapt the existing concept of reasoning behaviour and articulate its interpretation within the specific context of medical LLMs. We survey and categorise current state-of-the-art approaches for modeling and evaluating reasoning reasoning in medical LLMs. Additionally, we propose theoretical frameworks which can empower medical professionals or machine learning engineers to gain insight into the low-level reasoning operations of these previously obscure models. We also outline key open challenges facing the development of Large Reasoning Models. Conclusion: The subsequent increased transparency and trust in medical machine learning models by clinicians as well as patients will accelerate the integration, application as well as further development of medical AI for the healthcare system as a whole.},
 author = {Shamus Sim and Tyrone Chen},
 comment = {25 pages, 7 figures, 3 tables. Conceptualization, both authors. formal analysis, both authors. funding acquisition, both authors. investigation, both authors. resources, both authors. supervision, T.C.. validation, both authors. visualization, both authors. writing original draft, both authors. writing review and editing, both authors},
 doi = {},
 eprint = {2412.15748v2},
 journal = {arXiv preprint},
 title = {Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models},
 url = {http://arxiv.org/abs/2412.15748v2},
 year = {2024}
}

@article{2412.15828v2,
 abstract = {Integrating AI in healthcare can greatly improve patient care and system efficiency. However, the lack of explainability in AI systems (XAI) hinders their clinical adoption, especially in multimodal settings that use increasingly complex model architectures. Most existing XAI methods focus on unimodal models, which fail to capture cross-modal interactions crucial for understanding the combined impact of multiple data sources. Existing methods for quantifying cross-modal interactions are limited to two modalities, rely on labelled data, and depend on model performance. This is problematic in healthcare, where XAI must handle multiple data sources and provide individualised explanations. This paper introduces InterSHAP, a cross-modal interaction score that addresses the limitations of existing approaches. InterSHAP uses the Shapley interaction index to precisely separate and quantify the contributions of the individual modalities and their interactions without approximations. By integrating an open-source implementation with the SHAP package, we enhance reproducibility and ease of use. We show that InterSHAP accurately measures the presence of cross-modal interactions, can handle multiple modalities, and provides detailed explanations at a local level for individual samples. Furthermore, we apply InterSHAP to multimodal medical datasets and demonstrate its applicability for individualised explanations.},
 author = {Laura Wenderoth and Konstantin Hemker and Nikola Simidjievski and Mateja Jamnik},
 comment = {},
 doi = {10.1609/aaai.v39i20.35452},
 eprint = {2412.15828v2},
 journal = {arXiv preprint},
 title = {Measuring Cross-Modal Interactions in Multimodal Models},
 url = {http://arxiv.org/abs/2412.15828v2},
 year = {2024}
}

@article{2412.16003v2,
 abstract = {Explaining machine learning (ML) models using eXplainable AI (XAI) techniques has become essential to make them more transparent and trustworthy. This is especially important in high-stakes domains like healthcare, where understanding model decisions is critical to ensure ethical, sound, and trustworthy outcome predictions. However, users are often confused about which explanability method to choose for their specific use case. We present a comparative analysis of widely used explainability methods, Shapley Additive Explanations (SHAP) and Gradient-weighted Class Activation Mapping (Grad-CAM), within the domain of human activity recognition (HAR) utilizing graph convolutional networks (GCNs). By evaluating these methods on skeleton-based data from two real-world datasets, including a healthcare-critical cerebral palsy (CP) case, this study provides vital insights into both approaches' strengths, limitations, and differences, offering a roadmap for selecting the most appropriate explanation method based on specific models and applications. We quantitatively and quantitatively compare these methods, focusing on feature importance ranking, interpretability, and model sensitivity through perturbation experiments. While SHAP provides detailed input feature attribution, Grad-CAM delivers faster, spatially oriented explanations, making both methods complementary depending on the application's requirements. Given the importance of XAI in enhancing trust and transparency in ML models, particularly in sensitive environments like healthcare, our research demonstrates how SHAP and Grad-CAM could complement each other to provide more interpretable and actionable model explanations.},
 author = {Felix Tempel and Daniel Groos and Espen Alexander F. Ihlen and Lars Adde and Inga Strümke},
 comment = {},
 doi = {10.1007/s10489-025-06968-3},
 eprint = {2412.16003v2},
 journal = {arXiv preprint},
 title = {Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition},
 url = {http://arxiv.org/abs/2412.16003v2},
 year = {2024}
}

@article{2412.16098v2,
 abstract = {Detecting and analyzing complex patterns in multivariate time-series data is crucial for decision-making in urban and environmental system operations. However, challenges arise from the high dimensionality, intricate complexity, and interconnected nature of complex patterns, which hinder the understanding of their underlying physical processes. Existing AI methods often face limitations in interpretability, computational efficiency, and scalability, reducing their applicability in real-world scenarios. This paper proposes a novel visual analytics framework that integrates two generative AI models, Temporal Fusion Transformer (TFT) and Variational Autoencoders (VAEs), to reduce complex patterns into lower-dimensional latent spaces and visualize them in 2D using dimensionality reduction techniques such as PCA, t-SNE, and UMAP with DBSCAN. These visualizations, presented through coordinated and interactive views and tailored glyphs, enable intuitive exploration of complex multivariate temporal patterns, identifying patterns' similarities and uncover their potential correlations for a better interpretability of the AI outputs. The framework is demonstrated through a case study on power grid signal data, where it identifies multi-label grid event signatures, including faults and anomalies with diverse root causes. Additionally, novel metrics and visualizations are introduced to validate the models and evaluate the performance, efficiency, and consistency of latent maps generated by TFT and VAE under different configurations. These analyses provide actionable insights for model parameter tuning and reliability improvements. Comparative results highlight that TFT achieves shorter run times and superior scalability to diverse time-series data shapes compared to VAE. This work advances fault diagnosis in multivariate time series, fostering explainable AI to support critical system operations.},
 author = {Haowen Xu and Ali Boyaci and Jianming Lian and Aaron Wilson},
 comment = {},
 doi = {},
 eprint = {2412.16098v2},
 journal = {arXiv preprint},
 title = {Explainable AI for Multivariate Time Series Pattern Exploration: Latent Space Visual Analytics with Temporal Fusion Transformer and Variational Autoencoders in Power Grid Event Diagnosis},
 url = {http://arxiv.org/abs/2412.16098v2},
 year = {2024}
}

@article{2412.16196v1,
 abstract = {Today, crop diversification in agriculture is a critical issue to meet the increasing demand for food and improve food safety and quality. This issue is considered to be the most important challenge for the next generation of agriculture due to the diminishing natural resources, the limited arable land, and unpredictable climatic conditions caused by climate change. In this paper, we employ emerging technologies such as the Internet of Things (IoT), machine learning (ML), and explainable artificial intelligence (XAI) to improve operational efficiency and productivity in the agricultural sector. Specifically, we propose an edge computing-based explainable crop recommendation system, AgroXAI, which suggests suitable crops for a region based on weather and soil conditions. In this system, we provide local and global explanations of ML model decisions with methods such as ELI5, LIME, SHAP, which we integrate into ML models. More importantly, we provide regional alternative crop recommendations with the counterfactual explainability method. In this way, we envision that our proposed AgroXAI system will be a platform that provides regional crop diversity in the next generation agriculture.},
 author = {Ozlem Turgut and Ibrahim Kok and Suat Ozdemir},
 comment = {Accepted in 2024 IEEE International Conference on Big Data (IEEE BigData), 10 pages, 9 Figures, 5 Tables},
 doi = {},
 eprint = {2412.16196v1},
 journal = {arXiv preprint},
 title = {AgroXAI: Explainable AI-Driven Crop Recommendation System for Agriculture 4.0},
 url = {http://arxiv.org/abs/2412.16196v1},
 year = {2024}
}

@article{2501.00154v1,
 abstract = {Formal XAI is an emerging field that focuses on providing explanations with mathematical guarantees for the decisions made by machine learning models. A significant amount of work in this area is centered on the computation of "sufficient reasons". Given a model $M$ and an input instance $\vec{x}$, a sufficient reason for the decision $M(\vec{x})$ is a subset $S$ of the features of $\vec{x}$ such that for any instance $\vec{z}$ that has the same values as $\vec{x}$ for every feature in $S$, it holds that $M(\vec{x}) = M(\vec{z})$. Intuitively, this means that the features in $S$ are sufficient to fully justify the classification of $\vec{x}$ by $M$. For sufficient reasons to be useful in practice, they should be as small as possible, and a natural way to reduce the size of sufficient reasons is to consider a probabilistic relaxation; the probability of $M(\vec{x}) = M(\vec{z})$ must be at least some value $δ\in (0,1]$, for a random instance $\vec{z}$ that coincides with $\vec{x}$ on the features in $S$. Computing small $δ$-sufficient reasons ($δ$-SRs) is known to be a theoretically hard problem; even over decision trees--traditionally deemed simple and interpretable models--strong inapproximability results make the efficient computation of small $δ$-SRs unlikely. We propose the notion of $(δ, ε)$-SR, a simple relaxation of $δ$-SRs, and show that this kind of explanation can be computed efficiently over linear models.},
 author = {Bernardo Subercaseaux and Marcelo Arenas and Kuldeep S Meel},
 comment = {Extended version of AAAI paper},
 doi = {},
 eprint = {2501.00154v1},
 journal = {arXiv preprint},
 title = {Probabilistic Explanations for Linear Models},
 url = {http://arxiv.org/abs/2501.00154v1},
 year = {2024}
}

@article{2501.00464v1,
 abstract = {Malaria remains a significant global health burden, particularly in resource-limited regions where timely and accurate diagnosis is critical to effective treatment and control. Deep Learning (DL) has emerged as a transformative tool for automating malaria detection and it offers high accuracy and scalability. However, the effectiveness of these models is constrained by challenges in data quality and model generalization including imbalanced datasets, limited diversity and annotation variability. These issues reduce diagnostic reliability and hinder real-world applicability.
  This article provides a comprehensive analysis of these challenges and their implications for malaria detection performance. Key findings highlight the impact of data imbalances which can lead to a 20\% drop in F1-score and regional biases which significantly hinder model generalization. Proposed solutions, such as GAN-based augmentation, improved accuracy by 15-20\% by generating synthetic data to balance classes and enhance dataset diversity. Domain adaptation techniques, including transfer learning, further improved cross-domain robustness by up to 25\% in sensitivity.
  Additionally, the development of diverse global datasets and collaborative data-sharing frameworks is emphasized as a cornerstone for equitable and reliable malaria diagnostics. The role of explainable AI techniques in improving clinical adoption and trustworthiness is also underscored. By addressing these challenges, this work advances the field of AI-driven malaria detection and provides actionable insights for researchers and practitioners. The proposed solutions aim to support the development of accessible and accurate diagnostic tools, particularly for resource-constrained populations.},
 author = {Kiswendsida Kisito Kabore and Desire Guel},
 comment = {22 pages, 17 figures, 17 tables, In: Journal of Sensor Networks and Data Communications (JSNDC). ISSN: 2994-6433. DOI: 10.33140/JSNDC.04.03.09},
 doi = {10.33140/JSNDC.04.03.09},
 eprint = {2501.00464v1},
 journal = {arXiv preprint},
 title = {Addressing Challenges in Data Quality and Model Generalization for Malaria Detection},
 url = {http://arxiv.org/abs/2501.00464v1},
 year = {2024}
}

@article{2501.00537v1,
 abstract = {Explainable Artificial Intelligence (XAI) plays an important role in improving the transparency and reliability of complex machine learning models, especially in critical domains such as cybersecurity. Despite the prevalence of heuristic interpretation methods such as SHAP and LIME, these techniques often lack formal guarantees and may produce inconsistent local explanations. To fulfill this need, few tools have emerged that use formal methods to provide formal explanations. Among these, XReason uses a SAT solver to generate formal instance-level explanation for XGBoost models. In this paper, we extend the XReason tool to support LightGBM models as well as class-level explanations. Additionally, we implement a mechanism to generate and detect adversarial examples in XReason. We evaluate the efficiency and accuracy of our approach on the CICIDS-2017 dataset, a widely used benchmark for detecting network attacks.},
 author = {Amira Jemaa and Adnan Rashid and Sofiene Tahar},
 comment = {International Congress on Information and Communication Technology (ICICT), Lecture Notes in Networks and Systems (LNNS), Springer, 2025},
 doi = {},
 eprint = {2501.00537v1},
 journal = {arXiv preprint},
 title = {Extending XReason: Formal Explanations for Adversarial Detection},
 url = {http://arxiv.org/abs/2501.00537v1},
 year = {2024}
}

@article{2501.00636v1,
 abstract = {Layer fusion techniques are critical to improving the inference efficiency of deep neural networks (DNN) for deployment. Fusion aims to lower inference costs by reducing data transactions between an accelerator's on-chip buffer and DRAM. This is accomplished by grouped execution of multiple operations like convolution and activations together into single execution units - fusion groups. However, on-chip buffer capacity limits fusion group size and optimizing fusion on whole DNNs requires partitioning into multiple fusion groups. Finding the optimal groups is a complex problem where the presence of invalid solutions hampers traditional search algorithms and demands robust approaches. In this paper we incorporate Explainable AI, specifically Graph Explanation Techniques (GET), into layer fusion. Given an invalid fusion group, we identify the operations most responsible for group invalidity, then use this knowledge to recursively split the original fusion group via a greedy tree-based algorithm to minimize DRAM access. We pair our scheme with common algorithms and optimize DNNs on two types of layer fusion: Line-Buffer Depth First (LBDF) and Branch Requirement Reduction (BRR). Experiments demonstrate the efficacy of our scheme on several popular and classical convolutional neural networks like ResNets and MobileNets. Our scheme achieves over 20% DRAM Access reduction on EfficientNet-B3.},
 author = {Keith G. Mills and Muhammad Fetrat Qharabagh and Weichen Qiu and Fred X. Han and Mohammad Salameh and Wei Lu and Shangling Jui and Di Niu},
 comment = {DAC'23 WIP Poster; 8 pages, 5 Figures 5 Tables},
 doi = {},
 eprint = {2501.00636v1},
 journal = {arXiv preprint},
 title = {Applying Graph Explanation to Operator Fusion},
 url = {http://arxiv.org/abs/2501.00636v1},
 year = {2024}
}

@article{2501.00777v3,
 abstract = {Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an important finding for future research in this direction.},
 author = {Qianli Wang and Nils Feldhus and Simon Ostermann and Luis Felipe Villa-Arenas and Sebastian Möller and Vera Schmitt},
 comment = {ACL 2025 Findings; camera-ready version},
 doi = {},
 eprint = {2501.00777v3},
 journal = {arXiv preprint},
 title = {FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation},
 url = {http://arxiv.org/abs/2501.00777v3},
 year = {2025}
}

@article{2501.01209v1,
 abstract = {Deep learning models (DLMs) achieve increasingly high performance both on structured and unstructured data. They significantly extended applicability of machine learning to various domains. Their success in making predictions, detecting patterns and generating new data made significant impact on science and industry. Despite these accomplishments, DLMs are difficult to explain because of their enormous size. In this work, we propose a novel framework for post-hoc explaining and relating DLMs using redescriptions. The framework allows cohort analysis of arbitrary DLMs by identifying statistically significant redescriptions of neuron activations. It allows coupling neurons to a set of target labels or sets of descriptive attributes, relating layers within a single DLM or associating different DLMs. The proposed framework is independent of the artificial neural network architecture and can work with more complex target labels (e.g. multi-label or multi-target scenario). Additionally, it can emulate both pedagogical and decompositional approach to rule extraction. The aforementioned properties of the proposed framework can increase explainability and interpretability of arbitrary DLMs by providing different information compared to existing explainable-AI approaches.},
 author = {Matej Mihelčić and Ivan Grubišić and Miha Keber},
 comment = {},
 doi = {},
 eprint = {2501.01209v1},
 journal = {arXiv preprint},
 title = {A redescription mining framework for post-hoc explaining and relating deep learning models},
 url = {http://arxiv.org/abs/2501.01209v1},
 year = {2025}
}

@article{2501.01516v1,
 abstract = {Explainable AI (XAI) has seen a surge in recent interest with the proliferation of powerful but intractable black-box models. Moreover, XAI has come under fire for techniques that may not offer reliable explanations. As many of the methods in XAI are themselves models, adversarial examples have been prominent in the literature surrounding the effectiveness of XAI, with the objective of these examples being to alter the explanation while maintaining the output of the original model. For explanations in natural language, it is natural to use measures found in the domain of information retrieval for use with ranked lists to guide the adversarial XAI process. We show that the standard implementation of these measures are poorly suited for the comparison of explanations in adversarial XAI and amend them by using information that is discarded, the synonymity of perturbed words. This synonymity weighting produces more accurate estimates of the actual weakness of XAI methods to adversarial examples.},
 author = {Christopher Burger},
 comment = {10 pages, 2 figures, 4 tables},
 doi = {},
 eprint = {2501.01516v1},
 journal = {arXiv preprint},
 title = {Improving Robustness Estimates in Natural Language Explainable AI though Synonymity Weighted Similarity Measures},
 url = {http://arxiv.org/abs/2501.01516v1},
 year = {2025}
}

@article{2501.01576v3,
 abstract = {The integration of machine learning (ML) into chemistry offers transformative potential in the design of molecules with targeted properties. However, the focus has often been on creating highly efficient predictive models, sometimes at the expense of interpretability. In this study, we leverage explainable AI techniques to explore the rational design of boron-based Lewis acids, which play a pivotal role in organic reactions due to their electron-ccepting properties. Using Fluoride Ion Affinity as a proxy for Lewis acidity, we developed interpretable ML models based on chemically meaningful descriptors, including ab initio computed features and substituent-based parameters derived from the Hammett linear free-energy relationship. By constraining the chemical space to well-defined molecular scaffolds, we achieved highly accurate predictions (mean absolute error < 6 kJ/mol), surpassing conventional black-box deep learning models in low-data regimes. Interpretability analyses of the models shed light on the origin of Lewis acidity in these compounds and identified actionable levers to modulate it through the nature and positioning of substituents on the molecular scaffold. This work bridges ML and chemist's way of thinking, demonstrating how explainable models can inspire molecular design and enhance scientific understanding of chemical reactivity.},
 author = {Juliette Fenogli and Laurence Grimaud and Rodolphe Vuilleumier},
 comment = {Main text is 14 pages, 7 figures, 1 scheme. Supporting information is 25 pages. For associated code and datasets, see https://github.com/jfenogli/XAI_boron_LA},
 doi = {},
 eprint = {2501.01576v3},
 journal = {arXiv preprint},
 title = {Constructing and explaining machine learning models for chemistry: example of the exploration and design of boron-based Lewis acids},
 url = {http://arxiv.org/abs/2501.01576v3},
 year = {2025}
}

@article{2501.02042v1,
 abstract = {Recent work has investigated the concept of adversarial attacks on explainable AI (XAI) in the NLP domain with a focus on examining the vulnerability of local surrogate methods such as Lime to adversarial perturbations or small changes on the input of a machine learning (ML) model. In such attacks, the generated explanation is manipulated while the meaning and structure of the original input remain similar under the ML model. Such attacks are especially alarming when XAI is used as a basis for decision making (e.g., prescribing drugs based on AI medical predictors) or for legal action (e.g., legal dispute involving AI software). Although weaknesses across many XAI methods have been shown to exist, the reasons behind why remain little explored. Central to this XAI manipulation is the similarity measure used to calculate how one explanation differs from another. A poor choice of similarity measure can lead to erroneous conclusions about the stability or adversarial robustness of an XAI method. Therefore, this work investigates a variety of similarity measures designed for text-based ranked lists referenced in related work to determine their comparative suitability for use. We find that many measures are overly sensitive, resulting in erroneous estimates of stability. We then propose a weighting scheme for text-based data that incorporates the synonymity between the features within an explanation, providing more accurate estimates of the actual weakness of XAI methods to adversarial examples.},
 author = {Christopher Burger and Charles Walter and Thai Le and Lingwei Chen},
 comment = {12 pages, 1 figure, 4 tables. arXiv admin note: substantial text overlap with arXiv:2406.15839. substantial text overlap with arXiv:2501.01516},
 doi = {},
 eprint = {2501.02042v1},
 journal = {arXiv preprint},
 title = {Towards Robust and Accurate Stability Estimation of Local Surrogate Models in Text-based Explainable AI},
 url = {http://arxiv.org/abs/2501.02042v1},
 year = {2025}
}

@article{2501.02891v2,
 abstract = {Humour styles can have either a negative or a positive impact on well-being. Given the importance of these styles to mental health, significant research has been conducted on their automatic identification. However, the automated machine learning models used for this purpose are black boxes, making their prediction decisions opaque. Clarity and transparency are vital in the field of mental health. This paper presents an explainable AI (XAI) framework for understanding humour style classification, building upon previous work in computational humour analysis. Using the best-performing single model (ALI+XGBoost) from prior research, we apply comprehensive XAI techniques to analyse how linguistic, emotional, and semantic features contribute to humour style classification decisions. Our analysis reveals distinct patterns in how different humour styles are characterised and misclassified, with particular emphasis on the challenges in distinguishing affiliative humour from other styles. Through detailed examination of feature importance, error patterns, and misclassification cases, we identify key factors influencing model decisions, including emotional ambiguity, context misinterpretation, and target identification. The framework demonstrates significant utility in understanding model behaviour, achieving interpretable insights into the complex interplay of features that define different humour styles. Our findings contribute to both the theoretical understanding of computational humour analysis and practical applications in mental health, content moderation, and digital humanities research.},
 author = {Mary Ogbuka Kenneth and Foaad Khosmood and Abbas Edalat},
 comment = {},
 doi = {10.46298/jdmdh.15031},
 eprint = {2501.02891v2},
 journal = {arXiv preprint},
 title = {Explaining Humour Style Classifications: An XAI Approach to Understanding Computational Humour Analysis},
 url = {http://arxiv.org/abs/2501.02891v2},
 year = {2025}
}

@article{2501.03041v1,
 abstract = {We propose Group Shapley, a metric that extends the classical individual-level Shapley value framework to evaluate the importance of feature groups, addressing the structured nature of predictors commonly found in business and economic data. More importantly, we develop a significance testing procedure based on a three-cumulant chi-square approximation and establish the asymptotic properties of the test statistics for Group Shapley values. Our approach can effectively handle challenging scenarios, including sparse or skewed distributions and small sample sizes, outperforming alternative tests such as the Wald test. Simulations confirm that the proposed test maintains robust empirical size and demonstrates enhanced power under diverse conditions. To illustrate the method's practical relevance in advancing Explainable AI, we apply our framework to bond recovery rate predictions using a global dataset (1996-2023) comprising 2,094 observations and 98 features, grouped into 16 subgroups and five broader categories: bond characteristics, firm fundamentals, industry-specific factors, market-related variables, and macroeconomic indicators. Our results identify the market-related variables group as the most influential. Furthermore, Lorenz curves and Gini indices reveal that Group Shapley assigns feature importance more equitably compared to individual Shapley values.},
 author = {Jingyi Wang and Ying Chen and Paolo Giudici},
 comment = {},
 doi = {},
 eprint = {2501.03041v1},
 journal = {arXiv preprint},
 title = {Group Shapley with Robust Significance Testing and Its Application to Bond Recovery Rate Prediction},
 url = {http://arxiv.org/abs/2501.03041v1},
 year = {2025}
}

@article{2501.03203v1,
 abstract = {This study seeks to enhance academic integrity by providing tools to detect AI-generated content in student work using advanced technologies. The findings promote transparency and accountability, helping educators maintain ethical standards and supporting the responsible integration of AI in education. A key contribution of this work is the generation of the CyberHumanAI dataset, which has 1000 observations, 500 of which are written by humans and the other 500 produced by ChatGPT. We evaluate various machine learning (ML) and deep learning (DL) algorithms on the CyberHumanAI dataset comparing human-written and AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT). Results demonstrate that traditional ML algorithms, specifically XGBoost and Random Forest, achieve high performance (83% and 81% accuracies respectively). Results also show that classifying shorter content seems to be more challenging than classifying longer content. Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow). Meanwhile AI-generated text is characterized by more abstract and formal terms (e.g., realm and employ). Finally, a comparative analysis with GPTZero show that our narrowly focused, simple, and fine-tuned model can outperform generalized systems like GPTZero. The proposed model achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when tasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a tendency to classify challenging and small-content cases as either mixed or unrecognized while our proposed model showed a more balanced performance across the three classes.},
 author = {Ayat A. Najjar and Huthaifa I. Ashqar and Omar A. Darwish and Eman Hammad},
 comment = {},
 doi = {},
 eprint = {2501.03203v1},
 journal = {arXiv preprint},
 title = {Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity},
 url = {http://arxiv.org/abs/2501.03203v1},
 year = {2025}
}

@article{2501.03212v1,
 abstract = {The development of Generative AI Large Language Models (LLMs) raised the alarm regarding identifying content produced through generative AI or humans. In one case, issues arise when students heavily rely on such tools in a manner that can affect the development of their writing or coding skills. Other issues of plagiarism also apply. This study aims to support efforts to detect and identify textual content generated using LLM tools. We hypothesize that LLMs-generated text is detectable by machine learning (ML), and investigate ML models that can recognize and differentiate texts generated by multiple LLMs tools. We leverage several ML and Deep Learning (DL) algorithms such as Random Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable Artificial Intelligence (XAI) to understand the important features in attribution. Our method is divided into 1) binary classification to differentiate between human-written and AI-text, and 2) multi classification, to differentiate between human-written text and the text generated by the five different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity). Results show high accuracy in the multi and binary classification. Our model outperformed GPTZero with 98.5\% accuracy to 78.3\%. Notably, GPTZero was unable to recognize about 4.2\% of the observations, but our model was able to recognize the complete test dataset. XAI results showed that understanding feature importance across different classes enables detailed author/source profiles. Further, aiding in attribution and supporting plagiarism detection by highlighting unique stylistic and structural elements ensuring robust content originality verification.},
 author = {Ayat Najjar and Huthaifa I. Ashqar and Omar Darwish and Eman Hammad},
 comment = {},
 doi = {},
 eprint = {2501.03212v1},
 journal = {arXiv preprint},
 title = {Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text},
 url = {http://arxiv.org/abs/2501.03212v1},
 year = {2025}
}

@article{2501.03282v1,
 abstract = {Uncertainty quantification (UQ) is a critical aspect of artificial intelligence (AI) systems, particularly in high-risk domains such as healthcare, autonomous systems, and financial technology, where decision-making processes must account for uncertainty. This review explores the evolution of uncertainty quantification techniques in AI, distinguishing between aleatoric and epistemic uncertainties, and discusses the mathematical foundations and methods used to quantify these uncertainties. We provide an overview of advanced techniques, including probabilistic methods, ensemble learning, sampling-based approaches, and generative models, while also highlighting hybrid approaches that integrate domain-specific knowledge. Furthermore, we examine the diverse applications of UQ across various fields, emphasizing its impact on decision-making, predictive accuracy, and system robustness. The review also addresses key challenges such as scalability, efficiency, and integration with explainable AI, and outlines future directions for research in this rapidly developing area. Through this comprehensive survey, we aim to provide a deeper understanding of UQ's role in enhancing the reliability, safety, and trustworthiness of AI systems.},
 author = {Tianyang Wang and Yunze Wang and Jun Zhou and Benji Peng and Xinyuan Song and Charles Zhang and Xintian Sun and Qian Niu and Junyu Liu and Silin Chen and Keyu Chen and Ming Li and Pohsun Feng and Ziqian Bi and Ming Liu and Yichao Zhang and Cheng Fei and Caitlyn Heqi Yin and Lawrence KQ Yan},
 comment = {14 pages},
 doi = {},
 eprint = {2501.03282v1},
 journal = {arXiv preprint},
 title = {From Aleatoric to Epistemic: Exploring Uncertainty Quantification Techniques in Artificial Intelligence},
 url = {http://arxiv.org/abs/2501.03282v1},
 year = {2025}
}

@article{2501.03923v1,
 abstract = {Neurodegenerative diseases (NDDs) are complex and lack effective treatment due to their poorly understood mechanism. The increasingly used data analysis from Single nucleus RNA Sequencing (snRNA-seq) allows to explore transcriptomic events at a single cell level, yet face challenges in interpreting the mechanisms underlying a disease. On the other hand, Neural Network (NN) models can handle complex data to offer insights but can be seen as black boxes with poor interpretability. In this context, explainable AI (XAI) emerges as a solution that could help to understand disease-associated mechanisms when combined with efficient NN models. However, limited research explores XAI in single-cell data. In this work, we implement a method for identifying disease-related genes and the mechanistic explanation of disease progression based on NN model combined with SHAP. We analyze available Huntington's disease (HD) data to identify both HD-altered genes and mechanisms by adding Gene Set Enrichment Analysis (GSEA) comparing two methods, differential gene expression analysis (DGE) and NN combined with SHAP approach. Our results show that DGE and SHAP approaches offer both common and differential sets of altered genes and pathways, reinforcing the usefulness of XAI methods for a broader perspective of disease.},
 author = {Mohammad Usman and Olga Varea and Petia Radeva and Josep Canals and Jordi Abante and Daniel Ortiz},
 comment = {},
 doi = {},
 eprint = {2501.03923v1},
 journal = {arXiv preprint},
 title = {Explainable AI model reveals disease-related mechanisms in single-cell RNA-seq data},
 url = {http://arxiv.org/abs/2501.03923v1},
 year = {2025}
}

@article{2501.04009v2,
 abstract = {Deep Learning systems excel in complex tasks but often lack transparency, limiting their use in critical applications. Counterfactual explanations, a core tool within eXplainable Artificial Intelligence (XAI), offer insights into model decisions by identifying minimal changes to an input to alter its predicted outcome. However, existing methods for time series data are limited by univariate assumptions, rigid constraints on modifications, or lack of validity guarantees. This paper introduces Multi-SpaCE, a multi-objective counterfactual explanation method for multivariate time series. Using non-dominated ranking genetic algorithm II (NSGA-II), Multi-SpaCE balances proximity, sparsity, plausibility, and contiguity. Unlike most methods, it ensures perfect validity, supports multivariate data and provides a Pareto front of solutions, enabling flexibility to different end-user needs. Comprehensive experiments in diverse datasets demonstrate the ability of Multi-SpaCE to consistently achieve perfect validity and deliver superior performance compared to existing methods.},
 author = {Mario Refoyo and David Luengo},
 comment = {},
 doi = {},
 eprint = {2501.04009v2},
 journal = {arXiv preprint},
 title = {Multi-SpaCE: Multi-Objective Subsequence-based Sparse Counterfactual Explanations for Multivariate Time Series Classification},
 url = {http://arxiv.org/abs/2501.04009v2},
 year = {2024}
}

@article{2501.04067v1,
 abstract = {Formula One (F1) race strategy takes place in a high-pressure and fast-paced environment where split-second decisions can drastically affect race results. Two of the core decisions of race strategy are when to make pit stops (i.e. replace the cars' tyres) and which tyre compounds (hard, medium or soft, in normal conditions) to select. The optimal pit stop decisions can be determined by estimating the tyre degradation of these compounds, which in turn can be computed from the energy applied to each tyre, i.e. the tyre energy. In this work, we trained deep learning models, using the Mercedes-AMG PETRONAS F1 team's historic race data consisting of telemetry, to forecast tyre energies during races. Additionally, we fitted XGBoost, a decision tree-based machine learning algorithm, to the same dataset and compared the results, with both giving impressive performance. Furthermore, we incorporated two different explainable AI methods, namely feature importance and counterfactual explanations, to gain insights into the reasoning behind the forecasts. Our contributions thus result in an explainable, automated method which could assist F1 teams in optimising their race strategy.},
 author = {Jamie Todd and Junqi Jiang and Aaron Russo and Steffen Winkler and Stuart Sale and Joseph McMillan and Antonio Rago},
 comment = {9 pages, 9 figures. Copyright ACM 2025. This is the authors' version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record will be published in SAC 2025, http://dx.doi.org/10.1145/3672608.3707765},
 doi = {10.1145/3672608.3707765},
 eprint = {2501.04067v1},
 journal = {arXiv preprint},
 title = {Explainable Time Series Prediction of Tyre Energy in Formula One Race Strategy},
 url = {http://arxiv.org/abs/2501.04067v1},
 year = {2025}
}

@article{2501.05325v1,
 abstract = {Explainable AI (XAI) provides methods to understand non-interpretable machine learning models. However, we have little knowledge about what legal experts expect from these explanations, including their legal compliance with, and value against European Union legislation. To close this gap, we present the Explanation Dialogues, an expert focus study to uncover the expectations, reasoning, and understanding of legal experts and practitioners towards XAI, with a specific focus on the European General Data Protection Regulation. The study consists of an online questionnaire and follow-up interviews, and is centered around a use-case in the credit domain. We extract both a set of hierarchical and interconnected codes using grounded theory, and present the standpoints of the participating experts towards XAI. We find that the presented explanations are hard to understand and lack information, and discuss issues that can arise from the different interests of the data controller and subject. Finally, we present a set of recommendations for developers of XAI methods, and indications of legal areas of discussion. Among others, recommendations address the presentation, choice, and content of an explanation, technical risks as well as the end-user, while we provide legal pointers to the contestability of explanations, transparency thresholds, intellectual property rights as well as the relationship between involved parties.},
 author = {Laura State and Alejandra Bringas Colmenarejo and Andrea Beretta and Salvatore Ruggieri and Franco Turini and Stephanie Law},
 comment = {Artificial Intelligence and Law (Springer Nature)},
 doi = {},
 eprint = {2501.05325v1},
 journal = {arXiv preprint},
 title = {The explanation dialogues: an expert focus study to understand requirements towards explanations within the GDPR},
 url = {http://arxiv.org/abs/2501.05325v1},
 year = {2025}
}

@article{2501.05387v1,
 abstract = {Encrypted network communication ensures confidentiality, integrity, and privacy between endpoints. However, attackers are increasingly exploiting encryption to conceal malicious behavior. Detecting unknown encrypted malicious traffic without decrypting the payloads remains a significant challenge. In this study, we investigate the integration of explainable artificial intelligence (XAI) techniques to detect malicious network traffic. We employ ensemble learning models to identify malicious activity using multi-view features extracted from various aspects of encrypted communication. To effectively represent malicious communication, we compiled a robust dataset with 1,127 unique connections, more than any other available open-source dataset, and spanning 54 malware families. Our models were benchmarked against the CTU-13 dataset, achieving performance of over 99% accuracy, precision, and F1-score. Additionally, the eXtreme Gradient Boosting (XGB) model demonstrated 99.32% accuracy, 99.53% precision, and 99.43% F1-score on our custom dataset. By leveraging Shapley Additive Explanations (SHAP), we identified that the maximum packet size, mean inter-arrival time of packets, and transport layer security version used are the most critical features for the global model explanation. Furthermore, key features were identified as important for local explanations across both datasets for individual traffic samples. These insights provide a deeper understanding of the model decision-making process, enhancing the transparency and reliability of detecting malicious encrypted traffic.},
 author = {Sileshi Nibret Zeleke and Amsalu Fentie Jember and Mario Bochicchio},
 comment = {Accepted and presented on PanAfriCon AI 2024},
 doi = {},
 eprint = {2501.05387v1},
 journal = {arXiv preprint},
 title = {Integrating Explainable AI for Effective Malware Detection in Encrypted Network Traffic},
 url = {http://arxiv.org/abs/2501.05387v1},
 year = {2025}
}

@article{2501.05471v1,
 abstract = {The increasing complexity of machine learning models in computer vision, particularly in face verification, requires the development of explainable artificial intelligence (XAI) to enhance interpretability and transparency. This study extends previous work by integrating semantic concepts derived from human cognitive processes into XAI frameworks to bridge the comprehension gap between model outputs and human understanding. We propose a novel approach combining global and local explanations, using semantic features defined by user-selected facial landmarks to generate similarity maps and textual explanations via large language models (LLMs). The methodology was validated through quantitative experiments and user feedback, demonstrating improved interpretability. Results indicate that our semantic-based approach, particularly the most detailed set, offers a more nuanced understanding of model decisions than traditional methods. User studies highlight a preference for our semantic explanations over traditional pixelbased heatmaps, emphasizing the benefits of human-centric interpretability in AI. This work contributes to the ongoing efforts to create XAI frameworks that align AI models behaviour with human cognitive processes, fostering trust and acceptance in critical applications.},
 author = {Miriam Doh and Caroline Mazini Rodrigues and N. Boutry and L. Najman and Matei Mancas and Bernard Gosselin},
 comment = {},
 doi = {},
 eprint = {2501.05471v1},
 journal = {arXiv preprint},
 title = {Found in Translation: semantic approaches for enhancing AI interpretability in face verification},
 url = {http://arxiv.org/abs/2501.05471v1},
 year = {2025}
}

@article{2501.06077v1,
 abstract = {Causal inference has recently gained notable attention across various fields like biology, healthcare, and environmental science, especially within explainable artificial intelligence (xAI) systems, for uncovering the causal relationships among multiple variables and outcomes. Yet, it has not been fully recognized and deployed in the manufacturing systems. In this paper, we introduce an explainable, scalable, and flexible federated Bayesian learning framework, \texttt{xFBCI}, designed to explore causality through treatment effect estimation in distributed manufacturing systems. By leveraging federated Bayesian learning, we efficiently estimate posterior of local parameters to derive the propensity score for each client without accessing local private data. These scores are then used to estimate the treatment effect using propensity score matching (PSM). Through simulations on various datasets and a real-world Electrohydrodynamic (EHD) printing data, we demonstrate that our approach outperforms standard Bayesian causal inference methods and several state-of-the-art federated learning benchmarks.},
 author = {Xiaofeng Xiao and Khawlah Alharbi and Pengyu Zhang and Hantang Qin and Xubo Yue},
 comment = {26 pages},
 doi = {},
 eprint = {2501.06077v1},
 journal = {arXiv preprint},
 title = {Explainable Federated Bayesian Causal Inference and Its Application in Advanced Manufacturing},
 url = {http://arxiv.org/abs/2501.06077v1},
 year = {2025}
}

@article{2501.06099v1,
 abstract = {Detecting anomalies in energy consumption data is crucial for identifying energy waste, equipment malfunction, and overall, for ensuring efficient energy management. Machine learning, and specifically deep learning approaches, have been greatly successful in anomaly detection; however, they are black-box approaches that do not provide transparency or explanations. SHAP and its variants have been proposed to explain these models, but they suffer from high computational complexity (SHAP) or instability and inconsistency (e.g., Kernel SHAP). To address these challenges, this paper proposes an explainability approach for anomalies in energy consumption data that focuses on context-relevant information. The proposed approach leverages existing explainability techniques, focusing on SHAP variants, together with global feature importance and weighted cosine similarity to select background dataset based on the context of each anomaly point. By focusing on the context and most relevant features, this approach mitigates the instability of explainability algorithms. Experimental results across 10 different machine learning models, five datasets, and five XAI techniques, demonstrate that our method reduces the variability of explanations providing consistent explanations. Statistical analyses confirm the robustness of our approach, showing an average reduction in variability of approximately 38% across multiple datasets.},
 author = {Mohammad Noorchenarboo and Katarina Grolinger},
 comment = {26 pages, 8 figures},
 doi = {10.1016/j.enbuild.2024.115177},
 eprint = {2501.06099v1},
 journal = {arXiv preprint},
 title = {Explaining Deep Learning-based Anomaly Detection in Energy Consumption Data by Focusing on Contextually Relevant Data},
 url = {http://arxiv.org/abs/2501.06099v1},
 year = {2025}
}

@article{2501.06210v1,
 abstract = {This study explores using Natural Language Processing in aviation safety, focusing on machine learning algorithms to enhance safety measures. There are currently May 2024, 34 Scopus results from the keyword search natural language processing and aviation safety. Analyzing these studies allows us to uncover trends in the methodologies, findings and implications of NLP in aviation. Both qualitative and quantitative tools have been used to investigate the current state of literature on NLP for aviation safety. The qualitative analysis summarises the research motivations, objectives, and outcomes, showing how NLP can be utilized to help identify critical safety issues and improve aviation safety. This study also identifies research gaps and suggests areas for future exploration, providing practical recommendations for the aviation industry. We discuss challenges in implementing NLP in aviation safety, such as the need for large, annotated datasets, and the difficulty in interpreting complex models. We propose solutions like active learning for data annotation and explainable AI for model interpretation. Case studies demonstrate the successful application of NLP in improving aviation safety, highlighting its potential to make aviation safer and more efficient.},
 author = {Aziida Nanyonga and Keith Joiner and Ugur Turhan and Graham Wild},
 comment = {},
 doi = {},
 eprint = {2501.06210v1},
 journal = {arXiv preprint},
 title = {Applications of natural language processing in aviation safety: A review and qualitative analysis},
 url = {http://arxiv.org/abs/2501.06210v1},
 year = {2025}
}

@article{2501.06222v1,
 abstract = {Acknowledging the effects of outdoor air pollution, the literature inadequately addresses indoor air pollution's impacts. Despite daily health risks, existing research primarily focused on monitoring, lacking accuracy in pinpointing indoor pollution sources. In our research work, we thoroughly investigated the influence of indoor activities on pollution levels. A survey of 143 participants revealed limited awareness of indoor air pollution. Leveraging 65 days of diverse data encompassing activities like incense stick usage, indoor smoking, inadequately ventilated cooking, excessive AC usage, and accidental paper burning, we developed a comprehensive monitoring system. We identify pollutant sources and effects with high precision through clustering analysis and interpretability models (LIME and SHAP). Our method integrates Decision Trees, Random Forest, Naive Bayes, and SVM models, excelling at 99.8% accuracy with Decision Trees. Continuous 24-hour data allows personalized assessments for targeted pollution reduction strategies, achieving 91% accuracy in predicting activities and pollution exposure.},
 author = {Pritisha Sarkar and Kushalava reddy Jala and Mousumi Saha},
 comment = {},
 doi = {},
 eprint = {2501.06222v1},
 journal = {arXiv preprint},
 title = {Can Explainable AI Assess Personalized Health Risks from Indoor Air Pollution?},
 url = {http://arxiv.org/abs/2501.06222v1},
 year = {2025}
}

@article{2501.06571v1,
 abstract = {Multivariate anomaly detection finds its importance in diverse applications. Despite the existence of many detectors to solve this problem, one cannot simply define why an obtained anomaly inferred by the detector is anomalous. This reasoning is required for network operators to understand the root cause of the anomaly and the remedial action that should be taken to counteract its occurrence. Existing solutions in explainable AI may give cues to features that influence an anomaly, but they do not formulate generalizable rules that can be assessed by a domain expert. Furthermore, not all outliers are anomalous in a business sense. There is an unfulfilled need for a system that can interpret anomalies predicted by a multivariate anomaly detector and map these patterns to actionable rules. This paper aims to fulfill this need by proposing a semi-autonomous anomaly rule miner. The proposed method is applicable to both discrete and time series data and is tailored for radio access network (RAN) anomaly detection use cases. The proposed method is demonstrated in this paper with time series RAN data.},
 author = {Ebenezer R. H. P. Isaac and Joseph H. R. Isaac},
 comment = {},
 doi = {},
 eprint = {2501.06571v1},
 journal = {arXiv preprint},
 title = {Active Rule Mining for Multivariate Anomaly Detection in Radio Access Networks},
 url = {http://arxiv.org/abs/2501.06571v1},
 year = {2025}
}

@article{2501.08019v1,
 abstract = {As urbanization accelerates, open spaces are increasingly recognized for their role in enhancing sustainability and well-being, yet they remain underexplored compared to built spaces. This study introduces an AI-driven framework that integrates machine learning models (MLMs) and explainable AI techniques to optimize Sky View Factor (SVF) and visibility, key spatial metrics influencing thermal comfort and perceived safety in urban spaces. Unlike global optimization methods, which are computationally intensive and impractical for localized adjustments, this framework supports incremental design improvements with lower computational costs and greater flexibility. The framework employs SHapley Adaptive Explanations (SHAP) to analyze feature importance and Counterfactual Explanations (CFXs) to propose minimal design changes. Simulations tested five MLMs, identifying XGBoost as the most accurate, with building width, park area, and heights of surrounding buildings as critical for SVF, and distances from southern buildings as key for visibility. Compared to Genetic Algorithms, which required approximately 15/30 minutes across 3/4 generations to converge, the tested CFX approach achieved optimized results in 1 minute with a 5% RMSE error, demonstrating significantly faster performance and suitability for scalable retrofitting strategies. This interpretable and computationally efficient framework advances urban performance optimization, providing data-driven insights and practical retrofitting solutions for enhancing usability and environmental quality across diverse urban contexts.},
 author = {Pegah Eshraghi and Arman Nikkhah Dehnavi and Maedeh Mirdamadi and Riccardo Talami and Zahra-Sadat Zomorodian},
 comment = {36 pages},
 doi = {},
 eprint = {2501.08019v1},
 journal = {arXiv preprint},
 title = {An AI-driven framework for rapid and localized optimizations of urban open spaces},
 url = {http://arxiv.org/abs/2501.08019v1},
 year = {2025}
}

@article{2501.08169v1,
 abstract = {This study introduces an integrated approach to recognizing Arabic Sign Language (ArSL) using state-of-the-art deep learning models such as MobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced by explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and RGB Arabic Alphabets Sign Language (AASL) datasets are employed, with EfficientNet-B2 achieving peak accuracies of 99.48\% and 98.99\%, respectively. Key innovations include sophisticated data augmentation methods to mitigate class imbalance, implementation of stratified 5-fold cross-validation for better generalization, and the use of Grad-CAM for clear model decision transparency. The proposed system not only sets new benchmarks in recognition accuracy but also emphasizes interpretability, making it suitable for applications in healthcare, education, and inclusive communication technologies.},
 author = {Mazen Balat and Rewaa Awaad and Ahmed B. Zaky and Salah A. Aly},
 comment = {13 pages, 25 figures, 16 tables},
 doi = {},
 eprint = {2501.08169v1},
 journal = {arXiv preprint},
 title = {Revolutionizing Communication with Deep Learning and XAI for Enhanced Arabic Sign Language Recognition},
 url = {http://arxiv.org/abs/2501.08169v1},
 year = {2025}
}

@article{2501.08297v1,
 abstract = {The tree-width of a multivariate polynomial is the tree-width of the hypergraph with hyperedges corresponding to its terms. Multivariate polynomials of bounded tree-width have been studied by Makowsky and Meer as a new sparsity condition that allows for polynomial solvability of problems which are intractable in general. We consider a variation on this theme for Boolean variables. A representation of a Boolean function as the sign of a polynomial is called a polynomial threshold representation. We discuss Boolean functions representable as polynomial threshold functions of bounded tree-width and present two applications to Bayesian network classifiers, a probabilistic graphical model. Both applications are in Explainable Artificial Intelligence (XAI), the research area dealing with the black-box nature of many recent machine learning models. We also give a separation result between the representational power of positive and general polynomial threshold functions.},
 author = {Karine Chubarian and Johnny Joyce and Gyorgy Turan},
 comment = {22 pages, 3 figures. To be published in Festschrift in honor of Johann A. Makowsky},
 doi = {},
 eprint = {2501.08297v1},
 journal = {arXiv preprint},
 title = {Polynomial Threshold Functions of Bounded Tree-Width: Some Explainability and Complexity Aspects},
 url = {http://arxiv.org/abs/2501.08297v1},
 year = {2025}
}

@article{2501.08850v1,
 abstract = {Explaining the predictions of a deep neural network is a nontrivial task, yet high-quality explanations for predictions are often a prerequisite for practitioners to trust these models. Counterfactual explanations aim to explain predictions by finding the ''nearest'' in-distribution alternative input whose prediction changes in a pre-specified way. However, it remains an open question how to define this nearest alternative input, whose solution depends on both the domain (e.g. images, graphs, tabular data, etc.) and the specific application considered. For graphs, this problem is complicated i) by their discrete nature, as opposed to the continuous nature of state-of-the-art graph classifiers; and ii) by the node permutation group acting on the graphs. We propose a method to generate counterfactual explanations for any differentiable black-box graph classifier, utilizing a case-specific permutation equivariant graph variational autoencoder. We generate counterfactual explanations in a continuous fashion by traversing the latent space of the autoencoder across the classification boundary of the classifier, allowing for seamless integration of discrete graph structure and continuous graph attributes. We empirically validate the approach on three graph datasets, showing that our model is consistently high-performing and more robust than the baselines.},
 author = {Andreas Abildtrup Hansen and Paraskevas Pegios and Anna Calissano and Aasa Feragen},
 comment = {Published at Northern Lights Deep Learning Conference 2025},
 doi = {},
 eprint = {2501.08850v1},
 journal = {arXiv preprint},
 title = {Graph Counterfactual Explainable AI via Latent Space Traversal},
 url = {http://arxiv.org/abs/2501.08850v1},
 year = {2025}
}

@article{2501.09006v1,
 abstract = {Advances in the effectiveness of machine learning models have come at the cost of enormous complexity resulting in a poor understanding of how they function. Local surrogate methods have been used to approximate the workings of these complex models, but recent work has revealed their vulnerability to adversarial attacks where the explanation produced is appreciably different while the meaning and structure of the complex model's output remains similar. This prior work has focused on the existence of these weaknesses but not on their magnitude. Here we explore using an alternate search method with the goal of finding minimum viable perturbations, the fewest perturbations necessary to achieve a fixed similarity value between the original and altered text's explanation. Intuitively, a method that requires fewer perturbations to expose a given level of instability is inferior to one which requires more. This nuance allows for superior comparisons of the stability of explainability methods.},
 author = {Christopher Burger and Charles Walter},
 comment = {9 pages, 3 figures, 5 tables. arXiv admin note: text overlap with arXiv:2406.15839},
 doi = {},
 eprint = {2501.09006v1},
 journal = {arXiv preprint},
 title = {Improving Stability Estimates in Adversarial Explainable AI through Alternate Search Methods},
 url = {http://arxiv.org/abs/2501.09006v1},
 year = {2025}
}

@article{2501.09967v1,
 abstract = {Artificial Intelligence (AI) has continued to achieve tremendous success in recent times. However, the decision logic of these frameworks is often not transparent, making it difficult for stakeholders to understand, interpret or explain their behavior. This limitation hinders trust in machine learning systems and causes a general reluctance towards their adoption in practical applications, particularly in mission-critical domains like healthcare and autonomous driving. Explainable AI (XAI) techniques facilitate the explainability or interpretability of machine learning models, enabling users to discern the basis of the decision and possibly avert undesirable behavior. This comprehensive survey details the advancements of explainable AI methods, from inherently interpretable models to modern approaches for achieving interpretability of various black box models, including large language models (LLMs). Additionally, we review explainable AI techniques that leverage LLM and vision-language model (VLM) frameworks to automate or improve the explainability of other machine learning models. The use of LLM and VLM as interpretability methods particularly enables high-level, semantically meaningful explanations of model decisions and behavior. Throughout the paper, we highlight the scientific principles, strengths and weaknesses of state-of-the-art methods and outline different areas of improvement. Where appropriate, we also present qualitative and quantitative comparison results of various methods to show how they compare. Finally, we discuss the key challenges of XAI and directions for future research.},
 author = {Fuseini Mumuni and Alhassan Mumuni},
 comment = {},
 doi = {},
 eprint = {2501.09967v1},
 journal = {arXiv preprint},
 title = {Explainable artificial intelligence (XAI): from inherent explainability to large language models},
 url = {http://arxiv.org/abs/2501.09967v1},
 year = {2025}
}

@article{2501.10273v1,
 abstract = {In epidemiology, traditional statistical methods such as logistic regression, linear regression, and other parametric models are commonly employed to investigate associations between predictors and health outcomes. However, non-parametric machine learning techniques, such as deep neural networks (DNNs), coupled with explainable AI (XAI) tools, offer new opportunities for this task. Despite their potential, these methods face challenges due to the limited availability of high-quality, high-quantity data in this field. To address these challenges, we introduce SEANN, a novel approach for informed DNNs that leverages a prevalent form of domain-specific knowledge: Pooled Effect Sizes (PES). PESs are commonly found in published Meta-Analysis studies, in different forms, and represent a quantitative form of a scientific consensus. By direct integration within the learning procedure using a custom loss, we experimentally demonstrate significant improvements in the generalizability of predictive performances and the scientific plausibility of extracted relationships compared to a domain-knowledge agnostic neural network in a scarce and noisy data setting.},
 author = {Jean-Baptiste Guimbaud and Marc Plantevit and Léa Maître and Rémy Cazabet},
 comment = {},
 doi = {},
 eprint = {2501.10273v1},
 journal = {arXiv preprint},
 title = {SEANN: A Domain-Informed Neural Network for Epidemiological Insights},
 url = {http://arxiv.org/abs/2501.10273v1},
 year = {2025}
}

@article{2501.11094v2,
 abstract = {Suicidal ideation detection is crucial for preventing suicides, a leading cause of death worldwide. Many individuals express suicidal thoughts on social media, offering a vital opportunity for early detection through advanced machine learning techniques. The identification of suicidal ideation in social media text is improved by utilising a hybrid framework that integrates Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM), enhanced with an attention mechanism. To enhance the interpretability of the model's predictions, Explainable AI (XAI) methods are applied, with a particular focus on SHapley Additive exPlanations (SHAP), are incorporated. At first, the model managed to reach an accuracy of 92.81%. By applying fine-tuning and early stopping techniques, the accuracy improved to 94.29%. The SHAP analysis revealed key features influencing the model's predictions, such as terms related to mental health struggles. This level of transparency boosts the model's credibility while helping mental health professionals understand and trust the predictions. This work highlights the potential for improving the accuracy and interpretability of detecting suicidal tendencies, making a valuable contribution to the progress of mental health monitoring systems. It emphasizes the significance of blending powerful machine learning methods with explainability to develop reliable and impactful mental health solutions.},
 author = {Mohaiminul Islam Bhuiyan and Nur Shazwani Kamarudin and Nur Hafieza Ismail},
 comment = {},
 doi = {},
 eprint = {2501.11094v2},
 journal = {arXiv preprint},
 title = {Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model},
 url = {http://arxiv.org/abs/2501.11094v2},
 year = {2025}
}

@article{2501.11178v1,
 abstract = {This paper proposes a method for measuring conditional feature importance via generative modeling. In explainable artificial intelligence (XAI), conditional feature importance assesses the impact of a feature on a prediction model's performance given the information of other features. Model-agnostic post hoc methods to do so typically evaluate changes in the predictive performance under on-manifold feature value manipulations. Such procedures require creating feature values that respect conditional feature distributions, which can be challenging in practice. Recent advancements in generative modeling can facilitate this. For tabular data, which may consist of both categorical and continuous features, the adversarial random forest (ARF) stands out as a generative model that can generate on-manifold data points without requiring intensive tuning efforts or computational resources, making it a promising candidate model for subroutines in XAI methods. This paper proposes cARFi (conditional ARF feature importance), a method for measuring conditional feature importance through feature values sampled from ARF-estimated conditional distributions. cARFi requires only little tuning to yield robust importance scores that can flexibly adapt for conditional or marginal notions of feature importance, including straightforward extensions to condition on feature subsets and allows for inferring the significance of feature importances through statistical tests.},
 author = {Kristin Blesch and Niklas Koenen and Jan Kapar and Pegah Golchian and Lukas Burk and Markus Loecher and Marvin N. Wright},
 comment = {},
 doi = {},
 eprint = {2501.11178v1},
 journal = {arXiv preprint},
 title = {Conditional Feature Importance with Generative Modeling Using Adversarial Random Forests},
 url = {http://arxiv.org/abs/2501.11178v1},
 year = {2025}
}

@article{2501.11827v1,
 abstract = {With the rapid growth of generative AI in numerous applications, explainable AI (XAI) plays a crucial role in ensuring the responsible development and deployment of generative AI technologies. XAI has undergone notable advancements and widespread adoption in recent years, reflecting a concerted push to enhance the transparency, interpretability, and credibility of AI systems. Recent research emphasizes that a proficient XAI method should adhere to a set of criteria, primarily focusing on two key areas. Firstly, it should ensure the quality and fluidity of explanations, encompassing aspects like faithfulness, plausibility, completeness, and tailoring to individual needs. Secondly, the design principle of the XAI system or mechanism should cover the following factors such as reliability, resilience, the verifiability of its outputs, and the transparency of its algorithm. However, research in XAI for generative models remains relatively scarce, with little exploration into how such methods can effectively meet these criteria in that domain. In this work, we propose PXGen, a post-hoc explainable method for generative models. Given a model that needs to be explained, PXGen prepares two materials for the explanation, the Anchor set and intrinsic & extrinsic criteria. Those materials are customizable by users according to their purpose and requirements. Via the calculation of each criterion, each anchor has a set of feature values and PXGen provides examplebased explanation methods according to the feature values among all the anchors and illustrated and visualized to the users via tractable algorithms such as k-dispersion or k-center.},
 author = {Yen-Lung Huang and Ming-Hsi Weng and Hao-Tsung Yang},
 comment = {},
 doi = {},
 eprint = {2501.11827v1},
 journal = {arXiv preprint},
 title = {PXGen: A Post-hoc Explainable Method for Generative Models},
 url = {http://arxiv.org/abs/2501.11827v1},
 year = {2025}
}

@article{2501.12706v3,
 abstract = {Explainable Artificial Intelligence (XAI) techniques hold significant potential for enhancing the causal discovery process, which is crucial for understanding complex systems in areas like healthcare, economics, and artificial intelligence. However, no causal discovery methods currently incorporate explainability into their models to derive the causal graphs. Thus, in this paper we explore this innovative approach, as it offers substantial potential and represents a promising new direction worth investigating. Specifically, we introduce ReX, a causal discovery method that leverages machine learning (ML) models coupled with explainability techniques, specifically Shapley values, to identify and interpret significant causal relationships among variables. Comparative evaluations on synthetic datasets comprising continuous tabular data reveal that ReX outperforms state-of-the-art causal discovery methods across diverse data generation processes, including non-linear and additive noise models. Moreover, ReX was tested on the Sachs single-cell protein-signaling dataset, achieving a precision of 0.952 and recovering key causal relationships with no incorrect edges. Taking together, these results showcase ReX's effectiveness in accurately recovering true causal structures while minimizing false positive predictions, its robustness across diverse datasets, and its applicability to real-world problems. By combining ML and explainability techniques with causal discovery, ReX bridges the gap between predictive modeling and causal inference, offering an effective tool for understanding complex causal structures.},
 author = {Jesus Renero and Idoia Ochoa and Roberto Maestre},
 comment = {22 pages, 30 figures, Published in Elsevier's Pattern Recognition},
 doi = {10.1016/j.patcog.2025.112491},
 eprint = {2501.12706v3},
 journal = {arXiv preprint},
 title = {REX: Causal discovery based on machine learning and explainability techniques},
 url = {http://arxiv.org/abs/2501.12706v3},
 year = {2025}
}

@article{2501.13479v1,
 abstract = {Few-shot learning (FSL) enables machine learning models to generalize effectively with minimal labeled data, making it crucial for data-scarce domains such as healthcare, robotics, and natural language processing. Despite its potential, FSL faces challenges including sensitivity to initialization, difficulty in adapting to diverse domains, and vulnerability to noisy datasets. To address these issues, this paper introduces Adaptive Few-Shot Learning (AFSL), a framework that integrates advancements in meta-learning, domain alignment, noise resilience, and multi-modal integration. AFSL consists of four key modules: a Dynamic Stability Module for performance consistency, a Contextual Domain Alignment Module for domain adaptation, a Noise-Adaptive Resilience Module for handling noisy data, and a Multi-Modal Fusion Module for integrating diverse modalities. This work also explores strategies such as task-aware data augmentation, semi-supervised learning, and explainable AI techniques to enhance the applicability and robustness of FSL. AFSL provides scalable, reliable, and impactful solutions for real-world, high-stakes domains.},
 author = {Rishabh Agrawal},
 comment = {},
 doi = {},
 eprint = {2501.13479v1},
 journal = {arXiv preprint},
 title = {Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility},
 url = {http://arxiv.org/abs/2501.13479v1},
 year = {2025}
}

@article{2501.13552v1,
 abstract = {Artificial intelligence (AI) is expected to significantly enhance radio resource management (RRM) in sixth-generation (6G) networks. However, the lack of explainability in complex deep learning (DL) models poses a challenge for practical implementation. This paper proposes a novel explainable AI (XAI)- based framework for feature selection and model complexity reduction in a model-agnostic manner. Applied to a multi-agent deep reinforcement learning (MADRL) setting, our approach addresses the joint sub-band assignment and power allocation problem in cellular vehicle-to-everything (V2X) communications. We propose a novel two-stage systematic explainability framework leveraging feature relevance-oriented XAI to simplify the DRL agents. While the former stage generates a state feature importance ranking of the trained models using Shapley additive explanations (SHAP)-based importance scores, the latter stage exploits these importance-based rankings to simplify the state space of the agents by removing the least important features from the model input. Simulation results demonstrate that the XAI-assisted methodology achieves 97% of the original MADRL sum-rate performance while reducing optimal state features by 28%, average training time by 11%, and trainable weight parameters by 46% in a network with eight vehicular pairs.},
 author = {Nasir Khan and Asmaa Abdallah and Abdulkadir Celik and Ahmed M. Eltawil and Sinem Coleri},
 comment = {},
 doi = {},
 eprint = {2501.13552v1},
 journal = {arXiv preprint},
 title = {Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X Resource Allocation},
 url = {http://arxiv.org/abs/2501.13552v1},
 year = {2025}
}

@article{2501.13887v2,
 abstract = {Adding explanations to audio deepfake detection (ADD) models will boost their real-world application by providing insight on the decision making process. In this paper, we propose a relevancy-based explainable AI (XAI) method to analyze the predictions of transformer-based ADD models. We compare against standard Grad-CAM and SHAP-based methods, using quantitative faithfulness metrics as well as a partial spoof test, to comprehensively analyze the relative importance of different temporal regions in an audio. We consider large datasets, unlike previous works where only limited utterances are studied, and find that the XAI methods differ in their explanations. The proposed relevancy-based XAI method performs the best overall on a variety of metrics. Further investigation on the relative importance of speech/non-speech, phonetic content, and voice onsets/offsets suggest that the XAI results obtained from analyzing limited utterances don't necessarily hold when evaluated on large datasets.},
 author = {Petr Grinberg and Ankur Kumar and Surya Koppisetti and Gaurav Bharaj},
 comment = {Accepted to ICASSP 2025},
 doi = {10.1109/ICASSP49660.2025.10887568},
 eprint = {2501.13887v2},
 journal = {arXiv preprint},
 title = {What Does an Audio Deepfake Detector Focus on? A Study in the Time Domain},
 url = {http://arxiv.org/abs/2501.13887v2},
 year = {2025}
}

@article{2501.14271v2,
 abstract = {The scheme of adaptation via meta-learning is seen as an ingredient for solving the problem of data shortage or distribution shift in real-world applications, but it also brings the new risk of inappropriate updates of the model in the user environment, which increases the demand for explainability. Among the various types of XAI methods, establishing a method of explanation based on past experience in meta-learning requires special consideration due to its bi-level structure of training, which has been left unexplored. In this work, we propose influence functions for explaining meta-learning that measure the sensitivities of training tasks to adaptation and inference. We also argue that the approximation of the Hessian using the Gauss-Newton matrix resolves computational barriers peculiar to meta-learning. We demonstrate the adequacy of the method through experiments on task distinction and task distribution distinction using image classification tasks with MAML and Prototypical Network.},
 author = {Yoshihiro Mitsuka and Shadan Golestan and Zahin Sufiyan and Sheila Schoepp and Shotaro Miwa and Osmar R. Zaiane},
 comment = {22 pages; v2: modification in metadata},
 doi = {},
 eprint = {2501.14271v2},
 journal = {arXiv preprint},
 title = {TLXML: Task-Level Explanation of Meta-Learning via Influence Functions},
 url = {http://arxiv.org/abs/2501.14271v2},
 year = {2025}
}

@article{2501.14836v1,
 abstract = {In this paper we focus on the opacity issue of sub-symbolic machine learning predictors by promoting two complementary activities, namely, symbolic knowledge extraction (SKE) and injection (SKI) from and into sub-symbolic predictors. We consider as symbolic any language being intelligible and interpretable for both humans and computers. Accordingly, we propose general meta-models for both SKE and SKI, along with two taxonomies for the classification of SKE and SKI methods. By adopting an explainable artificial intelligence (XAI) perspective, we highlight how such methods can be exploited to mitigate the aforementioned opacity issue. Our taxonomies are attained by surveying and classifying existing methods from the literature, following a systematic approach, and by generalising the results of previous surveys targeting specific sub-topics of either SKE or SKI alone. More precisely, we analyse 132 methods for SKE and 117 methods for SKI, and we categorise them according to their purpose, operation, expected input/output data and predictor types. For each method, we also indicate the presence/lack of runnable software implementations. Our work may be of interest for data scientists aiming at selecting the most adequate SKE/SKI method for their needs, and also work as suggestions for researchers interested in filling the gaps of the current state of the art, as well as for developers willing to implement SKE/SKI-based technologies.},
 author = {Giovanni Ciatto and Federico Sabbatini and Andrea Agiollo and Matteo Magnini and Andrea Omicini},
 comment = {},
 doi = {10.1145/3645103},
 eprint = {2501.14836v1},
 journal = {arXiv preprint},
 title = {Symbolic Knowledge Extraction and Injection with Sub-symbolic Predictors: A Systematic Literature Review},
 url = {http://arxiv.org/abs/2501.14836v1},
 year = {2025}
}

@article{2501.15374v1,
 abstract = {The black-box nature of large language models (LLMs) necessitates the development of eXplainable AI (XAI) techniques for transparency and trustworthiness. However, evaluating these techniques remains a challenge. This study presents a general evaluation framework using four key metrics: Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We assess the effectiveness of six explainability techniques from five different XAI categories model simplification (LIME), perturbation-based methods (SHAP), gradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance Propagation (LRP), and attention mechanisms-based explainability methods (Attention Mechanism Visualization, AMV) across five encoder-based language models: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using the IMDB Movie Reviews and Tweet Sentiment Extraction (TSE) datasets. Our findings show that the model simplification-based XAI method (LIME) consistently outperforms across multiple metrics and models, significantly excelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness, and consistency as the complexity of large language models increases. AMV demonstrates the best Robustness, with scores as low as 0.0020. It also excels in Consistency, achieving near-perfect scores of 0.9999 across all models. Regarding Contrastivity, LRP performs the best, particularly on more complex models, with scores up to 0.9371.},
 author = {Melkamu Abay Mersha and Mesay Gemeda Yigezu and Jugal Kalita},
 comment = {},
 doi = {10.1016/j.knosys.2025.113042},
 eprint = {2501.15374v1},
 journal = {arXiv preprint},
 title = {Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models},
 url = {http://arxiv.org/abs/2501.15374v1},
 year = {2025}
}

@article{2501.15734v1,
 abstract = {Network slicing aims to enhance flexibility and efficiency in next-generation wireless networks by allocating the right resources to meet the diverse requirements of various applications. Managing these slices with machine learning (ML) algorithms has emerged as a promising approach however explainability has been a challenge. To this end, several Explainable Artificial Intelligence (XAI) frameworks have been proposed to address the opacity in decision-making in many ML methods. In this paper, we propose a Prioritized Value-Decomposition Network (PVDN) as an XAI-driven approach for resource allocation in a multi-agent network slicing system. The PVDN method decomposes the global value function into individual contributions and prioritizes slice outputs, providing an explanation of how resource allocation decisions impact system performance. By incorporating XAI, PVDN offers valuable insights into the decision-making process, enabling network operators to better understand, trust, and optimize slice management strategies. Through simulations, we demonstrate the effectiveness of the PVDN approach with improving the throughput by 67% and 16%, while reducing latency by 35% and 22%, compared to independent and VDN-based resource allocation methods.},
 author = {Shavbo Salehi and Pedro Enrique Iturria-Rivera and Medhat Elsayed and Majid Bavand and Raimundas Gaigalas and Yigit Ozcan and Melike Erol-Kantarci},
 comment = {},
 doi = {},
 eprint = {2501.15734v1},
 journal = {arXiv preprint},
 title = {Prioritized Value-Decomposition Network for Explainable AI-Enabled Network Slicing},
 url = {http://arxiv.org/abs/2501.15734v1},
 year = {2025}
}

@article{2501.16357v1,
 abstract = {The widespread use of artificial intelligence deep neural networks in fields such as medicine and engineering necessitates understanding their decision-making processes. Current explainability methods often produce inconsistent results and struggle to highlight essential signals influencing model inferences. This paper introduces the Evolutionary Independent Deterministic Explanation (EVIDENCE) theory, a novel approach offering a deterministic, model-independent method for extracting significant signals from black-box models. EVIDENCE theory, grounded in robust mathematical formalization, is validated through empirical tests on diverse datasets, including COVID-19 audio diagnostics, Parkinson's disease voice recordings, and the George Tzanetakis music classification dataset (GTZAN). Practical applications of EVIDENCE include improving diagnostic accuracy in healthcare and enhancing audio signal analysis. For instance, in the COVID-19 use case, EVIDENCE-filtered spectrograms fed into a frozen Residual Network with 50 layers improved precision by 32% for positive cases and increased the area under the curve (AUC) by 16% compared to baseline models. For Parkinson's disease classification, EVIDENCE achieved near-perfect precision and sensitivity, with a macro average F1-Score of 0.997. In the GTZAN, EVIDENCE maintained a high AUC of 0.996, demonstrating its efficacy in filtering relevant features for accurate genre classification. EVIDENCE outperformed other Explainable Artificial Intelligence (XAI) methods such as LIME, SHAP, and GradCAM in almost all metrics. These findings indicate that EVIDENCE not only improves classification accuracy but also provides a transparent and reproducible explanation mechanism, crucial for advancing the trustworthiness and applicability of AI systems in real-world settings.},
 author = {Vincenzo Dentamaro and Paolo Giglio and Donato Impedovo and Giuseppe Pirlo},
 comment = {20 pages, 4 figures},
 doi = {},
 eprint = {2501.16357v1},
 journal = {arXiv preprint},
 title = {EVolutionary Independent DEtermiNistiC Explanation},
 url = {http://arxiv.org/abs/2501.16357v1},
 year = {2025}
}

@article{2501.16638v1,
 abstract = {Any exploit taking advantage of zero-day is called a zero-day attack. Previous research and social media trends show a massive demand for research in zero-day attack detection. This paper analyzes Machine Learning (ML) and Deep Learning (DL) based approaches to create Intrusion Detection Systems (IDS) and scrutinizing them using Explainable AI (XAI) by training an explainer based on randomly sampled data from the testing set. The focus is on using the KDD99 dataset, which has the most research done among all the datasets for detecting zero-day attacks. The paper aims to synthesize the dataset to have fewer classes for multi-class classification, test ML and DL approaches on pattern recognition, establish the robustness and dependability of the model, and establish the interpretability and scalability of the model. We evaluated the performance of four multilayer perceptron (MLP) trained on the KDD99 dataset, including baseline ML models, weighted ML models, truncated ML models, and weighted truncated ML models. Our results demonstrate that the truncated ML model achieves the highest accuracy (99.62%), precision, and recall, while weighted truncated ML model shows lower accuracy (97.26%) but better class representation (less bias) among all the classes with improved unweighted recall score. We also used Shapely Additive exPlanations (SHAP) to train explainer for our truncated models to check for feature importance among the two weighted and unweighted models.},
 author = {Ashim Dahal and Prabin Bajgai and Nick Rahimi},
 comment = {},
 doi = {10.1007/978-3-031-86637-1_5},
 eprint = {2501.16638v1},
 journal = {arXiv preprint},
 title = {Analysis of Zero Day Attack Detection Using MLP and XAI},
 url = {http://arxiv.org/abs/2501.16638v1},
 year = {2025}
}

@article{2501.16944v2,
 abstract = {Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning (ML) prediction tasks involving graph-structured data, their interpretability remains challenging. In explainable artificial intelligence (XAI), the Shapley Value (SV) is the predominant method to quantify contributions of individual features to a ML model's output. Addressing the limitations of SVs in complex prediction models, Shapley Interactions (SIs) extend the SV to groups of features. In this work, we explain single graph predictions of GNNs with SIs that quantify node contributions and interactions among multiple nodes. By exploiting the GNN architecture, we show that the structure of interactions in node embeddings are preserved for graph prediction. As a result, the exponential complexity of SIs depends only on the receptive fields, i.e. the message-passing ranges determined by the connectivity of the graph and the number of convolutional layers. Based on our theoretical results, we introduce GraphSHAP-IQ, an efficient approach to compute any-order SIs exactly. GraphSHAP-IQ is applicable to popular message passing techniques in conjunction with a linear global pooling and output layer. We showcase that GraphSHAP-IQ substantially reduces the exponential complexity of computing exact SIs on multiple benchmark datasets. Beyond exact computation, we evaluate GraphSHAP-IQ's approximation of SIs on popular GNN architectures and compare with existing baselines. Lastly, we visualize SIs of real-world water distribution networks and molecule structures using a SI-Graph.},
 author = {Maximilian Muschalik and Fabian Fumagalli and Paolo Frazzetto and Janine Strotherm and Luca Hermes and Alessandro Sperduti and Eyke Hüllermeier and Barbara Hammer},
 comment = {Preprint Version. Accepted at ICLR 2025},
 doi = {},
 eprint = {2501.16944v2},
 journal = {arXiv preprint},
 title = {Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks},
 url = {http://arxiv.org/abs/2501.16944v2},
 year = {2025}
}

@article{2501.17415v1,
 abstract = {In this paper, we introduce si4onnx, a package for performing selective inference on deep learning models. Techniques such as CAM in XAI and reconstruction-based anomaly detection using VAE can be interpreted as methods for identifying significant regions within input images. However, the identified regions may not always carry meaningful significance. Therefore, evaluating the statistical significance of these regions represents a crucial challenge in establishing the reliability of AI systems. si4onnx is a Python package that enables straightforward implementation of hypothesis testing with controlled type I error rates through selective inference. It is compatible with deep learning models constructed using common frameworks such as PyTorch and TensorFlow.},
 author = {Teruyuki Katsuoka and Tomohiro Shiraishi and Daiki Miwa and Shuichi Nishino and Ichiro Takeuchi},
 comment = {35pages, 3figures},
 doi = {},
 eprint = {2501.17415v1},
 journal = {arXiv preprint},
 title = {si4onnx: A Python package for Selective Inference in Deep Learning Models},
 url = {http://arxiv.org/abs/2501.17415v1},
 year = {2025}
}

@article{2501.17896v1,
 abstract = {Data science has emerged as fourth paradigm of scientific exploration. However many machine learning models operate as black boxes offering limited insight into the reasoning behind their predictions. This lack of transparency is one of the drawbacks to generate new knowledge from data. Recently Kolmogorov-Arnold Network or KAN has been proposed as an alternative model which embeds explainable AI. This study demonstrates the potential of KAN for new scientific exploration. KAN along with five other popular supervised machine learning models are applied to the well-known problem of airfoil lift prediction in aerospace engineering. Standard data generated from an earlier study on 2900 different airfoils is used. KAN performed the best with an R2 score of 96.17 percent on the test data, surpassing both the baseline model and Multi Layer Perceptron. Explainability of KAN is shown by pruning and symbolizing the model resulting in an equation for coefficient of lift in terms of input variables. The explainable information retrieved from KAN model is found to be consistent with the known physics of lift generation by airfoil thus demonstrating its potential to aid in scientific exploration.},
 author = {Sudhanva Kulkarni},
 comment = {3 pages, 2 tables, 3 figures},
 doi = {},
 eprint = {2501.17896v1},
 journal = {arXiv preprint},
 title = {Explainable Machine Learning: An Illustration of Kolmogorov-Arnold Network Model for Airfoil Lift Prediction},
 url = {http://arxiv.org/abs/2501.17896v1},
 year = {2025}
}

@article{2501.18071v2,
 abstract = {Diabetes mellitus (DM) is a global health issue of significance that must be diagnosed as early as possible and managed well. This study presents a framework for diabetes prediction using Machine Learning (ML) models, complemented with eXplainable Artificial Intelligence (XAI) tools, to investigate both the predictive accuracy and interpretability of the predictions from ML models. Data Preprocessing is based on the Synthetic Minority Oversampling Technique (SMOTE) and feature scaling used on the Diabetes Binary Health Indicators dataset to deal with class imbalance and variability of clinical features. The ensemble model provided high accuracy, with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General Health, Income, and Physical Activity were the most influential predictors obtained from the model explanations. The results of this study suggest that ML combined with XAI is a promising means of developing accurate and computationally transparent tools for use in healthcare systems.},
 author = {Pir Bakhsh Khokhar and Viviana Pentangelo and Fabio Palomba and Carmine Gravino},
 comment = {},
 doi = {},
 eprint = {2501.18071v2},
 journal = {arXiv preprint},
 title = {Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence},
 url = {http://arxiv.org/abs/2501.18071v2},
 year = {2025}
}

@article{2501.18523v2,
 abstract = {Machine learning (ML) with in situ diagnostics offers a transformative approach to accelerate, understand, and control thin film synthesis by uncovering relationships between synthesis conditions and material properties. In this study, we demonstrate the application of deep learning to predict the stoichiometry of Sr$_{2x}$Ti$_{2(1-x)}$O$_{3}$ thin films using reflection high-energy electron diffraction images acquired during pulsed laser deposition. A gated convolutional neural network trained for regression of the Sr atomic fraction achieved accurate predictions with a small dataset of 31 samples. Explainable AI techniques revealed a previously unknown correlation between diffraction streak features and cation stoichiometry in Sr$_{2x}$Ti$_{2(1-x)}$O$_{3}$ thin films. Our results demonstrate how ML can be used to transform a ubiquitous in situ diagnostic tool, that is usually limited to qualitative assessments, into a quantitative surrogate measurement of continuously valued thin film properties. Such methods are critically needed to enable real-time control, autonomous workflows, and accelerate traditional synthesis approaches.},
 author = {Sumner B. Harris and Patrick T. Gemperline and Christopher M. Rouleau and Rama K. Vasudevan and Ryan B. Comes},
 comment = {},
 doi = {10.1021/acs.nanolett.5c00787},
 eprint = {2501.18523v2},
 journal = {arXiv preprint},
 title = {Deep learning with reflection high-energy electron diffraction images to predict cation ratio in Sr$_{2x}$Ti$_{2(1-x)}$O$_{3}$ thin films},
 url = {http://arxiv.org/abs/2501.18523v2},
 year = {2025}
}

@article{2501.18887v3,
 abstract = {The increasing complexity of AI systems has made understanding their behavior critical. Numerous interpretability methods have been developed to attribute model behavior to three key aspects: input features, training data, and internal model components, which emerged from explainable AI, data-centric AI, and mechanistic interpretability, respectively. However, these attribution methods are studied and applied rather independently, resulting in a fragmented landscape of methods and terminology. This position paper argues that feature, data, and component attribution methods share fundamental similarities, and a unified view of them benefits both interpretability and broader AI research. To this end, we first analyze popular methods for these three types of attributions and present a unified view demonstrating that these seemingly distinct methods employ similar techniques (such as perturbations, gradients, and linear approximations) over different aspects and thus differ primarily in their perspectives rather than techniques. Then, we demonstrate how this unified view enhances understanding of existing attribution methods, highlights shared concepts and evaluation criteria among these methods, and leads to new research directions both in interpretability research, by addressing common challenges and facilitating cross-attribution innovation, and in AI more broadly, with applications in model editing, steering, and regulation.},
 author = {Shichang Zhang and Tessa Han and Usha Bhalla and Himabindu Lakkaraju},
 comment = {},
 doi = {},
 eprint = {2501.18887v3},
 journal = {arXiv preprint},
 title = {Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability},
 url = {http://arxiv.org/abs/2501.18887v3},
 year = {2025}
}

@article{2501.19114v3,
 abstract = {Principal Component Analysis (PCA) is a commonly used tool for dimension reduction and denoising. Therefore, it is also widely used on the data prior to training a neural network. However, this approach can complicate the explanation of eXplainable Artificial Intelligence (XAI) methods for the decision of the model. In this work, we analyze the potential issues with this approach and propose Principal Components-based Initialization (PCsInit), a strategy to incorporate PCA into the first layer of a neural network via initialization of the first layer in the network with the principal components, and its two variants PCsInit-Act and PCsInit-Sub. We will show that explanations using these strategies are more simple, direct and straightforward than using PCA prior to training a neural network on the principal components. We also show that the proposed techniques possess desirable theoretical properties. Moreover, as will be illustrated in the experiments, such training strategies can also allow further improvement of training via backpropagation compared to training neural networks on principal components.},
 author = {Nhan Phan and Thu Nguyen and Uyen Dang and Pål Halvorsen and Michael A. Riegler},
 comment = {},
 doi = {},
 eprint = {2501.19114v3},
 journal = {arXiv preprint},
 title = {Principal Components for Neural Network Initialization},
 url = {http://arxiv.org/abs/2501.19114v3},
 year = {2025}
}

@article{2502.00025v4,
 abstract = {Importance: Emergency department (ED) returns for mental health conditions pose a major healthcare burden, with 24-27% of patients returning within 30 days. Traditional machine learning models for predicting these returns often lack interpretability for clinical use.
  Objective: To assess whether integrating large language models (LLMs) with machine learning improves predictive accuracy and clinical interpretability of ED mental health return risk models.
  Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an academic medical center in the Deep South from January 2018 to December 2022.
  Main Outcomes and Measures: Two primary outcomes were evaluated: (1) 30-day ED return prediction accuracy and (2) model interpretability using a novel LLM-enhanced framework integrating SHAP (SHapley Additive exPlanations) values with clinical knowledge.
  Results: For chief complaint classification, LLaMA 3 (8B) with 10-shot learning outperformed traditional models (accuracy: 0.882, F1-score: 0.86). In SDoH classification, LLM-based models achieved 0.95 accuracy and 0.96 F1-score, with Alcohol, Tobacco, and Substance Abuse performing best (F1: 0.96-0.89), while Exercise and Home Environment showed lower performance (F1: 0.70-0.67). The LLM-based interpretability framework achieved 99% accuracy in translating model predictions into clinically relevant explanations. LLM-extracted features improved XGBoost AUC from 0.74 to 0.76 and AUC-PR from 0.58 to 0.61.
  Conclusions and Relevance: Integrating LLMs with machine learning models yielded modest but consistent accuracy gains while significantly enhancing interpretability through automated, clinically relevant explanations. This approach provides a framework for translating predictive analytics into actionable clinical insights.},
 author = {Abdulaziz Ahmed and Mohammad Saleem and Mohammed Alzeen and Badari Birur and Rachel E Fargason and Bradley G Burk and Ahmed Alhassan and Mohammed Ali Al-Garadi},
 comment = {},
 doi = {},
 eprint = {2502.00025v4},
 journal = {arXiv preprint},
 title = {Explainable AI for Mental Health Emergency Returns: Integrating LLMs with Predictive Modeling},
 url = {http://arxiv.org/abs/2502.00025v4},
 year = {2025}
}

@article{2502.00088v1,
 abstract = {Functionality or proxy-based approach is one of the used approaches to evaluate the quality of explainable artificial intelligence methods. It uses statistical methods, definitions and new developed metrics for the evaluation without human intervention. Among them, Selectivity or RemOve And Retrain (ROAR), and Permutation Importance (PI) are the most commonly used metrics to evaluate the quality of explainable artificial intelligence methods to highlight the most significant features in machine learning models. They state that the model performance should experience a sharp reduction if the most informative feature is removed from the model or permuted. However, the efficiency of both metrics is significantly affected by multicollinearity, number of significant features in the model and the accuracy of the model. This paper shows with empirical examples that both metrics suffer from the aforementioned limitations. Accordingly, we propose expected accuracy interval (EAI), a metric to predict the upper and lower bounds of the the accuracy of the model when ROAR or IP is implemented. The proposed metric found to be very useful especially with collinear features.},
 author = {Ahmed M. Salih},
 comment = {},
 doi = {},
 eprint = {2502.00088v1},
 journal = {arXiv preprint},
 title = {Re-Visiting Explainable AI Evaluation Metrics to Identify The Most Informative Features},
 url = {http://arxiv.org/abs/2502.00088v1},
 year = {2025}
}

@article{2502.00300v2,
 abstract = {Machine learning algorithms have shown promise in reducing bias in wind gust predictions, while still underpredicting high gusts. Uncertainty quantification (UQ) supports this issue by identifying when predictions are reliable or need cautious interpretation. Using data from 61 extratropical storms in the Northeastern USA, we introduce evidential neural network (ENN) as a novel approach for UQ in gust predictions, leveraging atmospheric variables from the Weather Research and Forecasting (WRF) model. Explainable AI techniques suggested that key predictive features contributed to higher uncertainty, which correlated strongly with storm intensity and spatial gust gradients. Compared to WRF, ENN demonstrated a 47% reduction in RMSE and allowed the construction of gust prediction intervals without an ensemble, successfully capturing at least 95% of observed gusts at 179 out of 266 stations. From an operational perspective, providing gust forecasts with quantified uncertainty enhances stakeholders' confidence in risk assessment and response planning for extreme gust events.},
 author = {Israt Jahan and John S. Schreck and David John Gagne and Charlie Becker and Marina Astitha},
 comment = {},
 doi = {10.1016/j.envsoft.2025.106595},
 eprint = {2502.00300v2},
 journal = {arXiv preprint},
 title = {Uncertainty Quantification of Wind Gust Predictions in the Northeast United States: An Evidential Neural Network and Explainable Artificial Intelligence Approach},
 url = {http://arxiv.org/abs/2502.00300v2},
 year = {2025}
}

@article{2502.00459v2,
 abstract = {Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio token level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks.},
 author = {Hyunju Kang and Geonhee Han and Yoonjae Jeong and Hogun Park},
 comment = {14 pages},
 doi = {10.1609/aaai.v39i17.33950},
 eprint = {2502.00459v2},
 journal = {arXiv preprint},
 title = {AudioGenX: Explainability on Text-to-Audio Generative Models},
 url = {http://arxiv.org/abs/2502.00459v2},
 year = {2025}
}

@article{2502.00973v1,
 abstract = {In this study, we introduce a novel method to predict mental health by building machine learning models for a non-invasive wearable device equipped with Laser Doppler Flowmetry (LDF) and Fluorescence Spectroscopy (FS) sensors. Besides, we present the corresponding dataset to predict mental health, e.g. depression, anxiety, and stress levels via the DAS-21 questionnaire. To our best knowledge, this is the world's largest and the most generalized dataset ever collected for both LDF and FS studies. The device captures cutaneous blood microcirculation parameters, and wavelet analysis of the LDF signal extracts key rhythmic oscillations. The dataset, collected from 132 volunteers aged 18-94 from 19 countries, explores relationships between physiological features, demographics, lifestyle habits, and health conditions. We employed a variety of machine learning methods to classify stress detection, in which LightGBM is identified as the most effective model for stress detection, achieving a ROC AUC of 0.7168 and a PR AUC of 0.8852. In addition, we also incorporated Explainable Artificial Intelligence (XAI) techniques into our analysis to investigate deeper insights into the model's predictions. Our results suggest that females, younger individuals and those with a higher Body Mass Index (BMI) or heart rate have a greater likelihood of experiencing mental health conditions like stress and anxiety. All related code and data are published online: https://github.com/leduckhai/Wearable_LDF-FS.},
 author = {Minh Ngoc Nguyen and Khai Le-Duc and Tan-Hanh Pham and Trang Nguyen and Quang Minh Luu and Ba Kien Tran and Truong-Son Hy and Viktor Dremin and Sergei Sokolovsky and Edik Rafailov},
 comment = {Preprint, 55 pages},
 doi = {},
 eprint = {2502.00973v1},
 journal = {arXiv preprint},
 title = {A Wearable Device Dataset for Mental Health Assessment Using Laser Doppler Flowmetry and Fluorescence Spectroscopy Sensors},
 url = {http://arxiv.org/abs/2502.00973v1},
 year = {2025}
}

@article{2502.03014v1,
 abstract = {The growing complexity of machine learning and deep learning models has led to an increased reliance on opaque "black box" systems, making it difficult to understand the rationale behind predictions. This lack of transparency is particularly challenging in high-stakes applications where interpretability is as important as accuracy. Post-hoc explanation methods are commonly used to interpret these models, but they are seldom rigorously evaluated, raising concerns about their reliability. The Python package xai_evals addresses this by providing a comprehensive framework for generating, benchmarking, and evaluating explanation methods across both tabular and image data modalities. It integrates popular techniques like SHAP, LIME, Grad-CAM, Integrated Gradients (IG), and Backtrace, while supporting evaluation metrics such as faithfulness, sensitivity, and robustness. xai_evals enhances the interpretability of machine learning models, fostering transparency and trust in AI systems. The library is open-sourced at https://pypi.org/project/xai-evals/ .},
 author = {Pratinav Seth and Yashwardhan Rathore and Neeraj Kumar Singh and Chintan Chitroda and Vinay Kumar Sankarapu},
 comment = {},
 doi = {},
 eprint = {2502.03014v1},
 journal = {arXiv preprint},
 title = {xai_evals : A Framework for Evaluating Post-Hoc Local Explanation Methods},
 url = {http://arxiv.org/abs/2502.03014v1},
 year = {2025}
}

@article{2502.03200v1,
 abstract = {Tree-based and rule-based machine learning models play pivotal roles in explainable artificial intelligence (XAI) due to their unique ability to provide explanations in the form of tree or rule sets that are easily understandable and interpretable, making them essential for applications in which trust in model decisions is necessary. These transparent models are typically used in surrogate modeling, a post-hoc XAI approach for explaining the logic of black-box models, enabling users to comprehend and trust complex predictive systems while maintaining competitive performance. This study proposes the Cost-Sensitive Rule and Tree Extraction (CORTEX) method, a novel rule-based XAI algorithm grounded in the multi-class cost-sensitive decision tree (CSDT) method. The original version of the CSDT is extended to classification problems with more than two classes by inducing the concept of an n-dimensional class-dependent cost matrix. The performance of CORTEX as a rule-extractor XAI method is compared to other post-hoc tree and rule extraction methods across several datasets with different numbers of classes. Several quantitative evaluation metrics are employed to assess the explainability of generated rule sets. Our findings demonstrate that CORTEX is competitive with other tree-based methods and can be superior to other rule-based methods across different datasets. The extracted rule sets suggest the advantages of using the CORTEX method over other methods by producing smaller rule sets with shorter rules on average across datasets with a diverse number of classes. Overall, the results underscore the potential of CORTEX as a powerful XAI tool for scenarios that require the generation of clear, human-understandable rules while maintaining good predictive performance.},
 author = {Marija Kopanja and Miloš Savić and Luca Longo},
 comment = {},
 doi = {},
 eprint = {2502.03200v1},
 journal = {arXiv preprint},
 title = {CORTEX: A Cost-Sensitive Rule and Tree Extraction Method},
 url = {http://arxiv.org/abs/2502.03200v1},
 year = {2025}
}

@article{2502.04695v2,
 abstract = {Reliable explainability is not only a technical goal but also a cornerstone of private AI governance. As AI models enter high-stakes sectors, private actors such as auditors, insurers, certification bodies, and procurement agencies require standardized evaluation metrics to assess trustworthiness. However, current XAI evaluation metrics remain fragmented and prone to manipulation, which undermines accountability and compliance. We argue that standardized metrics can function as governance primitives, embedding auditability and accountability within AI systems for effective private oversight. Building upon prior work in XAI benchmarking, we identify key limitations in ensuring faithfulness, tamper resistance, and regulatory alignment. Furthermore, interpretability can directly support model alignment by providing a verifiable means of ensuring behavioral integrity in General Purpose AI (GPAI) systems. This connection between interpretability and alignment positions XAI metrics as both technical and regulatory instruments that help prevent alignment faking, a growing concern among oversight bodies. We propose a Governance by Metrics paradigm that treats explainability evaluation as a central mechanism of private AI governance. Our framework introduces a hierarchical model linking transparency, tamper resistance, scalability, and legal alignment, extending evaluation from model introspection toward systemic accountability. Through conceptual synthesis and alignment with governance standards, we outline a roadmap for integrating explainability metrics into continuous AI assurance pipelines that serve both private oversight and regulatory needs.},
 author = {Pratinav Seth and Vinay Kumar Sankarapu},
 comment = {Accepted at first EurIPS Workshop on Private AI Governance},
 doi = {},
 eprint = {2502.04695v2},
 journal = {arXiv preprint},
 title = {Bridging the Gap in XAI-Why Reliable Metrics Matter for Explainability and Compliance},
 url = {http://arxiv.org/abs/2502.04695v2},
 year = {2025}
}

@article{2502.05718v1,
 abstract = {Around 50 percent of Irelands rural population relies on unregulated private wells vulnerable to agricultural runoff and untreated wastewater. High national rates of Shiga toxin-producing Escherichia coli (STEC) and other waterborne illnesses have been linked to well water exposure. Periodic well testing is essential for public health, yet the lack of government incentives places the financial burden on households. Understanding environmental, cognitive, and material factors influencing well-testing behavior is critical.
  This study employs Agent-Based Modeling (ABM) to simulate policy interventions based on national survey data. The ABM framework, designed for private well-testing behavior, integrates a Deep Q-network reinforcement learning model and Explainable AI (XAI) for decision-making insights. Key features were selected using Recursive Feature Elimination (RFE) with 10-fold cross-validation, while SHAP (Shapley Additive Explanations) provided further interpretability for policy recommendations.
  Fourteen policy scenarios were tested. The most effective, Free Well Testing plus Communication Campaign, increased participation to 435 out of 561 agents, from a baseline of approximately 5 percent, with rapid behavioral adaptation. Free Well Testing plus Regulation also performed well, with 433 out of 561 agents initiating well testing. Free testing alone raised participation to over 75 percent, with some agents testing multiple times annually. Scenarios with free well testing achieved faster learning efficiency, converging in 1000 episodes, while others took 2000 episodes, indicating slower adaptation.
  This research demonstrates the value of ABM and XAI in public health policy, providing a framework for evaluating behavioral interventions in environmental health.},
 author = {Rabia Asghar and Simon Mooney and Eoin O Neill and Paul Hynds},
 comment = {},
 doi = {},
 eprint = {2502.05718v1},
 journal = {arXiv preprint},
 title = {Using agent-based models and EXplainable Artificial Intelligence (XAI) to simulate social behaviors and policy intervention scenarios: A case study of private well users in Ireland},
 url = {http://arxiv.org/abs/2502.05718v1},
 year = {2025}
}

@article{2502.06775v2,
 abstract = {The trade-off between accuracy and interpretability has long been a challenge in machine learning (ML). This tension is particularly significant for emerging interpretable-by-design methods, which aim to redesign ML algorithms for trustworthy interpretability but often sacrifice accuracy in the process. In this paper, we address this gap by investigating the impact of deviations in concept representations-an essential component of interpretable models-on prediction performance and propose a novel framework to mitigate these effects. The framework builds on the principle of optimizing concept embeddings under constraints that preserve interpretability. Using a generative model as a test-bed, we rigorously prove that our algorithm achieves zero loss while progressively enhancing the interpretability of the resulting model. Additionally, we evaluate the practical performance of our proposed framework in generating explainable predictions for image classification tasks across various benchmarks. Compared to existing explainable methods, our approach not only improves prediction accuracy while preserving model interpretability across various large-scale benchmarks but also achieves this with significantly lower computational cost.},
 author = {Geyu Liang and Senne Michielssen and Salar Fattahi},
 comment = {},
 doi = {},
 eprint = {2502.06775v2},
 journal = {arXiv preprint},
 title = {Enhancing Performance of Explainable AI Models with Constrained Concept Refinement},
 url = {http://arxiv.org/abs/2502.06775v2},
 year = {2025}
}

@article{2502.07214v1,
 abstract = {In decision-making systems, algorithmic recourse aims to identify minimal-cost actions to alter an individual features, thereby obtaining a desired outcome. This empowers individuals to understand, question, or alter decisions that negatively affect them. However, due to the variety and sensitivity of system environments and individual personalities, quantifying the cost of a single function is nearly impossible while considering multiple criteria situations. Most current recourse mechanisms use gradient-based methods that assume cost functions are differentiable, often not applicable in real-world scenarios, resulting in sub-optimal solutions that compromise various criteria. These solutions are typically intractable and lack rigorous theoretical foundations, raising concerns regarding interpretability, reliability, and transparency from the explainable AI (XAI) perspective.
  To address these issues, this work proposes an algorithmic recourse framework that handles non-differentiable and discrete multi-cost functions. By formulating recourse as a multi-objective optimization problem and assigning weights to different criteria based on their importance, our method identifies Pareto optimal recourse recommendations. To demonstrate scalability, we incorporate the concept of epsilon-net, proving the ability to find approximated Pareto optimal actions. Experiments show the trade-off between different criteria and the methods scalability in large graphs. Compared to current heuristic practices, our approach provides a stronger theoretical foundation and better aligns recourse suggestions with real-world requirements.},
 author = {Wen-Ling Chen and Hong-Chang Huang and Kai-Hung Lin and Shang-Wei Hwang and Hao-Tsung Yang},
 comment = {},
 doi = {},
 eprint = {2502.07214v1},
 journal = {arXiv preprint},
 title = {Pareto Optimal Algorithmic Recourse in Multi-cost Function},
 url = {http://arxiv.org/abs/2502.07214v1},
 year = {2025}
}

@article{2502.07400v1,
 abstract = {In this study, we propose Explainable Multimodal Machine Learning (EMML), which integrates the analysis of diverse data types (multimodal data) using factor analysis for feature extraction with Explainable AI (XAI), for carbon nanotube (CNT) fibers prepared from aqueous dispersions. This method is a powerful approach to elucidate the mechanisms governing material properties, where multi-stage fabrication conditions and multiscale structures have complex influences. Thus, in our case, this approach helps us understand how different processing steps and structures at various scales impact the final properties of CNT fibers. The analysis targeted structures ranging from the nanoscale to the macroscale, including aggregation size distributions of CNT dispersions and the effective length of CNTs. Furthermore, because some types of data were difficult to interpret using standard methods, challenging-to-interpret distribution data were analyzed using Negative Matrix Factorization (NMF) for extracting key features that determine the outcome. Contribution analysis with SHapley Additive exPlanations (SHAP) demonstrated that small, uniformly distributed aggregates are crucial for improving fracture strength, while CNTs with long effective lengths are significant factors for enhancing electrical conductivity. The analysis also identified thresholds and trends for these key factors to assist in defining the conditions needed to optimize CNT fiber properties. EMML is not limited to CNT fibers but can be applied to the design of other materials derived from nanomaterials, making it a useful tool for developing a wide range of advanced materials. This approach provides a foundation for advancing data-driven materials research.},
 author = {Daisuke Kimura and Naoko Tajima and Toshiya Okazaki and Shun Muroga},
 comment = {33 pages, 9 figures},
 doi = {},
 eprint = {2502.07400v1},
 journal = {arXiv preprint},
 title = {Explainable Multimodal Machine Learning for Revealing Structure-Property Relationships in Carbon Nanotube Fibers},
 url = {http://arxiv.org/abs/2502.07400v1},
 year = {2025}
}

@article{2502.08821v2,
 abstract = {The recent surge in advanced generative models, such as diffusion models and generative adversarial networks (GANs), has led to an alarming rise in AI-generated images across various domains on the web. While such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. Additionally, the uncredited use of AI-generated images in media and marketing has sparked significant backlash from online communities. In response to this, we introduce DejAIvu, a Chrome Web extension that combines real-time AI-generated image detection with saliency-based explainability while users browse the web. Using an ONNX-optimized deep learning model, DejAIvu automatically analyzes images on websites such as Google Images, identifies AI-generated content using model inference, and overlays a saliency heatmap to highlight AI-related artifacts. Our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that AI detection is both transparent and interpretable. We also evaluate DejAIvu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing AI image accountability. The code for this system can be found at https://github.com/Noodulz/dejAIvu.},
 author = {Jocelyn Dzuong},
 comment = {5 pages, 3 figures. Accepted to IJCAI 2025 Demo Track. Revised version will be uploaded soon},
 doi = {},
 eprint = {2502.08821v2},
 journal = {arXiv preprint},
 title = {DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with Saliency Maps},
 url = {http://arxiv.org/abs/2502.08821v2},
 year = {2025}
}

@article{2502.09340v1,
 abstract = {The growing interest in eXplainable Artificial Intelligence (XAI) has prompted research into models with built-in interpretability, the most prominent of which are part-prototype models. Part-Prototype Models (PPMs) make decisions by comparing an input image to a set of learned prototypes, providing human-understandable explanations in the form of ``this looks like that''. Despite their inherent interpretability, PPMS are not yet considered a valuable alternative to post-hoc models. In this survey, we investigate the reasons for this and provide directions for future research. We analyze papers from 2019 to 2024, and derive a taxonomy of the challenges that current PPMS face. Our analysis shows that the open challenges are quite diverse. The main concern is the quality and quantity of prototypes. Other concerns are the lack of generalization to a variety of tasks and contexts, and general methodological issues, including non-standardized evaluation. We provide ideas for future research in five broad directions: improving predictive performance, developing novel architectures grounded in theory, establishing frameworks for human-AI collaboration, aligning models with humans, and establishing metrics and benchmarks for evaluation. We hope that this survey will stimulate research and promote intrinsically interpretable models for application domains. Our list of surveyed papers is available at https://github.com/aix-group/ppm-survey.},
 author = {Khawla Elhadri and Tomasz Michalski and Adam Wróbel and Jörg Schlötterer and Bartosz Zieliński and Christin Seifert},
 comment = {},
 doi = {},
 eprint = {2502.09340v1},
 journal = {arXiv preprint},
 title = {This looks like what? Challenges and Future Research Directions for Part-Prototype Models},
 url = {http://arxiv.org/abs/2502.09340v1},
 year = {2025}
}

@article{2502.09849v3,
 abstract = {Explainable Artificial Intelligence (XAI) is essential for the transparency and clinical adoption of Clinical Decision Support Systems (CDSS). However, the real-world effectiveness of existing XAI methods remains limited and is inconsistently evaluated. This study conducts a systematic PRISMA-guided survey of 31 human-centered evaluations (HCE) of XAI applied to CDSS, classifying them by XAI methodology, evaluation design, and adoption barrier. Our findings reveal that most existing studies employ post-hoc, model-agnostic approaches such as SHAP and Grad-CAM, typically assessed through small-scale clinician studies. The results show that over 80% of the studies adopt post-hoc, model-agnostic approaches such as SHAP and Grad-CAM, and that clinician sample sizes remain below 25 participants. The findings indicate that explanations generally improve clinician trust and diagnostic confidence, but frequently increase cognitive load and exhibit misalignment with domain reasoning processes. To bridge these gaps, we propose a stakeholder-centric evaluation framework that integrates socio-technical principles and human-computer interaction to guide the future development of clinically viable and trustworthy XAI-based CDSS.},
 author = {Alessandro Gambetti and Qiwei Han and Hong Shen and Claudia Soares},
 comment = {19 pages, 2 tables, 4 figures},
 doi = {},
 eprint = {2502.09849v3},
 journal = {arXiv preprint},
 title = {A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems},
 url = {http://arxiv.org/abs/2502.09849v3},
 year = {2025}
}

@article{2502.10230v1,
 abstract = {Process discovery aims to automatically derive process models from historical execution data (event logs). While various process discovery algorithms have been proposed in the last 25 years, there is no consensus on a dominating discovery algorithm. Selecting the most suitable discovery algorithm remains a challenge due to competing quality measures and diverse user requirements. Manually selecting the most suitable process discovery algorithm from a range of options for a given event log is a time-consuming and error-prone task. This paper introduces ProReco, a Process discovery Recommender system designed to recommend the most appropriate algorithm based on user preferences and event log characteristics. ProReco incorporates state-of-the-art discovery algorithms, extends the feature pools from previous work, and utilizes eXplainable AI (XAI) techniques to provide explanations for its recommendations.},
 author = {Tsung-Hao Huang and Tarek Junied and Marco Pegoraro and Wil M. P. van der Aalst},
 comment = {8 pages, 5 figures, 9 references},
 doi = {10.1007/978-3-031-61000-4_11},
 eprint = {2502.10230v1},
 journal = {arXiv preprint},
 title = {ProReco: A Process Discovery Recommender System},
 url = {http://arxiv.org/abs/2502.10230v1},
 year = {2025}
}

@article{2502.10311v1,
 abstract = {Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small "proxy set" of simple models, which can act as a generative global explanation. This reduction procedure, ExplainReduce, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics.},
 author = {Lauri Seppäläinen and Mudong Guo and Kai Puolamäki},
 comment = {22 pages with a 7 page appendix, 7 + 5 figures, 2 tables. The datasets and source code used in the paper are available at https://github.com/edahelsinki/explainreduce},
 doi = {},
 eprint = {2502.10311v1},
 journal = {arXiv preprint},
 title = {ExplainReduce: Summarising local explanations via proxies},
 url = {http://arxiv.org/abs/2502.10311v1},
 year = {2025}
}

@article{2502.11205v1,
 abstract = {Housing and household characteristics are key determinants of social and economic well-being, yet our understanding of their interrelationships remains limited. This study addresses this knowledge gap by developing a deep contrastive learning (DCL) model to infer housing-household relationships using the American Community Survey (ACS) Public Use Microdata Sample (PUMS). More broadly, the proposed model is suitable for a class of problems where the goal is to learn joint relationships between two distinct entities without explicitly labeled ground truth data. Our proposed dual-encoder DCL approach leverages co-occurrence patterns in PUMS and introduces a bisect K-means clustering method to overcome the absence of ground truth labels. The dual-encoder DCL architecture is designed to handle the semantic differences between housing (building) and household (people) features while mitigating noise introduced by clustering. To validate the model, we generate a synthetic ground truth dataset and conduct comprehensive evaluations. The model further demonstrates its superior performance in capturing housing-household relationships in Delaware compared to state-of-the-art methods. A transferability test in North Carolina confirms its generalizability across diverse sociodemographic and geographic contexts. Finally, the post-hoc explainable AI analysis using SHAP values reveals that tenure status and mortgage information play a more significant role in housing-household matching than traditionally emphasized factors such as the number of persons and rooms.},
 author = {Xiao Qian and Shangjia Dong and Rachel Davidson},
 comment = {},
 doi = {},
 eprint = {2502.11205v1},
 journal = {arXiv preprint},
 title = {Deep Contrastive Learning for Feature Alignment: Insights from Housing-Household Relationship Inference},
 url = {http://arxiv.org/abs/2502.11205v1},
 year = {2025}
}

@article{2502.12222v1,
 abstract = {The eXplainable Artificial Intelligence (XAI) research predominantly concentrates to provide explainations about AI model decisions, especially Deep Learning (DL) models. However, there is a growing interest in using XAI techniques to automatically improve the performance of the AI systems themselves.
  This paper proposes IMPACTX, a novel approach that leverages XAI as a fully automated attention mechanism, without requiring external knowledge or human feedback. Experimental results show that IMPACTX has improved performance respect to the standalone ML model by integrating an attention mechanism based an XAI method outputs during the model training. Furthermore, IMPACTX directly provides proper feature attribution maps for the model's decisions, without relying on external XAI methods during the inference process.
  Our proposal is evaluated using three widely recognized DL models (EfficientNet-B2, MobileNet, and LeNet-5) along with three standard image datasets: CIFAR-10, CIFAR-100, and STL-10. The results show that IMPACTX consistently improves the performance of all the inspected DL models across all evaluated datasets, and it directly provides appropriate explanations for its responses.},
 author = {Andrea Apicella and Salvatore Giugliano and Francesco Isgrò and Roberto Prevete},
 comment = {in peer review},
 doi = {},
 eprint = {2502.12222v1},
 journal = {arXiv preprint},
 title = {IMPACTX: Improving Model Performance by Appropriately predicting CorrecT eXplanations},
 url = {http://arxiv.org/abs/2502.12222v1},
 year = {2025}
}

@article{2502.12525v1,
 abstract = {Explainable AI (XAI) is critical for ensuring transparency, accountability, and trust in machine learning systems as black-box models are increasingly deployed within high-stakes domains. Among XAI methods, Shapley values are widely used for their fairness and consistency axioms. However, prevalent Shapley value approximation methods commonly rely on abstract baselines or computationally intensive calculations, which can limit their interpretability and scalability. To address such challenges, we propose Pairwise Shapley Values, a novel framework that grounds feature attributions in explicit, human-relatable comparisons between pairs of data instances proximal in feature space. Our method introduces pairwise reference selection combined with single-value imputation to deliver intuitive, model-agnostic explanations while significantly reducing computational overhead. Here, we demonstrate that Pairwise Shapley Values enhance interpretability across diverse regression and classification scenarios--including real estate pricing, polymer property prediction, and drug discovery datasets. We conclude that the proposed methods enable more transparent AI systems and advance the real-world applicability of XAI.},
 author = {Jiaxin Xu and Hung Chau and Angela Burden},
 comment = {},
 doi = {},
 eprint = {2502.12525v1},
 journal = {arXiv preprint},
 title = {From Abstract to Actionable: Pairwise Shapley Values for Explainable AI},
 url = {http://arxiv.org/abs/2502.12525v1},
 year = {2025}
}

@article{2502.12563v1,
 abstract = {Encoding implicit language presents a challenge for language models, especially in high-risk domains where maintaining high precision is important. Automated detection of online child grooming is one such critical domain, where predators manipulate victims using a combination of explicit and implicit language to convey harmful intentions. While recent studies have shown the potential of Transformer language models like SBERT for preemptive grooming detection, they primarily depend on surface-level features and approximate real victim grooming processes using vigilante and law enforcement conversations. The question of whether these features and approximations are reasonable has not been addressed thus far. In this paper, we address this gap and study whether SBERT can effectively discern varying degrees of grooming risk inherent in conversations, and evaluate its results across different participant groups. Our analysis reveals that while fine-tuning aids language models in learning to assign grooming scores, they show high variance in predictions, especially for contexts containing higher degrees of grooming risk. These errors appear in cases that 1) utilize indirect speech pathways to manipulate victims and 2) lack sexually explicit content. This finding underscores the necessity for robust modeling of indirect speech acts by language models, particularly those employed by predators.},
 author = {Geetanjali Bihani and Tatiana Ringenberg and Julia Rayz},
 comment = {9 pages, 2 figures. Accepted for publication in the Proceedings of the NAFIPS International Conference on Fuzzy Systems, Soft Computing, and Explainable AI. NAFIPS'2024},
 doi = {},
 eprint = {2502.12563v1},
 journal = {arXiv preprint},
 title = {Evaluating Language Models on Grooming Risk Estimation Using Fuzzy Theory},
 url = {http://arxiv.org/abs/2502.12563v1},
 year = {2025}
}

@article{2502.12576v1,
 abstract = {With the advent of social media, children are becoming increasingly vulnerable to the risk of grooming in online settings. Detecting grooming instances in an online conversation poses a significant challenge as the interactions are not necessarily sexually explicit, since the predators take time to build trust and a relationship with their victim. Moreover, predators evade detection using indirect and coded language. While previous studies have fine-tuned Transformers to automatically identify grooming in chat conversations, they overlook the impact of coded and indirect language on model predictions, and how these align with human perceptions of grooming. In this paper, we address this gap and evaluate bi-encoders on the task of classifying different degrees of grooming risk in chat contexts, for three different participant groups, i.e. law enforcement officers, real victims, and decoys. Using a fuzzy-theoretic framework, we map human assessments of grooming behaviors to estimate the actual degree of grooming risk. Our analysis reveals that fine-tuned models fail to tag instances where the predator uses indirect speech pathways and coded language to evade detection. Further, we find that such instances are characterized by a higher presence of out-of-vocabulary (OOV) words in samples, causing the model to misclassify. Our findings highlight the need for more robust models to identify coded language from noisy chat inputs in grooming contexts.},
 author = {Geetanjali Bihani and Julia Rayz},
 comment = {8 pages, 2 figures. Accepted for publication in the Proceedings of the NAFIPS International Conference on Fuzzy Systems, Soft Computing, and Explainable AI. NAFIPS'2024},
 doi = {},
 eprint = {2502.12576v1},
 journal = {arXiv preprint},
 title = {A Fuzzy Evaluation of Sentence Encoders on Grooming Risk Classification},
 url = {http://arxiv.org/abs/2502.12576v1},
 year = {2025}
}

@article{2502.15403v1,
 abstract = {Obtaining high-quality explanations of a model's output enables developers to identify and correct biases, align the system's behavior with human values, and ensure ethical compliance. Explainable Artificial Intelligence (XAI) practitioners rely on specific measures to gauge the quality of such explanations. These measures assess key attributes, such as how closely an explanation aligns with a model's decision process (faithfulness), how accurately it pinpoints the relevant input features (localization), and its consistency across different cases (robustness). Despite providing valuable information, these measures do not fully address a critical practitioner's concern: how does the quality of a given explanation compare to other potential explanations? Traditionally, the quality of an explanation has been assessed by comparing it to a randomly generated counterpart. This paper introduces an alternative: the Quality Gap Estimate (QGE). The QGE method offers a direct comparison to what can be viewed as the `inverse' explanation, one that conceptually represents the antithesis of the original explanation. Our extensive testing across multiple model architectures, datasets, and established quality metrics demonstrates that the QGE method is superior to the traditional approach. Furthermore, we show that QGE enhances the statistical reliability of these quality assessments. This advance represents a significant step toward a more insightful evaluation of explanations that enables a more effective inspection of a model's behavior.},
 author = {Carlos Eiras-Franco and Anna Hedström and Marina M. -C. Höhne},
 comment = {Accepted to AAAI 2025},
 doi = {10.1609/aaai.v39i26.34935},
 eprint = {2502.15403v1},
 journal = {arXiv preprint},
 title = {Evaluate with the Inverse: Efficient Approximation of Latent Explanation Quality Distribution},
 url = {http://arxiv.org/abs/2502.15403v1},
 year = {2025}
}

@article{2502.15568v2,
 abstract = {In this study, we examine the reliability of AI-based Voting Advice Applications (VAAs) and large language models (LLMs) in providing objective political information. Our analysis is based upon a comparison with party responses to 38 statements of the Wahl-O-Mat, a well-established German online tool that helps inform voters by comparing their views with political party positions. For the LLMs, we identify significant biases. They exhibit a strong alignment (over 75% on average) with left-wing parties and a substantially lower alignment with center-right (smaller 50%) and right-wing parties (around 30%). Furthermore, for the VAAs, intended to objectively inform voters, we found substantial deviations from the parties' stated positions in Wahl-O-Mat: While one VAA deviated in 25% of cases, another VAA showed deviations in more than 50% of cases. For the latter, we even observed that simple prompt injections led to severe hallucinations, including false claims such as non-existent connections between political parties and right-wing extremist ties.},
 author = {Ina Dormuth and Sven Franke and Marlies Hafer and Tim Katzke and Alexander Marx and Emmanuel Müller and Daniel Neider and Markus Pauly and Jérôme Rutinowski},
 comment = {},
 doi = {10.1007/978-3-032-08333-3_4},
 eprint = {2502.15568v2},
 journal = {arXiv preprint},
 title = {A Cautionary Tale About "Neutrally" Informative AI Tools Ahead of the 2025 Federal Elections in Germany},
 url = {http://arxiv.org/abs/2502.15568v2},
 year = {2025}
}

@article{2502.15827v2,
 abstract = {Accurate prediction of shear strength parameters in Municipal Solid Waste (MSW) remains a critical challenge in geotechnical engineering due to the heterogeneous nature of waste materials and their temporal evolution through degradation processes. This paper presents a novel explainable artificial intelligence (XAI) framework for evaluating cohesion and friction angle across diverse MSW compositional profiles. The proposed model integrates a multi-layer perceptron architecture with SHAP (SHapley Additive exPlanations) analysis to provide transparent insights into how specific waste components influence strength characteristics. Training data encompassed large-scale direct shear tests across various waste compositions and degradation states. The model demonstrated superior predictive accuracy compared to traditional gradient boosting methods, achieving mean absolute percentage errors of 7.42% and 14.96% for friction angle and cohesion predictions, respectively. Through SHAP analysis, the study revealed that fibrous materials and particle size distribution were primary drivers of shear strength variation, with food waste and plastics showing significant but non-linear effects. The model's explainability component successfully quantified these relationships, enabling evidence-based recommendations for waste management practices. This research bridges the gap between advanced machine learning and geotechnical engineering practice, offering a reliable tool for rapid assessment of MSW mechanical properties while maintaining interpretability for engineering decision-making.},
 author = {Parichat Suknark and Sompote Youwaib and Tipok Kitkobsin and Sirintornthep Towprayoon and Chart Chiemchaisri and Komsilp Wangyao},
 comment = {},
 doi = {},
 eprint = {2502.15827v2},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence Model for Evaluating Shear Strength Parameters of Municipal Solid Waste Across Diverse Compositional Profiles},
 url = {http://arxiv.org/abs/2502.15827v2},
 year = {2025}
}

@article{2502.15898v1,
 abstract = {Medicare fraud poses a substantial challenge to healthcare systems, resulting in significant financial losses and undermining the quality of care provided to legitimate beneficiaries. This study investigates the use of machine learning (ML) to enhance Medicare fraud detection, addressing key challenges such as class imbalance, high-dimensional data, and evolving fraud patterns. A dataset comprising inpatient claims, outpatient claims, and beneficiary details was used to train and evaluate five ML models: Random Forest, KNN, LDA, Decision Tree, and AdaBoost. Data preprocessing techniques included resampling SMOTE method to address the class imbalance, feature selection for dimensionality reduction, and aggregation of diagnostic and procedural codes. Random Forest emerged as the best-performing model, achieving a training accuracy of 99.2% and validation accuracy of 98.8%, and F1-score (98.4%). The Decision Tree also performed well, achieving a validation accuracy of 96.3%. KNN and AdaBoost demonstrated moderate performance, with validation accuracies of 79.2% and 81.1%, respectively, while LDA struggled with a validation accuracy of 63.3% and a low recall of 16.6%. The results highlight the importance of advanced resampling techniques, feature engineering, and adaptive learning in detecting Medicare fraud effectively. This study underscores the potential of machine learning in addressing the complexities of fraud detection. Future work should explore explainable AI and hybrid models to improve interpretability and performance, ensuring scalable and reliable fraud detection systems that protect healthcare resources and beneficiaries.},
 author = {Dorsa Farahmandazad and Kasra Danesh},
 comment = {},
 doi = {},
 eprint = {2502.15898v1},
 journal = {arXiv preprint},
 title = {ML-Driven Approaches to Combat Medicare Fraud: Advances in Class Imbalance Solutions, Feature Engineering, Adaptive Learning, and Business Impact},
 url = {http://arxiv.org/abs/2502.15898v1},
 year = {2025}
}

@article{2502.16083v1,
 abstract = {In the context of explainable artificial intelligence (XAI), limited research has identified role-specific explanation needs. This study investigates the explanation needs of data scientists, who are responsible for training, testing, deploying, and maintaining machine learning (ML) models in AI systems. The research aims to determine specific explanation content of data scientists. A task analysis identified user goals and proactive user tasks. Using explanation questions, task-specific explanation needs and content were identified. From these individual explanations, we developed a mental model for explanations, which was validated and revised through a qualitative study (n=12). In a second quantitative study (n=12), we examined which explanation intents (reason, comparison, accuracy, prediction, trust) require which type of explanation content from the mental model. The findings are: F1: Explanation content for data scientists comes from the application domain, system domain, and AI domain. F2: Explanation content can be complex and should be organized sequentially and/or in hierarchies (novelty claim). F3: Explanation content includes context, inputs, evidence, attributes, ranked list, interim results, efficacy principle, and input/output relationships (novelty claim). F4: Explanation content should be organized as a causal story. F5: Standardized explanation questions ensure complete coverage of explanation needs (novelty claim). F6: Refining mental models for explanations increases significantly its quality (novelty claim).},
 author = {Helmut Degen and Ziran Min and Parinitha Nagaraja},
 comment = {47 pages with 19 figures and 6 tables},
 doi = {},
 eprint = {2502.16083v1},
 journal = {arXiv preprint},
 title = {How to explain it to data scientists? A mixed-methods user study about explainable AI, using mental models for explanations},
 url = {http://arxiv.org/abs/2502.16083v1},
 year = {2025}
}

@article{2502.17022v2,
 abstract = {As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contribute the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through systematic empirical analysis across multiple datasets, model architectures, and perturbation strategies, we reveal previously overlooked class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions, offering particular value for class-imbalanced datasets. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common.},
 author = {Gregor Baer and Isel Grau and Chao Zhang and Pieter Van Gorp},
 comment = {Accepted at The World Conference on eXplainable Artificial Intelligence (XAI-2025)},
 doi = {10.1007/978-3-032-08330-2_14},
 eprint = {2502.17022v2},
 journal = {arXiv preprint},
 title = {Class-Dependent Perturbation Effects in Evaluating Time Series Attributions},
 url = {http://arxiv.org/abs/2502.17022v2},
 year = {2025}
}

@article{2502.17357v1,
 abstract = {Lennard-Jones (LJ) fluids serve as an important theoretical framework for understanding molecular interactions. Binary LJ fluids, where two distinct species of particles interact based on the LJ potential, exhibit rich phase behavior and provide valuable insights of complex fluid mixtures. Here we report the construction and utility of an artificial intelligence (AI) model for binary LJ fluids, focusing on their effectiveness in predicting radial distribution functions (RDFs) across a range of conditions. The RDFs of a binary mixture with varying compositions and temperatures are collected from molecular dynamics (MD) simulations to establish and validate the AI model. In this AI pipeline, RDFs are discretized in order to reduce the output dimension of the model. This, in turn, improves the efficacy, and reduce the complexity of an AI RDF model. The model is shown to predict RDFs for many unknown mixtures very accurately, especially outside the training temperature range. Our analysis suggests that the particle size ratio has a higher order impact on the microstructure of a binary mixture. We also highlight the areas where the fidelity of the AI model is low when encountering new regimes with different underlying physics.},
 author = {Israrul H Hashmi and Rahul Karmakar and Marripelli Maniteja and Kumar Ayush and Tarak K. Patra},
 comment = {},
 doi = {},
 eprint = {2502.17357v1},
 journal = {arXiv preprint},
 title = {An Explainable AI Model for Binary LJ Fluids},
 url = {http://arxiv.org/abs/2502.17357v1},
 year = {2025}
}

@article{2502.17469v2,
 abstract = {The analysis of lifelogs can yield valuable insights into an individual's daily life, particularly with regard to their health and well-being. The accurate assessment of quality of life is necessitated by the use of diverse sensors and precise synchronization. To rectify this issue, this study proposes the image-based sleep quality and stress level estimation flow (PixleepFlow). PixleepFlow employs a conversion methodology into composite image data to examine sleep patterns and their impact on overall health. Experiments were conducted using lifelog datasets to ascertain the optimal combination of data formats. In addition, we identified which sensor information has the greatest influence on the quality of life through Explainable Artificial Intelligence(XAI). As a result, PixleepFlow produced more significant results than various data formats. This study was part of a written-based competition, and the additional findings from the lifelog dataset are detailed in Section Section IV. More information about PixleepFlow can be found at https://github.com/seongjiko/Pixleep.},
 author = {Younghoon Na and Seunghun Oh and Seongji Ko and Hyunkyung Lee},
 comment = {},
 doi = {},
 eprint = {2502.17469v2},
 journal = {arXiv preprint},
 title = {PixleepFlow: A Pixel-Based Lifelog Framework for Predicting Sleep Quality and Stress Level},
 url = {http://arxiv.org/abs/2502.17469v2},
 year = {2025}
}

@article{2502.17999v1,
 abstract = {Sensor-based Human Activity Recognition (HAR) in smart home environments is crucial for several applications, especially in the healthcare domain. The majority of the existing approaches leverage deep learning models. While these approaches are effective, the rationale behind their outputs is opaque. Recently, eXplainable Artificial Intelligence (XAI) approaches emerged to provide intuitive explanations to the output of HAR models. To the best of our knowledge, these approaches leverage classic deep models like CNNs or RNNs. Recently, Graph Neural Networks (GNNs) proved to be effective for sensor-based HAR. However, existing approaches are not designed with explainability in mind. In this work, we propose the first explainable Graph Neural Network explicitly designed for smart home HAR. Our results on two public datasets show that this approach provides better explanations than state-of-the-art methods while also slightly improving the recognition rate.},
 author = {Michele Fiori and Davide Mor and Gabriele Civitarese and Claudio Bettini},
 comment = {This is a preprint. Paper accepted for publication at the 21st EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services (Mobiquitous)},
 doi = {},
 eprint = {2502.17999v1},
 journal = {arXiv preprint},
 title = {GNN-XAR: A Graph Neural Network for Explainable Activity Recognition in Smart Homes},
 url = {http://arxiv.org/abs/2502.17999v1},
 year = {2025}
}

@article{2503.00234v3,
 abstract = {The widespread adoption of machine learning systems has raised critical concerns about fairness and bias, making mitigating harmful biases essential for AI development. In this paper, we investigate the relationship between debiasing and removing artifacts in neural networks for computer vision tasks. First, we introduce a set of novel XAI-based metrics that analyze saliency maps to assess shifts in a model's decision-making process. Then, we demonstrate that successful debiasing methods systematically redirect model focus away from protected attributes. Finally, we show that techniques originally developed for artifact removal can be effectively repurposed for improving fairness. These findings provide evidence for the existence of a bidirectional connection between ensuring fairness and removing artifacts corresponding to protected attributes.},
 author = {Lukasz Sztukiewicz and Ignacy Stępka and Michał Wiliński and Jerzy Stefanowski},
 comment = {},
 doi = {},
 eprint = {2503.00234v3},
 journal = {arXiv preprint},
 title = {Investigating the Relationship Between Debiasing and Artifact Removal using Saliency Maps},
 url = {http://arxiv.org/abs/2503.00234v3},
 year = {2025}
}

@article{2503.00863v1,
 abstract = {Clinical trial eligibility matching is a critical yet often labor-intensive and error-prone step in medical research, as it ensures that participants meet precise criteria for safe and reliable study outcomes. Recent advances in Natural Language Processing (NLP) have shown promise in automating and improving this process by rapidly analyzing large volumes of unstructured clinical text and structured electronic health record (EHR) data. In this paper, we present a systematic overview of current NLP methodologies applied to clinical trial eligibility screening, focusing on data sources, annotation practices, machine learning approaches, and real-world implementation challenges. A comprehensive literature search (spanning Google Scholar, Mendeley, and PubMed from 2015 to 2024) yielded high-quality studies, each demonstrating the potential of techniques such as rule-based systems, named entity recognition, contextual embeddings, and ontology-based normalization to enhance patient matching accuracy. While results indicate substantial improvements in screening efficiency and precision, limitations persist regarding data completeness, annotation consistency, and model scalability across diverse clinical domains. The review highlights how explainable AI and standardized ontologies can bolster clinician trust and broaden adoption. Looking ahead, further research into advanced semantic and temporal representations, expanded data integration, and rigorous prospective evaluations is necessary to fully realize the transformative potential of NLP in clinical trial recruitment.},
 author = {Muhammad Talha Sharif and Abdul Rehman},
 comment = {},
 doi = {},
 eprint = {2503.00863v1},
 journal = {arXiv preprint},
 title = {Systematic Literature Review on Clinical Trial Eligibility Matching},
 url = {http://arxiv.org/abs/2503.00863v1},
 year = {2025}
}

@article{2503.00892v1,
 abstract = {We introduce Riemannian Integrated Gradients (RIG); an extension of Integrated Gradients (IG) to Riemannian manifolds. We demonstrate that RIG restricts to IG when the Riemannian manifold is Euclidean space. We show that feature attribution can be phrased as an eigenvalue problem where attributions correspond to eigenvalues of a symmetric endomorphism.},
 author = {Federico Costanza and Lachlan Simpson},
 comment = {},
 doi = {},
 eprint = {2503.00892v1},
 journal = {arXiv preprint},
 title = {Riemannian Integrated Gradients: A Geometric View of Explainable AI},
 url = {http://arxiv.org/abs/2503.00892v1},
 year = {2025}
}

@article{2503.01591v1,
 abstract = {This review systematically examines deep learning applications in financial asset management. Unlike prior reviews, this study focuses on identifying emerging trends, such as the integration of explainable artificial intelligence (XAI) and deep reinforcement learning (DRL), and their transformative potential. It highlights new developments, including hybrid models (e.g., transformer-based architectures) and the growing use of alternative data sources such as ESG indicators and sentiment analysis. These advancements challenge traditional financial paradigms and set the stage for a deeper understanding of the evolving landscape. We use the Scopus database to select the most relevant articles published from 2018 to 2023. The inclusion criteria encompassed articles that explicitly apply deep learning models within financial asset management. We excluded studies focused on physical assets. This review also outlines our methodology for evaluating the relevance and impact of the included studies, including data sources and analytical methods. Our search identified 934 articles, with 612 meeting the inclusion criteria based on their focus and methodology. The synthesis of results from these articles provides insights into the effectiveness of deep learning models in improving portfolio performance and price forecasting accuracy. The review highlights the broad applicability and potential enhancements deep learning offers to financial asset management. Despite some limitations due to the scope of model application and variation in methodological rigour, the overall evidence supports deep learning as a valuable tool in this field. Our systematic review underscores the progressive integration of deep learning in financial asset management, suggesting a trajectory towards more sophisticated and impactful applications.},
 author = {Pedro Reis and Ana Paula Serra and João Gama},
 comment = {},
 doi = {},
 eprint = {2503.01591v1},
 journal = {arXiv preprint},
 title = {The Role of Deep Learning in Financial Asset Management: A Systematic Review},
 url = {http://arxiv.org/abs/2503.01591v1},
 year = {2025}
}

@article{2503.02126v1,
 abstract = {In this study, eXplainable Artificial Intelligence (XAI) methods are applied to analyze flow fields obtained through PIV measurements of an axisymmetric turbulent jet. A convolutional neural network (U-Net) was trained to predict velocity fields at subsequent time steps. Three XAI methods: SHapley Additive explanations (SHAP), Gradient-SHAP, and Grad-CAM were employed to identify the flow field regions relevant for prediction. SHAP requires predefined segmentation of the flow field into relevant regions, while Gradient-SHAP and Grad-CAM avoid this bias by generating gradient-based heatmaps. The results show that the most relevant structures do not necessarily coincide with regions of maximum vorticity but rather with those exhibiting moderate vorticity, highlighting the critical role of these regions in energy transfer and jet dynamics. Additionally, structures with high turbulent dissipation values are identified as the most significant. Gradient-SHAP and Grad-CAM methods reveal a uniform spatial distribution of relevant regions, emphasizing the contribution of nearly circular structures to turbulent mixing. This study advances the understanding of turbulent dynamics through XAI tools, providing an innovative approach to correlate machine learning models with physical phenomena.},
 author = {Enrico Amico and Lorenzo Matteucci and Gioacchino Cafiero},
 comment = {},
 doi = {},
 eprint = {2503.02126v1},
 journal = {arXiv preprint},
 title = {Data-Driven Insights into Jet Turbulence: Explainable AI Approaches},
 url = {http://arxiv.org/abs/2503.02126v1},
 year = {2025}
}

@article{2503.02773v1,
 abstract = {In this paper, we propose a new theoretical approach to Explainable AI. Following the Scientific Method, this approach consists in formulating on the basis of empirical evidence, a mathematical model to explain and predict the behaviors of Neural Networks. We apply the method to a case study created in a controlled environment, which we call Prime Convolutional Model (p-Conv for short). p-Conv operates on a dataset consisting of the first one million natural numbers and is trained to identify the congruence classes modulo a given integer $m$. Its architecture uses a convolutional-type neural network that contextually processes a sequence of $B$ consecutive numbers to each input. We take an empirical approach and exploit p-Conv to identify the congruence classes of numbers in a validation set using different values for $m$ and $B$. The results show that the different behaviors of p-Conv (i.e., whether it can perform the task or not) can be modeled mathematically in terms of $m$ and $B$. The inferred mathematical model reveals interesting patterns able to explain when and why p-Conv succeeds in performing task and, if not, which error pattern it follows.},
 author = {Francesco Panelli and Doaa Almhaithawi and Tania Cerquitelli and Alessandro Bellini},
 comment = {},
 doi = {},
 eprint = {2503.02773v1},
 journal = {arXiv preprint},
 title = {Prime Convolutional Model: Breaking the Ground for Theoretical Explainability},
 url = {http://arxiv.org/abs/2503.02773v1},
 year = {2025}
}

@article{2503.03113v2,
 abstract = {Comprehensive forecasts of space tourism demand are crucial for businesses to optimize strategies and customer experiences in this burgeoning industry. Traditional methods struggle to capture the complex factors influencing an individual's decision to travel to space. In this paper, we propose an explainable and trustworthy artificial intelligence framework to address the challenge of predicting space tourism demand by following the National Institute of Standards and Technology guidelines. We develop a novel machine learning network, called SpaceNet, capable of learning wide-range dependencies in data and allowing us to analyze the relationships between various factors such as age, income, and risk tolerance. We investigate space travel demand in the US, categorizing it into four types: no travel, moon travel, suborbital, and orbital travel. To this end, we collected 1860 data points in many states and cities with different ages and then conducted our experiment with the data. From our experiments, the SpaceNet achieves an average ROC-AUC of 0.82 $\pm$ 0.088, indicating strong classification performance. Our investigation demonstrated that travel price, age, annual income, gender, and fatality probability are important features in deciding whether a person wants to travel or not. Beyond demand forecasting, we use explainable AI to provide interpretation for the travel-type decisions of an individual, offering insights into the factors driving interest in space travel, which is not possible with traditional classification methods. This knowledge enables businesses to tailor marketing strategies and optimize service offerings in this rapidly evolving market. To the best of our knowledge, this is the first work to implement an explainable and interpretable AI framework for investigating the factors influencing space tourism.},
 author = {Tan-Hanh Pham and Jingchen Bi and Rodrigo Mesa-Arango and Kim-Doang Nguyen},
 comment = {15 pages},
 doi = {},
 eprint = {2503.03113v2},
 journal = {arXiv preprint},
 title = {Predicting Space Tourism Demand Using Explainable AI},
 url = {http://arxiv.org/abs/2503.03113v2},
 year = {2025}
}

@article{2503.04283v1,
 abstract = {As predictive machine learning models become increasingly adopted and advanced, their role has evolved from merely predicting outcomes to actively shaping them. This evolution has underscored the importance of Trustworthy AI, highlighting the necessity to extend our focus beyond mere accuracy and toward a comprehensive understanding of these models' behaviors within the specific contexts of their applications. To further progress in explainability, we introduce Poem, Prefetched Offline Explanation Model, a model-agnostic, local explainability algorithm for image data. The algorithm generates exemplars, counterexemplars and saliency maps to provide quick and effective explanations suitable for time-sensitive scenarios. Leveraging an existing local algorithm, \poem{} infers factual and counterfactual rules from data to create illustrative examples and opposite scenarios with an enhanced stability by design. A novel mechanism then matches incoming test points with an explanation base and produces diverse exemplars, informative saliency maps and believable counterexemplars. Experimental results indicate that Poem outperforms its predecessor Abele in speed and ability to generate more nuanced and varied exemplars alongside more insightful saliency maps and valuable counterexemplars.},
 author = {Fabio Michele Russo and Carlo Metta and Anna Monreale and Salvatore Rinzivillo and Fabio Pinelli},
 comment = {},
 doi = {10.1007/978-3-031-78980-9_11},
 eprint = {2503.04283v1},
 journal = {arXiv preprint},
 title = {Explainable AI in Time-Sensitive Scenarios: Prefetched Offline Explanation Model},
 url = {http://arxiv.org/abs/2503.04283v1},
 year = {2025}
}

@article{2503.04343v1,
 abstract = {While XAI focuses on providing AI explanations to humans, can the reverse - humans explaining their judgments to AI - foster richer, synergistic human-AI systems? This paper explores various forms of human inputs to AI and examines how human explanations can guide machine learning models toward automated judgments and explanations that align more closely with human concepts.},
 author = {Alan Dix and Tommaso Turchi and Ben Wilson and Anna Monreale and Matt Roach},
 comment = {},
 doi = {},
 eprint = {2503.04343v1},
 journal = {arXiv preprint},
 title = {Talking Back -- human input and explanations to interactive AI systems},
 url = {http://arxiv.org/abs/2503.04343v1},
 year = {2025}
}

@article{2503.05050v2,
 abstract = {The increasing complexity of LLMs presents significant challenges to their transparency and interpretability, necessitating the use of eXplainable AI (XAI) techniques to enhance trustworthiness and usability. This study introduces a comprehensive evaluation framework with four novel metrics for assessing the effectiveness of five XAI techniques across five LLMs and two downstream tasks. We apply this framework to evaluate several XAI techniques LIME, SHAP, Integrated Gradients, Layer-wise Relevance Propagation (LRP), and Attention Mechanism Visualization (AMV) using the IMDB Movie Reviews and Tweet Sentiment Extraction datasets. The evaluation focuses on four key metrics: Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. Our results show that LIME consistently achieves high scores across multiple LLMs and evaluation metrics, while AMV demonstrates superior Robustness and near-perfect Consistency. LRP excels in Contrastivity, particularly with more complex models. Our findings provide valuable insights into the strengths and limitations of different XAI methods, offering guidance for developing and selecting appropriate XAI techniques for LLMs.},
 author = {Melkamu Abay Mersha and Mesay Gemeda Yigezu and Hassan Shakil and Ali K. AlShami and Sanghyun Byun and Jugal Kalita},
 comment = {arXiv admin note: substantial text overlap with arXiv:2501.15374},
 doi = {},
 eprint = {2503.05050v2},
 journal = {arXiv preprint},
 title = {A Unified Framework with Novel Metrics for Evaluating the Effectiveness of XAI Techniques in LLMs},
 url = {http://arxiv.org/abs/2503.05050v2},
 year = {2025}
}

@article{2503.05303v1,
 abstract = {Machine learning (ML) models serve as powerful tools for threat detection and mitigation; however, they also introduce potential new risks. Adversarial input can exploit these models through standard interfaces, thus creating new attack pathways that threaten critical network operations. As ML advancements progress, adversarial strategies become more advanced, and conventional defenses such as adversarial training are costly in computational terms and often fail to provide real-time detection. These methods typically require a balance between robustness and model performance, which presents challenges for applications that demand instant response. To further investigate this vulnerability, we suggest a novel strategy for detecting and mitigating adversarial attacks using eXplainable Artificial Intelligence (XAI). This approach is evaluated in real time within intrusion detection systems (IDS), leading to the development of a zero-touch mitigation strategy. Additionally, we explore various scenarios in the Radio Resource Control (RRC) layer within the Open Radio Access Network (O-RAN) framework, emphasizing the critical need for enhanced mitigation techniques to strengthen IDS defenses against advanced threats and implement a zero-touch mitigation solution. Extensive testing across different scenarios in the RRC layer of the O-RAN infrastructure validates the ability of the framework to detect and counteract integrated RRC-layer attacks when paired with adversarial strategies, emphasizing the essential need for robust defensive mechanisms to strengthen IDS against complex threats.},
 author = {Betül Güvenç Paltun and Ramin Fuladi and Rim El Malki},
 comment = {},
 doi = {},
 eprint = {2503.05303v1},
 journal = {arXiv preprint},
 title = {Robust Intrusion Detection System with Explainable Artificial Intelligence},
 url = {http://arxiv.org/abs/2503.05303v1},
 year = {2025}
}

@article{2503.05789v1,
 abstract = {Algorithmic solutions have significant potential to improve decision-making across various domains, from healthcare to e-commerce. However, the widespread adoption of these solutions is hindered by a critical challenge: the lack of human-interpretable explanations. Current approaches to Explainable AI (XAI) predominantly focus on complex machine learning models, often producing brittle and non-intuitive explanations. This project proposes a novel approach to developing explainable algorithms by starting with optimization problems, specifically the assignment problem. The developed software library enriches basic algorithms with human-understandable explanations through four key methodologies: generating meaningful alternative solutions, creating robust solutions through input perturbation, generating concise decision trees and providing reports with comprehensive explanation of the results. Currently developed tools are often designed with specific clustering algorithms in mind, which limits their adaptability and flexibility to incorporate alternative techniques. Additionally, many of these tools fail to integrate expert knowledge, which could enhance the clustering process by providing valuable insights and context. This lack of adaptability and integration can hinder the effectiveness and robustness of the clustering outcomes in various applications. The represents a step towards making algorithmic solutions more transparent, trustworthy, and accessible. By collaborating with industry partners in sectors such as sales, we demonstrate the practical relevance and transformative potential of our approach.},
 author = {Zuzanna Bączek and Michał Bizoń and Aneta Pawelec and Piotr Sankowski},
 comment = {},
 doi = {},
 eprint = {2503.05789v1},
 journal = {arXiv preprint},
 title = {EXALT: EXplainable ALgorithmic Tools for Optimization Problems},
 url = {http://arxiv.org/abs/2503.05789v1},
 year = {2025}
}

@article{2503.06463v1,
 abstract = {As cannabis use has increased in recent years, researchers have come to rely on sophisticated machine learning models to predict cannabis use behavior and its impact on health. However, many artificial intelligence (AI) models lack transparency and interpretability due to their opaque nature, limiting their trust and adoption in real-world medical applications, such as clinical decision support systems (CDSS). To address this issue, this paper enhances algorithm explainability underlying CDSS by integrating multiple Explainable Artificial Intelligence (XAI) methods and applying causal inference techniques to clarify the model' predictive decisions under various scenarios. By providing deeper interpretability of the XAI outputs using Large Language Models (LLMs), we provide users with more personalized and accessible insights to overcome the challenges posed by AI's "black box" nature. Our system dynamically adjusts feedback based on user queries and emotional states, combining text-based sentiment analysis with real-time facial emotion recognition to ensure responses are empathetic, context-adaptive, and user-centered. This approach bridges the gap between the learning demands of interpretability and the need for intuitive understanding, enabling non-technical users such as clinicians and clinical researchers to interact effectively with AI models.} Ultimately, this approach improves usability, enhances perceived trustworthiness, and increases the impact of CDSS in healthcare applications.},
 author = {Tongze Zhang and Tammy Chung and Anind Dey and Sang Won Bae},
 comment = {},
 doi = {},
 eprint = {2503.06463v1},
 journal = {arXiv preprint},
 title = {AXAI-CDSS : An Affective Explainable AI-Driven Clinical Decision Support System for Cannabis Use},
 url = {http://arxiv.org/abs/2503.06463v1},
 year = {2025}
}

@article{2503.07784v1,
 abstract = {Explainable AI is a crucial component for edge services, as it ensures reliable decision making based on complex AI models. Surrogate models are a prominent approach of XAI where human-interpretable models, such as a linear regression model, are trained to approximate a complex (black-box) model's predictions. This paper delves into the balance between the predictive accuracy of complex AI models and their approximation by surrogate ones, advocating that both these models benefit from being learned simultaneously. We derive a joint (bi-level) training scheme for both models and we introduce a new algorithm based on multi-objective optimization (MOO) to simultaneously minimize both the complex model's prediction error and the error between its outputs and those of the surrogate. Our approach leads to improvements that exceed 99% in the approximation of the black-box model through the surrogate one, as measured by the metric of Fidelity, for a compromise of less than 3% absolute reduction in the black-box model's predictive accuracy, compared to single-task and multi-task learning baselines. By improving Fidelity, we can derive more trustworthy explanations of the complex model's outcomes from the surrogate, enabling reliable AI applications for intelligent services at the network edge.},
 author = {Foivos Charalampakos and Thomas Tsouparopoulos and Iordanis Koutsopoulos},
 comment = {},
 doi = {},
 eprint = {2503.07784v1},
 journal = {arXiv preprint},
 title = {Joint Explainability-Performance Optimization With Surrogate Models for AI-Driven Edge Services},
 url = {http://arxiv.org/abs/2503.07784v1},
 year = {2025}
}

@article{2503.08420v1,
 abstract = {Objective. This paper presents an overview of generalizable and explainable artificial intelligence (XAI) in deep learning (DL) for medical imaging, aimed at addressing the urgent need for transparency and explainability in clinical applications.
  Methodology. We propose to use four CNNs in three medical datasets (brain tumor, skin cancer, and chest x-ray) for medical image classification tasks. In addition, we perform paired t-tests to show the significance of the differences observed between different methods. Furthermore, we propose to combine ResNet50 with five common XAI techniques to obtain explainable results for model prediction, aiming at improving model transparency. We also involve a quantitative metric (confidence increase) to evaluate the usefulness of XAI techniques.
  Key findings. The experimental results indicate that ResNet50 can achieve feasible accuracy and F1 score in all datasets (e.g., 86.31\% accuracy in skin cancer). Furthermore, the findings show that while certain XAI methods, such as XgradCAM, effectively highlight relevant abnormal regions in medical images, others, like EigenGradCAM, may perform less effectively in specific scenarios. In addition, XgradCAM indicates higher confidence increase (e.g., 0.12 in glioma tumor) compared to GradCAM++ (0.09) and LayerCAM (0.08).
  Implications. Based on the experimental results and recent advancements, we outline future research directions to enhance the robustness and generalizability of DL models in the field of biomedical imaging.},
 author = {Ahmad Chaddad and Yan Hu and Yihang Wu and Binbin Wen and Reem Kateb},
 comment = {Published in Current Opinion in Biomedical Engineering},
 doi = {10.1016/j.cobme.2024.100567},
 eprint = {2503.08420v1},
 journal = {arXiv preprint},
 title = {Generalizable and Explainable Deep Learning for Medical Image Computing: An Overview},
 url = {http://arxiv.org/abs/2503.08420v1},
 year = {2025}
}

@article{2503.09199v1,
 abstract = {Group Equivariant Non-Expansive Operators (GENEOs) have emerged as mathematical tools for constructing networks for Machine Learning and Artificial Intelligence. Recent findings suggest that such models can be inserted within the domain of eXplainable Artificial Intelligence (XAI) due to their inherent interpretability. In this study, we aim to verify this claim with respect to GENEOnet, a GENEO network developed for an application in computational biochemistry by employing various statistical analyses and experiments. Such experiments first allow us to perform a sensitivity analysis on GENEOnet's parameters to test their significance. Subsequently, we show that GENEOnet exhibits a significantly higher proportion of equivariance compared to other methods. Lastly, we demonstrate that GENEOnet is on average robust to perturbations arising from molecular dynamics. These results collectively serve as proof of the explainability, trustworthiness, and robustness of GENEOnet and confirm the beneficial use of GENEOs in the context of Trustworthy Artificial Intelligence.},
 author = {Giovanni Bocchi and Patrizio Frosini and Alessandra Micheletti and Alessandro Pedretti and Carmen Gratteri and Filippo Lunghini and Andrea Rosario Beccari and Carmine Talarico},
 comment = {},
 doi = {10.1080/02331888.2025.2478203},
 eprint = {2503.09199v1},
 journal = {arXiv preprint},
 title = {GENEOnet: Statistical analysis supporting explainability and trustworthiness},
 url = {http://arxiv.org/abs/2503.09199v1},
 year = {2025}
}

@article{2503.10650v2,
 abstract = {The rise of social media has significantly increased the prevalence of cyberbullying (CB), posing serious risks to both mental and physical well-being. Effective detection systems are essential for mitigating its impact. While several machine learning (ML) models have been developed, few incorporate victims' psychological, demographic, and behavioral factors alongside bullying comments to assess severity. In this study, we propose an AI model intregrating user-specific attributes, including psychological factors (self-esteem, anxiety, depression), online behavior (internet usage, disciplinary history), and demographic attributes (race, gender, ethnicity), along with social media comments. Additionally, we introduce a re-labeling technique that categorizes social media comments into three severity levels: Not Bullying, Mild Bullying, and Severe Bullying, considering user-specific factors.Our LSTM model is trained using 146 features, incorporating emotional, topical, and word2vec representations of social media comments as well as user-level attributes and it outperforms existing baseline models, achieving the highest accuracy of 98\% and an F1-score of 0.97. To identify key factors influencing the severity of cyberbullying, we employ explainable AI techniques (SHAP and LIME) to interpret the model's decision-making process. Our findings reveal that, beyond hate comments, victims belonging to specific racial and gender groups are more frequently targeted and exhibit higher incidences of depression, disciplinary issues, and low self-esteem. Additionally, individuals with a prior history of bullying are at a greater risk of becoming victims of cyberbullying.},
 author = {Tabia Tanzin Prama and Jannatul Ferdaws Amrin and Md. Mushfique Anwar and Iqbal H. Sarker},
 comment = {},
 doi = {},
 eprint = {2503.10650v2},
 journal = {arXiv preprint},
 title = {AI Enabled User-Specific Cyberbullying Severity Detection with Explainability},
 url = {http://arxiv.org/abs/2503.10650v2},
 year = {2025}
}

@article{2503.11282v1,
 abstract = {Alzheimer's disease, a neurodegenerative disorder, is associated with neural, genetic, and proteomic factors while affecting multiple cognitive and behavioral faculties. Traditional AD prediction largely focuses on univariate disease outcomes, such as disease stages and severity. Multimodal data encode broader disease information than a single modality and may, therefore, improve disease prediction; but they often contain missing values. Recent "deeper" machine learning approaches show promise in improving prediction accuracy, yet the biological relevance of these models needs to be further charted. Integrating missing data analysis, predictive modeling, multimodal data analysis, and explainable AI, we propose OPTIMUS, a predictive, modular, and explainable machine learning framework, to unveil the many-to-many predictive pathways between multimodal input data and multivariate disease outcomes amidst missing values. OPTIMUS first applies modality-specific imputation to uncover data from each modality while optimizing overall prediction accuracy. It then maps multimodal biomarkers to multivariate outcomes using machine-learning and extracts biomarkers respectively predictive of each outcome. Finally, OPTIMUS incorporates XAI to explain the identified multimodal biomarkers. Using data from 346 cognitively normal subjects, 608 persons with mild cognitive impairment, and 251 AD patients, OPTIMUS identifies neural and transcriptomic signatures that jointly but differentially predict multivariate outcomes related to executive function, language, memory, and visuospatial function. Our work demonstrates the potential of building a predictive and biologically explainable machine-learning framework to uncover multimodal biomarkers that capture disease profiles across varying cognitive landscapes. The results improve our understanding of the complex many-to-many pathways in AD.},
 author = {Christelle Schneuwly Diaz and Duy-Thanh Vu and Julien Bodelet and Duy-Cat Can and Guillaume Blanc and Haiting Jiang and Lin Yao and Guiseppe Pantaleo and ADNI and Oliver Y. Chén},
 comment = {},
 doi = {},
 eprint = {2503.11282v1},
 journal = {arXiv preprint},
 title = {OPTIMUS: Predicting Multivariate Outcomes in Alzheimer's Disease Using Multi-modal Data amidst Missing Values},
 url = {http://arxiv.org/abs/2503.11282v1},
 year = {2025}
}

@article{2503.12435v1,
 abstract = {In recent years, network slicing has embraced artificial intelligence (AI) models to manage the growing complexity of communication networks. In such a situation, AI-driven zero-touch network automation should present a high degree of flexibility and viability, especially when deployed in live production networks. However, centralized controllers suffer from high data communication overhead due to the vast amount of user data, and most network slices are reluctant to share private data. In federated learning systems, selecting trustworthy clients to participate in training is critical for ensuring system performance and reliability. The present paper proposes a new approach to client selection by leveraging an XAI method to guarantee scalable and fast operation of federated learning based analytic engines that implement slice-level resource provisioning at the RAN-Edge in a non-IID scenario. Attributions from XAI are used to guide the selection of devices participating in training. This approach enhances network trustworthiness for users and addresses the black-box nature of neural network models. The simulations conducted outperformed the standard approach in terms of both convergence time and computational cost, while also demonstrating high scalability.},
 author = {Martino Chiarani and Swastika Roy and Christos Verikoukis and Fabrizio Granelli},
 comment = {8 pages, 7 Figures},
 doi = {},
 eprint = {2503.12435v1},
 journal = {arXiv preprint},
 title = {XAI-Driven Client Selection for Federated Learning in Scalable 6G Network Slicing},
 url = {http://arxiv.org/abs/2503.12435v1},
 year = {2025}
}

@article{2503.12525v1,
 abstract = {In recent years, there has been a growing interest in explainable AI methods. We want not only to make accurate predictions using sophisticated neural networks but also to understand what the model's decision is based on. One of the fundamental levels of interpretability is to provide counterfactual examples explaining the rationale behind the decision and identifying which features, and to what extent, must be modified to alter the model's outcome. To address these requirements, we introduce HyConEx, a classification model based on deep hypernetworks specifically designed for tabular data. Owing to its unique architecture, HyConEx not only provides class predictions but also delivers local interpretations for individual data samples in the form of counterfactual examples that steer a given sample toward an alternative class. While many explainable methods generated counterfactuals for external models, there have been no interpretable classifiers simultaneously producing counterfactual samples so far. HyConEx achieves competitive performance on several metrics assessing classification accuracy and fulfilling the criteria of a proper counterfactual attack. This makes HyConEx a distinctive deep learning model, which combines predictions and explainers as an all-in-one neural network. The code is available at https://github.com/gmum/HyConEx.},
 author = {Patryk Marszałek and Ulvi Movsum-zada and Oleksii Furman and Kamil Książek and Przemysław Spurek and Marek Śmieja},
 comment = {},
 doi = {},
 eprint = {2503.12525v1},
 journal = {arXiv preprint},
 title = {HyConEx: Hypernetwork classifier with counterfactual explanations},
 url = {http://arxiv.org/abs/2503.12525v1},
 year = {2025}
}

@article{2503.13389v1,
 abstract = {This study proposes an autoencoder approach to extract latent features from cone penetration test profiles to evaluate the potential of incorporating CPT data in an AI model. We employ autoencoders to compress 200 CPT profiles of soil behavior type index (Ic) and normalized cone resistance (qc1Ncs) into ten latent features while preserving critical information. We then utilize the extracted latent features with site parameters to train XGBoost models for predicting lateral spreading occurrences in the 2011 Christchurch earthquake. Models using the latent CPT features outperformed models with conventional CPT metrics or no CPT data, achieving over 83% accuracy. Explainable AI revealed the most crucial latent feature corresponding to soil behavior between 1-3 meter depths, highlighting this depth range's criticality for liquefaction evaluation. The autoencoder approach provides an automated technique for condensing CPT profiles into informative latent features for machine-learning liquefaction models.},
 author = {Cheng-Hsi Hsiao and Ellen Rathje and Krishna Kumar},
 comment = {},
 doi = {},
 eprint = {2503.13389v1},
 journal = {arXiv preprint},
 title = {Investigating the effect of CPT in lateral spreading prediction using Explainable AI},
 url = {http://arxiv.org/abs/2503.13389v1},
 year = {2025}
}

@article{2503.14231v1,
 abstract = {Chinese porcelain holds immense historical and cultural value, making its accurate classification essential for archaeological research and cultural heritage preservation. Traditional classification methods rely heavily on expert analysis, which is time-consuming, subjective, and difficult to scale. This paper explores the application of DL and transfer learning techniques to automate the classification of porcelain artifacts across four key attributes: dynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks (CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their performance with and without pre-trained weights. Our results demonstrate that transfer learning significantly enhances classification accuracy, particularly for complex tasks like type classification, where models trained from scratch exhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high accuracy and robustness across all tasks, while VGG16 struggles with more diverse classifications. We further discuss the impact of dataset limitations and propose future directions, including domain-specific pre-training, integration of attention mechanisms, explainable AI methods, and generalization to other cultural artifacts.},
 author = {Ziyao Ling and Giovanni Delnevo and Paola Salomoni and Silvia Mirri},
 comment = {},
 doi = {},
 eprint = {2503.14231v1},
 journal = {arXiv preprint},
 title = {Multi-task Learning for Identification of Porcelain in Song and Yuan Dynasties},
 url = {http://arxiv.org/abs/2503.14231v1},
 year = {2025}
}

@article{2503.14567v1,
 abstract = {Raman spectroscopy is becoming more common for medical diagnostics with deep learning models being increasingly used to leverage its full potential. However, the opaque nature of such models and the sensitivity of medical diagnosis together with regulatory requirements necessitate the need for explainable AI tools. We introduce SpecReX, specifically adapted to explaining Raman spectra. SpecReX uses the theory of actual causality to rank causal responsibility in a spectrum, quantified by iteratively refining mutated versions of the spectrum and testing if it retains the original classification. The explanations provided by SpecReX take the form of a responsibility map, highlighting spectral regions most responsible for the model to make a correct classification. To assess the validity of SpecReX, we create increasingly complex simulated spectra, in which a "ground truth" signal is seeded, to train a classifier. We then obtain SpecReX explanations and compare the results with another explainability tool. By using simulated spectra we establish that SpecReX localizes to the known differences between classes, under a number of conditions. This provides a foundation on which we can find the spectral features which differentiate disease classes. This is an important first step in proving the validity of SpecReX.},
 author = {Nathan Blake and David A. Kelly and Akchunya Chanchal and Sarah Kapllani-Mucaj and Geraint Thomas and Hana Chockler},
 comment = {AAAI Workshop on Health Intelligencee (W3PHIAI-25)},
 doi = {},
 eprint = {2503.14567v1},
 journal = {arXiv preprint},
 title = {SpecReX: Explainable AI for Raman Spectroscopy},
 url = {http://arxiv.org/abs/2503.14567v1},
 year = {2025}
}

@article{2503.15901v1,
 abstract = {The potential for catastrophic collision makes near-Earth asteroids (NEAs) a serious concern. Planetary defense depends on accurately classifying potentially hazardous asteroids (PHAs), however the complexity of the data hampers conventional techniques. This work offers a sophisticated method for accurately predicting hazards by combining machine learning, deep learning, explainable AI (XAI), and anomaly detection. Our approach extracts essential parameters like size, velocity, and trajectory from historical and real-time asteroid data. A hybrid algorithm improves prediction accuracy by combining several cutting-edge models. A forecasting module predicts future asteroid behavior, and Monte Carlo simulations evaluate the likelihood of collisions. Timely mitigation is made possible by a real-time alarm system that notifies worldwide monitoring stations. This technique enhances planetary defense efforts by combining real-time alarms with sophisticated predictive modeling.},
 author = {Amit Kumar Mondal and Nafisha Aslam and Prasenjit Maji and Hemanta Kumar Mondal},
 comment = {17 pages, 12 figures},
 doi = {},
 eprint = {2503.15901v1},
 journal = {arXiv preprint},
 title = {A multi-model approach using XAI and anomaly detection to predict asteroid hazards},
 url = {http://arxiv.org/abs/2503.15901v1},
 year = {2025}
}

@article{2503.16583v1,
 abstract = {Approximate deep neural networks (AxDNNs) are promising for enhancing energy efficiency in real-world devices. One of the key contributors behind this enhanced energy efficiency in AxDNNs is the use of approximate multipliers. Unfortunately, the simulation of approximate multipliers does not usually scale well on CPUs and GPUs. As a consequence, this slows down the overall simulation of AxDNNs aimed at identifying the appropriate approximate multipliers to achieve high energy efficiency with a minimum accuracy loss. To address this problem, we present a novel XAI-Gen methodology, which leverages the analytical model of the emerging hardware accelerator (e.g., Google TPU v4) and explainable artificial intelligence (XAI) to precisely identify the non-critical layers for approximation and quickly discover the appropriate approximate multipliers for AxDNN layers. Our results show that XAI-Gen achieves up to 7x lower energy consumption with only 1-2% accuracy loss. We also showcase the effectiveness of the XAI-Gen approach through a neural architecture search (XAI-NAS) case study. Interestingly, XAI-NAS achieves 40\% higher energy efficiency with up to 5x less execution time when compared to the state-of-the-art NAS methods for generating AxDNNs.},
 author = {Ayesha Siddique and Khurram Khalil and Khaza Anuarul Hoque},
 comment = {This paper has been accepted in the ISQED 2025 conference},
 doi = {},
 eprint = {2503.16583v1},
 journal = {arXiv preprint},
 title = {Explainable AI-Guided Efficient Approximate DNN Generation for Multi-Pod Systolic Arrays},
 url = {http://arxiv.org/abs/2503.16583v1},
 year = {2025}
}

@article{2503.16622v2,
 abstract = {Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning of machine learning models. In IoT systems, XAI improves the transparency of models processing sensor data from multiple heterogeneous devices, ensuring end-users understand and trust their outputs. Among the many applications, XAI has also been applied to sensor-based Activities of Daily Living (ADLs) recognition in smart homes. Existing approaches highlight which sensor events are most important for each predicted activity, using simple rules to convert these events into natural language explanations for non-expert users. However, these methods produce rigid explanations lacking natural language flexibility and are not scalable. With the recent rise of Large Language Models (LLMs), it is worth exploring whether they can enhance explanation generation, considering their proven knowledge of human activities. This paper investigates potential approaches to combine XAI and LLMs for sensor-based ADL recognition. We evaluate if LLMs can be used: a) as explainable zero-shot ADL recognition models, avoiding costly labeled data collection, and b) to automate the generation of explanations for existing data-driven XAI approaches when training data is available and the goal is higher recognition rates. Our critical evaluation provides insights into the benefits and challenges of using LLMs for explainable ADL recognition.},
 author = {Michele Fiori and Gabriele Civitarese and Priyankar Choudhary and Claudio Bettini},
 comment = {},
 doi = {},
 eprint = {2503.16622v2},
 journal = {arXiv preprint},
 title = {Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation},
 url = {http://arxiv.org/abs/2503.16622v2},
 year = {2025}
}

@article{2503.17623v1,
 abstract = {Road fatalities pose significant public safety and health challenges worldwide, with pedestrians being particularly vulnerable in vehicle-pedestrian crashes due to disparities in physical and performance characteristics. This study employs explainable artificial intelligence (XAI) to identify key factors contributing to pedestrian fatalities across the five U.S. states with the highest crash rates (2018-2022). It compares them to the five states with the lowest fatality rates. Using data from the Fatality Analysis Reporting System (FARS), the study applies machine learning techniques-including Decision Trees, Gradient Boosting Trees, Random Forests, and XGBoost-to predict contributing factors to pedestrian fatalities. To address data imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) is utilized, while SHapley Additive Explanations (SHAP) values enhance model interpretability. The results indicate that age, alcohol and drug use, location, and environmental conditions are significant predictors of pedestrian fatalities. The XGBoost model outperformed others, achieving a balanced accuracy of 98 %, accuracy of 90 %, precision of 92 %, recall of 90 %, and an F1 score of 91 %. Findings reveal that pedestrian fatalities are more common in mid-block locations and areas with poor visibility, with older adults and substance-impaired individuals at higher risk. These insights can inform policymakers and urban planners in implementing targeted safety measures, such as improved lighting, enhanced pedestrian infrastructure, and stricter traffic law enforcement, to reduce fatalities and improve public safety.},
 author = {Methusela Sulle and Judith Mwakalonge and Gurcan Comert and Saidi Siuhi and Nana Kankam Gyimah},
 comment = {22 pages, 5 figures},
 doi = {},
 eprint = {2503.17623v1},
 journal = {arXiv preprint},
 title = {Unraveling Pedestrian Fatality Patterns: A Comparative Study with Explainable AI},
 url = {http://arxiv.org/abs/2503.17623v1},
 year = {2025}
}

@article{2503.18185v1,
 abstract = {Counterfactual explanations have emerged as a prominent method in Explainable Artificial Intelligence (XAI), providing intuitive and actionable insights into Machine Learning model decisions. In contrast to other traditional feature attribution methods that assess the importance of input variables, counterfactual explanations focus on identifying the minimal changes required to alter a model's prediction, offering a ``what-if'' analysis that is close to human reasoning. In the context of XAI, counterfactuals enhance transparency, trustworthiness and fairness, offering explanations that are not just interpretable but directly applicable in the decision-making processes.
  In this paper, we present a novel framework that integrates perturbation theory and statistical mechanics to generate minimal counterfactual explanations in explainable AI. We employ a local Taylor expansion of a Machine Learning model's predictive function and reformulate the counterfactual search as an energy minimization problem over a complex landscape. In sequence, we model the probability of candidate perturbations leveraging the Boltzmann distribution and use simulated annealing for iterative refinement. Our approach systematically identifies the smallest modifications required to change a model's prediction while maintaining plausibility. Experimental results on benchmark datasets for cybersecurity in Internet of Things environments, demonstrate that our method provides actionable, interpretable counterfactuals and offers deeper insights into model sensitivity and decision boundaries in high-dimensional spaces.},
 author = {Spyridon Evangelatos and Eleni Veroni and Vasilis Efthymiou and Christos Nikolopoulos and Georgios Th. Papadopoulos and Panagiotis Sarigiannidis},
 comment = {},
 doi = {},
 eprint = {2503.18185v1},
 journal = {arXiv preprint},
 title = {Exploring Energy Landscapes for Minimal Counterfactual Explanations: Applications in Cybersecurity and Beyond},
 url = {http://arxiv.org/abs/2503.18185v1},
 year = {2025}
}

@article{2503.20595v1,
 abstract = {Counterfactual explanations have been successfully applied to create human interpretable explanations for various black-box models. They are handy for tasks in the image domain, where the quality of the explanations benefits from recent advances in generative models. Although counterfactual explanations have been widely applied to classification models, their application to regression tasks remains underexplored. We present two methods to create counterfactual explanations for image regression tasks using diffusion-based generative models to address challenges in sparsity and quality: 1) one based on a Denoising Diffusion Probabilistic Model that operates directly in pixel-space and 2) another based on a Diffusion Autoencoder operating in latent space. Both produce realistic, semantic, and smooth counterfactuals on CelebA-HQ and a synthetic data set, providing easily interpretable insights into the decision-making process of the regression model and reveal spurious correlations. We find that for regression counterfactuals, changes in features depend on the region of the predicted value. Large semantic changes are needed for significant changes in predicted values, making it harder to find sparse counterfactuals than with classifiers. Moreover, pixel space counterfactuals are more sparse while latent space counterfactuals are of higher quality and allow bigger semantic changes.},
 author = {Trung Duc Ha and Sidney Bender},
 comment = {24 Pages, 5 Figures, Accepted at 3rd World Conference on eXplainable Artificial Intelligence (xAI-2025), Code and reproduction instructions available on GitHub, see https://github.com/DevinTDHa/Diffusion-Counterfactuals-for-Image-Regressors},
 doi = {},
 eprint = {2503.20595v1},
 journal = {arXiv preprint},
 title = {Diffusion Counterfactuals for Image Regressors},
 url = {http://arxiv.org/abs/2503.20595v1},
 year = {2025}
}

@article{2503.20796v1,
 abstract = {Sophisticated phishing attacks have emerged as a major cybersecurity threat, becoming more common and difficult to prevent. Though machine learning techniques have shown promise in detecting phishing attacks, they function mainly as "black boxes" without revealing their decision-making rationale. This lack of transparency erodes the trust of users and diminishes their effective threat response. We present EXPLICATE: a framework that enhances phishing detection through a three-component architecture: an ML-based classifier using domain-specific features, a dual-explanation layer combining LIME and SHAP for complementary feature-level insights, and an LLM enhancement using DeepSeek v3 to translate technical explanations into accessible natural language. Our experiments show that EXPLICATE attains 98.4 % accuracy on all metrics, which is on par with existing deep learning techniques but has better explainability. High-quality explanations are generated by the framework with an accuracy of 94.2 % as well as a consistency of 96.8\% between the LLM output and model prediction. We create EXPLICATE as a fully usable GUI application and a light Chrome extension, showing its applicability in many deployment situations. The research shows that high detection performance can go hand-in-hand with meaningful explainability in security applications. Most important, it addresses the critical divide between automated AI and user trust in phishing detection systems.},
 author = {Bryan Lim and Roman Huerta and Alejandro Sotelo and Anthonie Quintela and Priyanka Kumar},
 comment = {},
 doi = {},
 eprint = {2503.20796v1},
 journal = {arXiv preprint},
 title = {EXPLICATE: Enhancing Phishing Detection through Explainable AI and LLM-Powered Interpretability},
 url = {http://arxiv.org/abs/2503.20796v1},
 year = {2025}
}

@article{2503.23486v1,
 abstract = {This paper systematically explores the advancements in adaptive trip route planning and travel time estimation (TTE) through Artificial Intelligence (AI). With the increasing complexity of urban transportation systems, traditional navigation methods often struggle to accommodate dynamic user preferences, real-time traffic conditions, and scalability requirements. This study explores the contributions of established AI techniques, including Machine Learning (ML), Reinforcement Learning (RL), and Graph Neural Networks (GNNs), alongside emerging methodologies like Meta-Learning, Explainable AI (XAI), Generative AI, and Federated Learning. In addition to highlighting these innovations, the paper identifies critical challenges such as ethical concerns, computational scalability, and effective data integration, which must be addressed to advance the field. The paper concludes with recommendations for leveraging AI to build efficient, transparent, and sustainable navigation systems.},
 author = {Nikil Jayasuriya and Deshan Sumanathilaka},
 comment = {6 pages, 2 figures, 1 table},
 doi = {},
 eprint = {2503.23486v1},
 journal = {arXiv preprint},
 title = {A Systematic Decade Review of Trip Route Planning with Travel Time Estimation based on User Preferences and Behavior},
 url = {http://arxiv.org/abs/2503.23486v1},
 year = {2025}
}

@article{2503.24365v1,
 abstract = {As neural networks become dominant in essential systems, Explainable Artificial Intelligence (XAI) plays a crucial role in fostering trust and detecting potential misbehavior of opaque models. LIME (Local Interpretable Model-agnostic Explanations) is among the most prominent model-agnostic approaches, generating explanations by approximating the behavior of black-box models around specific instances. Despite its popularity, LIME faces challenges related to fidelity, stability, and applicability to domain-specific problems. Numerous adaptations and enhancements have been proposed to address these issues, but the growing number of developments can be overwhelming, complicating efforts to navigate LIME-related research. To the best of our knowledge, this is the first survey to comprehensively explore and collect LIME's foundational concepts and known limitations. We categorize and compare its various enhancements, offering a structured taxonomy based on intermediate steps and key issues. Our analysis provides a holistic overview of advancements in LIME, guiding future research and helping practitioners identify suitable approaches. Additionally, we provide a continuously updated interactive website (https://patrick-knab.github.io/which-lime-to-trust/), offering a concise and accessible overview of the survey.},
 author = {Patrick Knab and Sascha Marton and Udo Schlegel and Christian Bartelt},
 comment = {Accepted at the 3rd World Conference on eXplainable Artificial Intelligence (XAI 2025)},
 doi = {},
 eprint = {2503.24365v1},
 journal = {arXiv preprint},
 title = {Which LIME should I trust? Concepts, Challenges, and Solutions},
 url = {http://arxiv.org/abs/2503.24365v1},
 year = {2025}
}

@article{2504.00125v1,
 abstract = {Large Language Models (LLMs) offer a promising approach to enhancing Explainable AI (XAI) by transforming complex machine learning outputs into easy-to-understand narratives, making model predictions more accessible to users, and helping bridge the gap between sophisticated model behavior and human interpretability. AI models, such as state-of-the-art neural networks and deep learning models, are often seen as "black boxes" due to a lack of transparency. As users cannot fully understand how the models reach conclusions, users have difficulty trusting decisions from AI models, which leads to less effective decision-making processes, reduced accountabilities, and unclear potential biases. A challenge arises in developing explainable AI (XAI) models to gain users' trust and provide insights into how models generate their outputs. With the development of Large Language Models, we want to explore the possibilities of using human language-based models, LLMs, for model explainabilities. This survey provides a comprehensive overview of existing approaches regarding LLMs for XAI, and evaluation techniques for LLM-generated explanation, discusses the corresponding challenges and limitations, and examines real-world applications. Finally, we discuss future directions by emphasizing the need for more interpretable, automated, user-centric, and multidisciplinary approaches for XAI via LLMs.},
 author = {Ahsan Bilal and David Ebert and Beiyu Lin},
 comment = {This manuscript is intended for submission to ACM Transactions on Intelligent Systems and Technology},
 doi = {},
 eprint = {2504.00125v1},
 journal = {arXiv preprint},
 title = {LLMs for Explainable AI: A Comprehensive Survey},
 url = {http://arxiv.org/abs/2504.00125v1},
 year = {2025}
}

@article{2504.00395v3,
 abstract = {Deep neural networks trained through end-to-end learning have achieved remarkable success across various domains in the past decade. However, the end-to-end learning strategy, originally designed to minimize predictive loss in a black-box manner, faces two fundamental limitations: the struggle to form explainable representations in a self-supervised manner, and the inability to compress information rigorously following the Minimum Description Length (MDL) principle. These two limitations point to a deeper issue: an end-to-end learning model is not able to "understand" what it learns. In this paper, we establish a novel theory connecting these two limitations. We design the Spectrum VAE, a novel deep learning architecture whose minimum description length (MDL) can be rigorously evaluated. Then, we introduce the concept of latent dimension combinations, or what we term spiking patterns, and demonstrate that the observed spiking patterns should be as few as possible based on the training data in order for the Spectrum VAE to achieve the MDL. Finally, our theory demonstrates that when the MDL is achieved with respect to the given data distribution, the Spectrum VAE will naturally produce explainable latent representations of the data. In other words, explainable representations--or "understanding"--can emerge in a self-supervised manner simply by making the deep network obey the MDL principle. In our opinion, this also implies a deeper insight: To understand is to compress. At its core, our theory advocates for a shift in the training objective of deep networks: not only to minimize predictive loss, but also to minimize the description length regarding the given data. That is, a deep network should not only learn, but also understand what it learns. This work is entirely theoretical and aims to inspire future research toward self-supervised, explainable AI grounded in the MDL principle.},
 author = {Canlin Zhang and Xiuwen Liu},
 comment = {},
 doi = {},
 eprint = {2504.00395v3},
 journal = {arXiv preprint},
 title = {A Theory of Machine Understanding via the Minimum Description Length Principle},
 url = {http://arxiv.org/abs/2504.00395v3},
 year = {2025}
}

@article{2504.00795v1,
 abstract = {Machine learning (ML) is becoming increasingly popular in meteorological decision-making. Although the literature on explainable artificial intelligence (XAI) is growing steadily, user-centered XAI studies have not extend to this domain yet. This study defines three requirements for explanations of black-box models in meteorology through user studies: statistical model performance for different rainfall scenarios to identify model bias, model reasoning, and the confidence of model outputs. Appropriate XAI methods are mapped to each requirement, and the generated explanations are tested quantitatively and qualitatively. An XAI interface system is designed based on user feedback. The results indicate that the explanations increase decision utility and user trust. Users prefer intuitive explanations over those based on XAI algorithms even for potentially easy-to-recognize examples. These findings can provide evidence for future research on user-centered XAI algorithms, as well as a basis to improve the usability of AI systems in practice.},
 author = {Soyeon Kim and Junho Choi and Yeji Choi and Subeen Lee and Artyom Stitsyuk and Minkyoung Park and Seongyeop Jeong and Youhyun Baek and Jaesik Choi},
 comment = {19 pages, 16 figures},
 doi = {10.1007/978-3-031-48057-7_7},
 eprint = {2504.00795v1},
 journal = {arXiv preprint},
 title = {Explainable AI-Based Interface System for Weather Forecasting Model},
 url = {http://arxiv.org/abs/2504.00795v1},
 year = {2025}
}

@article{2504.00930v1,
 abstract = {We propose a novel eXplainable AI algorithm to compute faithful, easy-to-understand, and complete global decision rules from local explanations for tabular data by combining XAI methods with closed frequent itemset mining. Our method can be used with any local explainer that indicates which dimensions are important for a given sample for a given black-box decision. This property allows our algorithm to choose among different local explainers, addressing the disagreement problem, \ie the observation that no single explanation method consistently outperforms others across models and datasets. Unlike usual experimental methodology, our evaluation also accounts for the Rashomon effect in model explainability. To this end, we demonstrate the robustness of our approach in finding suitable rules for nearly all of the 700 black-box models we considered across 14 benchmark datasets. The results also show that our method exhibits improved runtime, high precision and F1-score while generating compact and complete rules.},
 author = {Sebastian Müller and Vanessa Toborek and Tamás Horváth and Christian Bauckhage},
 comment = {},
 doi = {},
 eprint = {2504.00930v1},
 journal = {arXiv preprint},
 title = {CFIRE: A General Method for Combining Local Explanations},
 url = {http://arxiv.org/abs/2504.00930v1},
 year = {2025}
}

@article{2504.02016v2,
 abstract = {The study of neural networks from the perspective of Fourier features has garnered significant attention. While existing analytical research suggests that neural networks tend to learn low-frequency features, a clear attribution method for identifying the specific learned Fourier features has remained elusive. To bridge this gap, we propose a novel Fourier feature attribution method grounded in signal decomposition theory. Additionally, we analyze the differences between game-theoretic attribution metrics for Fourier and spatial domain features, demonstrating that game-theoretic evaluation metrics are better suited for Fourier-based feature attribution.
  Our experiments show that Fourier feature attribution exhibits superior feature selection capabilities compared to spatial domain attribution methods. For instance, in the case of Vision Transformers (ViTs) on the ImageNet dataset, only $8\%$ of the Fourier features are required to maintain the original predictions for $80\%$ of the samples. Furthermore, we compare the specificity of features identified by our method against traditional spatial domain attribution methods. Results reveal that Fourier features exhibit greater intra-class concentration and inter-class distinctiveness, indicating their potential for more efficient classification and explainable AI algorithms.},
 author = {Zechen Liu and Feiyang Zhang and Wei Song and Xiang Li and Wei Wei},
 comment = {13 pages, 2 figures},
 doi = {},
 eprint = {2504.02016v2},
 journal = {arXiv preprint},
 title = {Fast Fourier Correlation is a Highly Efficient and Accurate Feature Attribution Algorithm from the Perspective of Control Theory and Game Theory},
 url = {http://arxiv.org/abs/2504.02016v2},
 year = {2025}
}

@article{2504.02019v2,
 abstract = {Additive feature explanations rely primarily on game-theoretic notions such as the Shapley value by viewing features as cooperating players. The Shapley value's popularity in and outside of explainable AI stems from its axiomatic uniqueness. However, its computational complexity severely limits practicability. Most works investigate the uniform approximation of all features' Shapley values, needlessly consuming samples for insignificant features. In contrast, identifying the $k$ most important features can already be sufficiently insightful and yields the potential to leverage algorithmic opportunities connected to the field of multi-armed bandits. We propose Comparable Marginal Contributions Sampling (CMCS), a method for the top-$k$ identification problem utilizing a new sampling scheme taking advantage of correlated observations. We conduct experiments to showcase the efficacy of our method in compared to competitive baselines. Our empirical findings reveal that estimation quality for the approximate-all problem does not necessarily transfer to top-$k$ identification and vice versa.},
 author = {Patrick Kolpaczki and Tim Nielen and Eyke Hüllermeier},
 comment = {},
 doi = {},
 eprint = {2504.02019v2},
 journal = {arXiv preprint},
 title = {Antithetic Sampling for Top-k Shapley Identification},
 url = {http://arxiv.org/abs/2504.02019v2},
 year = {2025}
}

@article{2504.02151v2,
 abstract = {This paper introduces a novel framework that accelerates the discovery of actionable relationships in high-dimensional temporal data by integrating machine learning (ML), explainable AI (XAI), and natural language processing (NLP) to enhance data quality and streamline workflows. Traditional methods often fail to recognize complex temporal relationships, leading to noisy, redundant, or biased datasets. Our approach combines ML-driven pruning to identify and mitigate low-quality samples, XAI-based interpretability to validate critical feature interactions, and NLP for future contextual validation, reducing the time required to uncover actionable insights by 40-60%. Evaluated on real-world agricultural and synthetic datasets, the framework significantly improves performance metrics (e.g., MSE, R2, MAE) and computational efficiency, with hardware-agnostic scalability across diverse platforms. While long-term real-world impacts (e.g., cost savings, sustainability gains) are pending, this methodology provides an immediate pathway to accelerate data-centric AI in dynamic domains like agriculture and energy, enabling faster iteration cycles for domain experts.},
 author = {Jiztom Kavalakkatt Francis and Matthew J Darr},
 comment = {7 pages},
 doi = {},
 eprint = {2504.02151v2},
 journal = {arXiv preprint},
 title = {Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP},
 url = {http://arxiv.org/abs/2504.02151v2},
 year = {2025}
}

@article{2504.02606v1,
 abstract = {Explainable AI (xAI) interventions aim to improve interpretability for complex black-box models, not only to improve user trust but also as a means to extract scientific insights from high-performing predictive systems. In molecular property prediction, counterfactual explanations offer a way to understand predictive behavior by highlighting which minimal perturbations in the input molecular structure cause the greatest deviation in the predicted property. However, such explanations only allow for meaningful scientific insights if they reflect the distribution of the true underlying property -- a feature we define as counterfactual truthfulness. To increase this truthfulness, we propose the integration of uncertainty estimation techniques to filter counterfactual candidates with high predicted uncertainty. Through computational experiments with synthetic and real-world datasets, we demonstrate that traditional uncertainty estimation methods, such as ensembles and mean-variance estimation, can already substantially reduce the average prediction error and increase counterfactual truthfulness, especially for out-of-distribution settings. Our results highlight the importance and potential impact of incorporating uncertainty estimation into explainability methods, especially considering the relatively high effectiveness of low-effort interventions like model ensembles.},
 author = {Jonas Teufel and Annika Leinweber and Pascal Friederich},
 comment = {24 pages, 5 figures, 4 tabels, accepted at the 3rd xAI World Conference},
 doi = {},
 eprint = {2504.02606v1},
 journal = {arXiv preprint},
 title = {Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification},
 url = {http://arxiv.org/abs/2504.02606v1},
 year = {2025}
}

@article{2504.02685v1,
 abstract = {Out-of-Distribution (OOD) detection is a critical task in machine learning, particularly in safety-sensitive applications where model failures can have serious consequences. However, current OOD detection methods often suffer from restrictive distributional assumptions, limited scalability, and a lack of interpretability. To address these challenges, we propose STOOD-X, a two-stage methodology that combines a Statistical nonparametric Test for OOD Detection with eXplainability enhancements. In the first stage, STOOD-X uses feature-space distances and a Wilcoxon-Mann-Whitney test to identify OOD samples without assuming a specific feature distribution. In the second stage, it generates user-friendly, concept-based visual explanations that reveal the features driving each decision, aligning with the BLUE XAI paradigm. Through extensive experiments on benchmark datasets and multiple architectures, STOOD-X achieves competitive performance against state-of-the-art post hoc OOD detectors, particularly in high-dimensional and complex settings. In addition, its explainability framework enables human oversight, bias detection, and model debugging, fostering trust and collaboration between humans and AI systems. The STOOD-X methodology therefore offers a robust, explainable, and scalable solution for real-world OOD detection tasks.},
 author = {Iván Sevillano-García and Julián Luengo and Francisco Herrera},
 comment = {18 pages, 7 Figures},
 doi = {},
 eprint = {2504.02685v1},
 journal = {arXiv preprint},
 title = {STOOD-X methodology: using statistical nonparametric test for OOD Detection Large-Scale datasets enhanced with explainability},
 url = {http://arxiv.org/abs/2504.02685v1},
 year = {2025}
}

@article{2504.03230v3,
 abstract = {Alzheimer's disease (AD) leads to progressive cognitive decline, making early detection crucial for effective intervention. While deep learning models have shown high accuracy in AD diagnosis, their lack of interpretability limits clinical trust and adoption. This paper introduces a novel pre-model approach leveraging Jacobian Maps (JMs) within a multi-modal framework to enhance explainability and trustworthiness in AD detection. By capturing localized brain volume changes, JMs establish meaningful correlations between model predictions and well-known neuroanatomical biomarkers of AD. We validate JMs through experiments comparing a 3D CNN trained on JMs versus on traditional preprocessed data, which demonstrates superior accuracy. We also employ 3D Grad-CAM analysis to provide both visual and quantitative insights, further showcasing improved interpretability and diagnostic reliability.},
 author = {Yasmine Mustafa and Mohamed Elmahallawy and Tie Luo},
 comment = {PM4B 2025 Best Paper},
 doi = {},
 eprint = {2504.03230v3},
 journal = {arXiv preprint},
 title = {Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection},
 url = {http://arxiv.org/abs/2504.03230v3},
 year = {2025}
}

@article{2504.03736v1,
 abstract = {Understanding uncertainty in Explainable AI (XAI) is crucial for building trust and ensuring reliable decision-making in Machine Learning models. This paper introduces a unified framework for quantifying and interpreting Uncertainty in XAI by defining a general explanation function $e_θ(x, f)$ that captures the propagation of uncertainty from key sources: perturbations in input data and model parameters. By using both analytical and empirical estimates of explanation variance, we provide a systematic means of assessing the impact uncertainty on explanations. We illustrate the approach using a first-order uncertainty propagation as the analytical estimator. In a comprehensive evaluation across heterogeneous datasets, we compare analytical and empirical estimates of uncertainty propagation and evaluate their robustness. Extending previous work on inconsistencies in explanations, our experiments identify XAI methods that do not reliably capture and propagate uncertainty. Our findings underscore the importance of uncertainty-aware explanations in high-stakes applications and offer new insights into the limitations of current XAI methods. The code for the experiments can be found in our repository at https://github.com/TeodorChiaburu/UXAI},
 author = {Teodor Chiaburu and Felix Bießmann and Frank Haußer},
 comment = {23 pages, 10 figures, accepted at WCXAI 2025 Istanbul},
 doi = {},
 eprint = {2504.03736v1},
 journal = {arXiv preprint},
 title = {Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators},
 url = {http://arxiv.org/abs/2504.03736v1},
 year = {2025}
}

@article{2504.03744v2,
 abstract = {This paper introduces Multi-Output LOcal Narrative Explanation (MOLONE), a novel comparative explanation method designed to enhance preference selection in human-in-the-loop Preference Bayesian optimization (PBO). The preference elicitation in PBO is a non-trivial task because it involves navigating implicit trade-offs between vector-valued outcomes, subjective priorities of decision-makers, and decision-makers' uncertainty in preference selection. Existing explainable AI (XAI) methods for BO primarily focus on input feature importance, neglecting the crucial role of outputs (objectives) in human preference elicitation. MOLONE addresses this gap by providing explanations that highlight both input and output importance, enabling decision-makers to understand the trade-offs between competing objectives and make more informed preference selections. MOLONE focuses on local explanations, comparing the importance of input features and outcomes across candidate samples within a local neighborhood of the search space, thus capturing nuanced differences relevant to preference-based decision-making. We evaluate MOLONE within a PBO framework using benchmark multi-objective optimization functions, demonstrating its effectiveness in improving convergence compared to noisy preference selections. Furthermore, a user study confirms that MOLONE significantly accelerates convergence in human-in-the-loop scenarios by facilitating more efficient identification of preferred options.},
 author = {Tanmay Chakraborty and Christian Wirth and Christin Seifert},
 comment = {Accepted at The World Conference on eXplainable Artificial Intelligence 2025 (XAI-2025)},
 doi = {},
 eprint = {2504.03744v2},
 journal = {arXiv preprint},
 title = {Comparative Explanations: Explanation Guided Decision Making for Human-in-the-Loop Preference Selection},
 url = {http://arxiv.org/abs/2504.03744v2},
 year = {2025}
}

@article{2504.03978v1,
 abstract = {Concept-based eXplainable AI (C-XAI) is a rapidly growing research field that enhances AI model interpretability by leveraging intermediate, human-understandable concepts. This approach not only enhances model transparency but also enables human intervention, allowing users to interact with these concepts to refine and improve the model's performance. Concept Bottleneck Models (CBMs) explicitly predict concepts before making final decisions, enabling interventions to correct misclassified concepts. While CBMs remain effective in Out-Of-Distribution (OOD) settings with intervention, they struggle to match the performance of black-box models. Concept Embedding Models (CEMs) address this by learning concept embeddings from both concept predictions and input data, enhancing In-Distribution (ID) accuracy but reducing the effectiveness of interventions, especially in OOD scenarios. In this work, we propose the Variational Concept Embedding Model (V-CEM), which leverages variational inference to improve intervention responsiveness in CEMs. We evaluated our model on various textual and visual datasets in terms of ID performance, intervention responsiveness in both ID and OOD settings, and Concept Representation Cohesiveness (CRC), a metric we propose to assess the quality of the concept embedding representations. The results demonstrate that V-CEM retains CEM-level ID performance while achieving intervention effectiveness similar to CBM in OOD settings, effectively reducing the gap between interpretability (intervention) and generalization (performance).},
 author = {Francesco De Santis and Gabriele Ciravegna and Philippe Bich and Danilo Giordano and Tania Cerquitelli},
 comment = {Paper accepted at: The 3rd World Conference on Explainable Artificial Intelligence},
 doi = {},
 eprint = {2504.03978v1},
 journal = {arXiv preprint},
 title = {V-CEM: Bridging Performance and Intervenability in Concept-based Models},
 url = {http://arxiv.org/abs/2504.03978v1},
 year = {2025}
}

@article{2504.04262v1,
 abstract = {Chronic Kidney Disease (CKD) is a major global health issue which is affecting million people around the world and with increasing rate of mortality. Mitigation of progression of CKD and better patient outcomes requires early detection. Nevertheless, limitations lie in traditional diagnostic methods, especially in resource constrained settings. This study proposes an advanced machine learning approach to enhance CKD detection by evaluating four models: Random Forest (RF), Multi-Layer Perceptron (MLP), Logistic Regression (LR), and a fine-tuned CatBoost algorithm. Specifically, among these, the fine-tuned CatBoost model demonstrated the best overall performance having an accuracy of 98.75%, an AUC of 0.9993 and a Kappa score of 97.35% of the studies. The proposed CatBoost model has used a nature inspired algorithm such as Simulated Annealing to select the most important features, Cuckoo Search to adjust outliers and grid search to fine tune its settings in such a way to achieve improved prediction accuracy. Features significance is explained by SHAP-a well-known XAI technique-for gaining transparency in the decision-making process of proposed model and bring up trust in diagnostic systems. Using SHAP, the significant clinical features were identified as specific gravity, serum creatinine, albumin, hemoglobin, and diabetes mellitus. The potential of advanced machine learning techniques in CKD detection is shown in this research, particularly for low income and middle-income healthcare settings where prompt and correct diagnoses are vital. This study seeks to provide a highly accurate, interpretable, and efficient diagnostic tool to add to efforts for early intervention and improved healthcare outcomes for all CKD patients.},
 author = {Md. Ehsanul Haque and S. M. Jahidul Islam and Jeba Maliha and Md. Shakhauat Hossan Sumon and Rumana Sharmin and Sakib Rokoni},
 comment = {8 page, 8 figures , conference : 14th IEEE International Conference on Communication Systems and Network Technologies (CSNT2025)},
 doi = {},
 eprint = {2504.04262v1},
 journal = {arXiv preprint},
 title = {Improving Chronic Kidney Disease Detection Efficiency: Fine Tuned CatBoost and Nature-Inspired Algorithms with Explainable AI},
 url = {http://arxiv.org/abs/2504.04262v1},
 year = {2025}
}

@article{2504.04541v1,
 abstract = {Machine Learning methods have extensively evolved to support industrial big data methods and their corresponding need in gas turbine maintenance and prognostics. However, most unsupervised methods need extensively labeled data to perform predictions across many dimensions. The cutting edge of small and medium applications do not necessarily maintain operational sensors and data acquisition with rising costs and diminishing profits. We propose a framework to make sensor maintenance priority decisions using a combination of SHAP, UMAP, Fuzzy C-means clustering. An aerospace jet engine dataset is used as a case study.},
 author = {Bharadwaj Dogga and Anoop Sathyan and Kelly Cohen},
 comment = {},
 doi = {},
 eprint = {2504.04541v1},
 journal = {arXiv preprint},
 title = {A model agnostic eXplainable AI based fuzzy framework for sensor constrained Aerospace maintenance applications},
 url = {http://arxiv.org/abs/2504.04541v1},
 year = {2025}
}

@article{2504.04737v3,
 abstract = {In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.},
 author = {Shubham Kumar Nigam and Balaramamahanthi Deepak Patnaik and Shivam Mishra and Noel Shallum and Kripabandhu Ghosh and Arnab Bhattacharya},
 comment = {Paper accepted in the AACL-IJCNLP 2025 conference},
 doi = {},
 eprint = {2504.04737v3},
 journal = {arXiv preprint},
 title = {TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context},
 url = {http://arxiv.org/abs/2504.04737v3},
 year = {2025}
}

@article{2504.04749v2,
 abstract = {Cancer remains one of the leading causes of mortality worldwide, necessitating accurate diagnosis and prognosis. Whole Slide Imaging (WSI) has become an integral part of clinical workflows with advancements in digital pathology. While various studies have utilized WSIs, their extracted features may not fully capture the most relevant pathological information, and their lack of interpretability limits clinical adoption.
  In this paper, we propose PATH-X, a framework that integrates Vision Transformers (ViT) and Autoencoders with SHAP (Shapley Additive Explanations) to enhance model explainability for patient stratification and risk prediction using WSIs from The Cancer Genome Atlas (TCGA). A representative image slice is selected from each WSI, and numerical feature embeddings are extracted using Google's pre-trained ViT. These features are then compressed via an autoencoder and used for unsupervised clustering and classification tasks. Kaplan-Meier survival analysis is applied to evaluate stratification into two and three risk groups. SHAP is used to identify key contributing features, which are mapped onto histopathological slices to provide spatial context.
  PATH-X demonstrates strong performance in breast and glioma cancers, where a sufficient number of WSIs enabled robust stratification. However, performance in lung cancer was limited due to data availability, emphasizing the need for larger datasets to enhance model reliability and clinical applicability.},
 author = {Ahmad Hussein and Mukesh Prasad and Ali Anaissi and Ali Braytee},
 comment = {11 pages},
 doi = {},
 eprint = {2504.04749v2},
 journal = {arXiv preprint},
 title = {Vision Transformers with Autoencoders and Explainable AI for Cancer Patient Risk Stratification Using Whole Slide Imaging},
 url = {http://arxiv.org/abs/2504.04749v2},
 year = {2025}
}

@article{2504.06055v2,
 abstract = {Improving energy efficiency in residential buildings is critical to combating climate change and reducing greenhouse gas emissions. Retrofitting existing buildings, which contribute a significant share of energy use, is therefore a key priority, especially in regions with outdated building stock. Artificial Intelligence (AI) and Machine Learning (ML) can automate retrofit decision-making and find retrofit strategies. However, their use faces challenges of data availability, model transparency, and compliance with national and EU AI regulations including the AI act, ethics guidelines and the ALTAI. This paper presents a trustworthy-by-design ML-based decision support framework that recommends energy efficiency strategies for residential buildings using minimal user-accessible inputs. The framework merges Conditional Tabular Generative Adversarial Networks (CTGAN) to augment limited and imbalanced data with a neural network-based multi-label classifier that predicts potential combinations of retrofit actions. To support explanation and trustworthiness, an Explainable AI (XAI) layer using SHapley Additive exPlanations (SHAP) clarifies the rationale behind recommendations and guides feature engineering. Two case studies validate performance and generalization: the first leveraging a well-established, large EPC dataset for England and Wales; the second using a small, imbalanced post-retrofit dataset from Latvia (RETROFIT-LAT). Results show that the framework can handle diverse data conditions and improve performance up to 53% compared to the baseline. Overall, the proposed framework provides a feasible, interpretable, and trustworthy AI system for building retrofit decision support through assured performance, usability, and transparency to aid stakeholders in prioritizing effective energy investments and support regulation-compliant, data-driven innovation in sustainable energy transition.},
 author = {Panagiota Rempi and Sotiris Pelekis and Alexandros Menelaos Tzortzis and Evangelos Spiliotis and Evangelos Karakolis and Christos Ntanos and Dimitris Askounis},
 comment = {},
 doi = {},
 eprint = {2504.06055v2},
 journal = {arXiv preprint},
 title = {A Trustworthy By Design Classification Model for Building Energy Retrofit Decision Support},
 url = {http://arxiv.org/abs/2504.06055v2},
 year = {2025}
}

@article{2504.06299v1,
 abstract = {Aim: This study aims to enhance interpretability and explainability of multi-modal prediction models integrating imaging and tabular patient data.
  Methods: We adapt the xAI methods Grad-CAM and Occlusion to multi-modal, partly interpretable deep transformation models (dTMs). DTMs combine statistical and deep learning approaches to simultaneously achieve state-of-the-art prediction performance and interpretable parameter estimates, such as odds ratios for tabular features. Based on brain imaging and tabular data from 407 stroke patients, we trained dTMs to predict functional outcome three months after stroke. We evaluated the models using different discriminatory metrics. The adapted xAI methods were used to generated explanation maps for identification of relevant image features and error analysis.
  Results: The dTMs achieve state-of-the-art prediction performance, with area under the curve (AUC) values close to 0.8. The most important tabular predictors of functional outcome are functional independence before stroke and NIHSS on admission, a neurological score indicating stroke severity. Explanation maps calculated from brain imaging dTMs for functional outcome highlighted critical brain regions such as the frontal lobe, which is known to be linked to age which in turn increases the risk for unfavorable outcomes. Similarity plots of the explanation maps revealed distinct patterns which give insight into stroke pathophysiology, support developing novel predictors of stroke outcome and enable to identify false predictions.
  Conclusion: By adapting methods for explanation maps to dTMs, we enhanced the explainability of multi-modal and partly interpretable prediction models. The resulting explanation maps facilitate error analysis and support hypothesis generation regarding the significance of specific image regions in outcome prediction.},
 author = {Jonas Brändli and Maurice Schneeberger and Lisa Herzog and Loran Avci and Nordin Dari and Martin Häansel and Hakim Baazaoui and Pascal Bühler and Susanne Wegener and Beate Sick},
 comment = {},
 doi = {},
 eprint = {2504.06299v1},
 journal = {arXiv preprint},
 title = {Going beyond explainability in multi-modal stroke outcome prediction models},
 url = {http://arxiv.org/abs/2504.06299v1},
 year = {2025}
}

@article{2504.06306v1,
 abstract = {Cancer remains a leading global health challenge and a major cause of mortality. This study leverages machine learning (ML) to predict the survivability of cancer patients with metastatic patterns using the comprehensive MSK-MET dataset, which includes genomic and clinical data from 25,775 patients across 27 cancer types. We evaluated five ML models-XGBoost, Naïve Bayes, Decision Tree, Logistic Regression, and Random Fores using hyperparameter tuning and grid search. XGBoost emerged as the best performer with an area under the curve (AUC) of 0.82. To enhance model interpretability, SHapley Additive exPlanations (SHAP) were applied, revealing key predictors such as metastatic site count, tumor mutation burden, fraction of genome altered, and organ-specific metastases. Further survival analysis using Kaplan-Meier curves, Cox Proportional Hazards models, and XGBoost Survival Analysis identified significant predictors of patient outcomes, offering actionable insights for clinicians. These findings could aid in personalized prognosis and treatment planning, ultimately improving patient care.},
 author = {Polycarp Nalela and Deepthi Rao and Praveen Rao},
 comment = {},
 doi = {},
 eprint = {2504.06306v1},
 journal = {arXiv preprint},
 title = {Predicting Survivability of Cancer Patients with Metastatic Patterns Using Explainable AI},
 url = {http://arxiv.org/abs/2504.06306v1},
 year = {2025}
}

@article{2504.06369v2,
 abstract = {Electric power grids are essential components of modern life, delivering reliable power to end-users while adhering to a multitude of engineering constraints and requirements. In grid operations, the Optimal Power Flow problem plays a key role in determining cost-effective generator dispatch that satisfies load demands and operational limits. However, due to stressed operating conditions, volatile demand profiles, and increased generation from intermittent energy sources, this optimization problem may become infeasible, posing risks such as voltage instability and line overloads. This study proposes a learning framework that combines machine learning with counterfactual explanations to automatically diagnose and restore feasibility in the OPF problem. Our method provides transparent and actionable insights by methodically identifying infeasible conditions and suggesting minimal demand response actions. We evaluate the proposed approach on IEEE 30-bus and 300-bus systems, demonstrating its capability to recover feasibility with high success rates and generating diverse corrective options, appropriate for real-time decision-making. These preliminary findings illustrate the potential of combining classical optimization with explainable AI techniques to enhance grid reliability and resilience.},
 author = {Mostafa Mohammadian and Anna Van Boven and Kyri Baker},
 comment = {},
 doi = {},
 eprint = {2504.06369v2},
 journal = {arXiv preprint},
 title = {Restoring Feasibility in Power Grid Optimization: A Counterfactual ML Approach},
 url = {http://arxiv.org/abs/2504.06369v2},
 year = {2025}
}

@article{2504.06791v1,
 abstract = {Understanding the decisions made and actions taken by increasingly complex AI system remains a key challenge. This has led to an expanding field of research in explainable artificial intelligence (XAI), highlighting the potential of explanations to enhance trust, support adoption, and meet regulatory standards. However, the question of what constitutes a "good" explanation is dependent on the goals, stakeholders, and context. At a high level, psychological insights such as the concept of mental model alignment can offer guidance, but success in practice is challenging due to social and technical factors. As a result of this ill-defined nature of the problem, explanations can be of poor quality (e.g. unfaithful, irrelevant, or incoherent), potentially leading to substantial risks. Instead of fostering trust and safety, poorly designed explanations can actually cause harm, including wrong decisions, privacy violations, manipulation, and even reduced AI adoption. Therefore, we caution stakeholders to beware of explanations of AI: while they can be vital, they are not automatically a remedy for transparency or responsible AI adoption, and their misuse or limitations can exacerbate harm. Attention to these caveats can help guide future research to improve the quality and impact of AI explanations.},
 author = {David Martens and Galit Shmueli and Theodoros Evgeniou and Kevin Bauer and Christian Janiesch and Stefan Feuerriegel and Sebastian Gabel and Sofie Goethals and Travis Greene and Nadja Klein and Mathias Kraus and Niklas Kühl and Claudia Perlich and Wouter Verbeke and Alona Zharova and Patrick Zschech and Foster Provost},
 comment = {This work was inspired by Dagstuhl Seminar 24342},
 doi = {},
 eprint = {2504.06791v1},
 journal = {arXiv preprint},
 title = {Beware of "Explanations" of AI},
 url = {http://arxiv.org/abs/2504.06791v1},
 year = {2025}
}

@article{2504.06979v1,
 abstract = {This study developed an accurate artificial intelligence model for predicting future height in children and adolescents using anthropometric and body composition data from the GP Cohort Study (588,546 measurements from 96,485 children aged 7-18). The model incorporated anthropometric measures, body composition, standard deviation scores, and growth velocity parameters, with performance evaluated using RMSE, MAE, and MAPE. Results showed high accuracy with males achieving average RMSE, MAE, and MAPE of 2.51 cm, 1.74 cm, and 1.14%, and females showing 2.28 cm, 1.68 cm, and 1.13%, respectively. Explainable AI approaches identified height SDS, height velocity, and soft lean mass velocity as crucial predictors. The model generated personalized growth curves by estimating individual-specific height trajectories, offering a robust tool for clinical decision support, early identification of growth disorders, and optimization of growth outcomes.},
 author = {Dohyun Chun and Hae Woon Jung and Jongho Kang and Woo Young Jang and Jihun Kim},
 comment = {23 pages, 7 figures, 2 tables},
 doi = {},
 eprint = {2504.06979v1},
 journal = {arXiv preprint},
 title = {Artificial Intelligence for Pediatric Height Prediction Using Large-Scale Longitudinal Body Composition Data},
 url = {http://arxiv.org/abs/2504.06979v1},
 year = {2025}
}

@article{2504.07011v1,
 abstract = {In this study, we introduce the Fuzzy Additive Model (FAM) and FAM with Explainability (FAME) as a solution for Explainable Artificial Intelligence (XAI). The family consists of three layers: (1) a Projection Layer that compresses the input space, (2) a Fuzzy Layer built upon Single Input-Single Output Fuzzy Logic Systems (SFLS), where SFLS functions as subnetworks within an additive index model, and (3) an Aggregation Layer. This architecture integrates the interpretability of SFLS, which uses human-understandable if-then rules, with the explainability of input-output relationships, leveraging the additive model structure. Furthermore, using SFLS inherently addresses issues such as the curse of dimensionality and rule explosion. To further improve interpretability, we propose a method for sculpting antecedent space within FAM, transforming it into FAME. We show that FAME captures the input-output relationships with fewer active rules, thus improving clarity. To learn the FAM family, we present a deep learning framework. Through the presented comparative results, we demonstrate the promising potential of FAME in reducing model complexity while retaining interpretability, positioning it as a valuable tool for XAI.},
 author = {Omer Bahadir Gokmen and Yusuf Guven and Tufan Kumbasar},
 comment = {in the IEEE International Conference on Fuzzy Systems, 2025},
 doi = {},
 eprint = {2504.07011v1},
 journal = {arXiv preprint},
 title = {FAME: Introducing Fuzzy Additive Models for Explainable AI},
 url = {http://arxiv.org/abs/2504.07011v1},
 year = {2025}
}

@article{2504.07107v1,
 abstract = {User profiling, the practice of collecting user information for personalized recommendations, has become widespread, driving progress in technology. However, this growth poses a threat to user privacy, as devices often collect sensitive data without their owners' awareness. This article aims to consolidate knowledge on user profiling, exploring various approaches and associated challenges. Through the lens of two companies sharing user data and an analysis of 18 popular Android applications in India across various categories, including $\textit{Social, Education, Entertainment, Travel, Shopping and Others}$, the article unveils privacy vulnerabilities. Further, the article propose an enhanced machine learning framework, employing decision trees and neural networks, that improves state-of-the-art classifiers in detecting personal information exposure. Leveraging the XAI (explainable artificial intelligence) algorithm LIME (Local Interpretable Model-agnostic Explanations), it enhances interpretability, crucial for reliably identifying sensitive data. Results demonstrate a noteworthy performance boost, achieving a $75.01\%$ accuracy with a reduced training time of $3.62$ seconds for neural networks. Concluding, the paper suggests research directions to strengthen digital security measures.},
 author = {Rishika Kohli and Shaifu Gupta and Manoj Singh Gaur},
 comment = {46 Pages, 8 tables, 9 figures},
 doi = {},
 eprint = {2504.07107v1},
 journal = {arXiv preprint},
 title = {Guarding Digital Privacy: Exploring User Profiling and Security Enhancements},
 url = {http://arxiv.org/abs/2504.07107v1},
 year = {2025}
}

@article{2504.08150v1,
 abstract = {This study addresses the challenge of predicting post-stroke rigidity by emphasizing feature interactions through graph-based explainable AI. Post-stroke rigidity, characterized by increased muscle tone and stiffness, significantly affects survivors' mobility and quality of life. Despite its prevalence, early prediction remains limited, delaying intervention. We analyze 519K stroke hospitalization records from the Healthcare Cost and Utilization Project dataset, where 43% of patients exhibited rigidity. We compare traditional approaches such as Logistic Regression, XGBoost, and Transformer with graph-based models like Graphormer and Graph Attention Network. These graph models inherently capture feature interactions and incorporate intrinsic or post-hoc explainability. Our results show that graph-based methods outperform others (AUROC 0.75), identifying key predictors such as NIH Stroke Scale and APR-DRG mortality risk scores. They also uncover interactions missed by conventional models. This research provides a novel application of graph-based XAI in stroke prognosis, with potential to guide early identification and personalized rehabilitation strategies.},
 author = {Jiawei Xu and Yonggeon Lee and Anthony Elkommos Youssef and Eunjin Yun and Tinglin Huang and Tianjian Guo and Hamidreza Saber and Rex Ying and Ying Ding},
 comment = {Jiawei Xu and Yonggeon Lee contributed equally to this work},
 doi = {},
 eprint = {2504.08150v1},
 journal = {arXiv preprint},
 title = {Beyond Feature Importance: Feature Interactions in Predicting Post-Stroke Rigidity with Graph Explainable AI},
 url = {http://arxiv.org/abs/2504.08150v1},
 year = {2025}
}

@article{2504.08377v4,
 abstract = {We consider a model for explainable AI in which an explanation for a prediction $h(x)=y$ consists of a subset $S'$ of the training data (if it exists) such that all classifiers $h' \in H$ that make at most $b$ mistakes on $S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has label $y$ under the assumption that (1) the target function $h^\star$ belongs to $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example, if $b=0$ and $H$ is the family of linear classifiers in $\mathbb{R}^d$, and if $x$ lies inside the convex hull of the positive data points in $S$ (and hence every consistent linear classifier labels $x$ as positive), then Carathéodory's theorem states that $x$ lies inside the convex hull of $d+1$ of those points. So, a set $S'$ of size $d+1$ could be released as an explanation for a positive prediction, and would serve as a short proof of correctness of the prediction under the assumption of realizability.
  In this work, we consider this problem more generally, for general hypothesis classes $H$ and general values $b\geq 0$. We define the notion of the robust hollow star number of $H$ (which generalizes the standard hollow star number), and show that it precisely characterizes the worst-case size of the smallest certificate achievable, and analyze its size for natural classes. We also consider worst-case distributional bounds on certificate size, as well as distribution-dependent bounds that we show tightly control the sample size needed to get a certificate for any given test example. In particular, we define a notion of the certificate coefficient $\varepsilon_x$ of an example $x$ with respect to a data distribution $D$ and target function $h^\star$, and prove matching upper and lower bounds on sample size as a function of $\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.},
 author = {Avrim Blum and Steve Hanneke and Chirag Pabbaraju and Donya Saless},
 comment = {Fixed Crefs, added reference to open question on tolerance Carathéodory, other minor changes},
 doi = {},
 eprint = {2504.08377v4},
 journal = {arXiv preprint},
 title = {Proofs as Explanations: Short Certificates for Reliable Predictions},
 url = {http://arxiv.org/abs/2504.08377v4},
 year = {2025}
}

@article{2504.08553v1,
 abstract = {As machine learning models are increasingly considered for high-stakes domains, effective explanation methods are crucial to ensure that their prediction strategies are transparent to the user. Over the years, numerous metrics have been proposed to assess quality of explanations. However, their practical applicability remains unclear, in particular due to a limited understanding of which specific aspects each metric rewards. In this paper we propose a new framework based on spectral analysis of explanation outcomes to systematically capture the multifaceted properties of different explanation techniques. Our analysis uncovers two distinct factors of explanation quality-stability and target sensitivity-that can be directly observed through spectral decomposition. Experiments on both MNIST and ImageNet show that popular evaluation techniques (e.g., pixel-flipping, entropy) partially capture the trade-offs between these factors. Overall, our framework provides a foundational basis for understanding explanation quality, guiding the development of more reliable techniques for evaluating explanations.},
 author = {Johannes Maeß and Grégoire Montavon and Shinichi Nakajima and Klaus-Robert Müller and Thomas Schnake},
 comment = {14 pages, 5 figures, Accepted at XAI World Conference 2025},
 doi = {},
 eprint = {2504.08553v1},
 journal = {arXiv preprint},
 title = {Uncovering the Structure of Explanation Quality with Spectral Analysis},
 url = {http://arxiv.org/abs/2504.08553v1},
 year = {2025}
}

@article{2504.08602v1,
 abstract = {The thriving research field of concept-based explainable artificial intelligence (C-XAI) investigates how human-interpretable semantic concepts embed in the latent spaces of deep neural networks (DNNs). Post-hoc approaches therein use a set of examples to specify a concept, and determine its embeddings in DNN latent space using data driven techniques. This proved useful to uncover biases between different target (foreground or concept) classes. However, given that the background is mostly uncontrolled during training, an important question has been left unattended so far: Are/to what extent are state-of-the-art, data-driven post-hoc C-XAI approaches themselves prone to biases with respect to their backgrounds? E.g., wild animals mostly occur against vegetation backgrounds, and they seldom appear on roads. Even simple and robust C-XAI methods might abuse this shortcut for enhanced performance. A dangerous performance degradation of the concept-corner cases of animals on the road could thus remain undiscovered. This work validates and thoroughly confirms that established Net2Vec-based concept segmentation techniques frequently capture background biases, including alarming ones, such as underperformance on road scenes. For the analysis, we compare 3 established techniques from the domain of background randomization on >50 concepts from 2 datasets, and 7 diverse DNN architectures. Our results indicate that even low-cost setups can provide both valuable insight and improved background robustness.},
 author = {Gesina Schwalbe and Georgii Mikriukov and Edgar Heinert and Stavros Gerolymatos and Mert Keser and Alois Knoll and Matthias Rottmann and Annika Mütze},
 comment = {camera-ready version for 3rd World Conference on eXplainable Artificial Intelligence; 5 figures, 6 tables; code available at: https://github.com/gesina/bg_randomized_loce},
 doi = {},
 eprint = {2504.08602v1},
 journal = {arXiv preprint},
 title = {On Background Bias of Post-Hoc Concept Embeddings in Computer Vision DNNs},
 url = {http://arxiv.org/abs/2504.08602v1},
 year = {2025}
}

@article{2504.08824v1,
 abstract = {Colorectal cancer (CRC) ranks as the second leading cause of cancer-related deaths and the third most prevalent malignant tumour worldwide. Early detection of CRC remains problematic due to its non-specific and often embarrassing symptoms, which patients frequently overlook or hesitate to report to clinicians. Crucially, the stage at which CRC is diagnosed significantly impacts survivability, with a survival rate of 80-95\% for Stage I and a stark decline to 10\% for Stage IV. Unfortunately, in the UK, only 14.4\% of cases are diagnosed at the earliest stage (Stage I).
  In this study, we propose ColonScopeX, a machine learning framework utilizing explainable AI (XAI) methodologies to enhance the early detection of CRC and pre-cancerous lesions. Our approach employs a multimodal model that integrates signals from blood sample measurements, processed using the Savitzky-Golay algorithm for fingerprint smoothing, alongside comprehensive patient metadata, including medication history, comorbidities, age, weight, and BMI. By leveraging XAI techniques, we aim to render the model's decision-making process transparent and interpretable, thereby fostering greater trust and understanding in its predictions. The proposed framework could be utilised as a triage tool or a screening tool of the general population.
  This research highlights the potential of combining diverse patient data sources and explainable machine learning to tackle critical challenges in medical diagnostics.},
 author = {Natalia Sikora and Robert L. Manschke and Alethea M. Tang and Peter Dunstan and Dean A. Harris and Su Yang},
 comment = {Published to AAAI-25 Bridge Program},
 doi = {},
 eprint = {2504.08824v1},
 journal = {arXiv preprint},
 title = {ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer},
 url = {http://arxiv.org/abs/2504.08824v1},
 year = {2025}
}

@article{2504.08877v1,
 abstract = {Mild Cognitive Impairment (MCI) affects 12-18% of individuals over 60. MCI patients exhibit cognitive dysfunctions without significant daily functional loss. While MCI may progress to dementia, predicting this transition remains a clinical challenge due to limited and unreliable indicators. Behavioral changes, like in the execution of Activities of Daily Living (ADLs), can signal such progression. Sensorized smart homes and wearable devices offer an innovative solution for continuous, non-intrusive monitoring ADLs for MCI patients. However, current machine learning models for detecting behavioral changes lack transparency, hindering clinicians' trust. This paper introduces the SERENADE project, a European Union-funded initiative that aims to detect and explain behavioral changes associated with cognitive decline using explainable AI methods. SERENADE aims at collecting one year of data from 30 MCI patients living alone, leveraging AI to support clinical decision-making and offering a new approach to early dementia detection.},
 author = {Gabriele Civitarese and Michele Fiori and Andrea Arighi and Daniela Galimberti and Graziana Florio and Claudio Bettini},
 comment = {},
 doi = {},
 eprint = {2504.08877v1},
 journal = {arXiv preprint},
 title = {The SERENADE project: Sensor-Based Explainable Detection of Cognitive Decline},
 url = {http://arxiv.org/abs/2504.08877v1},
 year = {2025}
}

@article{2504.10169v1,
 abstract = {We review generalized additive models as a type of ``transparent'' model that has recently seen renewed interest in the deep learning community as neural additive models. We highlight multiple types of nonidentifiability in this model class and discuss challenges in interpretability, arguing for restraint when claiming ``interpretability'' or ``suitability for safety-critical applications'' of such models.},
 author = {Xinyu Zhang and Julien Martinelli and ST John},
 comment = {},
 doi = {},
 eprint = {2504.10169v1},
 journal = {arXiv preprint},
 title = {Challenges in interpretability of additive models},
 url = {http://arxiv.org/abs/2504.10169v1},
 year = {2025}
}

@article{2504.10343v1,
 abstract = {Tissue-of-origin signals dominate pan-cancer gene expression, often obscuring molecular features linked to patient survival. This hampers the discovery of generalizable biomarkers, as models tend to overfit tissue-specific patterns rather than capture survival-relevant signals. To address this, we propose a Domain-Adversarial Neural Network (DANN) trained on TCGA RNA-seq data to learn representations less biased by tissue and more focused on survival. Identifying tissue-independent genetic profiles is key to revealing core cancer programs. We assess the DANN using: (1) Standard SHAP, based on the original input space and DANN's mortality classifier; (2) A layer-aware strategy applied to hidden activations, including an unsupervised manifold from raw activations and a supervised manifold from mortality-specific SHAP values. Standard SHAP remains confounded by tissue signals due to biases inherent in its computation. The raw activation manifold was dominated by high-magnitude activations, which masked subtle tissue and mortality-related signals. In contrast, the layer-aware SHAP manifold offers improved low-dimensional representations of both tissue and mortality signals, independent of activation strength, enabling subpopulation stratification and pan-cancer identification of survival-associated genes.},
 author = {Cristian Padron-Manrique and Juan José Oropeza Valdez and Osbaldo Resendis-Antonio},
 comment = {},
 doi = {},
 eprint = {2504.10343v1},
 journal = {arXiv preprint},
 title = {Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin Signal in Pan-cancer Mortality Classification},
 url = {http://arxiv.org/abs/2504.10343v1},
 year = {2025}
}

@article{2504.10456v2,
 abstract = {Social interactions among classroom peers, represented as social learning networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN analysis has recently garnered attention, most existing approaches rely on centralized training, where data is aggregated and processed on a local/cloud server with direct access to raw data. However, in real-world educational settings, such direct access across multiple classrooms is often restricted due to privacy concerns. Furthermore, training models on isolated classroom data prevents the identification of common interaction patterns that exist across multiple classrooms, thereby limiting model performance. To address these challenges, we propose one of the first frameworks that integrates Federated Learning (FL), a distributed and collaborative machine learning (ML) paradigm, with SLNs derived from students' interactions in multiple classrooms' online forums to predict future link formations (i.e., interactions) among students. By leveraging FL, our approach enables collaborative model training across multiple classrooms while preserving data privacy, as it eliminates the need for raw data centralization. Recognizing that each classroom may exhibit unique student interaction dynamics, we further employ model personalization techniques to adapt the FL model to individual classroom characteristics. Our results demonstrate the effectiveness of our approach in capturing both shared and classroom-specific representations of student interactions in SLNs. Additionally, we utilize explainable AI (XAI) techniques to interpret model predictions, identifying key factors that influence link formation across different classrooms. These insights unveil the drivers of social learning interactions within a privacy-preserving, collaborative, and distributed ML framework -- an aspect that has not been explored before.},
 author = {Anurata Prabha Hridi and Muntasir Hoq and Zhikai Gao and Collin Lynch and Rajeev Sahay and Seyyedali Hosseinalipour and Bita Akram},
 comment = {Published in Educational Data Mining Conference (EDM) 2025},
 doi = {10.5281/zenodo.15870179},
 eprint = {2504.10456v2},
 journal = {arXiv preprint},
 title = {Privacy-Preserving Distributed Link Predictions Among Peers in Online Classrooms Using Federated Learning},
 url = {http://arxiv.org/abs/2504.10456v2},
 year = {2025}
}

@article{2504.13717v1,
 abstract = {This work aligns deep learning (DL) with human reasoning capabilities and needs to enable more efficient, interpretable, and robust image classification. We approach this from three perspectives: explainability, causality, and biological vision. Introduction and background open this work before diving into operative chapters. First, we assess neural networks' visualization techniques for medical images and validate an explainable-by-design method for breast mass classification. A comprehensive review at the intersection of XAI and causality follows, where we introduce a general scaffold to organize past and future research, laying the groundwork for our second perspective. In the causality direction, we propose novel modules that exploit feature co-occurrence in medical images, leading to more effective and explainable predictions. We further introduce CROCODILE, a general framework that integrates causal concepts, contrastive learning, feature disentanglement, and prior knowledge to enhance generalization. Lastly, we explore biological vision, examining how humans recognize objects, and propose CoCoReco, a connectivity-inspired network with context-aware attention mechanisms. Overall, our key findings include: (i) simple activation maximization lacks insight for medical imaging DL models; (ii) prototypical-part learning is effective and radiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak causal signals can be leveraged without a priori information to improve performance and interpretability; (v) our framework generalizes across medical domains and out-of-distribution data; (vi) incorporating biological circuit motifs improves human-aligned recognition. This work contributes toward human-aligned DL and highlights pathways to bridge the gap between research and clinical adoption, with implications for improved trust, diagnostic accuracy, and safe deployment.},
 author = {Gianluca Carloni},
 comment = {Personal adaptation and expansion of doctoral thesis (originally submitted in Oct 2024, revisioned in Jan 2025)},
 doi = {},
 eprint = {2504.13717v1},
 journal = {arXiv preprint},
 title = {Human-aligned Deep Learning: Explainability, Causality, and Biological Inspiration},
 url = {http://arxiv.org/abs/2504.13717v1},
 year = {2025}
}

@article{2504.13765v1,
 abstract = {This study investigates the extent to which Mel-Frequency Cepstral Coefficients (MFCCs) capture first language (L1) transfer in extended second language (L2) English speech. Speech samples from Mandarin and American English L1 speakers were extracted from the GMU Speech Accent Archive, converted to WAV format, and processed to obtain thirteen MFCCs per speaker. A multi-method analytic framework combining inferential statistics (t-tests, MANOVA, Canonical Discriminant Analysis) and machine learning (Random Forest classification) identified MFCC-1 (broadband energy), MFCC-2 (first formant region), and MFCC-5 (voicing and fricative energy) as the most discriminative features for distinguishing L1 backgrounds. A reduced-feature model using these MFCCs significantly outperformed the full-feature model, as confirmed by McNemar's test and non-overlapping confidence intervals. The findings empirically support the Perceptual Assimilation Model for L2 (PAM-L2) and the Speech Learning Model (SLM), demonstrating that L1-conditioned variation in L2 speech is both perceptually grounded and acoustically quantifiable. Methodologically, the study contributes to applied linguistics and explainable AI by proposing a transparent, data-efficient pipeline for L2 pronunciation modeling. The results also offer pedagogical implications for ESL/EFL instruction by highlighting L1-specific features that can inform intelligibility-oriented instruction, curriculum design, and speech assessment tools.},
 author = {Peyman Jahanbin},
 comment = {27 pages (including references), 4 figures, 1 table. Combines statistical inference and explainable machine learning to model L1 influence in L2 pronunciation using MFCC features. Methodology and code are openly available via Zenodo and OSF: Zenodo: https://doi.org/10.5281/zenodo.15186197 OSF: https://doi.org/10.17605/OSF.IO/4UXGM},
 doi = {},
 eprint = {2504.13765v1},
 journal = {arXiv preprint},
 title = {Modeling L1 Influence on L2 Pronunciation: An MFCC-Based Framework for Explainable Machine Learning and Pedagogical Feedback},
 url = {http://arxiv.org/abs/2504.13765v1},
 year = {2025}
}

@article{2504.15664v1,
 abstract = {Machine learning models tend to learn spurious features - features that strongly correlate with target labels but are not causal. Existing approaches to mitigate models' dependence on spurious features work in some cases, but fail in others. In this paper, we systematically analyze how and where neural networks encode spurious correlations. We introduce the neuron spurious score, an XAI-based diagnostic measure to quantify a neuron's dependence on spurious features. We analyze both convolutional neural networks (CNNs) and vision transformers (ViTs) using architecture-specific methods. Our results show that spurious features are partially disentangled, but the degree of disentanglement varies across model architectures. Furthermore, we find that the assumptions behind existing mitigation methods are incomplete. Our results lay the groundwork for the development of novel methods to mitigate spurious correlations and make AI models safer to use in practice.},
 author = {Phuong Quynh Le and Jörg Schlötterer and Christin Seifert},
 comment = {Accepted at The World Conference on eXplainable Artificial Intelligence 2025 (XAI-2025)},
 doi = {},
 eprint = {2504.15664v1},
 journal = {arXiv preprint},
 title = {An XAI-based Analysis of Shortcut Learning in Neural Networks},
 url = {http://arxiv.org/abs/2504.15664v1},
 year = {2025}
}

@article{2504.15670v1,
 abstract = {Machine learning methods find growing application in the reconstruction and analysis of data in high energy physics experiments. A modified convolutional autoencoder model was employed to identify and reconstruct the pulses from scintillating crystals. The model was further investigated using four xAI methods for deeper understanding of the underlying reconstruction mechanism. The results are discussed in detail, underlining the importance of xAI for knowledge gain and further improvement of the algorithms.},
 author = {Kalina Dimitrova and Venelin Kozhuharov and Peicho Petkov},
 comment = {},
 doi = {10.1088/1742-6596/3002/1/012005},
 eprint = {2504.15670v1},
 journal = {arXiv preprint},
 title = {Applicability Evaluation of Selected xAI Methods for Machine Learning Algorithms for Signal Parameters Extraction},
 url = {http://arxiv.org/abs/2504.15670v1},
 year = {2025}
}

@article{2504.16148v1,
 abstract = {Despite significant advancements in AI-driven educational systems and ongoing calls for responsible AI for education, several critical issues remain unresolved -- acting as the elephant in the room within AI in education, learning analytics, educational data mining, learning sciences, and educational psychology communities. This critical analysis identifies and examines nine persistent challenges that continue to undermine the fairness, transparency, and effectiveness of current AI methods and applications in education. These include: (1) the lack of clarity around what AI for education truly means -- often ignoring the distinct purposes, strengths, and limitations of different AI families -- and the trend of equating it with domain-agnostic, company-driven large language models; (2) the widespread neglect of essential learning processes such as motivation, emotion, and (meta)cognition in AI-driven learner modelling and their contextual nature; (3) limited integration of domain knowledge and lack of stakeholder involvement in AI design and development; (4) continued use of non-sequential machine learning models on temporal educational data; (5) misuse of non-sequential metrics to evaluate sequential models; (6) use of unreliable explainable AI methods to provide explanations for black-box models; (7) ignoring ethical guidelines in addressing data inconsistencies during model training; (8) use of mainstream AI methods for pattern discovery and learning analytics without systematic benchmarking; and (9) overemphasis on global prescriptions while overlooking localised, student-specific recommendations. Supported by theoretical and empirical research, we demonstrate how hybrid AI methods -- specifically neural-symbolic AI -- can address the elephant in the room and serve as the foundation for responsible, trustworthy AI systems in education.},
 author = {Danial Hooshyar and Gustav Šír and Yeongwook Yang and Eve Kikas and Raija Hämäläinen and Tommi Kärkkäinen and Dragan Gašević and Roger Azevedo},
 comment = {},
 doi = {10.1016/j.caeai.2025.100524},
 eprint = {2504.16148v1},
 journal = {arXiv preprint},
 title = {Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room},
 url = {http://arxiv.org/abs/2504.16148v1},
 year = {2025}
}

@article{2504.16192v1,
 abstract = {The continuous improvement in weather forecast skill over the past several decades is largely due to the increasing quantity of available satellite observations and their assimilation into operational forecast systems. Assimilating these observations requires observation operators in the form of radiative transfer models. Significant efforts have been dedicated to enhancing the computational efficiency of these models. Computational cost remains a bottleneck, and a large fraction of available data goes unused for assimilation. To address this, we used machine learning to build an efficient neural network based probabilistic emulator of the Community Radiative Transfer Model (CRTM), applied to the GOES Advanced Baseline Imager. The trained NN emulator predicts brightness temperatures output by CRTM and the corresponding error with respect to CRTM. RMSE of the predicted brightness temperature is 0.3 K averaged across all channels. For clear sky conditions, the RMSE is less than 0.1 K for 9 out of 10 infrared channels. The error predictions are generally reliable across a wide range of conditions. Explainable AI methods demonstrate that the trained emulator reproduces the relevant physics, increasing confidence that the model will perform well when presented with new data.},
 author = {Lucas Howard and Aneesh C. Subramanian and Gregory Thompson and Benjamin Johnson and Thomas Auligne},
 comment = {26 pages, 9 figures, 1 table},
 doi = {},
 eprint = {2504.16192v1},
 journal = {arXiv preprint},
 title = {Probabilistic Emulation of the Community Radiative Transfer Model Using Machine Learning},
 url = {http://arxiv.org/abs/2504.16192v1},
 year = {2025}
}

@article{2504.17179v1,
 abstract = {Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately detect objects and interpret their surroundings. However, even when trained using millions of miles of real-world data, AVs are often unable to detect rare failure modes (RFMs). The problem of RFMs is commonly referred to as the "long-tail challenge", due to the distribution of data including many instances that are very rarely seen. In this paper, we present a novel approach that utilizes advanced generative and explainable AI techniques to aid in understanding RFMs. Our methods can be used to enhance the robustness and reliability of AVs when combined with both downstream model training and testing. We extract segmentation masks for objects of interest (e.g., cars) and invert them to create environmental masks. These masks, combined with carefully crafted text prompts, are fed into a custom diffusion model. We leverage the Stable Diffusion inpainting model guided by adversarial noise optimization to generate images containing diverse environments designed to evade object detection models and expose vulnerabilities in AI systems. Finally, we produce natural language descriptions of the generated RFMs that can guide developers and policymakers to improve the safety and reliability of AV systems.},
 author = {Mohammad Zarei and Melanie A Jutras and Eliana Evans and Mike Tan and Omid Aaramoon},
 comment = {8 pages, 10 figures. Accepted to IEEE Conference on Artificial Intelligence (CAI), 2025},
 doi = {},
 eprint = {2504.17179v1},
 journal = {arXiv preprint},
 title = {AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models},
 url = {http://arxiv.org/abs/2504.17179v1},
 year = {2025}
}

@article{2504.17272v1,
 abstract = {Machine learning methods are being introduced at all stages of data reconstruction and analysis in various high-energy physics experiments. We present the development and application of convolutional neural networks with modified autoencoder architecture for the reconstruction of the pulse arrival time and amplitude in individual scintillating crystals in electromagnetic calorimeters and other detectors. The network performance is discussed as well as the application of xAI methods for further investigation of the algorithm and improvement of the output accuracy.},
 author = {Kalina Dimitrova and Venelin Kozhuharov and Peicho Petkov},
 comment = {This article was published in Particles, 2025, 8, 48, DOI: 10.3390/particles8020048},
 doi = {10.3390/particles8020048},
 eprint = {2504.17272v1},
 journal = {arXiv preprint},
 title = {Development and Explainability of Models for Machine-Learning-Based Reconstruction of Signals in Particle Detectors},
 url = {http://arxiv.org/abs/2504.17272v1},
 year = {2025}
}

@article{2504.17719v1,
 abstract = {Reliable uncertainty estimates are crucial in modern machine learning. Deep Gaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs hierarchically, offering promising methods for uncertainty quantification grounded in Bayesian principles. However, their empirical calibration and robustness under distribution shift relative to baselines like Deep Ensembles remain understudied. This work evaluates these models on regression (CASP dataset) and classification (ESR dataset) tasks, assessing predictive performance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL) and Expected Calibration Error (ECE), alongside robustness under various synthetic feature-level distribution shifts. Results indicate DSPPs provide strong in-distribution calibration leveraging their sigma point approximations. However, compared to Deep Ensembles, which demonstrated superior robustness in both per- formance and calibration under the tested shifts, the GP-based methods showed vulnerabilities, exhibiting particular sensitivity in the observed metrics. Our findings underscore ensembles as a robust baseline, suggesting that while deep GP methods offer good in-distribution calibration, their practical robustness under distribution shift requires careful evaluation. To facilitate reproducibility, we make our code available at https://github.com/matthjs/xai-gp.},
 author = {Matthijs van der Lende and Jeremias Lino Ferrao and Niclas Müller-Hof},
 comment = {},
 doi = {},
 eprint = {2504.17719v1},
 journal = {arXiv preprint},
 title = {Evaluating Uncertainty in Deep Gaussian Processes},
 url = {http://arxiv.org/abs/2504.17719v1},
 year = {2025}
}

@article{2504.18029v1,
 abstract = {The Open Radio Access Network (Open RAN) is an emerging idea -- transforming the traditional Radio Access Networks (RAN) that are monolithic and inflexible into more flexible and innovative. By leveraging open standard interfaces, data collection across all RAN layers becomes feasible, paving the way for the development of energy-efficient Open RAN architectures through Artificial Intelligence / Machine Learning (AI/ML). However, the inherent complexity and black-box nature of AI/ML models used for energy consumption prediction pose challenges in interpreting their underlying factors and relationships. This work presents an integration of eXplainable AI (XAI) to understand the key RAN parameters that contribute to energy consumption. Furthermore, the paper delves into the analysis of RAN parameters -- \emph{airtime}, \emph{goodput}, \emph{throughput}, \emph{buffer status report}, \emph{number of resource blocks}, and many others -- identified by XAI techniques, highlighting their significance in energy consumption.},
 author = {L. Malakalapalli and V. Gudepu and B. Chirumamilla and S. J. Yadhunandan and K. Kondepu},
 comment = {Accepted at FNFW 2024},
 doi = {},
 eprint = {2504.18029v1},
 journal = {arXiv preprint},
 title = {Integrating Explainable AI for Energy Efficient Open Radio Access Networks},
 url = {http://arxiv.org/abs/2504.18029v1},
 year = {2025}
}

@article{2504.18371v1,
 abstract = {The integration of unmanned aerial vehicles (UAVs) into cellular networks presents significant mobility management challenges, primarily due to frequent handovers caused by probabilistic line-of-sight conditions with multiple ground base stations (BSs). To tackle these challenges, reinforcement learning (RL)-based methods, particularly deep Q-networks (DQN), have been employed to optimize handover decisions dynamically. However, a major drawback of these learning-based approaches is their black-box nature, which limits interpretability in the decision-making process. This paper introduces an explainable AI (XAI) framework that incorporates Shapley Additive Explanations (SHAP) to provide deeper insights into how various state parameters influence handover decisions in a DQN-based mobility management system. By quantifying the impact of key features such as reference signal received power (RSRP), reference signal received quality (RSRQ), buffer status, and UAV position, our approach enhances the interpretability and reliability of RL-based handover solutions. To validate and compare our framework, we utilize real-world network performance data collected from UAV flight trials. Simulation results show that our method provides intuitive explanations for policy decisions, effectively bridging the gap between AI-driven models and human decision-makers.},
 author = {Irshad A. Meer and Bruno Hörmann and Mustafa Ozger and Fabien Geyer and Alberto Viseras and Dominic Schupke and Cicek Cavdar},
 comment = {Submitted to IEEE PIMRC 2025},
 doi = {},
 eprint = {2504.18371v1},
 journal = {arXiv preprint},
 title = {Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization},
 url = {http://arxiv.org/abs/2504.18371v1},
 year = {2025}
}

@article{2504.19027v2,
 abstract = {Explainable artificial intelligence (XAI) has become increasingly important in decision-critical domains such as healthcare, finance, and law. Counterfactual (CF) explanations, a key approach in XAI, provide users with actionable insights by suggesting minimal modifications to input features that lead to different model outcomes. Despite significant advancements, existing CF generation methods often struggle to balance proximity, diversity, and robustness, limiting their real-world applicability. A widely adopted framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but lacks robustness, making CF explanations sensitive to perturbations and domain constraints. To address these challenges, we introduce DiCE-Extended, an enhanced CF explanation framework that integrates multi-objective optimization techniques to improve robustness while maintaining interpretability. Our approach introduces a novel robustness metric based on the Dice-Sørensen coefficient, enabling stability under small input variations. Additionally, we refine CF generation using weighted loss components (lambda_p, lambda_d, lambda_r) to balance proximity, diversity, and robustness. We empirically validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch, TensorFlow). Results demonstrate improved CF validity, stability, and alignment with decision boundaries compared to standard DiCE-generated explanations. Our findings highlight the potential of DiCE-Extended in generating more reliable and interpretable CFs for high-stakes applications. Future work could explore adaptive optimization techniques and domain-specific constraints to further enhance CF generation in real-world scenarios},
 author = {Volkan Bakir and Polat Goktas and Sureyya Akyuz},
 comment = {5th international Conference on Modelling, Computation and Optimization in Information Systems and Management Sciences (MCO 2025), June 4-6, 2025, Metz, France},
 doi = {},
 eprint = {2504.19027v2},
 journal = {arXiv preprint},
 title = {DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning},
 url = {http://arxiv.org/abs/2504.19027v2},
 year = {2025}
}

@article{2504.19682v1,
 abstract = {Graph Neural Networks (GNNs) have emerged as an efficient alternative to convolutional approaches for vision tasks such as image classification, leveraging patch-based representations instead of raw pixels. These methods construct graphs where image patches serve as nodes, and edges are established based on patch similarity or classification relevance. Despite their efficiency, the explainability of GNN-based vision models remains underexplored, even though graphs are naturally interpretable. In this work, we analyze the semantic consistency of the graphs formed at different layers of GNN-based image classifiers, focusing on how well they preserve object structures and meaningful relationships. A comprehensive analysis is presented by quantifying the extent to which inter-layer graph connections reflect semantic similarity and spatial coherence. Explanations from standard and adversarial settings are also compared to assess whether they reflect the classifiers' robustness. Additionally, we visualize the flow of information across layers through heatmap-based visualization techniques, thereby highlighting the models' explainability. Our findings demonstrate that the decision-making processes of these models can be effectively explained, while also revealing that their reasoning does not necessarily align with human perception, especially in deeper layers.},
 author = {Nikolaos Chaidos and Angeliki Dimitriou and Nikolaos Spanos and Athanasios Voulodimos and Giorgos Stamou},
 comment = {13 pages, 3 figures, accepted for presentation at xAI-World-Conference 2025, code is available at https://github.com/nickhaidos/Vision-GNNs-Explainer},
 doi = {},
 eprint = {2504.19682v1},
 journal = {arXiv preprint},
 title = {Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification},
 url = {http://arxiv.org/abs/2504.19682v1},
 year = {2025}
}

@article{2504.20064v1,
 abstract = {The opaqueness of modern digital advertising, exemplified by platforms such as Meta Ads, raises concerns regarding their autonomous control over audience targeting, pricing structures, and ad relevancy assessments. Locked in their leading positions by network effects, ``Metas and Googles of the world'' attract countless advertisers who rely on intuition, with billions of dollars lost on ineffective social media ads. The platforms' algorithms use huge amounts of data unavailable to advertisers, and the algorithms themselves are opaque as well. This lack of transparency hinders the advertisers' ability to make informed decisions and necessitates efforts to promote transparency, standardize industry metrics, and strengthen regulatory frameworks. In this work, we propose novel ways to assist marketers in optimizing their advertising strategies via machine learning techniques designed to analyze and evaluate content, in particular, predict the click-through rates (CTR) of novel advertising content. Another important problem is that large volumes of data available in the competitive landscape, e.g., competitors' ads, impede the ability of marketers to derive meaningful insights. This leads to a pressing need for a novel approach that would allow us to summarize and comprehend complex data. Inspired by the success of ChatGPT in bridging the gap between large language models (LLMs) and a broader non-technical audience, we propose a novel system that facilitates marketers in data interpretation, called SODA, that merges LLMs with explainable AI, enabling better human-AI collaboration with an emphasis on the domain of digital marketing and advertising. By combining LLMs and explainability features, in particular modern text-image models, we aim to improve the synergy between human marketers and AI systems.},
 author = {Qi Yang and Marlo Ongpin and Sergey Nikolenko and Alfred Huang and Aleksandr Farseev},
 comment = {},
 doi = {},
 eprint = {2504.20064v1},
 journal = {arXiv preprint},
 title = {Against Opacity: Explainable AI and Large Language Models for Effective Digital Advertising},
 url = {http://arxiv.org/abs/2504.20064v1},
 year = {2025}
}

@article{2504.20687v1,
 abstract = {Evaluating synthetic tabular data is challenging, since they can differ from the real data in so many ways. There exist numerous metrics of synthetic data quality, ranging from statistical distances to predictive performance, often providing conflicting results. Moreover, they fail to explain or pinpoint the specific weaknesses in the synthetic data. To address this, we apply explainable AI (XAI) techniques to a binary detection classifier trained to distinguish real from synthetic data. While the classifier identifies distributional differences, XAI concepts such as feature importance and feature effects, analyzed through methods like permutation feature importance, partial dependence plots, Shapley values and counterfactual explanations, reveal why synthetic data are distinguishable, highlighting inconsistencies, unrealistic dependencies, or missing patterns. This interpretability increases transparency in synthetic data evaluation and provides deeper insights beyond conventional metrics, helping diagnose and improve synthetic data quality. We apply our approach to two tabular datasets and generative models, showing that it uncovers issues overlooked by standard evaluation techniques.},
 author = {Jan Kapar and Niklas Koenen and Martin Jullum},
 comment = {This is the accepted, post peer-reviewed version of the manuscript, accepted for publication in the proceedings after the Third World Conference on eXplainable Artificial Intelligence, XAI-2025. A link to the version of record will be included here upon publication},
 doi = {},
 eprint = {2504.20687v1},
 journal = {arXiv preprint},
 title = {What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models},
 url = {http://arxiv.org/abs/2504.20687v1},
 year = {2025}
}

@article{2504.20741v1,
 abstract = {Since the early days of the Explainable AI movement, post-hoc explanations have been praised for their potential to improve user understanding, promote trust, and reduce patient safety risks in black box medical AI systems. Recently, however, critics have argued that the benefits of post-hoc explanations are greatly exaggerated since they merely approximate, rather than replicate, the actual reasoning processes that black box systems take to arrive at their outputs. In this article, we aim to defend the value of post-hoc explanations against this recent critique. We argue that even if post-hoc explanations do not replicate the exact reasoning processes of black box systems, they can still improve users' functional understanding of black box systems, increase the accuracy of clinician-AI teams, and assist clinicians in justifying their AI-informed decisions. While post-hoc explanations are not a "silver bullet" solution to the black box problem in medical AI, we conclude that they remain a useful strategy for addressing the black box problem in medical AI.},
 author = {Joshua Hatherley and Lauritz Munch and Jens Christian Bjerring},
 comment = {},
 doi = {},
 eprint = {2504.20741v1},
 journal = {arXiv preprint},
 title = {In defence of post-hoc explanations in medical AI},
 url = {http://arxiv.org/abs/2504.20741v1},
 year = {2025}
}

@article{2504.20906v3,
 abstract = {The continuous monitoring of the interactions between cyber-physical components of any industrial control system (ICS) is required to secure automation of the system controls, and to guarantee plant processes are fail-safe and remain in an acceptably safe state. Safety is achieved by managing actuation (where electric signals are used to trigger physical movement), dependent on corresponding sensor readings; used as ground truth in decision making. Timely detection of anomalies (attacks, faults and unascertained states) in ICSs is crucial for the safe running of a plant, the safety of its personnel, and for the safe provision of any services provided. We propose an anomaly detection method that involves accurate linearization of the non-linear forms arising from sensor-actuator(s) relationships, primarily because solving linear models is easier and well understood. We accomplish this by using a well-known water treatment testbed as a use case. Our experiments show millisecond time response to detect anomalies, all of which are explainable and traceable; this simultaneous coupling of detection speed and explainability has not been achieved by other state of the art Artificial Intelligence (AI)/ Machine Learning (ML) models with eXplainable AI (XAI) used for the same purpose. Our methods explainability enables us to pin-point the sensor(s) and the actuation state(s) for which the anomaly was detected. The proposed algorithm showed an accuracy of 97.72% by flagging deviations within safe operation limits as non-anomalous; indicative that slower detectors with highest detection resolution is unnecessary, for systems whose safety boundaries provide leeway within safety limits.},
 author = {Sarad Venugopalan and Sridhar Adepu},
 comment = {},
 doi = {},
 eprint = {2504.20906v3},
 journal = {arXiv preprint},
 title = {GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems},
 url = {http://arxiv.org/abs/2504.20906v3},
 year = {2025}
}

@article{2504.21166v1,
 abstract = {The growing interest in automated movement analysis has presented new challenges in recognition of complex human activities including dance. This study focuses on dance style recognition using features extracted using Laban Movement Analysis. Previous studies for dance style recognition often focus on cross-frame movement analysis, which limits the ability to capture temporal context and dynamic transitions between movements. This gap highlights the need for a method that can add temporal context to LMA features. For this, we introduce a novel pipeline which combines 3D pose estimation, 3D human mesh reconstruction, and floor aware body modeling to effectively extract LMA features. To address the temporal limitation, we propose a sliding window approach that captures movement evolution across time in features. These features are then used to train various machine learning methods for classification, and their explainability explainable AI methods to evaluate the contribution of each feature to classification performance. Our proposed method achieves a highest classification accuracy of 99.18\% which shows that the addition of temporal context significantly improves dance style recognition performance.},
 author = {Muhammad Turab and Philippe Colantoni and Damien Muselet and Alain Tremeau},
 comment = {},
 doi = {},
 eprint = {2504.21166v1},
 journal = {arXiv preprint},
 title = {Dance Style Recognition Using Laban Movement Analysis},
 url = {http://arxiv.org/abs/2504.21166v1},
 year = {2025}
}

@article{2504.21457v1,
 abstract = {This work presents xEEGNet, a novel, compact, and explainable neural network for EEG data analysis. It is fully interpretable and reduces overfitting through major parameter reduction. As an applicative use case, we focused on classifying common dementia conditions, Alzheimer's and frontotemporal dementia, versus controls. xEEGNet is broadly applicable to other neurological conditions involving spectral alterations. We initially used ShallowNet, a simple and popular model from the EEGNet-family. Its structure was analyzed and gradually modified to move from a "black box" to a more transparent model, without compromising performance. The learned kernels and weights were examined from a clinical standpoint to assess medical relevance. Model variants, including ShallowNet and the final xEEGNet, were evaluated using robust Nested-Leave-N-Subjects-Out cross-validation for unbiased performance estimates. Variability across data splits was explained using embedded EEG representations, grouped by class and set, with pairwise separability to quantify group distinction. Overfitting was assessed through training-validation loss correlation and training speed. xEEGNet uses only 168 parameters, 200 times fewer than ShallowNet, yet retains interpretability, resists overfitting, achieves comparable median performance (-1.5%), and reduces variability across splits. This variability is explained by embedded EEG representations: higher accuracy correlates with greater separation between test set controls and Alzheimer's cases, without significant influence from training data. xEEGNet's ability to filter specific EEG bands, learn band-specific topographies, and use relevant spectral features demonstrates its interpretability. While large deep learning models are often prioritized for performance, this study shows smaller architectures like xEEGNet can be equally effective in EEG pathology classification.},
 author = {Andrea Zanola and Louis Fabrice Tshimanga and Federico Del Pup and Marco Baiesi and Manfredo Atzori},
 comment = {},
 doi = {10.1088/1741-2552/adf6e6},
 eprint = {2504.21457v1},
 journal = {arXiv preprint},
 title = {xEEGNet: Towards Explainable AI in EEG Dementia Classification},
 url = {http://arxiv.org/abs/2504.21457v1},
 year = {2025}
}

@article{2504.21700v3,
 abstract = {Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. These mechanisms maintain the integrity of LLM alignment by guaranteeing that the models respond safely and ethically. In response to this, attacks on LLMs are a significant threat to such protections, and many previous approaches have already demonstrated their effectiveness across diverse domains. Existing LLM attacks mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel approach that exploits these unique patterns to break the security and alignment constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our approach.},
 author = {Marco Arazzi and Vignesh Kumar Kembu and Antonino Nocera and Vinod P},
 comment = {},
 doi = {},
 eprint = {2504.21700v3},
 journal = {arXiv preprint},
 title = {XBreaking: Understanding how LLMs security alignment can be broken},
 url = {http://arxiv.org/abs/2504.21700v3},
 year = {2025}
}

@article{2505.00171v1,
 abstract = {Non-muscle-invasive bladder cancer (NMIBC) is a relentless challenge in oncology, with recurrence rates soaring as high as 70-80%. Each recurrence triggers a cascade of invasive procedures, lifelong surveillance, and escalating healthcare costs - affecting 460,000 individuals worldwide. However, existing clinical prediction tools remain fundamentally flawed, often overestimating recurrence risk and failing to provide personalized insights for patient management. In this work, we propose an interpretable deep learning framework that integrates vector embeddings and attention mechanisms to improve NMIBC recurrence prediction performance. We incorporate vector embeddings for categorical variables such as smoking status and intravesical treatments, allowing the model to capture complex relationships between patient attributes and recurrence risk. These embeddings provide a richer representation of the data, enabling improved feature interactions and enhancing prediction performance. Our approach not only enhances performance but also provides clinicians with patient-specific insights by highlighting the most influential features contributing to recurrence risk for each patient. Our model achieves accuracy of 70% with tabular data, outperforming conventional statistical methods while providing clinician-friendly patient-level explanations through feature attention. Unlike previous studies, our approach identifies new important factors influencing recurrence, such as surgical duration and hospital stay, which had not been considered in existing NMIBC prediction models.},
 author = {Saram Abbas and Naeem Soomro and Rishad Shafik and Rakesh Heer and Kabita Adhikari},
 comment = {7 pages, 5 figures, Accepted to be presented at the 47th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2025)},
 doi = {},
 eprint = {2505.00171v1},
 journal = {arXiv preprint},
 title = {Attention-enabled Explainable AI for Bladder Cancer Recurrence Prediction},
 url = {http://arxiv.org/abs/2505.00171v1},
 year = {2025}
}

@article{2505.00410v2,
 abstract = {The present research tackles the difficulty of predicting osteoporosis risk via machine learning (ML) approaches, emphasizing the use of explainable artificial intelligence (XAI) to improve model transparency. Osteoporosis is a significant public health concern, sometimes remaining untreated owing to its asymptomatic characteristics, and early identification is essential to avert fractures. The research assesses six machine learning classifiers: Random Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting and utilizes a dataset based on clinical, demographic, and lifestyle variables. The models are refined using GridSearchCV to calibrate hyperparameters, with the objective of enhancing predictive efficacy. XGBoost had the greatest accuracy (91%) among the evaluated models, surpassing others in precision (0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI approaches, such as SHAP, LIME, and Permutation Feature Importance, to elucidate the decision-making process of the optimal model. The study indicates that age is the primary determinant in forecasting osteoporosis risk, followed by hormonal alterations and familial history. These results corroborate clinical knowledge and affirm the models' therapeutic significance. The research underscores the significance of explainability in machine learning models for healthcare applications, guaranteeing that physicians can rely on the system's predictions. The report ultimately proposes directions for further research, such as validation across varied populations and the integration of supplementary biomarkers for enhanced predictive accuracy.},
 author = {Farhana Elias and Md Shihab Reza and Muhammad Zawad Mahmud and Samiha Islam and Shahran Rahman Alve},
 comment = {Submitted in an international conference},
 doi = {10.1109/QPAIN66474.2025.11172213},
 eprint = {2505.00410v2},
 journal = {arXiv preprint},
 title = {Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis},
 url = {http://arxiv.org/abs/2505.00410v2},
 year = {2025}
}

@article{2505.00591v1,
 abstract = {This chapter discusses the opportunities of eXplainable Artificial Intelligence (XAI) within the realm of spatial analysis. A key objective in spatial analysis is to model spatial relationships and infer spatial processes to generate knowledge from spatial data, which has been largely based on spatial statistical methods. More recently, machine learning offers scalable and flexible approaches that complement traditional methods and has been increasingly applied in spatial data science. Despite its advantages, machine learning is often criticized for being a black box, which limits our understanding of model behavior and output. Recognizing this limitation, XAI has emerged as a pivotal field in AI that provides methods to explain the output of machine learning models to enhance transparency and understanding. These methods are crucial for model diagnosis, bias detection, and ensuring the reliability of results obtained from machine learning models. This chapter introduces key concepts and methods in XAI with a focus on Shapley value-based approaches, which is arguably the most popular XAI method, and their integration with spatial analysis. An empirical example of county-level voting behaviors in the 2020 Presidential election is presented to demonstrate the use of Shapley values and spatial analysis with a comparison to multi-scale geographically weighted regression. The chapter concludes with a discussion on the challenges and limitations of current XAI techniques and proposes new directions.},
 author = {Ziqi Li},
 comment = {},
 doi = {},
 eprint = {2505.00591v1},
 journal = {arXiv preprint},
 title = {Explainable AI in Spatial Analysis},
 url = {http://arxiv.org/abs/2505.00591v1},
 year = {2025}
}

@article{2505.00802v1,
 abstract = {As Artificial Intelligence (AI) is increasingly used in areas that significantly impact human lives, concerns about fairness and transparency have grown, especially regarding their impact on protected groups. Recently, the intersection of explainability and fairness has emerged as an important area to promote responsible AI systems. This paper explores how explainability methods can be leveraged to detect and interpret unfairness. We propose a pipeline that integrates local post-hoc explanation methods to derive fairness-related insights. During the pipeline design, we identify and address critical questions arising from the use of explanations as bias detectors such as the relationship between distributive and procedural fairness, the effect of removing the protected attribute, the consistency and quality of results across different explanation methods, the impact of various aggregation strategies of local explanations on group fairness evaluations, and the overall trustworthiness of explanations as bias detectors. Our results show the potential of explanation methods used for fairness while highlighting the need to carefully consider the aforementioned critical aspects.},
 author = {Vasiliki Papanikou and Danae Pla Karidi and Evaggelia Pitoura and Emmanouil Panagiotou and Eirini Ntoutsi},
 comment = {},
 doi = {},
 eprint = {2505.00802v1},
 journal = {arXiv preprint},
 title = {Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration},
 url = {http://arxiv.org/abs/2505.00802v1},
 year = {2025}
}

@article{2505.01238v1,
 abstract = {As Natural Language Processing (NLP) models continue to evolve and become integral to high-stakes applications, ensuring their interpretability remains a critical challenge. Given the growing variety of explainability methods and diverse stakeholder requirements, frameworks that help stakeholders select appropriate explanations tailored to their specific use cases are increasingly important. To address this need, we introduce EvalxNLP, a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models. EvalxNLP integrates eight widely recognized explainability techniques from the Explainable AI (XAI) literature, enabling users to generate and evaluate explanations based on key properties such as faithfulness, plausibility, and complexity. Our framework also provides interactive, LLM-based textual explanations, facilitating user understanding of the generated explanations and evaluation outcomes. Human evaluation results indicate high user satisfaction with EvalxNLP, suggesting it is a promising framework for benchmarking explanation methods across diverse user groups. By offering a user-friendly and extensible platform, EvalxNLP aims at democratizing explainability tools and supporting the systematic comparison and advancement of XAI techniques in NLP.},
 author = {Mahdi Dhaini and Kafaite Zahra Hussain and Efstratios Zaradoukas and Gjergji Kasneci},
 comment = {Accepted to the xAI World Conference (2025) - System Demonstration},
 doi = {},
 eprint = {2505.01238v1},
 journal = {arXiv preprint},
 title = {EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models},
 url = {http://arxiv.org/abs/2505.01238v1},
 year = {2025}
}

@article{2505.01445v1,
 abstract = {If a product deviates from its desired properties in the injection moulding process, its root cause analysis can be aided by models that relate the input machine settings with the output quality characteristics. The machine learning models tested in the quality prediction are mostly black boxes; therefore, no direct explanation of their prognosis is given, which restricts their applicability in the quality control. The previously attempted explainability methods are either restricted to tree-based algorithms only or do not emphasize on the fact that some explainability methods can lead to wrong root cause identification of a product's deviation from its desired properties. This study first shows that the interactions among the multiple input machine settings do exist in real experimental data collected as per a central composite design. Then, the model-agnostic explainable AI methods are compared for the first time to show that different explainability methods indeed lead to different feature impact analysis in injection moulding. Moreover, it is shown that the better feature attribution translates to the correct cause identification and actionable insights for the injection moulding process. Being model agnostic, explanations on both random forest and multilayer perceptron are performed for the cause analysis, as both models have the mean absolute percentage error of less than 0.05% on the experimental dataset.},
 author = {Muhammad Muaz and Sameed Sajid and Tobias Schulze and Chang Liu and Nils Klasen and Benny Drescher},
 comment = {},
 doi = {10.1016/j.jmapro.2025.03.114},
 eprint = {2505.01445v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding},
 url = {http://arxiv.org/abs/2505.01445v1},
 year = {2025}
}

@article{2505.01488v1,
 abstract = {The increasing automation of traffic management systems has made them prime targets for cyberattacks, disrupting urban mobility and public safety. Traditional network-layer defenses are often inaccessible to transportation agencies, necessitating a machine learning-based approach that relies solely on traffic flow data. In this study, we simulate cyberattacks in a semi-realistic environment, using a virtualized traffic network to analyze disruption patterns. We develop a deep learning-based anomaly detection system, demonstrating that Longest Stop Duration and Total Jam Distance are key indicators of compromised signals. To enhance interpretability, we apply Explainable AI (XAI) techniques, identifying critical decision factors and diagnosing misclassification errors. Our analysis reveals two primary challenges: transitional data inconsistencies, where mislabeled recovery-phase traffic misleads the model, and model limitations, where stealth attacks in low-traffic conditions evade detection. This work enhances AI-driven traffic security, improving both detection accuracy and trustworthiness in smart transportation systems.},
 author = {Yujing Zhou and Marc L. Jacquet and Robel Dawit and Skyler Fabre and Dev Sarawat and Faheem Khan and Madison Newell and Yongxin Liu and Dahai Liu and Hongyun Chen and Jian Wang and Huihui Wang},
 comment = {},
 doi = {},
 eprint = {2505.01488v1},
 journal = {arXiv preprint},
 title = {Explainable Machine Learning for Cyberattack Identification from Traffic Flows},
 url = {http://arxiv.org/abs/2505.01488v1},
 year = {2025}
}

@article{2505.02828v3,
 abstract = {Explainable Artificial Intelligence (XAI) has emerged as a pillar of Trustworthy AI and aims to bring transparency in complex models that are opaque by nature. Despite the benefits of incorporating explanations in models, an urgent need is found in addressing the privacy concerns of providing this additional information to end users. In this article, we conduct a scoping review of existing literature to elicit details on the conflict between privacy and explainability. Using the standard methodology for scoping review, we extracted 57 articles from 1,943 studies published from January 2019 to December 2024. The review addresses 3 research questions to present readers with more understanding of the topic: (1) what are the privacy risks of releasing explanations in AI systems? (2) what current methods have researchers employed to achieve privacy preservation in XAI systems? (3) what constitutes a privacy preserving explanation? Based on the knowledge synthesized from the selected studies, we categorize the privacy risks and preservation methods in XAI and propose the characteristics of privacy preserving explanations to aid researchers and practitioners in understanding the requirements of XAI that is privacy compliant. Lastly, we identify the challenges in balancing privacy with other system desiderata and provide recommendations for achieving privacy preserving XAI. We expect that this review will shed light on the complex relationship of privacy and explainability, both being the fundamental principles of Trustworthy AI.},
 author = {Sonal Allana and Mohan Kankanhalli and Rozita Dara},
 comment = {Published in Transactions on Machine Learning Research: https://openreview.net/forum?id=q9nykJfzku},
 doi = {},
 eprint = {2505.02828v3},
 journal = {arXiv preprint},
 title = {Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review},
 url = {http://arxiv.org/abs/2505.02828v3},
 year = {2025}
}

@article{2505.02859v1,
 abstract = {Across various sectors applications of eXplainableAI (XAI) gained momentum as the increasing black-boxedness of prevailing Machine Learning (ML) models became apparent. In parallel, Large Language Models (LLMs) significantly developed in their abilities to understand human language and complex patterns. By combining both, this paper presents a novel reference architecture for the interpretation of XAI through an interactive chatbot powered by a fine-tuned LLM. We instantiate the reference architecture in the context of State-of-Health (SoH) prediction for batteries and validate its design in multiple evaluation and demonstration rounds. The evaluation indicates that the implemented prototype enhances the human interpretability of ML, especially for users with less experience with XAI.},
 author = {Jonas Bokstaller and Julia Altheimer and Julian Dormehl and Alina Buss and Jasper Wiltfang and Johannes Schneider and Maximilian Röglinger},
 comment = {},
 doi = {},
 eprint = {2505.02859v1},
 journal = {arXiv preprint},
 title = {Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI},
 url = {http://arxiv.org/abs/2505.02859v1},
 year = {2025}
}

@article{2505.03132v1,
 abstract = {Real-world machine learning models require rigorous evaluation before deployment, especially in safety-critical domains like autonomous driving and surveillance. The evaluation of machine learning models often focuses on data slices, which are subsets of the data that share a set of characteristics. Data slice finding automatically identifies conditions or data subgroups where models underperform, aiding developers in mitigating performance issues. Despite its popularity and effectiveness, data slicing for vision model validation faces several challenges. First, data slicing often needs additional image metadata or visual concepts, and falls short in certain computer vision tasks, such as object detection. Second, understanding data slices is a labor-intensive and mentally demanding process that heavily relies on the expert's domain knowledge. Third, data slicing lacks a human-in-the-loop solution that allows experts to form hypothesis and test them interactively. To overcome these limitations and better support the machine learning operations lifecycle, we introduce VISLIX, a novel visual analytics framework that employs state-of-the-art foundation models to help domain experts analyze slices in computer vision models. Our approach does not require image metadata or visual concepts, automatically generates natural language insights, and allows users to test data slice hypothesis interactively. We evaluate VISLIX with an expert study and three use cases, that demonstrate the effectiveness of our tool in providing comprehensive insights for validating object detection models.},
 author = {Xinyuan Yan and Xiwei Xuan and Jorge Piazentin Ono and Jiajing Guo and Vikram Mohanty and Shekar Arvind Kumar and Liang Gou and Bei Wang and Liu Ren},
 comment = {},
 doi = {},
 eprint = {2505.03132v1},
 journal = {arXiv preprint},
 title = {VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis},
 url = {http://arxiv.org/abs/2505.03132v1},
 year = {2025}
}

@article{2505.03201v3,
 abstract = {Integrated Gradients (IG) is a widely used attribution method in explainable AI, particularly in computer vision applications where reliable feature attribution is essential. A key limitation of IG is its sensitivity to the choice of baseline (reference) images. Multi-baseline extensions such as Expected Gradients (EG) assume uniform weighting over baselines, implicitly treating baseline images as equally informative. In high-dimensional vision models, this assumption often leads to noisy or unstable explanations. This paper proposes Weighted Integrated Gradients (WG), a principled approach that evaluates and weights baselines to enhance attribution reliability. WG introduces an unsupervised criterion for baseline suitability, enabling adaptive selection and weighting of baselines on a per-input basis. The method not only preserves core axiomatic properties of IG but also provides improved theoretical guarantees on the quality of explanation over EG. Experiments on commonly used image datasets and models show that WG consistently outperforms EG, yielding 10 to 35 percent improvements in attribution fidelity. WG further identifies informative baseline subsets, reducing unnecessary variability while maintaining high attribution accuracy. By moving beyond the idea that all baselines matter equally, Weighted Integrated Gradients offers a clearer and more reliable way to explain computer-vision models, improving both understanding and practical usability in explainable AI.},
 author = {Kien Tran Duc Tuan and Tam Nguyen Trong and Son Nguyen Hoang and Khoat Than and Anh Nguyen Duc},
 comment = {},
 doi = {},
 eprint = {2505.03201v3},
 journal = {arXiv preprint},
 title = {Enhancing Visual Feature Attribution via Weighted Integrated Gradients},
 url = {http://arxiv.org/abs/2505.03201v3},
 year = {2025}
}

@article{2505.04019v1,
 abstract = {The need to explain predictive models is well-established in modern machine learning. However, beyond model interpretability, understanding pre-processing methods is equally essential. Understanding how data modifications impact model performance improvements and potential biases and promoting a reliable pipeline is mandatory for developing robust machine learning solutions. Isolation Forest (iForest) is a widely used technique for outlier detection that performs well. Its effectiveness increases with the number of tree-based learners. However, this also complicates the explanation of outlier selection and the decision boundaries for inliers. This research introduces a novel Explainable AI (XAI) method, tackling the problem of global explainability. In detail, it aims to offer a global explanation for outlier detection to address its opaque nature. Our approach is based on the Decision Predicate Graph (DPG), which clarifies the logic of ensemble methods and provides both insights and a graph-based metric to explain how samples are identified as outliers using the proposed Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's explainability and provides a comprehensive view of the decision-making process, detailing which features contribute to outlier identification and how the model utilizes them. This method advances the state-of-the-art by providing insights into decision boundaries and a comprehensive view of holistic feature usage in outlier identification. -- thus promoting a fully explainable machine learning pipeline.},
 author = {Matteo Ceschin and Leonardo Arrighi and Luca Longo and Sylvio Barbon Junior},
 comment = {},
 doi = {},
 eprint = {2505.04019v1},
 journal = {arXiv preprint},
 title = {Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest},
 url = {http://arxiv.org/abs/2505.04019v1},
 year = {2025}
}

@article{2505.04627v2,
 abstract = {The rise of deep learning challenges the longstanding scientific ideal of insight - the human capacity to understand phenomena by uncovering underlying mechanisms. In many modern applications, accurate predictions no longer require interpretable models, prompting debate about whether explainability is a realistic or even meaningful goal. From our perspective in physics, we examine this tension through a concrete case study: a physics-informed neural network (PINN) trained on a rarefied gas dynamics problem governed by the Boltzmann equation. Despite the system's clear structure and well-understood governing laws, the trained network's weights resemble Gaussian-distributed random matrices, with no evident trace of the physical principles involved. This suggests that deep learning and traditional simulation may follow distinct cognitive paths to the same outcome - one grounded in mechanistic insight, the other in statistical interpolation. Our findings raise critical questions about the limits of explainable AI and whether interpretability can - or should-remain a universal standard in artificial reasoning.},
 author = {Jean-Michel Tucny and Mihir Durve and Sauro Succi},
 comment = {15 pages, 2 figures},
 doi = {},
 eprint = {2505.04627v2},
 journal = {arXiv preprint},
 title = {Is the end of Insight in Sight ?},
 url = {http://arxiv.org/abs/2505.04627v2},
 year = {2025}
}

@article{2505.06123v1,
 abstract = {Wasserstein distances provide a powerful framework for comparing data distributions. They can be used to analyze processes over time or to detect inhomogeneities within data. However, simply calculating the Wasserstein distance or analyzing the corresponding transport map (or coupling) may not be sufficient for understanding what factors contribute to a high or low Wasserstein distance. In this work, we propose a novel solution based on Explainable AI that allows us to efficiently and accurately attribute Wasserstein distances to various data components, including data subgroups, input features, or interpretable subspaces. Our method achieves high accuracy across diverse datasets and Wasserstein distance specifications, and its practical utility is demonstrated in two use cases.},
 author = {Philip Naumann and Jacob Kauffmann and Grégoire Montavon},
 comment = {},
 doi = {},
 eprint = {2505.06123v1},
 journal = {arXiv preprint},
 title = {Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena},
 url = {http://arxiv.org/abs/2505.06123v1},
 year = {2025}
}

@article{2505.06258v1,
 abstract = {Attribution algorithms are essential for enhancing the interpretability and trustworthiness of deep learning models by identifying key features driving model decisions. Existing frameworks, such as InterpretDL and OmniXAI, integrate multiple attribution methods but suffer from scalability limitations, high coupling, theoretical constraints, and lack of user-friendly implementations, hindering neural network transparency and interoperability. To address these challenges, we propose Attribution-Based Explainability (ABE), a unified framework that formalizes Fundamental Attribution Methods and integrates state-of-the-art attribution algorithms while ensuring compliance with attribution axioms. ABE enables researchers to develop novel attribution techniques and enhances interpretability through four customizable modules: Robustness, Interpretability, Validation, and Data & Model. This framework provides a scalable, extensible foundation for advancing attribution-based explainability and fostering transparent AI systems. Our code is available at: https://github.com/LMBTough/ABE-XAI.},
 author = {Zhiyu Zhu and Jiayu Zhang and Zhibo Jin and Fang Chen and Jianlong Zhou},
 comment = {},
 doi = {},
 eprint = {2505.06258v1},
 journal = {arXiv preprint},
 title = {ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability},
 url = {http://arxiv.org/abs/2505.06258v1},
 year = {2025}
}

@article{2505.06263v1,
 abstract = {The dynamic nature of human health and comfort calls for adaptive systems that respond to individual physiological needs in real time. This paper presents an AI-enhanced digital twin framework that integrates biometric signals, specifically electrocardiogram (ECG) data, with environmental parameters such as temperature, humidity, and ventilation. Leveraging IoT-enabled sensors and biometric monitoring devices, the system continuously acquires, synchronises, and preprocesses multimodal data streams to construct a responsive virtual replica of the physical environment. To validate this framework, a detailed case study is conducted using the MIT-BIH noise stress test dataset. ECG signals are filtered and segmented using dynamic sliding windows, followed by extracting heart rate variability (HRV) features such as SDNN, BPM, QTc, and LF/HF ratio. Relative deviation metrics are computed against clean baselines to quantify stress responses. A random forest classifier is trained to predict stress levels across five categories, and Shapley Additive exPlanations (SHAP) is used to interpret model behaviour and identify key contributing features. These predictions are mapped to a structured set of environmental interventions using a Five Level Stress Intervention Mapping, which activates multi-scale responses across personal, room, building, and landscape levels. This integration of physiological insight, explainable AI, and adaptive control establishes a new paradigm for health-responsive built environments. It lays the foundation for the future development of intelligent, personalised healing spaces.},
 author = {Yiping Meng and Yiming Sun},
 comment = {},
 doi = {},
 eprint = {2505.06263v1},
 journal = {arXiv preprint},
 title = {From Biometrics to Environmental Control: AI-Enhanced Digital Twins for Personalized Health Interventions in Healing Landscapes},
 url = {http://arxiv.org/abs/2505.06263v1},
 year = {2025}
}

@article{2505.06620v1,
 abstract = {There is a growing demand for the use of Artificial Intelligence (AI) and Machine Learning (ML) in healthcare, particularly as clinical decision support systems to assist medical professionals. However, the complexity of many of these models, often referred to as black box models, raises concerns about their safe integration into clinical settings as it is difficult to understand how they arrived at their predictions. This paper discusses insights and recommendations derived from an expert working group convened by the UK Medicine and Healthcare products Regulatory Agency (MHRA). The group consisted of healthcare professionals, regulators, and data scientists, with a primary focus on evaluating the outputs from different AI algorithms in clinical decision-making contexts. Additionally, the group evaluated findings from a pilot study investigating clinicians' behaviour and interaction with AI methods during clinical diagnosis. Incorporating AI methods is crucial for ensuring the safety and trustworthiness of medical AI devices in clinical settings. Adequate training for stakeholders is essential to address potential issues, and further insights and recommendations for safely adopting AI systems in healthcare settings are provided.},
 author = {Dima Alattal and Asal Khoshravan Azar and Puja Myles and Richard Branson and Hatim Abdulhussein and Allan Tucker},
 comment = {47 pages},
 doi = {},
 eprint = {2505.06620v1},
 journal = {arXiv preprint},
 title = {Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations},
 url = {http://arxiv.org/abs/2505.06620v1},
 year = {2025}
}

@article{2505.06906v1,
 abstract = {This paper presents a novel method for generating realistic counterfactual explanations (CFEs) in machine learning (ML)-based control for mobile robots using 2D LiDAR. ML models, especially artificial neural networks (ANNs), can provide advanced decision-making and control capabilities by learning from data. However, they often function as black boxes, making it challenging to interpret them. This is especially a problem in safety-critical control applications. To generate realistic CFEs, we parameterize the LiDAR space with simple shapes such as circles and rectangles, whose parameters are chosen by a genetic algorithm, and the configurations are transformed into LiDAR data by raycasting. Our model-agnostic approach generates CFEs in the form of synthetic LiDAR data that resembles a base LiDAR state but is modified to produce a pre-defined ML model control output based on a query from the user. We demonstrate our method on a mobile robot, the TurtleBot3, controlled using deep reinforcement learning (DRL) in real-world and simulated scenarios. Our method generates logical and realistic CFEs, which helps to interpret the DRL agent's decision making. This paper contributes towards advancing explainable AI in mobile robotics, and our method could be a tool for understanding, debugging, and improving ML-based autonomous control.},
 author = {Sindre Benjamin Remman and Anastasios M. Lekkas},
 comment = {Accepted for publication at the 2025 European Control Conference (ECC)},
 doi = {},
 eprint = {2505.06906v1},
 journal = {arXiv preprint},
 title = {Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR},
 url = {http://arxiv.org/abs/2505.06906v1},
 year = {2025}
}

@article{2505.07058v1,
 abstract = {Artificial Intelligence (AI) is rapidly expanding and integrating more into daily life to automate tasks, guide decision making, and enhance efficiency. However, complex AI models, which make decisions without providing clear explanations (known as the "black-box problem"), currently restrict trust and widespread adoption of AI. Explainable Artificial Intelligence (XAI) has emerged to address the black-box problem of making AI systems more interpretable and transparent so stakeholders can trust, verify, and act upon AI-based outcomes. Researchers have developed various techniques to foster XAI in the Software Development Lifecycle. However, there are gaps in applying XAI techniques in the Software Engineering phases. Literature review shows that 68% of XAI in Software Engineering research is focused on maintenance as opposed to 8% on software management and requirements. In this paper, we present a comprehensive survey of the applications of XAI methods such as concept-based explanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), rule extraction, attention mechanisms, counterfactual explanations, and example-based explanations to the different phases of the Software Development Life Cycle (SDLC), including requirements elicitation, design and development, testing and deployment, and evolution. To the best of our knowledge, this paper presents the first comprehensive survey of XAI techniques for every phase of the Software Development Life Cycle (SDLC). This survey aims to promote explainable AI in Software Engineering and facilitate the practical application of complex AI models in AI-driven software development.},
 author = {Lakshit Arora and Sanjay Surendranath Girija and Shashank Kapoor and Aman Raj and Dipen Pradhan and Ankit Shetgaonkar},
 comment = {Accepted to IEEE COMPSAC 2025},
 doi = {10.1109/COMPSAC65507.2025.00321},
 eprint = {2505.07058v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey},
 url = {http://arxiv.org/abs/2505.07058v1},
 year = {2025}
}

@article{2505.07534v1,
 abstract = {Visual Analytics (VA) integrates humans, data, and models as key actors in insight generation and data-driven decision-making. This position paper values and reflects on 16 VA process models and frameworks and makes nine high-level observations that motivate a fresh perspective on VA. The contribution is the HDMI Canvas, a perspective to VA that complements the strengths of existing VA process models and frameworks. It systematically characterizes diverse roles of humans, data, and models, and how these actors benefit from and contribute to VA processes. The descriptive power of the HDMI Canvas eases the differentiation between a series of VA building blocks, rather than describing general VA principles only. The canvas includes modern human-centered methodologies, including human knowledge externalization and forms of feedback loops, while interpretable and explainable AI highlight model contributions beyond their conventional outputs. The HDMI Canvas has generative power, guiding the design of new VA processes and is optimized for external stakeholders, improving VA outreach, interdisciplinary collaboration, and user-centered design. The utility of the HDMI Canvas is demonstrated through two preliminary case studies.},
 author = {Jürgen Bernard},
 comment = {7 pages, 5 figures, LaTeX; to appear at the 16th International EuroVis Workshop on Visual Analytics (EuroVA'25) as a position paper},
 doi = {},
 eprint = {2505.07534v1},
 journal = {arXiv preprint},
 title = {The Human-Data-Model Interaction Canvas for Visual Analytics},
 url = {http://arxiv.org/abs/2505.07534v1},
 year = {2025}
}

@article{2505.07910v2,
 abstract = {Despite the growing interest in Explainable Artificial Intelligence (XAI), explainability is rarely considered during hyperparameter tuning or neural architecture optimization, where the focus remains primarily on minimizing predictive loss. In this work, we introduce the novel concept of XAI consistency, defined as the agreement among different feature attribution methods, and propose new metrics to quantify it. For the first time, we integrate XAI consistency directly into the hyperparameter tuning objective, creating a multi-objective optimization framework that balances predictive performance with explanation robustness. Implemented within the Sequential Parameter Optimization Toolbox (SPOT), our approach uses both weighted aggregation and desirability-based strategies to guide model selection. Through our proposed framework and supporting tools, we explore the impact of incorporating XAI consistency into the optimization process. This enables us to characterize distinct regions in the architecture configuration space: one region with poor performance and comparatively low interpretability, another with strong predictive performance but weak interpretability due to low \gls{xai} consistency, and a trade-off region that balances both objectives by offering high interpretability alongside competitive performance. Beyond introducing this novel approach, our research provides a foundation for future investigations into whether models from the trade-off zone-balancing performance loss and XAI consistency-exhibit greater robustness by avoiding overfitting to training performance, thereby leading to more reliable predictions on out-of-distribution data.},
 author = {Alexander Hinterleitner and Thomas Bartz-Beielstein},
 comment = {},
 doi = {},
 eprint = {2505.07910v2},
 journal = {arXiv preprint},
 title = {Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization},
 url = {http://arxiv.org/abs/2505.07910v2},
 year = {2025}
}

@article{2505.08198v2,
 abstract = {Explainable artificial intelligence (XAI) is essential for trustworthy machine learning (ML), particularly in high-stakes domains such as healthcare and finance. Shapley value (SV) methods provide a principled framework for feature attribution in complex models but incur high computational costs, limiting their scalability in high-dimensional settings. We propose Stochastic Iterative Momentum for Shapley Value Approximation (SIM-Shapley), a stable and efficient SV approximation method inspired by stochastic optimization. We analyze variance theoretically, prove linear $Q$-convergence, and demonstrate improved empirical stability and low bias in practice on real-world datasets. In our numerical experiments, SIM-Shapley reduces computation time by up to 85% relative to state-of-the-art baselines while maintaining comparable feature attribution quality. Beyond feature attribution, our stochastic mini-batch iterative framework extends naturally to a broader class of sample average approximation problems, offering a new avenue for improving computational efficiency with stability guarantees. Code is publicly available at https://github.com/nliulab/SIM-Shapley.},
 author = {Wangxuan Fan and Siqi Li and Doudou Zhou and Yohei Okada and Chuan Hong and Molei Liu and Nan Liu},
 comment = {21 pages, 6 figures, 5 tables},
 doi = {},
 eprint = {2505.08198v2},
 journal = {arXiv preprint},
 title = {SIM-Shapley: A Stable and Computationally Efficient Approach to Shapley Value Approximation},
 url = {http://arxiv.org/abs/2505.08198v2},
 year = {2025}
}

@article{2505.08345v1,
 abstract = {Local feature-based explanations are a key component of the XAI toolkit. These explanations compute feature importance values relative to an ``interpretable'' feature representation. In tabular data, feature values themselves are often considered interpretable. This paper examines the impact of data engineering choices on local feature-based explanations. We demonstrate that simple, common data engineering techniques, such as representing age with a histogram or encoding race in a specific way, can manipulate feature importance as determined by popular methods like SHAP. Notably, the sensitivity of explanations to feature representation can be exploited by adversaries to obscure issues like discrimination. While the intuition behind these results is straightforward, their systematic exploration has been lacking. Previous work has focused on adversarial attacks on feature-based explainers by biasing data or manipulating models. To the best of our knowledge, this is the first study demonstrating that explainers can be misled by standard, seemingly innocuous data engineering techniques.},
 author = {Hyunseung Hwang and Andrew Bell and Joao Fonseca and Venetia Pliatsika and Julia Stoyanovich and Steven Euijong Whang},
 comment = {Accepted to ACM FAccT 2025},
 doi = {},
 eprint = {2505.08345v1},
 journal = {arXiv preprint},
 title = {SHAP-based Explanations are Sensitive to Feature Representation},
 url = {http://arxiv.org/abs/2505.08345v1},
 year = {2025}
}

@article{2505.08847v1,
 abstract = {Machine Learning as a Service (MLaaS) has gained important attraction as a means for deploying powerful predictive models, offering ease of use that enables organizations to leverage advanced analytics without substantial investments in specialized infrastructure or expertise. However, MLaaS platforms must be safeguarded against security and privacy attacks, such as model extraction (MEA) attacks. The increasing integration of explainable AI (XAI) within MLaaS has introduced an additional privacy challenge, as attackers can exploit model explanations particularly counterfactual explanations (CFs) to facilitate MEA. In this paper, we investigate the trade offs among model performance, privacy, and explainability when employing Differential Privacy (DP), a promising technique for mitigating CF facilitated MEA. We evaluate two distinct DP strategies: implemented during the classification model training and at the explainer during CF generation.},
 author = {Fatima Ezzeddine and Rinad Akel and Ihab Sbeity and Silvia Giordano and Marc Langheinrich and Omran Ayoub},
 comment = {},
 doi = {},
 eprint = {2505.08847v1},
 journal = {arXiv preprint},
 title = {On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction},
 url = {http://arxiv.org/abs/2505.08847v1},
 year = {2025}
}

@article{2505.09932v1,
 abstract = {The trajectory of artificial intelligence (AI) has been one of relentless acceleration, evolving from rudimentary rule-based systems to sophisticated, autonomous agents capable of complex reasoning and interaction. This whitepaper chronicles this remarkable journey, charting the key technological milestones--advancements in prompting, training methodologies, hardware capabilities, and architectural innovations--that have converged to create the AI agents of today. We argue that these agents, exemplified by systems like OpenAI's ChatGPT with plugins and xAI's Grok, represent a culminating phase in AI development, potentially constituting the "final generation" of intelligence as we currently conceive it. We explore the capabilities and underlying technologies of these agents, grounded in practical examples, while also examining the profound societal implications and the unprecedented pace of progress that suggests intelligence is now doubling approximately every six months. The paper concludes by underscoring the critical need for wisdom and foresight in navigating the opportunities and challenges presented by this powerful new era of intelligence.},
 author = {Kevin J McNamara and Rhea Pritham Marpu},
 comment = {},
 doi = {},
 eprint = {2505.09932v1},
 journal = {arXiv preprint},
 title = {Demystifying AI Agents: The Final Generation of Intelligence},
 url = {http://arxiv.org/abs/2505.09932v1},
 year = {2025}
}

@article{2505.10050v1,
 abstract = {Traditional machine learning models often prioritize predictive accuracy, often at the expense of model transparency and interpretability. The lack of transparency makes it difficult for organizations to comply with regulatory requirements and gain stakeholders trust. In this research, we propose a fraud detection framework that combines a stacking ensemble of well-known gradient boosting models: XGBoost, LightGBM, and CatBoost. In addition, explainable artificial intelligence (XAI) techniques are used to enhance the transparency and interpretability of the model's decisions. We used SHAP (SHapley Additive Explanations) for feature selection to identify the most important features. Further efforts were made to explain the model's predictions using Local Interpretable Model-Agnostic Explanation (LIME), Partial Dependence Plots (PDP), and Permutation Feature Importance (PFI). The IEEE-CIS Fraud Detection dataset, which includes more than 590,000 real transaction records, was used to evaluate the proposed model. The model achieved a high performance with an accuracy of 99% and an AUC-ROC score of 0.99, outperforming several recent related approaches. These results indicate that combining high prediction accuracy with transparent interpretability is possible and could lead to a more ethical and trustworthy solution in financial fraud detection.},
 author = {Fahad Almalki and Mehedi Masud},
 comment = {},
 doi = {},
 eprint = {2505.10050v1},
 journal = {arXiv preprint},
 title = {Financial Fraud Detection Using Explainable AI and Stacking Ensemble Methods},
 url = {http://arxiv.org/abs/2505.10050v1},
 year = {2025}
}

@article{2505.10167v3,
 abstract = {The emergence of hybrid quantum-classical machine learning (HQML) models opens new horizons of computational intelligence but their fundamental complexity frequently leads to black box behavior that undermines transparency and reliability in their application. Although XAI for quantum systems still in its infancy, a major research gap is evident in robust global and local explainability approaches that are designed for HQML architectures that employ quantized feature encoding followed by classical learning. The gap is the focus of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an explainer for explaining feature importance in these hybrid systems. Our model entails the creation of HQML models incorporating quantum feature maps, the use of Q-MEDLEY, which combines feature based inferences, preserving the quantum transformation stage and visualizing the resulting attributions. Our result shows that Q-MEDLEY delineates influential classical aspects in HQML models, as well as separates their noise, and competes well against established XAI techniques in classical validation settings. Ablation studies more significantly expose the virtues of the composite structure used in Q-MEDLEY. The implications of this work are critically important, as it provides a route to improve the interpretability and reliability of HQML models, thus promoting greater confidence and being able to engage in safer and more responsible use of quantum-enhanced AI technology.
  Our code and experiments are open-sourced at: https://github.com/GitsSaikat/QuXAI},
 author = {Saikat Barua and Mostafizur Rahman and Shehenaz Khaled and Md Jafor Sadek and Rafiul Islam and Shahnewaz Siddique},
 comment = {16 pages, 6 figures, 7 equations},
 doi = {},
 eprint = {2505.10167v3},
 journal = {arXiv preprint},
 title = {QuXAI: Explainers for Hybrid Quantum Machine Learning Models},
 url = {http://arxiv.org/abs/2505.10167v3},
 year = {2025}
}

@article{2505.10188v1,
 abstract = {As the field of healthcare increasingly adopts artificial intelligence, it becomes important to understand which types of explanations increase transparency and empower users to develop confidence and trust in the predictions made by machine learning (ML) systems. In shared decision-making scenarios where doctors cooperate with ML systems to reach an appropriate decision, establishing mutual trust is crucial. In this paper, we explore different approaches to generating explanations in eXplainable AI (XAI) and make their underlying arguments explicit so that they can be evaluated by medical experts. In particular, we present the findings of a user study conducted with physicians to investigate their perceptions of various types of AI-generated explanations in the context of diagnostic decision support. The study aims to identify the most effective and useful explanations that enhance the diagnostic process. In the study, medical doctors filled out a survey to assess different types of explanations. Further, an interview was carried out post-survey to gain qualitative insights on the requirements of explanations incorporated in diagnostic decision support. Overall, the insights gained from this study contribute to understanding the types of explanations that are most effective.},
 author = {Felix Liedeker and Olivia Sanchez-Graillet and Moana Seidler and Christian Brandt and Jörg Wellmer and Philipp Cimiano},
 comment = {Presented at 'The First Workshop on Natural Language Argument-Based Explanations', co-located with ECAI 2024},
 doi = {},
 eprint = {2505.10188v1},
 journal = {arXiv preprint},
 title = {A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support},
 url = {http://arxiv.org/abs/2505.10188v1},
 year = {2025}
}

@article{2505.10515v1,
 abstract = {Recently, post hoc explanation methods have emerged to enhance model transparency by attributing model outputs to input features. However, these methods face challenges due to their specificity to certain neural network architectures and data modalities. Existing explainable artificial intelligence (XAI) frameworks have attempted to address these challenges but suffer from several limitations. These include limited flexibility to diverse model architectures and data modalities due to hard-coded implementations, a restricted number of supported XAI methods because of the requirements for layer-specific operations of attribution methods, and sub-optimal recommendations of explanations due to the lack of evaluation and optimization phases. Consequently, these limitations impede the adoption of XAI technology in real-world applications, making it difficult for practitioners to select the optimal explanation method for their domain. To address these limitations, we introduce \textbf{PnPXAI}, a universal XAI framework that supports diverse data modalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI automatically detects model architectures, recommends applicable explanation methods, and optimizes hyperparameters for optimal explanations. We validate the framework's effectiveness through user surveys and showcase its versatility across various domains, including medicine and finance.},
 author = {Seongun Kim and Sol A Kim and Geonhyeong Kim and Enver Menadjiev and Chanwoo Lee and Seongwook Chung and Nari Kim and Jaesik Choi},
 comment = {},
 doi = {},
 eprint = {2505.10515v1},
 journal = {arXiv preprint},
 title = {PnPXAI: A Universal XAI Framework Providing Automatic Explanations Across Diverse Modalities and Models},
 url = {http://arxiv.org/abs/2505.10515v1},
 year = {2025}
}

@article{2505.10764v3,
 abstract = {Innovations in digital intelligence are transforming robotic surgery with more informed decision-making. Real-time awareness of surgical instrument presence and actions (e.g., cutting tissue) is essential for such systems. Yet, despite decades of research, most machine learning models for this task are trained on small datasets and still struggle to generalize. Recently, vision-Language Models (VLMs) have brought transformative advances in reasoning across visual and textual modalities. Their unprecedented generalization capabilities suggest great potential for advancing intelligent robotic surgery. However, surgical VLMs remain under-explored, and existing models show limited performance, highlighting the need for benchmark studies to assess their capabilities and limitations and to inform future development. To this end, we benchmark the zero-shot performance of several advanced VLMs on two public robotic-assisted laparoscopic datasets for instrument and action classification. Beyond standard evaluation, we integrate explainable AI to visualize VLM attention and uncover causal explanations behind their predictions. This provides a previously underexplored perspective in this field for evaluating the reliability of model predictions. We also propose several explainability analysis-based metrics to complement standard evaluations. Our analysis reveals that surgical VLMs, despite domain-specific training, often rely on weak contextual cues rather than clinically relevant visual evidence, highlighting the need for stronger visual and reasoning supervision in surgical applications.},
 author = {Jiajun Cheng and Xianwu Zhao and Sainan Liu and Xiaofan Yu and Ravi Prakash and Patrick J. Codd and Jonathan Elliott Katz and Shan Lin},
 comment = {},
 doi = {},
 eprint = {2505.10764v3},
 journal = {arXiv preprint},
 title = {SurgXBench: Explainable Vision-Language Model Benchmark for Surgery},
 url = {http://arxiv.org/abs/2505.10764v3},
 year = {2025}
}

@article{2505.10942v1,
 abstract = {Federated Learning (FL) has emerged as a powerful paradigm for collaborative model training while keeping client data decentralized and private. However, it is vulnerable to Data Reconstruction Attacks (DRA) such as "LoKI" and "Robbing the Fed", where malicious models sent from the server to the client can reconstruct sensitive user data. To counter this, we introduce DRArmor, a novel defense mechanism that integrates Explainable AI with targeted detection and mitigation strategies for DRA. Unlike existing defenses that focus on the entire model, DRArmor identifies and addresses the root cause (i.e., malicious layers within the model that send gradients with malicious intent) by analyzing their contribution to the output and detecting inconsistencies in gradient values. Once these malicious layers are identified, DRArmor applies defense techniques such as noise injection, pixelation, and pruning to these layers rather than the whole model, minimizing the attack surface and preserving client data privacy. We evaluate DRArmor's performance against the advanced LoKI attack across diverse datasets, including MNIST, CIFAR-10, CIFAR-100, and ImageNet, in a 200-client FL setup. Our results demonstrate DRArmor's effectiveness in mitigating data leakage, achieving high True Positive and True Negative Rates of 0.910 and 0.890, respectively. Additionally, DRArmor maintains an average accuracy of 87%, effectively protecting client privacy without compromising model performance. Compared to existing defense mechanisms, DRArmor reduces the data leakage rate by 62.5% with datasets containing 500 samples per client.},
 author = {Meghali Nandi and Arash Shaghaghi and Nazatul Haque Sultan and Gustavo Batista and Raymond K. Zhao and Sanjay Jha},
 comment = {Accepted to AsiaCCS 2025},
 doi = {},
 eprint = {2505.10942v1},
 journal = {arXiv preprint},
 title = {Nosy Layers, Noisy Fixes: Tackling DRAs in Federated Learning Systems using Explainable AI},
 url = {http://arxiv.org/abs/2505.10942v1},
 year = {2025}
}

@article{2505.10991v3,
 abstract = {Explainable Artificial Intelligence (XAI) is critical for attaining trust in the operation of AI systems. A key question of an AI system is ``why was this decision made this way''. Formal approaches to XAI use a formal model of the AI system to identify abductive explanations. While abductive explanations may be applicable to a large number of inputs sharing the same concrete values, more general explanations may be preferred for numeric inputs. So-called inflated abductive explanations give intervals for each feature ensuring that any input whose values fall withing these intervals is still guaranteed to make the same prediction. Inflated explanations cover a larger portion of the input space, and hence are deemed more general explanations. But there can be many (inflated) abductive explanations for an instance. Which is the best? In this paper, we show how to find a most general abductive explanation for an AI decision. This explanation covers as much of the input space as possible, while still being a correct formal explanation of the model's behaviour. Given that we only want to give a human one explanation for a decision, the most general explanation gives us the explanation with the broadest applicability, and hence the one most likely to seem sensible. (The paper has been accepted at IJCAI2025 conference.)},
 author = {Yacine Izza and Alexey Ignatiev and Sasha Rubin and Joao Marques-Silva and Peter J. Stuckey},
 comment = {},
 doi = {},
 eprint = {2505.10991v3},
 journal = {arXiv preprint},
 title = {Most General Explanations of Tree Ensembles (Extended Version)},
 url = {http://arxiv.org/abs/2505.10991v3},
 year = {2025}
}

@article{2505.11189v2,
 abstract = {Large language models (LLMs) can amplify misinformation, undermining societal goals like the UN SDGs. We study three documented drivers of misinformation (valence framing, information overload, and oversimplification) which are often shaped by one's default beliefs. Building on evidence that LLMs encode such defaults (e.g., "joy is positive," "math is complex") and can act as "bags of heuristics," we ask: can general belief-driven heuristics behind misinformative behaviour be recovered from LLMs as clear rules? A key obstacle is that global rule-extraction methods in explainable AI (XAI) are built for numerical inputs/outputs, not text. We address this by eliciting global LLM beliefs and mapping them to numerical scores via statistically reliable abstractions, thereby enabling off-the-shelf global XAI to detect belief-related heuristics in LLMs. To obtain ground truth, we hard-code bias-inducing nonlinear heuristics of increasing complexity (univariate, conjunctive, nonconvex) into popular LLMs (ChatGPT and Llama) via system instructions. This way, we find that RuleFit under-detects non-univariate biases, while global SHAP better approximates conjunctive ones but does not yield actionable rules. To bridge this gap, we propose RuleSHAP, a rule-extraction algorithm that couples global SHAP-value aggregations with rule induction to better capture non-univariate bias, improving heuristics detection over RuleFit by +94% (MRR@1) on average. Our results provide a practical pathway for revealing belief-driven biases in LLMs.},
 author = {Francesco Sovrano},
 comment = {},
 doi = {},
 eprint = {2505.11189v2},
 journal = {arXiv preprint},
 title = {Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP},
 url = {http://arxiv.org/abs/2505.11189v2},
 year = {2025}
}

@article{2505.11210v3,
 abstract = {Suppressor variables can influence model predictions without being dependent on the target outcome, and they pose a significant challenge for Explainable AI (XAI) methods. These variables may cause false-positive feature attributions, undermining the utility of explanations. Although effective remedies exist for linear models, their extension to non-linear models and instance-based explanations has remained limited. We introduce PatternLocal, a novel XAI technique that addresses this gap. PatternLocal begins with a locally linear surrogate, e.g., LIME, KernelSHAP, or gradient-based methods, and transforms the resulting discriminative model weights into a generative representation, thereby suppressing the influence of suppressor variables while preserving local fidelity. In extensive hyperparameter optimization on the XAI-TRIS benchmark, PatternLocal consistently outperformed other XAI methods and reduced false-positive attributions when explaining non-linear tasks, thereby enabling more reliable and actionable insights. We further evaluate PatternLocal on an EEG motor imagery dataset, demonstrating physiologically plausible explanations.},
 author = {Anders Gjølbye and Stefan Haufe and Lars Kai Hansen},
 comment = {Accepted at NeurIPS 2025. Code: https://github.com/gjoelbye/PatternLocal},
 doi = {},
 eprint = {2505.11210v3},
 journal = {arXiv preprint},
 title = {Minimizing False-Positive Attributions in Explanations of Non-Linear Models},
 url = {http://arxiv.org/abs/2505.11210v3},
 year = {2025}
}

@article{2505.12437v2,
 abstract = {Graph neural networks have become the de facto model for learning from structured data. However, the decision-making process of GNNs remains opaque to the end user, which undermines their use in safety-critical applications. Several explainable AI techniques for graphs have been developed to address this major issue. Focusing on graph classification, these explainers identify subgraph motifs that explain predictions. Therefore, a robust benchmarking of graph explainers is required to ensure that the produced explanations are of high quality, i.e., aligned with the GNN's decision process. However, current graph-XAI benchmarks are limited to simplistic synthetic datasets or a few real-world tasks curated by domain experts, hindering rigorous and reproducible evaluation, and consequently stalling progress in the field. To overcome these limitations, we propose a method to automate the construction of graph XAI benchmarks from generic graph classification datasets. Our approach leverages the Weisfeiler-Leman color refinement algorithm to efficiently perform approximate subgraph matching and mine class-discriminating motifs, which serve as proxy ground-truth class explanations. At the same time, we ensure that these motifs can be learned by GNNs because their discriminating power aligns with WL expressiveness. This work also introduces the OpenGraphXAI benchmark suite, which consists of 15 ready-made graph-XAI datasets derived by applying our method to real-world molecular classification datasets. The suite is available to the public along with a codebase to generate over 2,000 additional graph-XAI benchmarks. Finally, we present a use case that illustrates how the suite can be used to assess the effectiveness of a selection of popular graph explainers, demonstrating the critical role of a sufficiently large benchmark collection for improving the significance of experimental results.},
 author = {Michele Fontanesi and Alessio Micheli and Marco Podda and Domenico Tortorella},
 comment = {},
 doi = {},
 eprint = {2505.12437v2},
 journal = {arXiv preprint},
 title = {A method for the systematic generation of graph XAI benchmarks via Weisfeiler-Leman coloring},
 url = {http://arxiv.org/abs/2505.12437v2},
 year = {2025}
}

@article{2505.13118v1,
 abstract = {Cooperative game theory methods, notably Shapley values, have significantly enhanced machine learning (ML) interpretability. However, existing explainable AI (XAI) frameworks mainly attribute average model predictions, overlooking predictive uncertainty. This work addresses that gap by proposing a novel, model-agnostic uncertainty attribution (UA) method grounded in conformal prediction (CP). By defining cooperative games where CP interval properties-such as width and bounds-serve as value functions, we systematically attribute predictive uncertainty to input features. Extending beyond the traditional Shapley values, we use the richer class of Harsanyi allocations, and in particular the proportional Shapley values, which distribute attribution proportionally to feature importance. We propose a Monte Carlo approximation method with robust statistical guarantees to address computational feasibility, significantly improving runtime efficiency. Our comprehensive experiments on synthetic benchmarks and real-world datasets demonstrate the practical utility and interpretative depth of our approach. By combining cooperative game theory and conformal prediction, we offer a rigorous, flexible toolkit for understanding and communicating predictive uncertainty in high-stakes ML applications.},
 author = {Marouane Il Idrissi and Agathe Fernandes Machado and Ewen Gallic and Arthur Charpentier},
 comment = {},
 doi = {},
 eprint = {2505.13118v1},
 journal = {arXiv preprint},
 title = {Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals},
 url = {http://arxiv.org/abs/2505.13118v1},
 year = {2025}
}

@article{2505.13324v1,
 abstract = {Counterfactuals play a pivotal role in the two distinct data science fields of causal inference (CI) and explainable artificial intelligence (XAI). While the core idea behind counterfactuals remains the same in both fields--the examination of what would have happened under different circumstances--there are key differences in how they are used and interpreted. We introduce a formal definition that encompasses the multi-faceted concept of the counterfactual in CI and XAI. We then discuss how counterfactuals are used, evaluated, generated, and operationalized in CI vs. XAI, highlighting conceptual and practical differences. By comparing and contrasting the two, we hope to identify opportunities for cross-fertilization across CI and XAI.},
 author = {Galit Shmueli and David Martens and Jaewon Yoo and Travis Greene},
 comment = {},
 doi = {},
 eprint = {2505.13324v1},
 journal = {arXiv preprint},
 title = {From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI},
 url = {http://arxiv.org/abs/2505.13324v1},
 year = {2025}
}

@article{2505.14428v1,
 abstract = {The objective of this proposal is to bridge the gap between Deep Learning (DL) and System Dynamics (SD) by developing an interpretable neural system dynamics framework. While DL excels at learning complex models and making accurate predictions, it lacks interpretability and causal reliability. Traditional SD approaches, on the other hand, provide transparency and causal insights but are limited in scalability and require extensive domain knowledge. To overcome these limitations, this project introduces a Neural System Dynamics pipeline, integrating Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. This framework combines the predictive power of DL with the interpretability of traditional SD models, resulting in both causal reliability and scalability. The efficacy of the proposed pipeline will be validated through real-world applications of the EU-funded AutoMoTIF project, which is focused on autonomous multimodal transportation systems. The long-term goal is to collect actionable insights that support the integration of explainability and safety in autonomous systems.},
 author = {Riccardo D'Elia},
 comment = {To be submitted to CEUR-WS.org for publication in the Doctoral Consortium Proceedings of XAI 2025, The World Conference on Explainable Artificial Intelligence},
 doi = {},
 eprint = {2505.14428v1},
 journal = {arXiv preprint},
 title = {Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications},
 url = {http://arxiv.org/abs/2505.14428v1},
 year = {2025}
}

@article{2505.14510v3,
 abstract = {As machine learning models and autonomous agents are increasingly deployed in high-stakes, real-world domains such as healthcare, security, finance, and robotics, the need for transparent and trustworthy explanations has become critical. To ensure end-to-end transparency of AI decisions, we need models that are not only accurate but also fully explainable and human-tunable. We introduce BACON, a novel framework for automatically training explainable AI models for decision making problems using graded logic. BACON achieves high predictive accuracy while offering full structural transparency and precise, logic-based symbolic explanations, enabling effective human-AI collaboration and expert-guided refinement. We evaluate BACON with a diverse set of scenarios: classic Boolean approximation, Iris flower classification, house purchasing decisions and breast cancer diagnosis. In each case, BACON provides high-performance models while producing compact, human-verifiable decision logic. These results demonstrate BACON's potential as a practical and principled approach for delivering crisp, trustworthy explainable AI.},
 author = {Haishi Bai and Jozo Dujmovic and Jianwu Wang},
 comment = {},
 doi = {},
 eprint = {2505.14510v3},
 journal = {arXiv preprint},
 title = {BACON: A fully explainable AI model with graded logic for decision making problems},
 url = {http://arxiv.org/abs/2505.14510v3},
 year = {2025}
}

@article{2505.14659v1,
 abstract = {As healthcare systems increasingly adopt advanced wireless networks and connected devices, securing medical applications has become critical. The integration of Internet of Medical Things devices, such as robotic surgical tools, intensive care systems, and wearable monitors has enhanced patient care but introduced serious security risks. Cyberattacks on these devices can lead to life threatening consequences, including surgical errors, equipment failure, and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative role in healthcare through AI and cloud integration, it also raises new security concerns. This paper explores how explainable AI techniques like SHAP, LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve trust and transparency in 6G enabled healthcare. We support our approach with experimental analysis and highlight promising results.},
 author = {Navneet Kaur and Lav Gupta},
 comment = {},
 doi = {},
 eprint = {2505.14659v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks},
 url = {http://arxiv.org/abs/2505.14659v1},
 year = {2025}
}

@article{2505.14745v2,
 abstract = {Composites are amongst the most important materials manufactured today, as evidenced by their use in countless applications. In order to establish the suitability of composites in specific applications, finite element (FE) modelling, a numerical method based on partial differential equations, is the industry standard for assessing their mechanical properties. However, FE modelling is exceptionally costly from a computational viewpoint, a limitation which has led to efforts towards applying AI models to this task. However, in these approaches: the chosen model architectures were rudimentary, feed-forward neural networks giving limited accuracy; the studies focused on predicting elastic mechanical properties, without considering material strength limits; and the models lacked transparency, hindering trustworthiness by users. In this paper, we show that convolutional neural networks (CNNs) equipped with methods from explainable AI (XAI) can be successfully deployed to solve this problem. Our approach uses customised CNNs trained on a dataset we generate using transverse tension tests in FE modelling to predict composites' mechanical properties, i.e., Young's modulus and yield strength. We show empirically that our approach achieves high accuracy, outperforming a baseline, ResNet-34, in estimating the mechanical properties. We then use SHAP and Integrated Gradients, two post-hoc XAI methods, to explain the predictions, showing that the CNNs use the critical geometrical features that influence the composites' behaviour, thus allowing engineers to verify that the models are trustworthy by representing the science of composites.},
 author = {Varun Raaghav and Dimitrios Bikos and Antonio Rago and Francesca Toni and Maria Charalambides},
 comment = {9 pages, 6 figures. Accepted for publication at The 14th Conference on Prestigious Applications of Intelligent Systems (PAIS-2025)},
 doi = {},
 eprint = {2505.14745v2},
 journal = {arXiv preprint},
 title = {Explainable Prediction of the Mechanical Properties of Composites with CNNs},
 url = {http://arxiv.org/abs/2505.14745v2},
 year = {2025}
}

@article{2505.15516v1,
 abstract = {While eXplainable AI (XAI) has advanced significantly, few methods address interpretability in embedded vector spaces where dimensions represent complex abstractions. We introduce Distance Explainer, a novel method for generating local, post-hoc explanations of embedded spaces in machine learning models. Our approach adapts saliency-based techniques from RISE to explain the distance between two embedded data points by assigning attribution values through selective masking and distance-ranked mask filtering. We evaluate Distance Explainer on cross-modal embeddings (image-image and image-caption pairs) using established XAI metrics including Faithfulness, Sensitivity/Robustness, and Randomization. Experiments with ImageNet and CLIP models demonstrate that our method effectively identifies features contributing to similarity or dissimilarity between embedded data points while maintaining high robustness and consistency. We also explore how parameter tuning, particularly mask quantity and selection strategy, affects explanation quality. This work addresses a critical gap in XAI research and enhances transparency and trustworthiness in deep learning applications utilizing embedded spaces.},
 author = {Christiaan Meijer and E. G. Patrick Bos},
 comment = {33 pages, 19 figures. Submitted to JMLR. Method implementation: https://research-software-directory.org/software/distance-explainer},
 doi = {},
 eprint = {2505.15516v1},
 journal = {arXiv preprint},
 title = {Explainable embeddings with Distance Explainer},
 url = {http://arxiv.org/abs/2505.15516v1},
 year = {2025}
}

@article{2505.16103v1,
 abstract = {Keylogger detection involves monitoring for unusual system behaviors such as delays between typing and character display, analyzing network traffic patterns for data exfiltration. In this study, we provide a comprehensive analysis for keylogger detection with traditional machine learning models - SVC, Random Forest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes and advanced ensemble methods including Stacking, Blending and Voting. Moreover, feature selection approaches such as Information gain, Lasso L1 and Fisher Score are thoroughly assessed to improve predictive performance and lower computational complexity. The Keylogger Detection dataset from publicly available Kaggle website is used in this project. In addition to accuracy-based classification, this study implements the approach for model interpretation using Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to deliver finer explanations for how much each feature contributes in assisting or hindering the detection process. To evaluate the models result, we have used AUC score, sensitivity, Specificity, Accuracy and F1 score. The best performance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99, 100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is near-perfect classification with Fisher Score.},
 author = {Monirul Islam Mahmud},
 comment = {},
 doi = {},
 eprint = {2505.16103v1},
 journal = {arXiv preprint},
 title = {Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI},
 url = {http://arxiv.org/abs/2505.16103v1},
 year = {2025}
}

@article{2505.21513v1,
 abstract = {Machine learning models achieve high precision, but their decision-making processes often lack explainability. Furthermore, as model complexity increases, explainability typically decreases. Existing efforts to improve explainability primarily involve developing new eXplainable artificial intelligence (XAI) techniques or incorporating explainability constraints during training. While these approaches yield specific improvements, their applicability remains limited. In this work, we propose the Vision Transformer with artificial Astrocytes (ViTA). This training-free approach is inspired by neuroscience and enhances the reasoning of a pretrained deep neural network to generate more human-aligned explanations. We evaluated our approach employing two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the similarity between the heatmaps produced by the XAI techniques and a (human-aligned) ground truth. Our results consistently demonstrate that incorporating artificial astrocytes enhances the alignment of model explanations with human perception, leading to statistically significant improvements across all XAI techniques and metrics utilized.},
 author = {Nicolas Echevarrieta-Catalan and Ana Ribas-Rodriguez and Francisco Cedron and Odelia Schwartz and Vanessa Aguiar-Pulido},
 comment = {LXCV Workshop at IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) 2025},
 doi = {},
 eprint = {2505.21513v1},
 journal = {arXiv preprint},
 title = {Enhancing Vision Transformer Explainability Using Artificial Astrocytes},
 url = {http://arxiv.org/abs/2505.21513v1},
 year = {2025}
}

@article{2505.21589v1,
 abstract = {From uncertainty quantification to real-world object detection, we recognize the importance of machine learning algorithms, particularly in safety-critical domains such as autonomous driving or medical diagnostics. In machine learning, ambiguous data plays an important role in various machine learning domains. Optical illusions present a compelling area of study in this context, as they offer insight into the limitations of both human and machine perception. Despite this relevance, optical illusion datasets remain scarce. In this work, we introduce a novel dataset of optical illusions featuring intermingled animal pairs designed to evoke perceptual ambiguity. We identify generalizable visual concepts, particularly gaze direction and eye cues, as subtle yet impactful features that significantly influence model accuracy. By confronting models with perceptual ambiguity, our findings underscore the importance of concepts in visual learning and provide a foundation for studying bias and alignment between human and machine vision. To make this dataset useful for general purposes, we generate optical illusions systematically with different concepts discussed in our bias mitigation section. The dataset is accessible in Kaggle via https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333. Our source code can be found at https://github.com/KDD-OpenSource/Ambivision.git.},
 author = {Carina Newen and Luca Hinkamp and Maria Ntonti and Emmanuel Müller},
 comment = {19 pages, 18 figures},
 doi = {},
 eprint = {2505.21589v1},
 journal = {arXiv preprint},
 title = {Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI},
 url = {http://arxiv.org/abs/2505.21589v1},
 year = {2025}
}

@article{2505.22252v1,
 abstract = {Understanding the reasoning behind deep learning model predictions is crucial in cheminformatics and drug discovery, where molecular design determines their properties. However, current evaluation frameworks for Explainable AI (XAI) in this domain often rely on artificial datasets or simplified tasks, employing data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness. To address this, we introduce B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through a comprehensive evaluation using B-XAIC, we reveal limitations of existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain. This benchmark provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models.},
 author = {Magdalena Proszewska and Tomasz Danel and Dawid Rymarczyk},
 comment = {26 pages, 16 figures, 5 tables},
 doi = {},
 eprint = {2505.22252v1},
 journal = {arXiv preprint},
 title = {B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data},
 url = {http://arxiv.org/abs/2505.22252v1},
 year = {2025}
}

@article{2505.22541v1,
 abstract = {Deep neural networks form the backbone of artificial intelligence research, with potential to transform the human experience in areas ranging from autonomous driving to personal assistants, healthcare to education. However, their integration into the daily routines of real-world classrooms remains limited. It is not yet common for a teacher to assign students individualized homework targeting their specific weaknesses, provide students with instant feedback, or simulate student responses to a new exam question. While these models excel in predictive performance, this lack of adoption can be attributed to a significant weakness: the lack of explainability of model decisions, leading to a lack of trust from students, parents, and teachers. This thesis aims to bring human needs to the forefront of eXplainable AI (XAI) research, grounded in the concrete use case of personalized learning and teaching. We frame the contributions along two verticals: technical advances in XAI and their aligned human studies. We investigate explainability in AI for education, revealing systematic disagreements between post-hoc explainers and identifying a need for inherently interpretable model architectures. We propose four novel technical contributions in interpretability with a multimodal modular architecture (MultiModN), an interpretable mixture-of-experts model (InterpretCC), adversarial training for explainer stability, and a theory-driven LLM-XAI framework to present explanations to students (iLLuMinaTE), which we evaluate in diverse settings with professors, teachers, learning scientists, and university students. By combining empirical evaluations of existing explainers with novel architectural designs and human studies, our work lays a foundation for human-centric AI systems that balance state-of-the-art performance with built-in transparency and trust.},
 author = {Vinitra Swamy},
 comment = {PhD Thesis, EPFL (Computer Science)},
 doi = {},
 eprint = {2505.22541v1},
 journal = {arXiv preprint},
 title = {A Human-Centric Approach to Explainable AI for Personalized Education},
 url = {http://arxiv.org/abs/2505.22541v1},
 year = {2025}
}

@article{2505.23700v2,
 abstract = {Counterfactual explanations play a pivotal role in explainable artificial intelligence (XAI) by offering intuitive, human-understandable alternatives that elucidate machine learning model decisions. Despite their significance, existing methods for generating counterfactuals often require constant access to the predictive model, involve computationally intensive optimization for each instance and lack the flexibility to adapt to new user-defined constraints without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic, conditional generative framework that produces multiple diverse counterfactuals in a single forward pass. Leveraging conditional normalizing flows trained solely on labeled data, DiCoFlex addresses key limitations by enabling real-time user-driven customization of constraints such as sparsity and actionability at inference time. Extensive experiments on standard benchmark datasets show that DiCoFlex outperforms existing methods in terms of validity, diversity, proximity, and constraint adherence, making it a practical and scalable solution for counterfactual generation in sensitive decision-making domains.},
 author = {Oleksii Furman and Ulvi Movsum-zada and Patryk Marszalek and Maciej Zięba and Marek Śmieja},
 comment = {},
 doi = {},
 eprint = {2505.23700v2},
 journal = {arXiv preprint},
 title = {DiCoFlex: Model-agnostic diverse counterfactuals with flexible control},
 url = {http://arxiv.org/abs/2505.23700v2},
 year = {2025}
}

@article{2505.23917v2,
 abstract = {We propose a method for discovering and visualizing the differences between two learned representations, enabling more direct and interpretable model comparisons. We validate our method, which we call Representational Differences Explanations (RDX), by using it to compare models with known conceptual differences and demonstrate that it recovers meaningful distinctions where existing explainable AI (XAI) techniques fail. Applied to state-of-the-art models on challenging subsets of the ImageNet and iNaturalist datasets, RDX reveals both insightful representational differences and subtle patterns in the data. Although comparison is a cornerstone of scientific analysis, current tools in machine learning, namely post hoc XAI methods, struggle to support model comparison effectively. Our work addresses this gap by introducing an effective and explainable tool for contrasting model representations.},
 author = {Neehar Kondapaneni and Oisin Mac Aodha and Pietro Perona},
 comment = {9 pages, 6 figures, 21 supplementary pages, 14 supp figs},
 doi = {},
 eprint = {2505.23917v2},
 journal = {arXiv preprint},
 title = {Representational Difference Explanations},
 url = {http://arxiv.org/abs/2505.23917v2},
 year = {2025}
}

@article{2505.24612v1,
 abstract = {Explainability is crucial for improving the transparency of black-box machine learning models. With the advancement of explanation methods such as LIME and SHAP, various XAI performance metrics have been developed to evaluate the quality of explanations. However, different explainers can provide contrasting explanations for the same prediction, introducing trade-offs across conflicting quality metrics. Although available aggregation approaches improve robustness, reducing explanations' variability, very limited research employed a multi-criteria decision-making approach. To address this gap, this paper introduces a multi-criteria rank-based weighted aggregation method that balances multiple quality metrics simultaneously to produce an ensemble of explanation models. Furthermore, we propose rank-based versions of existing XAI metrics (complexity, faithfulness and stability) to better evaluate ranked feature importance explanations. Extensive experiments on publicly available datasets demonstrate the robustness of the proposed model across these metrics. Comparative analyses of various multi-criteria decision-making and rank aggregation algorithms showed that TOPSIS and WSUM are the best candidates for this use case.},
 author = {Sujoy Chatterjee and Everton Romanzini Colombo and Marcos Medeiros Raimundo},
 comment = {Accepted at the 2025 International Joint Conference on Neural Networks (IJCNN)},
 doi = {},
 eprint = {2505.24612v1},
 journal = {arXiv preprint},
 title = {Multi-criteria Rank-based Aggregation for Explainable AI},
 url = {http://arxiv.org/abs/2505.24612v1},
 year = {2025}
}

@article{2506.01059v1,
 abstract = {Feature attribution (FA) methods are widely used in explainable AI (XAI) to help users understand how the inputs of a machine learning model contribute to its outputs. However, different FA models often provide disagreeing importance scores for the same model. In the absence of ground truth or in-depth knowledge about the inner workings of the model, it is often difficult to meaningfully determine which of the different FA methods produce more suitable explanations in different contexts. As a step towards addressing this issue, we introduce the open-source XAI-Units benchmark, specifically designed to evaluate FA methods against diverse types of model behaviours, such as feature interactions, cancellations, and discontinuous outputs. Our benchmark provides a set of paired datasets and models with known internal mechanisms, establishing clear expectations for desirable attribution scores. Accompanied by a suite of built-in evaluation metrics, XAI-Units streamlines systematic experimentation and reveals how FA methods perform against distinct, atomic kinds of model reasoning, similar to unit tests in software engineering. Crucially, by using procedurally generated models tied to synthetic datasets, we pave the way towards an objective and reliable comparison of FA methods.},
 author = {Jun Rui Lee and Sadegh Emami and Michael David Hollins and Timothy C. H. Wong and Carlos Ignacio Villalobos Sánchez and Francesca Toni and Dekai Zhang and Adam Dejl},
 comment = {Accepted at FAccT 2025},
 doi = {10.1145/3715275.3732186},
 eprint = {2506.01059v1},
 journal = {arXiv preprint},
 title = {XAI-Units: Benchmarking Explainability Methods with Unit Tests},
 url = {http://arxiv.org/abs/2506.01059v1},
 year = {2025}
}

@article{2506.01662v1,
 abstract = {As AI regulations around the world intensify their focus on system safety, contestability has become a mandatory, yet ill-defined, safeguard. In XAI, "contestability" remains an empty promise: no formal definition exists, no algorithm guarantees it, and practitioners lack concrete guidance to satisfy regulatory requirements. Grounded in a systematic literature review, this paper presents the first rigorous formal definition of contestability in explainable AI, directly aligned with stakeholder requirements and regulatory mandates. We introduce a modular framework of by-design and post-hoc mechanisms spanning human-centered interfaces, technical architectures, legal processes, and organizational workflows. To operationalize our framework, we propose the Contestability Assessment Scale, a composite metric built on more than twenty quantitative criteria. Through multiple case studies across diverse application domains, we reveal where state-of-the-art systems fall short and show how our framework drives targeted improvements. By converting contestability from regulatory theory into a practical framework, our work equips practitioners with the tools to embed genuine recourse and accountability into AI systems.},
 author = {Catarina Moreira and Anna Palatkina and Dacia Braca and Dylan M. Walsh and Peter J. Leihn and Fang Chen and Nina C. Hubig},
 comment = {},
 doi = {},
 eprint = {2506.01662v1},
 journal = {arXiv preprint},
 title = {Explainable AI Systems Must Be Contestable: Here's How to Make It Happen},
 url = {http://arxiv.org/abs/2506.01662v1},
 year = {2025}
}

@article{2506.03267v1,
 abstract = {A prevailing approach to explain time series models is to generate attribution in time domain. A recent development in time series XAI is the concept of explanation spaces, where any model trained in the time domain can be interpreted with any existing XAI method in alternative domains, such as frequency. The prevailing approach is to present XAI attributions either in the time domain or in the domain where the attribution is most sparse. In this paper, we demonstrate that in certain cases, XAI methods can generate attributions that highlight fundamentally different features in the time and frequency domains that are not direct counterparts of one another. This suggests that both domains' attributions should be presented to achieve a more comprehensive interpretation. Thus it shows the necessity of multi-domain explanation. To quantify when such cases arise, we introduce the uncertainty principle (UP), originally developed in quantum mechanics and later studied in harmonic analysis and signal processing, to the XAI literature. This principle establishes a lower bound on how much a signal can be simultaneously localized in both the time and frequency domains. By leveraging this concept, we assess whether attributions in the time and frequency domains violate this bound, indicating that they emphasize distinct features. In other words, UP provides a sufficient condition that the time and frequency domain explanations do not match and, hence, should be both presented to the end user. We validate the effectiveness of this approach across various deep learning models, XAI methods, and a wide range of classification and forecasting datasets. The frequent occurrence of UP violations across various datasets and XAI methods highlights the limitations of existing approaches that focus solely on time-domain explanations. This underscores the need for multi-domain explanations as a new paradigm.},
 author = {Shahbaz Rezaei and Avishai Halev and Xin Liu},
 comment = {},
 doi = {},
 eprint = {2506.03267v1},
 journal = {arXiv preprint},
 title = {On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models},
 url = {http://arxiv.org/abs/2506.03267v1},
 year = {2025}
}

@article{2506.05035v1,
 abstract = {Recent explainable artificial intelligence (XAI) methods for time series primarily estimate point-wise attribution magnitudes, while overlooking the directional impact on predictions, leading to suboptimal identification of significant points. Our analysis shows that conventional Integrated Gradients (IG) effectively capture critical points with both positive and negative impacts on predictions. However, current evaluation metrics fail to assess this capability, as they inadvertently cancel out opposing feature contributions. To address this limitation, we propose novel evaluation metrics-Cumulative Prediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to systematically assess whether attribution methods accurately identify significant positive and negative points in time series XAI. Under these metrics, conventional IG outperforms recent counterparts. However, directly applying IG to time series data may lead to suboptimal outcomes, as generated paths ignore temporal relationships and introduce out-of-distribution samples. To overcome these challenges, we introduce TIMING, which enhances IG by incorporating temporal awareness while maintaining its theoretical properties. Extensive experiments on synthetic and real-world time series benchmarks demonstrate that TIMING outperforms existing time series XAI baselines. Our code is available at https://github.com/drumpt/TIMING.},
 author = {Hyeongwon Jang and Changhun Kim and Eunho Yang},
 comment = {ICML 2025 Spotlight Presentation; Code is available at https://github.com/drumpt/TIMING},
 doi = {},
 eprint = {2506.05035v1},
 journal = {arXiv preprint},
 title = {TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation},
 url = {http://arxiv.org/abs/2506.05035v1},
 year = {2025}
}

@article{2506.05286v1,
 abstract = {Transparency is a paramount concern in the medical field, prompting researchers to delve into the realm of explainable AI (XAI). Among these XAI methods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent space to human-understandable high-level concepts by generating a conceptual layer for extracting conceptual features, which has drawn much attention recently. However, existing methods rely solely on concept features to determine the model's predictions, which overlook the intrinsic feature embeddings within medical images. To address this utility gap between the original models and concept-based models, we propose Vision Concept Transformer (VCT). Furthermore, despite their benefits, CBMs have been found to negatively impact model performance and fail to provide stable explanations when faced with input perturbations, which limits their application in the medical field. To address this faithfulness issue, this paper further proposes the Stable Vision Concept Transformer (SVCT) based on VCT, which leverages the vision transformer (ViT) as its backbone and incorporates a conceptual layer. SVCT employs conceptual features to enhance decision-making capabilities by fusing them with image features and ensures model faithfulness through the integration of Denoised Diffusion Smoothing. Comprehensive experiments on four medical datasets demonstrate that our VCT and SVCT maintain accuracy while remaining interpretable compared to baselines. Furthermore, even when subjected to perturbations, our SVCT model consistently provides faithful explanations, thus meeting the needs of the medical field.},
 author = {Lijie Hu and Songning Lai and Yuan Hua and Shu Yang and Jingfeng Zhang and Di Wang},
 comment = {arXiv admin note: text overlap with arXiv:2304.06129 by other authors},
 doi = {},
 eprint = {2506.05286v1},
 journal = {arXiv preprint},
 title = {Stable Vision Concept Transformers for Medical Diagnosis},
 url = {http://arxiv.org/abs/2506.05286v1},
 year = {2025}
}

@article{2506.05958v1,
 abstract = {Water reuse is a key point when fresh water is a commodity in ever greater demand, but which is also becoming ever more available. Furthermore, the return of clean water to its natural environment is also mandatory. Therefore, wastewater treatment plants (WWTPs) are essential in any policy focused on these serious challenges.
  WWTPs are complex facilities which need to operate at their best to achieve their goals. Nowadays, they are largely monitored, generating large databases of historical data concerning their functioning over time. All this implies a large amount of embedded information which is not usually easy for plant managers to assimilate, correlate and understand; in other words, for them to know the global operation of the plant at any given time. At this point, the intelligent and Machine Learning (ML) approaches can give support for that need, managing all the data and translating them into manageable, interpretable and explainable knowledge about how the WWTP plant is operating at a glance.
  Here, an eXplainable Artificial Intelligence (XAI) based methodology is proposed and tested for a real WWTP, in order to extract explainable service knowledge concerning the operation modes of the WWTP managed by AQUAVALL, which is the public service in charge of the integral water cycle in the City Council of Valladolid (Castilla y León, Spain). By applying well-known approaches of XAI and ML focused on the challenge of WWTP, it has been possible to summarize a large number of historical databases through a few explained operation modes of the plant in a low-dimensional data space, showing the variables and facility units involved in each case.},
 author = {Alicia Beneyto-Rodriguez and Gregorio I. Sainz-Palmero and Marta Galende-Hernández and María J. Fuente and José M. Cuenca},
 comment = {},
 doi = {},
 eprint = {2506.05958v1},
 journal = {arXiv preprint},
 title = {Applying XAI based unsupervised knowledge discovering for Operation modes in a WWTP. A real case: AQUAVALL WWTP},
 url = {http://arxiv.org/abs/2506.05958v1},
 year = {2025}
}

@article{2506.06345v1,
 abstract = {Financial literacy is increasingly dependent on the ability to interpret complex financial data and utilize advanced forecasting tools. In this context, this study proposes a novel approach that combines transformer-based time series models with explainable artificial intelligence (XAI) to enhance the interpretability and accuracy of stock price predictions. The analysis focuses on the daily stock prices of the five highest-volume banks listed in the BIST100 index, along with XBANK and XU100 indices, covering the period from January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are employed, with input features enriched by technical indicators. SHAP and LIME techniques are used to provide transparency into the influence of individual features on model outputs. The results demonstrate the strong predictive capabilities of transformer models and highlight the potential of interpretable machine learning to empower individuals in making informed investment decisions and actively engaging in financial markets.},
 author = {Sukru Selim Calik and Andac Akyuz and Zeynep Hilal Kilimci and Kerem Colak},
 comment = {},
 doi = {},
 eprint = {2506.06345v1},
 journal = {arXiv preprint},
 title = {Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100},
 url = {http://arxiv.org/abs/2506.06345v1},
 year = {2025}
}

@article{2506.06680v3,
 abstract = {Infertility has a considerable impact on individuals' quality of life, affecting them socially and psychologically, with projections indicating a rise in the upcoming years. In vitro fertilization (IVF) emerges as one of the primary techniques within economically developed nations, employed to address the rising problem of low fertility. Expert embryologists conventionally grade embryos by reviewing blastocyst images to select the most optimal for transfer, yet this process is time-consuming and lacks efficiency. Blastocyst images provide a valuable resource for assessing embryo viability. In this study, we introduce an explainable artificial intelligence (XAI) framework for classifying embryos, employing a fusion of convolutional neural network (CNN) and long short-term memory (LSTM) architecture, referred to as CNN-LSTM. Utilizing deep learning, our model achieves high accuracy in embryo classification while maintaining interpretability through XAI.},
 author = {Radha Kodali and Venkata Rao Dhulipalla and Venkata Siva Kishor Tatavarty and Madhavi Nadakuditi and Bharadwaj Thiruveedhula and Suryanarayana Gunnam and Durga Prasad Bavirisetti and Gogulamudi Pradeep Reddy},
 comment = {},
 doi = {},
 eprint = {2506.06680v3},
 journal = {arXiv preprint},
 title = {Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment},
 url = {http://arxiv.org/abs/2506.06680v3},
 year = {2025}
}

@article{2506.06701v1,
 abstract = {Deep neural networks, particularly Transformers, have been widely adopted for predicting the functional properties of proteins. In this work, we focus on exploring whether Protein Transformers can capture biological intelligence among protein sequences. To achieve our goal, we first introduce a protein function dataset, namely Protein-FN, providing over 9000 protein data with meaningful labels. Second, we devise a new Transformer architecture, namely Sequence Protein Transformers (SPT), for computationally efficient protein function predictions. Third, we develop a novel Explainable Artificial Intelligence (XAI) technique called Sequence Score, which can efficiently interpret the decision-making processes of protein models, thereby overcoming the difficulty of deciphering biological intelligence bided in Protein Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only 5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3% on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset, all accomplished by training from scratch. Besides, our Sequence Score technique helps reveal that our SPT models can discover several meaningful patterns underlying the sequence structures of protein data, with these patterns aligning closely with the domain knowledge in the biology community. We have officially released our Protein-FN dataset on Hugging Face Datasets https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at https://github.com/fudong03/BioIntelligence.},
 author = {Fudong Lin and Wanrou Du and Jinchan Liu and Tarikul Milon and Shelby Meche and Wu Xu and Xiaoqi Qin and Xu Yuan},
 comment = {Accepted by European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2025)},
 doi = {},
 eprint = {2506.06701v1},
 journal = {arXiv preprint},
 title = {Do Protein Transformers Have Biological Intelligence?},
 url = {http://arxiv.org/abs/2506.06701v1},
 year = {2025}
}

@article{2506.07229v1,
 abstract = {Existing feature attribution methods like SHAP often suffer from global dependence, failing to capture true local model behavior. This paper introduces VARSHAP, a novel model-agnostic local feature attribution method which uses the reduction of prediction variance as the key importance metric of features. Building upon Shapley value framework, VARSHAP satisfies the key Shapley axioms, but, unlike SHAP, is resilient to global data distribution shifts. Experiments on synthetic and real-world datasets demonstrate that VARSHAP outperforms popular methods such as KernelSHAP or LIME, both quantitatively and qualitatively.},
 author = {Mateusz Gajewski and Mikołaj Morzy and Adam Karczmarz and Piotr Sankowski},
 comment = {},
 doi = {},
 eprint = {2506.07229v1},
 journal = {arXiv preprint},
 title = {VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution},
 url = {http://arxiv.org/abs/2506.07229v1},
 year = {2025}
}

@article{2506.07480v1,
 abstract = {Advanced Persistent Threats (APTs) represent a sophisticated and persistent cy-bersecurity challenge, characterized by stealthy, multi-phase, and targeted attacks aimed at compromising information systems over an extended period. Develop-ing an effective Intrusion Detection System (IDS) capable of detecting APTs at different phases relies on selecting network traffic features. However, not all of these features are directly related to the phases of APTs. Some network traffic features may be unrelated or have limited relevance to identifying malicious ac-tivity. Therefore, it is important to carefully select and analyze the most relevant features to improve the IDS performance. This work proposes a feature selection and classification model that integrates two prominent machine learning algo-rithms: SHapley Additive exPlanations (SHAP) and Extreme Gradient Boosting (XGBoost). The aim is to develop lightweight IDS based on a selected minimum number of influential features for detecting APTs at various phases. The pro-posed method also specifies the relevant features for each phase of APTs inde-pendently. Extensive experimental results on the SCVIC-APT-2021 dataset indi-cated that our proposed approach has improved performance compared to other standard techniques. Specifically, both the macro-average F1-score and recall reached 94% and 93 %, respectively, while reducing the complexity of the detec-tion model by selecting only 12 features out of 77.},
 author = {Bassam Noori Shaker and Bahaa Al-Musawi and Mohammed Falih Hassan},
 comment = {},
 doi = {},
 eprint = {2506.07480v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Enhancing IDS Against Advanced Persistent Kill Chain},
 url = {http://arxiv.org/abs/2506.07480v1},
 year = {2025}
}

@article{2506.08338v1,
 abstract = {The use of appropriate methods of Interpretable Machine Learning (IML) and eXplainable Artificial Intelligence (XAI) is essential for adopting black-box predictive models in fields where model and prediction explainability is required. As a novel tool for interpreting black-box models, we introduce the R package midr, which implements Maximum Interpretation Decomposition (MID). MID is a functional decomposition approach that derives a low-order additive representation of a black-box model by minimizing the squared error between the model's prediction function and this additive representation. midr enables learning from black-box models by constructing a global surrogate model with advanced analytical capabilities. After reviewing related work and the theoretical foundation of MID, we demonstrate the package's usage and discuss some of its key features.},
 author = {Ryoichi Asashiba and Reiji Kozuma and Hirokazu Iwasawa},
 comment = {20 pages, 10 figures},
 doi = {},
 eprint = {2506.08338v1},
 journal = {arXiv preprint},
 title = {midr: Learning from Black-Box Models by Maximum Interpretation Decomposition},
 url = {http://arxiv.org/abs/2506.08338v1},
 year = {2025}
}

@article{2506.11790v2,
 abstract = {Evaluating feature attribution methods represents a critical challenge in explainable AI (XAI), as researchers typically rely on perturbation-based metrics when ground truth is unavailable. However, recent work reveals that these evaluation metrics can show different performance across predicted classes within the same dataset. These "class-dependent evaluation effects" raise questions about whether perturbation analysis reliably measures attribution quality, with direct implications for XAI method development and evaluation trustworthiness. We investigate under which conditions these class-dependent effects arise by conducting controlled experiments with synthetic time series data where ground truth feature locations are known. We systematically vary feature types and class contrasts across binary classification tasks, then compare perturbation-based degradation scores with ground truth-based precision-recall metrics using multiple attribution methods. Our experiments demonstrate that class-dependent effects emerge with both evaluation approaches, even in simple scenarios with temporally localized features, triggered by basic variations in feature amplitude or temporal extent between classes. Most critically, we find that perturbation-based and ground truth metrics frequently yield contradictory assessments of attribution quality across classes, with weak correlations between evaluation approaches. These findings suggest that researchers should interpret perturbation-based metrics with care, as they may not always align with whether attributions correctly identify discriminating features. By showing this disconnect, our work points toward reconsidering what attribution evaluation actually measures and developing more rigorous evaluation methods that capture multiple dimensions of attribution quality.},
 author = {Gregor Baer and Isel Grau and Chao Zhang and Pieter Van Gorp},
 comment = {Accepted at TempXAI Workshop @ ECML-PKDD 2025 (Explainable AI for Time Series and Data Streams)},
 doi = {},
 eprint = {2506.11790v2},
 journal = {arXiv preprint},
 title = {Why Do Class-Dependent Evaluation Effects Occur with Time Series Feature Attributions? A Synthetic Data Investigation},
 url = {http://arxiv.org/abs/2506.11790v2},
 year = {2025}
}

@article{2506.11849v1,
 abstract = {With origins in game theory, probabilistic values like Shapley values, Banzhaf values, and semi-values have emerged as a central tool in explainable AI. They are used for feature attribution, data attribution, data valuation, and more. Since all of these values require exponential time to compute exactly, research has focused on efficient approximation methods using two techniques: Monte Carlo sampling and linear regression formulations. In this work, we present a new way of combining both of these techniques. Our approach is more flexible than prior algorithms, allowing for linear regression to be replaced with any function family whose probabilistic values can be computed efficiently. This allows us to harness the accuracy of tree-based models like XGBoost, while still producing unbiased estimates. From experiments across eight datasets, we find that our methods give state-of-the-art performance for estimating probabilistic values. For Shapley values, the error of our methods can be $6.5\times$ lower than Permutation SHAP (the most popular Monte Carlo method), $3.8\times$ lower than Kernel SHAP (the most popular linear regression method), and $2.6\times$ lower than Leverage SHAP (the prior state-of-the-art Shapley value estimator). For more general probabilistic values, we can obtain error $215\times$ lower than the best estimator from prior work.},
 author = {R. Teal Witter and Yurong Liu and Christopher Musco},
 comment = {},
 doi = {},
 eprint = {2506.11849v1},
 journal = {arXiv preprint},
 title = {Regression-adjusted Monte Carlo Estimators for Shapley Values and Probabilistic Values},
 url = {http://arxiv.org/abs/2506.11849v1},
 year = {2025}
}

@article{2506.11882v2,
 abstract = {Effective resource management and network slicing are essential to meet the diverse service demands of vehicular networks, including Enhanced Mobile Broadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC). This paper introduces an Explainable Deep Reinforcement Learning (XRL) framework for dynamic network slicing and resource allocation in vehicular networks, built upon a near-real-time RAN intelligent controller. By integrating a feature-based approach that leverages Shapley values and an attention mechanism, we interpret and refine the decisions of our reinforcementlearning agents, addressing key reliability challenges in vehicular communication systems. Simulation results demonstrate that our approach provides clear, real-time insights into the resource allocation process and achieves higher interpretability precision than a pure attention mechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC services increased from 78.0% to 80.13%, while that for eMBB services improved from 71.44% to 73.21%.},
 author = {Haochen Sun and Yifan Liu and Ahmed Al-Tahmeesschi and Swarna Chetty and Syed Ali Raza Zaidi and Avishek Nag and Hamed Ahmadi},
 comment = {To appear in Proceedings of IEEE PIMRC 2025. 6 pages, 4 figures},
 doi = {},
 eprint = {2506.11882v2},
 journal = {arXiv preprint},
 title = {An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing},
 url = {http://arxiv.org/abs/2506.11882v2},
 year = {2025}
}

@article{2506.12240v1,
 abstract = {Artificial Intelligence (AI) is rapidly embedded in critical decision-making systems, however their foundational ``black-box'' models require eXplainable AI (XAI) solutions to enhance transparency, which are mostly oriented to experts, making no sense to non-experts. Alarming evidence about AI's unprecedented human values risks brings forward the imperative need for transparent human-centered XAI solutions. In this work, we introduce a domain-, model-, explanation-agnostic, generalizable and reproducible framework that ensures both transparency and human-centered explanations tailored to the needs of both experts and non-experts. The framework leverages Large Language Models (LLMs) and employs in-context learning to convey domain- and explainability-relevant contextual knowledge into LLMs. Through its structured prompt and system setting, our framework encapsulates in one response explanations understandable by non-experts and technical information to experts, all grounded in domain and explainability principles. To demonstrate the effectiveness of our framework, we establish a ground-truth contextual ``thesaurus'' through a rigorous benchmarking with over 40 data, model, and XAI combinations for an explainable clustering analysis of a well-being scenario. Through a comprehensive quality and human-friendliness evaluation of our framework's explanations, we prove high content quality through strong correlations with ground-truth explanations (Spearman rank correlation=0.92) and improved interpretability and human-friendliness to non-experts through a user study (N=56). Our overall evaluation confirms trust in LLMs as HCXAI enablers, as our framework bridges the above Gaps by delivering (i) high-quality technical explanations aligned with foundational XAI methods and (ii) clear, efficient, and interpretable human-centered explanations for non-experts.},
 author = {Eva Paraschou and Ioannis Arapakis and Sofia Yfantidou and Sebastian Macaluso and Athena Vakali},
 comment = {Accepted for publication at The 3rd World Conference on eXplainable Artificial Intelligence. This version corresponds to the camera-ready manuscript submitted to the conference proceedings},
 doi = {},
 eprint = {2506.12240v1},
 journal = {arXiv preprint},
 title = {Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing Explainable AI},
 url = {http://arxiv.org/abs/2506.12240v1},
 year = {2025}
}

@article{2506.12250v1,
 abstract = {Classification of ceramic thin sections is fundamental for understanding ancient pottery production techniques, provenance, and trade networks. Although effective, traditional petrographic analysis is time-consuming. This study explores the application of deep learning models, specifically Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), as complementary tools to support the classification of Levantine ceramics based on their petrographic fabrics. A dataset of 1,424 thin section images from 178 ceramic samples belonging to several archaeological sites across the Levantine area, mostly from the Bronze Age, with few samples dating to the Iron Age, was used to train and evaluate these models. The results demonstrate that transfer learning significantly improves classification performance, with a ResNet18 model achieving 92.11% accuracy and a ViT reaching 88.34%. Explainability techniques, including Guided Grad-CAM and attention maps, were applied to interpret and visualize the models' decisions, revealing that both CNNs and ViTs successfully focus on key mineralogical features for the classification of the samples into their respective petrographic fabrics. These findings highlight the potential of explainable AI in archaeometric studies, providing a reproducible and efficient methodology for ceramic analysis while maintaining transparency in model decision-making.},
 author = {Sara Capriotti and Alessio Devoto and Simone Scardapane and Silvano Mignardi and Laura Medeghini},
 comment = {Accepted for publication in Machine Learning: Science and Technology},
 doi = {},
 eprint = {2506.12250v1},
 journal = {arXiv preprint},
 title = {Interpretable Classification of Levantine Ceramic Thin Sections via Neural Networks},
 url = {http://arxiv.org/abs/2506.12250v1},
 year = {2025}
}

@article{2506.12404v2,
 abstract = {Deep learning has significantly propelled the performance of ECG arrhythmia classification, yet its clinical adoption remains hindered by challenges in interpretability and deployment on resource-constrained edge devices. To bridge this gap, we propose EXGnet, a novel and reliable ECG arrhythmia classification network tailored for single-lead signals, specifically designed to balance high accuracy, explainability, and edge compatibility. EXGnet integrates XAI supervision during training via a normalized cross-correlation based loss, directing the model's attention to clinically relevant ECG regions, similar to a cardiologist's focus. This supervision is driven by automatically generated ground truth, derived through an innovative heart rate variability-based approach, without the need for manual annotation. To enhance classification accuracy without compromising deployment simplicity, we incorporate quantitative ECG features during training. These enrich the model with multi-domain knowledge but are excluded during inference, keeping the model lightweight for edge deployment. Additionally, we introduce an innovative multiresolution block to efficiently capture both short and long-term signal features while maintaining computational efficiency. Rigorous evaluation on the Chapman and Ningbo benchmark datasets validates the supremacy of EXGnet, which achieves average five-fold accuracies of 98.762% and 96.932%, and F1-scores of 97.910% and 95.527%, respectively. Comprehensive ablation studies and both quantitative and qualitative interpretability assessment confirm that the XAI guidance is pivotal, demonstrably enhancing the model's focus and trustworthiness. Overall, EXGnet sets a new benchmark by combining high-performance arrhythmia classification with interpretability, paving the way for more trustworthy and accessible portable ECG based health monitoring systems.},
 author = {Tushar Talukder Showrav and Soyabul Islam Lincoln and Md. Kamrul Hasan},
 comment = {17 pages, 8 figures},
 doi = {},
 eprint = {2506.12404v2},
 journal = {arXiv preprint},
 title = {EXGnet: a single-lead explainable-AI guided multiresolution network with train-only quantitative features for trustworthy ECG arrhythmia classification},
 url = {http://arxiv.org/abs/2506.12404v2},
 year = {2025}
}

@article{2506.13727v1,
 abstract = {Large Language Models (LLMs) are central to many contemporary AI applications, yet their extensive parameter counts pose significant challenges for deployment in memory- and compute-constrained environments. Recent works in eXplainable AI (XAI), particularly on attribution methods, suggest that interpretability can also enable model compression by identifying and removing components irrelevant to inference. In this paper, we leverage Layer-wise Relevance Propagation (LRP) to perform attribution-guided pruning of LLMs. While LRP has shown promise in structured pruning for vision models, we extend it to unstructured pruning in LLMs and demonstrate that it can substantially reduce model size with minimal performance loss. Our method is especially effective in extracting task-relevant subgraphs -- so-called ``circuits'' -- which can represent core functions (e.g., indirect object identification). Building on this, we introduce a technique for model correction, by selectively removing circuits responsible for spurious behaviors (e.g., toxic outputs). All in all, we gather these techniques as a uniform holistic framework and showcase its effectiveness and limitations through extensive experiments for compression, circuit discovery and model correction on Llama and OPT models, highlighting its potential for improving both model efficiency and safety. Our code is publicly available at https://github.com/erfanhatefi/SparC3.},
 author = {Sayed Mohammad Vakilzadeh Hatefi and Maximilian Dreyer and Reduan Achtibat and Patrick Kahardipraja and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin},
 comment = {Work in progress (10 pages manuscript, 3 pages references, 12 pages appendix)},
 doi = {},
 eprint = {2506.13727v1},
 journal = {arXiv preprint},
 title = {Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs},
 url = {http://arxiv.org/abs/2506.13727v1},
 year = {2025}
}

@article{2506.13900v1,
 abstract = {Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.},
 author = {Marouane Il Idrissi and Agathe Fernandes Machado and Arthur Charpentier},
 comment = {},
 doi = {},
 eprint = {2506.13900v1},
 journal = {arXiv preprint},
 title = {Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models},
 url = {http://arxiv.org/abs/2506.13900v1},
 year = {2025}
}

@article{2506.13904v1,
 abstract = {Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.
  This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.
  We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.
  The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.},
 author = {Ivania Donoso-Guzmán and Kristýna Sirka Kacafírková and Maxwell Szymanski and An Jacobs and Denis Parra and Katrien Verbert},
 comment = {},
 doi = {},
 eprint = {2506.13904v1},
 journal = {arXiv preprint},
 title = {A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare},
 url = {http://arxiv.org/abs/2506.13904v1},
 year = {2025}
}

@article{2506.14775v2,
 abstract = {As machine learning systems increasingly inform critical decisions, the need for human-understandable explanations grows. Current evaluations of Explainable AI (XAI) often prioritize technical fidelity over cognitive accessibility which critically affects users, in particular those with visual impairments. We propose CUE, a model for Cognitive Understanding of Explanations, linking explanation properties to cognitive sub-processes: legibility (perception), readability (comprehension), and interpretability (interpretation). In a study (N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we found comparable task performance but lower confidence/effort for visually impaired users. Unlike expected, these gaps were not mitigated and sometimes worsened by accessibility-focused color maps like Cividis. These results challenge assumptions about perceptual optimization and support the need for adaptive XAI interfaces. They also validate CUE by demonstrating that altering explanation legibility affects understandability. We contribute: (1) a formalized cognitive model for explanation understanding, (2) an integrated definition of human-centered explanation properties, and (3) empirical evidence motivating accessible, user-tailored XAI.},
 author = {Tobias Labarta and Nhi Hoang and Katharina Weitz and Wojciech Samek and Sebastian Lapuschkin and Leander Weber},
 comment = {10 pages, 5 figures (main text), 4 tables, 455-participant user study},
 doi = {},
 eprint = {2506.14775v2},
 journal = {arXiv preprint},
 title = {See What I Mean? CUE: A Cognitive Model of Understanding Explanations},
 url = {http://arxiv.org/abs/2506.14775v2},
 year = {2025}
}

@article{2506.14777v2,
 abstract = {This article introduces WebXAII, an open-source web framework designed to facilitate research on human interaction with eXplainable Artificial Intelligence (XAI) systems. The field of XAI is rapidly expanding, driven by the growing societal implications of the widespread adoption of AI (and in particular machine learning) across diverse applications. Researchers who study the interaction between humans and XAI techniques typically develop ad hoc interfaces in order to conduct their studies. These interfaces are usually not shared alongside the results of the studies, which limits their reusability and the reproducibility of experiments. In response, we design and implement WebXAII, a web-based platform that can embody full experimental protocols, meaning that it can present all aspects of the experiment to human participants and record their responses. The experimental protocols are translated into a composite architecture of generic views and modules, which offers a lot of flexibility. The architecture is defined in a structured configuration file, so that protocols can be implemented with minimal programming skills. We demonstrate that WebXAII can effectively embody relevant protocols, by reproducing the protocol of a state-of-the-art study of the literature.},
 author = {Jules Leguy and Pierre-Antoine Jean and Felipe Torres Figueroa and Sébastien Harispe},
 comment = {},
 doi = {},
 eprint = {2506.14777v2},
 journal = {arXiv preprint},
 title = {WebXAII: an open-source web framework to study human-XAI interaction},
 url = {http://arxiv.org/abs/2506.14777v2},
 year = {2025}
}

@article{2506.15408v1,
 abstract = {Modern AI systems frequently rely on opaque black-box models, most notably Deep Neural Networks, whose performance stems from complex architectures with millions of learned parameters. While powerful, their complexity poses a major challenge to trustworthiness, particularly due to a lack of transparency. Explainable AI (XAI) addresses this issue by providing human-understandable explanations of model behavior. However, to ensure their usefulness and trustworthiness, such explanations must be rigorously evaluated. Despite the growing number of XAI methods, the field lacks standardized evaluation protocols and consensus on appropriate metrics. To address this gap, we conduct a systematic literature review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a unified framework for the eValuation of XAI (VXAI). We identify 362 relevant publications and aggregate their contributions into 41 functionally similar metric groups. In addition, we propose a three-dimensional categorization scheme spanning explanation type, evaluation contextuality, and explanation quality desiderata. Our framework provides the most comprehensive and structured overview of VXAI to date. It supports systematic metric selection, promotes comparability across methods, and offers a flexible foundation for future extensions.},
 author = {David Dembinsky and Adriano Lucieri and Stanislav Frolov and Hiba Najjar and Ko Watanabe and Andreas Dengel},
 comment = {Submitted to TMLR, under review},
 doi = {},
 eprint = {2506.15408v1},
 journal = {arXiv preprint},
 title = {Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI},
 url = {http://arxiv.org/abs/2506.15408v1},
 year = {2025}
}

@article{2506.16199v1,
 abstract = {Explainable Artificial Intelligence (XAI) plays a critical role in fostering user trust and understanding in AI-driven systems. However, the design of effective XAI interfaces presents significant challenges, particularly for UX professionals who may lack technical expertise in AI or machine learning. Existing explanation methods, such as SHAP, LIME, and counterfactual explanations, often rely on complex technical language and assumptions that are difficult for non-expert users to interpret. To address these gaps, we propose a UX Research (UXR) Playbook for XAI - a practical framework aimed at supporting UX professionals in designing accessible, transparent, and trustworthy AI experiences. Our playbook offers actionable guidance to help bridge the gap between technical explainability methods and user centred design, empowering designers to create AI interactions that foster better understanding, trust, and responsible AI adoption.},
 author = {Mohammad Naiseh and Huseyin Dogan and Stephen Giff and Nan Jiang},
 comment = {},
 doi = {},
 eprint = {2506.16199v1},
 journal = {arXiv preprint},
 title = {Development of a persuasive User Experience Research (UXR) Point of View for Explainable Artificial Intelligence (XAI)},
 url = {http://arxiv.org/abs/2506.16199v1},
 year = {2025}
}

@article{2506.16443v1,
 abstract = {Physics-informed neural networks (PINNs) offer a powerful approach to solving partial differential equations (PDEs), which are ubiquitous in the quantitative sciences. Applied to both forward and inverse problems across various scientific domains, PINNs have recently emerged as a valuable tool in the field of scientific machine learning. A key aspect of their training is that the data -- spatio-temporal points sampled from the PDE's input domain -- are readily available. Influence functions, a tool from the field of explainable AI (XAI), approximate the effect of individual training points on the model, enhancing interpretability. In the present work, we explore the application of influence function-based sampling approaches for the training data. Our results indicate that such targeted resampling based on data attribution methods has the potential to enhance prediction accuracy in physics-informed neural networks, demonstrating a practical application of an XAI method in PINN training.},
 author = {Jonas R. Naujoks and Aleksander Krasowski and Moritz Weckbecker and Galip Ümit Yolcu and Thomas Wiegand and Sebastian Lapuschkin and Wojciech Samek and René P. Klausen},
 comment = {This article was presented at "The 3rd World Conference on eXplainable Artificial Intelligence" (2025)},
 doi = {10.1007/978-3-032-08324-1_17},
 eprint = {2506.16443v1},
 journal = {arXiv preprint},
 title = {Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks},
 url = {http://arxiv.org/abs/2506.16443v1},
 year = {2025}
}

@article{2506.16996v1,
 abstract = {Spatial dependence, referring to the correlation between variable values observed at different geographic locations, is one of the most fundamental characteristics of spatial data. The presence of spatial dependence violates the classical statistical assumption of independent and identically distributed observations and implies a high degree of information redundancy within spatial datasets. However, this redundancy can also be interpreted as structured information, which has been widely leveraged in spatial modeling, prediction, and explanation tasks. With the rise of geospatial big data and the rapid advancement of deep learning and large models, effectively modeling and characterizing spatial dependence has become essential for enhancing the performance of spatial analysis and uncovering latent spatial processes. From a data-driven perspective, this study proposes a novel interpretation: spatial dependence can be understood as the contribution of geographic location -- specifically, latitude and longitude -- to the observed variation in target variables. To validate this hypothesis, we conduct simulation experiments in which data are generated based on known spatial processes. We train machine learning models to predict variable values using only coordinate information. Subsequently, XAI techniques are employed to quantify the contribution of spatial features. The resulting importance scores are then compared with local indicators of spatial association (LISA). Across a range of spatial process settings, we observe consistently high correlations (greater than 0.94) between coordinate-based contributions and LISA values. These findings offer a new data-driven perspective on spatial dependence, bridging traditional spatial statistical approaches with modern machine learning techniques.},
 author = {Chuan Chen and Peng Luo},
 comment = {},
 doi = {},
 eprint = {2506.16996v1},
 journal = {arXiv preprint},
 title = {Reframing Spatial Dependence as Geographic Feature Attribution},
 url = {http://arxiv.org/abs/2506.16996v1},
 year = {2025}
}

@article{2506.17262v1,
 abstract = {Objective: (1) To assess whether ONH biomechanics improves prediction of three progressive visual field loss patterns in glaucoma; (2) to use explainable AI to identify strain-sensitive ONH regions contributing to these predictions.
  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects into four categories based on the presence of specific visual field defects: (1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full superior hemifield defect (N=25), and (4) other/non-specific defects (N=124). Automatic ONH tissue segmentation and digital volume correlation were used to compute IOP-induced neural tissue and lamina cribrosa (LC) strains. Biomechanical and structural features were input to a Geometric Deep Learning model. Three classification tasks were performed to detect: (1) superior nasal step, (2) superior partial arcuate, (3) full superior hemifield defect. For each task, the data were split into 80% training and 20% testing sets. Area under the curve (AUC) was used to assess performance. Explainable AI techniques were employed to highlight the ONH regions most critical to each classification.
  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain improved VF loss prediction beyond morphology alone. The inferior and inferotemporal rim were identified as key strain-sensitive regions, contributing most to visual field loss prediction and showing progressive expansion with increasing disease severity.
  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF loss patterns. Neuroretinal rim, rather than the LC, was the most critical region contributing to model predictions.},
 author = {Thanadet Chuangsuwanich and Monisha E. Nongpiur and Fabian A. Braeu and Tin A. Tun and Alexandre Thiery and Shamira Perera and Ching Lin Ho and Martin Buist and George Barbastathis and Tin Aung and Michaël J. A. Girard},
 comment = {},
 doi = {},
 eprint = {2506.17262v1},
 journal = {arXiv preprint},
 title = {AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma},
 url = {http://arxiv.org/abs/2506.17262v1},
 year = {2025}
}

@article{2506.19383v1,
 abstract = {This paper presents an intelligent and transparent AI-driven system for Credit Risk Assessment using three state-of-the-art ensemble machine learning models combined with Explainable AI (XAI) techniques. The system leverages XGBoost, LightGBM, and Random Forest algorithms for predictive analysis of loan default risks, addressing the challenges of model interpretability using SHAP and LIME. Preprocessing steps include custom imputation, one-hot encoding, and standardization. Class imbalance is managed using SMOTE, and hyperparameter tuning is performed with GridSearchCV. The model is evaluated on multiple performance metrics including ROC-AUC, precision, recall, and F1-score. LightGBM emerges as the most business-optimal model with the highest accuracy and best trade off between approval and default rates. Furthermore, the system generates applicant-specific XAI visual reports and business impact summaries to ensure transparent decision-making.},
 author = {Shreya and Harsh Pathak},
 comment = {15 pages, 8 Figures, 3 Tables},
 doi = {},
 eprint = {2506.19383v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence Credit Risk Assessment using Machine Learning},
 url = {http://arxiv.org/abs/2506.19383v1},
 year = {2025}
}

@article{2506.19732v1,
 abstract = {Neural networks now generate text, images, and speech with billions of parameters, producing a need to know how each neural unit contributes to these high-dimensional outputs. Existing explainable-AI methods, such as SHAP, attribute importance to inputs, but cannot quantify the contributions of neural units across thousands of output pixels, tokens, or logits. Here we close that gap with Multiperturbation Shapley-value Analysis (MSA), a model-agnostic game-theoretic framework. By systematically lesioning combinations of units, MSA yields Shapley Modes, unit-wise contribution maps that share the exact dimensionality of the model's output. We apply MSA across scales, from multi-layer perceptrons to the 56-billion-parameter Mixtral-8x7B and Generative Adversarial Networks (GAN). The approach demonstrates how regularisation concentrates computation in a few hubs, exposes language-specific experts inside the LLM, and reveals an inverted pixel-generation hierarchy in GANs. Together, these results showcase MSA as a powerful approach for interpreting, editing, and compressing deep neural networks.},
 author = {Shrey Dixit and Kayson Fakhar and Fatemeh Hadaeghi and Patrick Mineault and Konrad P. Kording and Claus C. Hilgetag},
 comment = {},
 doi = {},
 eprint = {2506.19732v1},
 journal = {arXiv preprint},
 title = {Who Does What in Deep Learning? Multidimensional Game-Theoretic Attribution of Function of Neural Units},
 url = {http://arxiv.org/abs/2506.19732v1},
 year = {2025}
}

@article{2506.19894v1,
 abstract = {Electricity markets are highly complex, involving lots of interactions and complex dependencies that make it hard to understand the inner workings of the market and what is driving prices. Econometric methods have been developed for this, white-box models, however, they are not as powerful as deep neural network models (DNN). In this paper, we use a DNN to forecast the price and then use XAI methods to understand the factors driving the price dynamics in the market. The objective is to increase our understanding of how different electricity markets work. To do that, we apply explainable methods such as SHAP and Gradient, combined with visual techniques like heatmaps (saliency maps) to analyse the behaviour and contributions of various features across five electricity markets. We introduce the novel concepts of SSHAP values and SSHAP lines to enhance the complex representation of high-dimensional tabular models.},
 author = {Antoine Pesenti and Aidan OSullivan},
 comment = {},
 doi = {10.1016/j.egyai.2025.100532},
 eprint = {2506.19894v1},
 journal = {arXiv preprint},
 title = {Explaining deep neural network models for electricity price forecasting with XAI},
 url = {http://arxiv.org/abs/2506.19894v1},
 year = {2025}
}

@article{2506.20102v2,
 abstract = {The convergence of Information Technology and Operational Technology has exposed Industrial Control Systems to adaptive, intelligent adversaries that render static defenses obsolete. This paper introduces the Adversarial Resilience Co-evolution (ARC) framework, addressing the "Trinity of Trust" comprising model fidelity, data integrity, and analytical resilience. ARC establishes a co-evolutionary arms race within a Fortified Secure Digital Twin (F-SCDT), where a Deep Reinforcement Learning "Red Agent" autonomously discovers attack paths while an ensemble-based "Blue Agent" is continuously hardened against these threats. Experimental validation on the Tennessee Eastman Process (TEP) and Secure Water Treatment (SWaT) testbeds demonstrates superior performance in detecting novel attacks, with F1-scores improving from 0.65 to 0.89 and detection latency reduced from over 1200 seconds to 210 seconds. A comprehensive ablation study reveals that the co-evolutionary process itself contributes a 27% performance improvement. By integrating Explainable AI and proposing a Federated ARC architecture, this work presents a necessary paradigm shift toward dynamic, self-improving security for critical infrastructure.},
 author = {Malikussaid and Sutiyo},
 comment = {6 pages, 2 figures, 4 equations, 1 algorithm, 3 tables, to be published in ISPACS 2025, unabridged version exists as arXiv:2506.20102v1},
 doi = {},
 eprint = {2506.20102v2},
 journal = {arXiv preprint},
 title = {Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox},
 url = {http://arxiv.org/abs/2506.20102v2},
 year = {2025}
}

@article{2506.20589v3,
 abstract = {Recent developments in the Internet of Bio-Nano Things (IoBNT) are laying the groundwork for innovative applications across the healthcare sector. Nanodevices designed to operate within the body, managed remotely via the internet, are envisioned to promptly detect and actuate on potential diseases. In this vision, an inherent challenge arises due to the limited capabilities of individual nanosensors; specifically, nanosensors must communicate with one another to collaborate as a cluster. Aiming to research the boundaries of the clustering capabilities, this survey emphasizes data-driven communication strategies in molecular communication (MC) channels as a means of linking nanosensors. Relying on the flexibility and robustness of machine learning (ML) methods to tackle the dynamic nature of MC channels, the MC research community frequently refers to neural network (NN) architectures. This interdisciplinary research field encompasses various aspects, including the use of NNs to facilitate communication in MC environments, their implementation at the nanoscale, explainable approaches for NNs, and dataset generation for training. Within this survey, we provide a comprehensive analysis of fundamental perspectives on recent trends in NN architectures for MC, the feasibility of their implementation at the nanoscale, applied explainable artificial intelligence (XAI) techniques, and the accessibility of datasets along with best practices for their generation. Additionally, we offer open-source code repositories that illustrate NN-based methods to support reproducible research for key MC scenarios. Finally, we identify emerging research challenges, such as robust NN architectures, biologically integrated NN modules, and scalable training strategies.},
 author = {Jorge Torres Gómez and Pit Hofmann and Lisa Y. Debus and Osman Tugay Başaran and Sebastian Lotter and Roya Khanzadeh and Stefan Angerbauer and Bige Deniz Unluturk and Sergi Abadal and Werner Haselmayr and Frank H. P. Fitzek and Robert Schober and Falko Dressler},
 comment = {Paper submitted to IEEE Communications Surveys & Tutorials},
 doi = {},
 eprint = {2506.20589v3},
 journal = {arXiv preprint},
 title = {Communicating Smartly in Molecular Communication Environments: Neural Networks in the Internet of Bio-Nano Things},
 url = {http://arxiv.org/abs/2506.20589v3},
 year = {2025}
}

@article{2506.20831v1,
 abstract = {Combining attention with recurrence has shown to be valuable in sequence modeling, including hydrological predictions. Here, we explore the strength of Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks in rainfall-runoff modeling. We train ten randomly initialized models, TFT and LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five subsets of the Caravan dataset, each representing catchments in the US, Australia, Brazil, Great Britain, and Chile. Then, the performance of the models, their variability regarding the catchment attributes, and the difference according to the datasets are assessed. Our findings show that TFT slightly outperforms LSTM, especially in simulating the midsection and peak of hydrographs. Furthermore, we show the ability of TFT to handle longer sequences and why it can be a better candidate for higher or larger catchments. Being an explainable AI technique, TFT identifies the key dynamic and static variables, providing valuable scientific insights. However, both TFT and LSTM exhibit a considerable drop in performance with the Caravan dataset, indicating possible data quality issues. Overall, the study highlights the potential of TFT in improving hydrological modeling and understanding.},
 author = {Sinan Rasiya Koya and Tirthankar Roy},
 comment = {},
 doi = {},
 eprint = {2506.20831v1},
 journal = {arXiv preprint},
 title = {Efficacy of Temporal Fusion Transformers for Runoff Simulation},
 url = {http://arxiv.org/abs/2506.20831v1},
 year = {2025}
}

@article{2506.20916v1,
 abstract = {Deep reinforcement learning has been extensively studied in decision-making processes and has demonstrated superior performance over conventional approaches in various fields, including radar resource management (RRM). However, a notable limitation of neural networks is their ``black box" nature and recent research work has increasingly focused on explainable AI (XAI) techniques to describe the rationale behind neural network decisions. One promising XAI method is local interpretable model-agnostic explanations (LIME). However, the sampling process in LIME ignores the correlations between features. In this paper, we propose a modified LIME approach that integrates deep learning (DL) into the sampling process, which we refer to as DL-LIME. We employ DL-LIME within deep reinforcement learning for radar resource management. Numerical results show that DL-LIME outperforms conventional LIME in terms of both fidelity and task performance, demonstrating superior performance with both metrics. DL-LIME also provides insights on which factors are more important in decision making for radar resource management.},
 author = {Ziyang Lu and M. Cenk Gursoy and Chilukuri K. Mohan and Pramod K. Varshney},
 comment = {},
 doi = {},
 eprint = {2506.20916v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning},
 url = {http://arxiv.org/abs/2506.20916v1},
 year = {2025}
}

@article{2506.22443v1,
 abstract = {Rule-based models offer interpretability but struggle with complex data, while deep neural networks excel in performance yet lack transparency. This work investigates a neuro-symbolic rule learning neural network named RL-Net that learns interpretable rule lists through neural optimization, applied for the first time to radar-based hand gesture recognition (HGR). We benchmark RL-Net against a fully transparent rule-based system (MIRA) and an explainable black-box model (XentricAI), evaluating accuracy, interpretability, and user adaptability via transfer learning. Our results show that RL-Net achieves a favorable trade-off, maintaining strong performance (93.03% F1) while significantly reducing rule complexity. We identify optimization challenges specific to rule pruning and hierarchy bias and propose stability-enhancing modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical middle ground between transparency and performance. This study highlights the real-world feasibility of neuro-symbolic models for interpretable HGR and offers insights for extending explainable AI to edge-deployable sensing systems.},
 author = {Sarah Seifi and Tobias Sukianto and Cecilia Carbonelli and Lorenzo Servadei and Robert Wille},
 comment = {8 pages, 3 figures, accepted at the late-breaking work track at the XAI-2025 third World Conference of Explainable AI},
 doi = {},
 eprint = {2506.22443v1},
 journal = {arXiv preprint},
 title = {Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition},
 url = {http://arxiv.org/abs/2506.22443v1},
 year = {2025}
}

@article{2506.23767v2,
 abstract = {Every publicly traded U.S. company files an annual 10-K report containing critical insights into financial health and risk. We propose Tiny eXplainable Risk Assessor (TinyXRA), a lightweight and explainable transformer-based model that automatically assesses company risk from these reports. Unlike prior work that relies solely on the standard deviation of excess returns (adjusted for the Fama-French model), which indiscriminately penalizes both upside and downside risk, TinyXRA incorporates skewness, kurtosis, and the Sortino ratio for more comprehensive risk assessment. We leverage TinyBERT as our encoder to efficiently process lengthy financial documents, coupled with a novel dynamic, attention-based word cloud mechanism that provides intuitive risk visualization while filtering irrelevant terms. This lightweight design ensures scalable deployment across diverse computing environments with real-time processing capabilities for thousands of financial documents which is essential for production systems with constrained computational resources. We employ triplet loss for risk quartile classification, improving over pairwise loss approaches in existing literature by capturing both the direction and magnitude of risk differences. Our TinyXRA achieves state-of-the-art predictive accuracy across seven test years on a dataset spanning 2013-2024, while providing transparent and interpretable risk assessments. We conduct comprehensive ablation studies to evaluate our contributions and assess model explanations both quantitatively by systematically removing highly attended words and sentences, and qualitatively by examining explanation coherence. The paper concludes with findings, practical implications, limitations, and future research directions. Our code is available at https://github.com/Chen-XueWen/TinyXRA.},
 author = {Xue Wen Tan and Stanley Kok},
 comment = {},
 doi = {},
 eprint = {2506.23767v2},
 journal = {arXiv preprint},
 title = {Explainable AI for Comprehensive Risk Assessment for Financial Reports: A Lightweight Hierarchical Transformer Network Approach},
 url = {http://arxiv.org/abs/2506.23767v2},
 year = {2025}
}

@article{2507.01056v1,
 abstract = {Flooding can damage pavement infrastructure significantly, causing both immediate and long-term structural and functional issues. This research investigates how flooding events affect pavement deterioration, specifically focusing on measuring pavement roughness by the International Roughness Index (IRI). To quantify these effects, we utilized 20 years of pavement condition data from TxDOT's PMIS database, which is integrated with flood event data, including duration and spatial extent. Statistical analyses were performed to compare IRI values before and after flooding and to calculate the deterioration rates influenced by flood exposure. Moreover, we applied Explainable Artificial Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of flooding on pavement performance. The results demonstrate that flood-affected pavements experience a more rapid increase in roughness compared to non-flooded sections. These findings emphasize the need for proactive flood mitigation strategies, including improved drainage systems, flood-resistant materials, and preventative maintenance, to enhance pavement resilience in vulnerable regions.},
 author = {Lidan Peng and Lu Gao and Feng Hong and Jingran Sun},
 comment = {},
 doi = {10.3390/buildings15091452},
 eprint = {2507.01056v1},
 journal = {arXiv preprint},
 title = {Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI},
 url = {http://arxiv.org/abs/2507.01056v1},
 year = {2025}
}

@article{2507.01068v2,
 abstract = {This research discusses the critical need for early detection and treatment for early prediction of Freezing of Gaits (FOG) utilizing a wearable sensor technology powered with LoRa communication. The system consisted of an Esp-32 microcontroller, in which the trained model is utilized utilizing the Micromlgen Python library. The research investigates accurate FOG classification based on pertinent clinical data by utilizing machine learning (ML) algorithms like Catboost, XGBoost, and Extra Tree classifiers. The XGBoost could classify with approximately 97% accuracy, along with 96% for the catboost and 90% for the Extra Trees Classifier model. The SHAP analysis interpretability shows that GYR SI degree is the most affecting factor in the prediction of the diseases. These results show the possibility of monitoring and identifying the affected person with tracking location on GPS and providing aid as an assistive technology for aiding the affected. The developed sensor-based technology has great potential for real-world problem solving in the field of healthcare and biomedical technology enhancements.},
 author = {Biplov Paneru},
 comment = {},
 doi = {},
 eprint = {2507.01068v2},
 journal = {arXiv preprint},
 title = {Wearable Sensor-Based IoT XAI Framework for Predicting Freezing of Gait in Parkinsons Disease},
 url = {http://arxiv.org/abs/2507.01068v2},
 year = {2025}
}

@article{2507.01282v1,
 abstract = {The recent boom of large language models (LLMs) has re-ignited the hope that artificial intelligence (AI) systems could aid medical diagnosis. Yet despite dazzling benchmark scores, LLM assistants have yet to deliver measurable improvements at the bedside. This scoping review aims to highlight the areas where AI is limited to make practical contributions in the clinical setting, specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom provide actionable, interpretable guidance, eroding clinician trust. Adjacent use of LLMs by physicians did not result in better diagnostic accuracy or speed. Key limitations trace to the data-driven paradigm: black-box outputs which lack transparency, vulnerability to hallucinations, and weak causal reasoning. Hybrid approaches that combine statistical learning with expert rule-based knowledge, and involve clinicians throughout the process help bring back interpretability. They also fit better with existing clinical workflows, as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking predictions to clinically meaningful causes. This can be done through neuro-symbolic or hybrid AI that combines the language ability of LLMs with human causal expertise. AI researchers have addressed this direction, with explainable AI and neuro-symbolic AI being the next logical steps in further advancement in AI. However, they are still based on data-driven knowledge integration instead of human-in-the-loop approaches. Future research should measure success not only by accuracy but by improvements in clinician understanding, workflow fit, and patient outcomes. A better understanding of what helps improve human-computer interactions is greatly needed for AI systems to become part of clinical practice.},
 author = {Matthew JY Kang and Wenli Yang and Monica R Roberts and Byeong Ho Kang and Charles B Malpas},
 comment = {},
 doi = {},
 eprint = {2507.01282v1},
 journal = {arXiv preprint},
 title = {Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care},
 url = {http://arxiv.org/abs/2507.01282v1},
 year = {2025}
}

@article{2507.02005v1,
 abstract = {This research introduces a unified approach combining Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. It integrates expert-driven feature engineering with algorithmic feature creation to enhance accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient boosting, random forests, and neural networks - were trained using AutoML under three feature schemes: domain-informed, algorithmic, and combined. This allowed a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE $\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $Δσ_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527% within the engineering-relevant 0 - 150 MPa domain. The denser-feature model ($\mathcal M_3$) showed minor gains during training but poorer generalization, while the simpler base-feature model ($\mathcal M_1$) performed comparably, confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress range $Δσ_i$, yield strength $R_{eH}$, and post-weld treatment (TIG dressing vs. as-welded) as dominant predictors. Secondary geometric factors - plate width, throat thickness, stiffener height - also significantly affected fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate, interpretable, and robust fatigue strength models for welded steel structures. It bridges data-driven modeling with engineering validation, enabling AI-assisted design and assessment. Future work will explore probabilistic fatigue life modeling and integration into digital twin environments.},
 author = {Michael A. Kraus and Helen Bartsch},
 comment = {},
 doi = {10.1016/j.ijfatigue.2025.109324},
 eprint = {2507.02005v1},
 journal = {arXiv preprint},
 title = {Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener},
 url = {http://arxiv.org/abs/2507.02005v1},
 year = {2025}
}

@article{2507.02342v2,
 abstract = {This study proposes DeltaSHAP, a novel explainable artificial intelligence (XAI) algorithm specifically designed for online patient monitoring systems. In clinical environments, discovering the causes driving patient risk evolution is critical for timely intervention, yet existing XAI methods fail to address the unique requirements of clinical time series explanation tasks. To this end, DeltaSHAP addresses three key clinical needs: explaining the changes in the consecutive predictions rather than isolated prediction scores, providing both magnitude and direction of feature attributions, and delivering these insights in real time. By adapting Shapley values to temporal settings, our approach accurately captures feature coalition effects. It further attributes prediction changes using only the actually observed feature combinations, making it efficient and practical for time-sensitive clinical applications. We also introduce new evaluation metrics to evaluate the faithfulness of the attributions for online time series, and demonstrate through experiments on online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI methods in both explanation quality as 62% and computational efficiency as 33% time reduction on the MIMIC-III decompensation benchmark. We release our code at https://github.com/AITRICS/DeltaSHAP.},
 author = {Changhun Kim and Yechan Mun and Sangchul Hahn and Eunho Yang},
 comment = {Accepted to ICML 2025 Workshop on Actionable Interpretability. Code is available at https://github.com/AITRICS/DeltaSHAP},
 doi = {},
 eprint = {2507.02342v2},
 journal = {arXiv preprint},
 title = {DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values},
 url = {http://arxiv.org/abs/2507.02342v2},
 year = {2025}
}

@article{2507.03197v2,
 abstract = {CD8+ "killer" T cells and CD4+ "helper" T cells play a central role in the adaptive immune system by recognizing antigens presented by Major Histocompatibility Complex (pMHC) molecules via T Cell Receptors (TCRs). Modeling binding between T cells and the pMHC complex is fundamental to understanding basic mechanisms of human immune response as well as in developing therapies. While transformer-based models such as TULIP have achieved impressive performance in this domain, their black-box nature precludes interpretability and thus limits a deeper mechanistic understanding of T cell response. Most existing post-hoc explainable AI (XAI) methods are confined to encoder-only, co-attention, or model-specific architectures and cannot handle encoder-decoder transformers used in TCR-pMHC modeling. To address this gap, we propose Quantifying Cross-Attention Interaction (QCAI), a new post-hoc method designed to interpret the cross-attention mechanisms in transformer decoders. Quantitative evaluation is a challenge for XAI methods; we have compiled TCR-XAI, a benchmark consisting of 274 experimentally determined TCR-pMHC structures to serve as ground truth for binding. Using these structures we compute physical distances between relevant amino acid residues in the TCR-pMHC interaction region and evaluate how well our method and others estimate the importance of residues in this region across the dataset. We show that QCAI achieves state-of-the-art performance on both interpretability and prediction accuracy under the TCR-XAI benchmark.},
 author = {Jiarui Li and Zixiang Yin and Haley Smith and Zhengming Ding and Samuel J. Landry and Ramgopal R. Mettu},
 comment = {},
 doi = {},
 eprint = {2507.03197v2},
 journal = {arXiv preprint},
 title = {Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding},
 url = {http://arxiv.org/abs/2507.03197v2},
 year = {2025}
}

@article{2507.03318v2,
 abstract = {Explainable artificial intelligence (XAI) approaches have been increasingly applied in drug discovery to learn molecular representations and identify substructures driving property predictions. However, building end-to-end explainable models for structure-activity relationship (SAR) modeling for compound property prediction faces many challenges, such as the limited number of compound-protein interaction activity data for specific protein targets, and plenty of subtle changes in molecular configuration sites significantly affecting molecular properties. We exploit pairs of molecules with activity cliffs that share scaffolds but differ at substituent sites, characterized by large potency differences for specific protein targets. We propose a framework by implementing graph neural networks (GNNs) to leverage property and structure information from activity cliff pairs to predict compound-protein affinity (i.e., half maximal inhibitory concentration, IC50). To enhance model performance and explainability, we train GNNs with structure-aware loss functions using group lasso and sparse group lasso regularizations, which prune and highlight molecular subgraphs relevant to activity differences. We applied this framework to activity cliff data of molecules targeting three proto-oncogene tyrosine-protein kinase Src proteins (PDB IDs: 1O42, 2H8H, 4MXO). Our approach improved property prediction by integrating common and uncommon node information with sparse group lasso, as reflected in reduced root mean squared error (RMSE) and improved Pearson's correlation coefficient (PCC). Applying regularizations also enhances feature attribution for GNN by boosting graph-level global direction scores and improving atom-level coloring accuracy. These advances strengthen model interpretability in drug discovery pipelines, particularly for identifying critical molecular substructures in lead optimization.},
 author = {Zanyu Shi and Yang Wang and Pathum Weerawarna and Jie Zhang and Timothy Richardson and Yijie Wang and Kun Huang},
 comment = {15 pages, 7 figures},
 doi = {},
 eprint = {2507.03318v2},
 journal = {arXiv preprint},
 title = {Structure-Aware Compound-Protein Affinity Prediction via Graph Neural Network with Group Lasso Regularization},
 url = {http://arxiv.org/abs/2507.03318v2},
 year = {2025}
}

@article{2507.04528v1,
 abstract = {Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating the risk of non-transparency in the decision-making process of black-box Artificial Intelligence (AI) systems. However, despite the benefits, XAI methods are found to leak the privacy of individuals whose data is used in training or querying the models. Researchers have demonstrated privacy attacks that exploit explanations to infer sensitive personal information of individuals. Currently there is a lack of defenses against known privacy attacks targeting explanations when vulnerable XAI are used in production and machine learning as a service system. To address this gap, in this article, we explore Privacy Enhancing Technologies (PETs) as a defense mechanism against attribute inference on explanations provided by feature-based XAI methods. We empirically evaluate 3 types of PETs, namely synthetic training data, differentially private training and noise addition, on two categories of feature-based XAI. Our evaluation determines different responses from the mitigation methods and side-effects of PETs on other system properties such as utility and performance. In the best case, PETs integration in explanations reduced the risk of the attack by 49.47%, while maintaining model utility and explanation quality. Through our evaluation, we identify strategies for using PETs in XAI for maximizing benefits and minimizing the success of this privacy attack on sensitive personal information.},
 author = {Sonal Allana and Rozita Dara and Xiaodong Lin and Pulei Xiong},
 comment = {Under peer review},
 doi = {},
 eprint = {2507.04528v1},
 journal = {arXiv preprint},
 title = {Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence},
 url = {http://arxiv.org/abs/2507.04528v1},
 year = {2025}
}

@article{2507.05286v1,
 abstract = {Deep neural networks (DNNs) have demonstrated remarkable performance in many tasks but it often comes at a high computational cost and memory usage. Compression techniques, such as pruning and quantization, are applied to reduce the memory footprint of DNNs and make it possible to accommodate them on resource-constrained edge devices. Recently, explainable artificial intelligence (XAI) methods have been introduced with the purpose of understanding and explaining AI methods. XAI can be utilized to get to know the inner functioning of DNNs, such as the importance of different neurons and features in the overall performance of DNNs. In this paper, a novel DNN compression approach using XAI is proposed to efficiently reduce the DNN model size with negligible accuracy loss. In the proposed approach, the importance score of DNN parameters (i.e. weights) are computed using a gradient-based XAI technique called Layer-wise Relevance Propagation (LRP). Then, the scores are used to compress the DNN as follows: 1) the parameters with the negative or zero importance scores are pruned and removed from the model, 2) mixed-precision quantization is applied to quantize the weights with higher/lower score with higher/lower number of bits. The experimental results show that, the proposed compression approach reduces the model size by 64% while the accuracy is improved by 42% compared to the state-of-the-art XAI-based compression method.},
 author = {Kimia Soroush and Mohsen Raji and Behnam Ghavami},
 comment = {},
 doi = {},
 eprint = {2507.05286v1},
 journal = {arXiv preprint},
 title = {Compressing Deep Neural Networks Using Explainable AI},
 url = {http://arxiv.org/abs/2507.05286v1},
 year = {2025}
}

@article{2507.05636v2,
 abstract = {Graph learning has rapidly evolved into a critical subfield of machine learning and artificial intelligence (AI). Its development began with early graph-theoretic methods, gaining significant momentum with the advent of graph neural networks (GNNs). Over the past decade, progress in scalable architectures, dynamic graph modeling, multimodal learning, generative AI, explainable AI (XAI), and responsible AI has broadened the applicability of graph learning to various challenging environments. Graph learning is significant due to its ability to model complex, non-Euclidean relationships that traditional machine learning struggles to capture, thus better supporting real-world applications ranging from drug discovery and fraud detection to recommender systems and scientific reasoning. However, challenges like scalability, generalization, heterogeneity, interpretability, and trustworthiness must be addressed to unlock its full potential. This survey provides a comprehensive introduction to graph learning, focusing on key dimensions including scalable, temporal, multimodal, generative, explainable, and responsible graph learning. We review state-of-the-art techniques for efficiently handling large-scale graphs, capturing dynamic temporal dependencies, integrating heterogeneous data modalities, generating novel graph samples, and enhancing interpretability to foster trust and transparency. We also explore ethical considerations, such as privacy and fairness, to ensure responsible deployment of graph learning models. Additionally, we identify and discuss emerging topics, highlighting recent integration of graph learning and other AI paradigms and offering insights into future directions. This survey serves as a valuable resource for researchers and practitioners seeking to navigate the rapidly evolving landscape of graph learning.},
 author = {Feng Xia and Ciyuan Peng and Jing Ren and Falih Gozi Febrinanto and Renqiang Luo and Vidya Saikrishna and Shuo Yu and Xiangjie Kong},
 comment = {185 pages},
 doi = {10.1561/2000000137},
 eprint = {2507.05636v2},
 journal = {arXiv preprint},
 title = {Graph Learning},
 url = {http://arxiv.org/abs/2507.05636v2},
 year = {2025}
}

@article{2507.06751v1,
 abstract = {This position paper looks at differences between the current understandings of human-centered explainability and explainability AI. We discuss current ideas in both fields, as well as the differences and opportunities we discovered. As an example of combining both, we will present preliminary work on a new algebraic machine learning approach. We are excited to continue discussing design opportunities for human-centered explainability (HCx) and xAI with the broader HCxAI community.},
 author = {Janin Koch and Vitor Fortes Rey},
 comment = {},
 doi = {},
 eprint = {2507.06751v1},
 journal = {arXiv preprint},
 title = {Combining Human-centred Explainability and Explainable AI},
 url = {http://arxiv.org/abs/2507.06751v1},
 year = {2025}
}

@article{2507.06819v3,
 abstract = {Prototype models are an important method for explainable artificial intelligence (XAI) and interpretable machine learning. In this paper, we perform an in-depth analysis of a set of prominent prototype models including ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive set of metrics. In addition to applying standard metrics from literature, we propose several new metrics to further complement the analysis of model interpretability. In our experimentation, we apply the set of prototype models on a diverse set of datasets including fine-grained classification, Non-IID settings and multi-label classification to further contrast the performance. Furthermore, we also provide our code as an open-source library (https://github.com/uos-sis/quanproto), which facilitates simple application of the metrics itself, as well as extensibility -- providing the option for easily adding new metrics and models.},
 author = {Philipp Schlinge and Steffen Meinert and Martin Atzmueller},
 comment = {},
 doi = {10.1007/s13218-025-00900-0},
 eprint = {2507.06819v3},
 journal = {arXiv preprint},
 title = {Comprehensive Evaluation of Prototype Neural Networks},
 url = {http://arxiv.org/abs/2507.06819v3},
 year = {2025}
}

@article{2507.07148v1,
 abstract = {Explainable artificial intelligence (XAI) has become increasingly important in biomedical image analysis to promote transparency, trust, and clinical adoption of DL models. While several surveys have reviewed XAI techniques, they often lack a modality-aware perspective, overlook recent advances in multimodal and vision-language paradigms, and provide limited practical guidance. This survey addresses this gap through a comprehensive and structured synthesis of XAI methods tailored to biomedical image analysis.We systematically categorize XAI methods, analyzing their underlying principles, strengths, and limitations within biomedical contexts. A modality-centered taxonomy is proposed to align XAI methods with specific imaging types, highlighting the distinct interpretability challenges across modalities. We further examine the emerging role of multimodal learning and vision-language models in explainable biomedical AI, a topic largely underexplored in previous work. Our contributions also include a summary of widely used evaluation metrics and open-source frameworks, along with a critical discussion of persistent challenges and future directions. This survey offers a timely and in-depth foundation for advancing interpretable DL in biomedical image analysis.},
 author = {Getamesay Haile Dagnaw and Yanming Zhu and Muhammad Hassan Maqsood and Wencheng Yang and Xingshuai Dong and Xuefei Yin and Alan Wee-Chung Liew},
 comment = {},
 doi = {},
 eprint = {2507.07148v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey},
 url = {http://arxiv.org/abs/2507.07148v1},
 year = {2025}
}

@article{2507.07276v1,
 abstract = {Along with accurate prediction, understanding the contribution of each feature to the making of the prediction, i.e., the importance of the feature, is a desirable and arguably necessary component of a machine learning model. For a complex model such as a random forest, such importances are not innate -- as they are, e.g., with linear regression. Efficient methods have been created to provide such capabilities, with one of the most popular among them being permutation feature importance due to its efficiency, model-agnostic nature, and perceived intuitiveness. However, permutation feature importance has been shown to be misleading in the presence of dependent features as a result of the creation of unrealistic observations when permuting the dependent features. In this work, we develop TRIP (Test for Reliable Interpretation via Permutation), a test requiring minimal assumptions that is able to detect unreliable permutation feature importance scores that are the result of model extrapolation. To build on this, we demonstrate how the test can be complemented in order to allow its use in high dimensional settings. Through testing on simulated data and applications, our results show that the test can be used to reliably detect when permutation feature importance scores are unreliable.},
 author = {Aaron Foote and Danny Krizanc},
 comment = {Accepted at the Workshop on Explainable Artificial Intelligence (XAI) at IJCAI 2025},
 doi = {},
 eprint = {2507.07276v1},
 journal = {arXiv preprint},
 title = {TRIP: A Nonparametric Test to Diagnose Biased Feature Importance Scores},
 url = {http://arxiv.org/abs/2507.07276v1},
 year = {2025}
}

@article{2507.07373v2,
 abstract = {In this work, we study the problem pertaining to personalized classification of subclinical atherosclerosis by developing a hierarchical graph neural network framework to leverage two characteristic modalities of a patient: clinical features within the context of the cohort, and molecular data unique to individual patients. Current graph-based methods for disease classification detect patient-specific molecular fingerprints, but lack consistency and comprehension regarding cohort-wide features, which are an essential requirement for understanding pathogenic phenotypes across diverse atherosclerotic trajectories. Furthermore, understanding patient subtypes often considers clinical feature similarity in isolation, without integration of shared pathogenic interdependencies among patients. To address these challenges, we introduce ATHENA: Atherosclerosis Through Hierarchical Explainable Neural Network Analysis, which constructs a novel hierarchical network representation through integrated modality learning; subsequently, it optimizes learned patient-specific molecular fingerprints that reflect individual omics data, enforcing consistency with cohort-wide patterns. With a primary clinical dataset of 391 patients, we demonstrate that this heterogeneous alignment of clinical features with molecular interaction patterns has significantly boosted subclinical atherosclerosis classification performance across various baselines by up to 13% in area under the receiver operating curve (AUC) and 20% in F1 score. Taken together, ATHENA enables mechanistically-informed patient subtype discovery through explainable AI (XAI)-driven subnetwork clustering; this novel integration framework strengthens personalized intervention strategies, thereby improving the prediction of atherosclerotic disease progression and management of their clinical actionable outcomes.},
 author = {Irsyad Adam and Steven Swee and Erika Yilin and Ethan Ji and William Speier and Dean Wang and Alex Bui and Wei Wang and Karol Watson and Peipei Ping},
 comment = {},
 doi = {},
 eprint = {2507.07373v2},
 journal = {arXiv preprint},
 title = {Atherosclerosis through Hierarchical Explainable Neural Network Analysis},
 url = {http://arxiv.org/abs/2507.07373v2},
 year = {2025}
}

@article{2507.07666v1,
 abstract = {Enzyme mining is rapidly evolving as a data-driven strategy to identify biocatalysts with tailored functions from the vast landscape of uncharacterized proteins. The integration of machine learning into these workflows enables high-throughput prediction of enzyme functions, including Enzyme Commission numbers, Gene Ontology terms, substrate specificity, and key catalytic properties such as kinetic parameters, optimal temperature, pH, solubility, and thermophilicity. This review provides a systematic overview of state-of-the-art machine learning models and highlights representative case studies that demonstrate their effectiveness in accelerating enzyme discovery.
  Despite notable progress, current approaches remain limited by data scarcity, model generalizability, and interpretability. We discuss emerging strategies to overcome these challenges, including multi-task learning, integration of multi-modal data, and explainable AI. Together, these developments establish ML-guided enzyme mining as a scalable and predictive framework for uncovering novel biocatalysts, with broad applications in biocatalysis, biotechnology, and synthetic biology.},
 author = {Yanzi Zhang and Felix Moorhoff and Sizhe Qiu and Wenjuan Dong and David Medina-Ortiz and Jing Zhao and Mehdi D. Davari},
 comment = {},
 doi = {},
 eprint = {2507.07666v1},
 journal = {arXiv preprint},
 title = {Machine Learning-Driven Enzyme Mining: Opportunities, Challenges, and Future Perspectives},
 url = {http://arxiv.org/abs/2507.07666v1},
 year = {2025}
}

@article{2507.07857v1,
 abstract = {Causality has gained popularity in recent years. It has helped improve the performance, reliability, and interpretability of machine learning models. However, recent literature on explainable artificial intelligence (XAI) has faced criticism. The classical XAI and causality literature focuses on understanding which factors contribute to which consequences. While such knowledge is valuable for researchers and engineers, it is not what non-expert users expect as explanations. Instead, these users often await facts that cause the target consequences, i.e., actual causes. Formalizing this notion is still an open problem. Additionally, identifying actual causes is reportedly an NP-complete problem, and there are too few practical solutions to approximate formal definitions. We propose a set of algorithms to identify actual causes with a polynomial complexity and an adjustable level of precision and exhaustiveness. Our experiments indicate that the algorithms (1) identify causes for different categories of systems that are not handled by existing approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be adjusted to gain more precision and exhaustiveness with more computation time.},
 author = {Samuel Reyd and Ada Diaconescu and Jean-Louis Dessalles},
 comment = {},
 doi = {},
 eprint = {2507.07857v1},
 journal = {arXiv preprint},
 title = {Searching for actual causes: Approximate algorithms with adjustable precision},
 url = {http://arxiv.org/abs/2507.07857v1},
 year = {2025}
}

@article{2507.08829v1,
 abstract = {Deep Neural Networks (DNNs) are widely employed in safety-critical domains, where ensuring their reliability is essential. Triple Modular Redundancy (TMR) is an effective technique to enhance the reliability of DNNs in the presence of bit-flip faults. In order to handle the significant overhead of TMR, it is applied selectively on the parameters and components with the highest contribution at the model output. Hence, the accuracy of the selection criterion plays the key role on the efficiency of TMR. This paper presents an efficient TMR approach to enhance the reliability of DNNs against bit-flip faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can provide valuable insights about the importance of individual neurons and weights in the performance of the network, they can be applied as the selection metric in TMR techniques. The proposed method utilizes a low-cost, gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to calculate importance scores for DNN parameters. These scores are then used to enhance the reliability of the model, with the most critical weights being protected by TMR. The proposed approach is evaluated on two DNN models, VGG16 and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate that the method can protect the AlexNet model at a bit error rate of 10-4, achieving over 60% reliability improvement while maintaining the same overhead as state-of-the-art methods.},
 author = {Kimia Soroush and Nastaran Shirazi and Mohsen Raji},
 comment = {},
 doi = {},
 eprint = {2507.08829v1},
 journal = {arXiv preprint},
 title = {Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI},
 url = {http://arxiv.org/abs/2507.08829v1},
 year = {2025}
}

@article{2507.09545v1,
 abstract = {The usage of eXplainable Artificial Intelligence (XAI) methods has become essential in practical applications, given the increasing deployment of Artificial Intelligence (AI) models and the legislative requirements put forward in the latest years. A fundamental but often underestimated aspect of the explanations is their robustness, a key property that should be satisfied in order to trust the explanations. In this study, we provide some preliminary insights on evaluating the reliability of explanations in the specific case of unbalanced datasets, which are very frequent in high-risk use-cases, but at the same time considerably challenging for both AI models and XAI methods. We propose a simple evaluation focused on the minority class (i.e. the less frequent one) that leverages on-manifold generation of neighbours, explanation aggregation and a metric to test explanation consistency. We present a use-case based on a tabular dataset with numerical features focusing on the occurrence of frost events.},
 author = {Ilaria Vascotto and Valentina Blasone and Alex Rodriguez and Alessandro Bonaita and Luca Bortolussi},
 comment = {Late Breaking Work presented at the 3rd World Conference on eXplainable Artificial Intelligence (XAI2025)},
 doi = {},
 eprint = {2507.09545v1},
 journal = {arXiv preprint},
 title = {Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events},
 url = {http://arxiv.org/abs/2507.09545v1},
 year = {2025}
}

@article{2507.09754v2,
 abstract = {Transcription Factor Binding Site (TFBS) prediction is crucial for understanding gene regulation and various biological processes. This study introduces a novel Mixture of Experts (MoE) approach for TFBS prediction, integrating multiple pre-trained Convolutional Neural Network (CNN) models, each specializing in different TFBS patterns. We evaluate the performance of our MoE model against individual expert models on both in-distribution and out-of-distribution (OOD) datasets, using six randomly selected transcription factors (TFs) for OOD testing. Our results demonstrate that the MoE model achieves competitive or superior performance across diverse TF binding sites, particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA) statistical test confirms the significance of these performance differences. Additionally, we introduce ShiftSmooth, a novel attribution mapping technique that provides more robust model interpretability by considering small shifts in input sequences. Through comprehensive explainability analysis, we show that ShiftSmooth offers superior attribution for motif discovery and localization compared to traditional Vanilla Gradient methods. Our work presents an efficient, generalizable, and interpretable solution for TFBS prediction, potentially enabling new discoveries in genome biology and advancing our understanding of transcriptional regulation.},
 author = {Aakash Tripathi and Ian E. Nielsen and Muhammad Umer and Ravi P. Ramachandran and Ghulam Rasool},
 comment = {},
 doi = {},
 eprint = {2507.09754v2},
 journal = {arXiv preprint},
 title = {Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts},
 url = {http://arxiv.org/abs/2507.09754v2},
 year = {2025}
}

@article{2507.11185v1,
 abstract = {Heart disease remains a major global health concern, particularly in regions with limited access to medical resources and diagnostic facilities. Traditional diagnostic methods often fail to accurately identify and manage heart disease risks, leading to adverse outcomes. Machine learning has the potential to significantly enhance the accuracy, efficiency, and speed of heart disease diagnosis. In this study, we proposed a comprehensive framework that combines classification models for heart disease detection and regression models for risk prediction. We employed the Heart Disease dataset, which comprises 1,035 cases. To address the issue of class imbalance, the Synthetic Minority Oversampling Technique (SMOTE) was applied, resulting in the generation of an additional 100,000 synthetic data points. Performance metrics, including accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to evaluate the model's effectiveness. Among the classification models, Random Forest emerged as the standout performer, achieving an accuracy of 97.2% on real data and 97.6% on synthetic data. For regression tasks, Linear Regression demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic datasets, respectively, with the lowest error metrics. Additionally, Explainable AI techniques were employed to enhance the interpretability of the models. This study highlights the potential of machine learning to revolutionize heart disease diagnosis and risk prediction, thereby facilitating early intervention and enhancing clinical decision-making.},
 author = {Md. Emon Akter Sourov and Md. Sabbir Hossen and Pabon Shaha and Mohammad Minoar Hossain and Md Sadiq Iqbal},
 comment = {This paper has been accepted at the IEEE QPAIN 2025. The final version will be available in the IEEE Xplore Digital Library},
 doi = {},
 eprint = {2507.11185v1},
 journal = {arXiv preprint},
 title = {An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment},
 url = {http://arxiv.org/abs/2507.11185v1},
 year = {2025}
}

@article{2507.11570v1,
 abstract = {Objective: To develop and evaluate machine learning (ML) models for predicting length of stay (LOS) in elective spine surgery, with a focus on the benefits of temporal modeling and model interpretability. Materials and Methods: We compared traditional ML models (e.g., linear regression, random forest, support vector machine (SVM), and XGBoost) with our developed model, SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an attention, using structured perioperative electronic health records (EHR) data. Performance was evaluated using the coefficient of determination (R2), and key predictors were identified using explainable AI. Results: SurgeryLSTM achieved the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85) and baseline models. The attention mechanism improved interpretability by dynamically identifying influential temporal segments within preoperative clinical sequences, allowing clinicians to trace which events or features most contributed to each LOS prediction. Key predictors of LOS included bone disorder, chronic kidney disease, and lumbar fusion identified as the most impactful predictors of LOS. Discussion: Temporal modeling with attention mechanisms significantly improves LOS prediction by capturing the sequential nature of patient data. Unlike static models, SurgeryLSTM provides both higher accuracy and greater interpretability, which are critical for clinical adoption. These results highlight the potential of integrating attention-based temporal models into hospital planning workflows. Conclusion: SurgeryLSTM presents an effective and interpretable AI solution for LOS prediction in elective spine surgery. Our findings support the integration of temporal, explainable ML approaches into clinical decision support systems to enhance discharge readiness and individualized patient care.},
 author = {Ha Na Cho and Sairam Sutari and Alexander Lopez and Hansen Bow and Kai Zheng},
 comment = {},
 doi = {10.1093/jamiaopen/ooaf079},
 eprint = {2507.11570v1},
 journal = {arXiv preprint},
 title = {SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery},
 url = {http://arxiv.org/abs/2507.11570v1},
 year = {2025}
}

@article{2507.12599v1,
 abstract = {The success of recent Artificial Intelligence (AI) models has been accompanied by the opacity of their internal mechanisms, due notably to the use of deep neural networks. In order to understand these internal mechanisms and explain the output of these AI models, a set of methods have been proposed, grouped under the domain of eXplainable AI (XAI). This paper focuses on a sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims to explain the actions of an agent that has learned by reinforcement learning. We propose an intuitive taxonomy based on two questions "What" and "How". The first question focuses on the target that the method explains, while the second relates to the way the explanation is provided. We use this taxonomy to provide a state-of-the-art review of over 250 papers. In addition, we present a set of domains close to XRL, which we believe should get attention from the community. Finally, we identify some needs for the field of XRL.},
 author = {Léo Saulières},
 comment = {69 pages, 19 figures},
 doi = {},
 eprint = {2507.12599v1},
 journal = {arXiv preprint},
 title = {A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs},
 url = {http://arxiv.org/abs/2507.12599v1},
 year = {2025}
}

@article{2507.13090v1,
 abstract = {Robust XAI techniques should ideally be simultaneously deterministic, model agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability technique, with guaranteed convergency. MUPAX measure theoretic formulation gives principled feature importance attribution through structured perturbation analysis that discovers inherent input patterns and eliminates spurious relationships. We evaluate MUPAX on an extensive range of data modalities and tasks: audio classification (1D), image classification (2D), volumetric medical image analysis (3D), and anatomical landmark detection, demonstrating dimension agnostic effectiveness. The rigorous convergence guarantees extend to any loss function and arbitrary dimensions, making MUPAX applicable to virtually any problem context for AI. By contrast with other XAI methods that typically decrease performance when masking, MUPAX not only preserves but actually enhances model accuracy by capturing only the most important patterns of the original data. Extensive benchmarking against the state of the XAI art demonstrates MUPAX ability to generate precise, consistent and understandable explanations, a crucial step towards explainable and trustworthy AI systems. The source code will be released upon publication.},
 author = {Vincenzo Dentamaro and Felice Franchini and Giuseppe Pirlo and Irina Voiculescu},
 comment = {},
 doi = {},
 eprint = {2507.13090v1},
 journal = {arXiv preprint},
 title = {MUPAX: Multidimensional Problem Agnostic eXplainable AI},
 url = {http://arxiv.org/abs/2507.13090v1},
 year = {2025}
}

@article{2507.14180v1,
 abstract = {In line with the AI-native 6G vision, explainability and robustness are crucial for building trust and ensuring reliable performance in millimeter-wave (mmWave) systems. Efficient beam alignment is essential for initial access, but deep learning (DL) solutions face challenges, including high data collection overhead, hardware constraints, lack of explainability, and susceptibility to adversarial attacks. This paper proposes a robust and explainable DL-based beam alignment engine (BAE) for mmWave multiple-input multiple output (MIMO) systems. The BAE uses received signal strength indicator (RSSI) measurements from wide beams to predict the best narrow beam, reducing the overhead of exhaustive beam sweeping. To overcome the challenge of real-world data collection, this work leverages a site-specific digital twin (DT) to generate synthetic channel data closely resembling real-world environments. A model refinement via transfer learning is proposed to fine-tune the pre-trained model residing in the DT with minimal real-world data, effectively bridging mismatches between the digital replica and real-world environments. To reduce beam training overhead and enhance transparency, the framework uses deep Shapley additive explanations (SHAP) to rank input features by importance, prioritizing key spatial directions and minimizing beam sweeping. It also incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a credibility metric for detecting out-of-distribution inputs and ensuring robust, transparent decision-making. Experimental results show that the proposed framework reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by up to 8.5x, achieving near-optimal spectral efficiency and transparent decision making compared to traditional softmax based DL models.},
 author = {Nasir Khan and Asmaa Abdallah and Abdulkadir Celik and Ahmed M. Eltawil and Sinem Coleri},
 comment = {},
 doi = {},
 eprint = {2507.14180v1},
 journal = {arXiv preprint},
 title = {Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems},
 url = {http://arxiv.org/abs/2507.14180v1},
 year = {2025}
}

@article{2507.14744v1,
 abstract = {Automated machine learning systems efficiently streamline model selection but often focus on a single best-performing model, overlooking explanation uncertainty, an essential concern in human centered explainable AI. To address this, we propose a novel framework that incorporates model multiplicity into explanation generation by aggregating partial dependence profiles (PDP) from a set of near optimal models, known as the Rashomon set. The resulting Rashomon PDP captures interpretive variability and highlights areas of disagreement, providing users with a richer, uncertainty aware view of feature effects. To evaluate its usefulness, we introduce two quantitative metrics, the coverage rate and the mean width of confidence intervals, to evaluate the consistency between the standard PDP and the proposed Rashomon PDP. Experiments on 35 regression datasets from the OpenML CTR23 benchmark suite show that in most cases, the Rashomon PDP covers less than 70% of the best model's PDP, underscoring the limitations of single model explanations. Our findings suggest that Rashomon PDP improves the reliability and trustworthiness of model interpretations by adding additional information that would otherwise be neglected. This is particularly useful in high stakes domains where transparency and confidence are critical.},
 author = {Mustafa Cavus and Jan N. van Rijn and Przemysław Biecek},
 comment = {Accepted at 28th International Conference on Discovery Science 2025},
 doi = {10.1007/978-3-032-05461-6_29},
 eprint = {2507.14744v1},
 journal = {arXiv preprint},
 title = {Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML},
 url = {http://arxiv.org/abs/2507.14744v1},
 year = {2025}
}

@article{2507.16065v3,
 abstract = {Machine learning (ML), including deep learning (DL) and radiomics-based methods, is increasingly used for cancer outcome prediction with PET and SPECT imaging. However, the comparative performance of handcrafted radiomics features (HRF), deep radiomics features (DRF), DL models, and hybrid fusion approaches remains inconsistent across clinical applications. This systematic review analyzed 226 studies published from 2020 to 2025 that applied ML to PET or SPECT imaging for outcome prediction. Each study was evaluated using a 59-item framework covering dataset construction, feature extraction, validation methods, interpretability, and risk of bias. We extracted key details including model type, cancer site, imaging modality, and performance metrics such as accuracy and area under the curve (AUC). PET-based studies (95%) generally outperformed those using SPECT, likely due to higher spatial resolution and sensitivity. DRF models achieved the highest mean accuracy (0.862), while fusion models yielded the highest AUC (0.861). ANOVA confirmed significant differences in performance (accuracy: p=0.0006, AUC: p=0.0027). Common limitations included inadequate handling of class imbalance (59%), missing data (29%), and low population diversity (19%). Only 48% of studies adhered to IBSI standards. These findings highlight the need for standardized pipelines, improved data quality, and explainable AI to support clinical integration.},
 author = {Mohammad R. Salmanpour and Somayeh Sadat Mehrnia and Sajad Jabarzadeh Ghandilu and Sonya Falahati and Shahram Taeb and Ghazal Mousavi and Mehdi Maghsoudi and Ahmad Shariftabrizi and Ilker Hacihaliloglu and Arman Rahmim},
 comment = {},
 doi = {},
 eprint = {2507.16065v3},
 journal = {arXiv preprint},
 title = {Handcrafted vs. Deep Radiomics vs. Fusion vs. Deep Learning: A Comprehensive Review of Machine Learning -Based Cancer Outcome Prediction in PET and SPECT Imaging},
 url = {http://arxiv.org/abs/2507.16065v3},
 year = {2025}
}

@article{2507.16069v1,
 abstract = {Learning-based surrogate models have become a practical alternative to high-fidelity CFD solvers, but their latent representations remain opaque and hinder adoption in safety-critical or regulation-bound settings. This work introduces a posthoc interpretability framework for graph-based surrogate models used in computational fluid dynamics (CFD) by leveraging sparse autoencoders (SAEs). By obtaining an overcomplete basis in the node embedding space of a pretrained surrogate, the method extracts a dictionary of interpretable latent features. The approach enables the identification of monosemantic concepts aligned with physical phenomena such as vorticity or flow structures, offering a model-agnostic pathway to enhance explainability and trustworthiness in CFD applications.},
 author = {Yeping Hu and Shusen Liu},
 comment = {Accepted by IJCAI 2025 Workshop on Explainable Artificial Intelligence (XAI)},
 doi = {},
 eprint = {2507.16069v1},
 journal = {arXiv preprint},
 title = {Interpreting CFD Surrogates through Sparse Autoencoders},
 url = {http://arxiv.org/abs/2507.16069v1},
 year = {2025}
}

@article{2507.17161v1,
 abstract = {Modern network intrusion detection systems (NIDS) frequently utilize the predictive power of complex deep learning models. However, the "black-box" nature of such deep learning methods adds a layer of opaqueness that hinders the proper understanding of detection decisions, trust in the decisions and prevent timely countermeasures against such attacks. Explainable AI (XAI) methods provide a solution to this problem by providing insights into the causes of the predictions. The majority of the existing XAI methods provide explanations which are not convenient to convert into actionable countermeasures. In this work, we propose a novel diffusion-based counterfactual explanation framework that can provide actionable explanations for network intrusion attacks. We evaluated our proposed algorithm against several other publicly available counterfactual explanation algorithms on 3 modern network intrusion datasets. To the best of our knowledge, this work also presents the first comparative analysis of existing counterfactual explanation algorithms within the context of network intrusion detection systems. Our proposed method provide minimal, diverse counterfactual explanations out of the tested counterfactual explanation algorithms in a more efficient manner by reducing the time to generate explanations. We also demonstrate how counterfactual explanations can provide actionable explanations by summarizing them to create a set of global rules. These rules are actionable not only at instance level but also at the global level for intrusion attacks. These global counterfactual rules show the ability to effectively filter out incoming attack queries which is crucial for efficient intrusion detection and defense mechanisms.},
 author = {Vinura Galwaduge and Jagath Samarabandu},
 comment = {},
 doi = {},
 eprint = {2507.17161v1},
 journal = {arXiv preprint},
 title = {Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection},
 url = {http://arxiv.org/abs/2507.17161v1},
 year = {2025}
}

@article{2507.17799v1,
 abstract = {Voice disorders affect a significant portion of the population, and the ability to diagnose them using automated, non-invasive techniques would represent a substantial advancement in healthcare, improving the quality of life of patients. Recent studies have demonstrated that artificial intelligence models, particularly Deep Neural Networks (DNNs), can effectively address this task. However, due to their complexity, the decision-making process of such models often remain opaque, limiting their trustworthiness in clinical contexts. This paper investigates an alternative approach based on Explainable AI (XAI), a field that aims to improve the interpretability of DNNs by providing different forms of explanations. Specifically, this works focuses on concept-based models such as Concept Bottleneck Model (CBM) and Concept Embedding Model (CEM) and how they can achieve performance comparable to traditional deep learning methods, while offering a more transparent and interpretable decision framework.},
 author = {Davide Ghia and Gabriele Ciravegna and Alkis Koudounas and Marco Fantini and Erika Crosetti and Giovanni Succo and Tania Cerquitelli},
 comment = {},
 doi = {},
 eprint = {2507.17799v1},
 journal = {arXiv preprint},
 title = {A Concept-based approach to Voice Disorder Detection},
 url = {http://arxiv.org/abs/2507.17799v1},
 year = {2025}
}

@article{2507.18738v1,
 abstract = {Fair and dynamic energy allocation in community microgrids remains a critical challenge, particularly when serving socio-economically diverse participants. Static optimization and cost-sharing methods often fail to adapt to evolving inequities, leading to participant dissatisfaction and unsustainable cooperation. This paper proposes a novel framework that integrates multi-objective mixed-integer linear programming (MILP), cooperative game theory, and a dynamic equity-adjustment mechanism driven by reinforcement learning (RL). At its core, the framework utilizes a bi-level optimization model grounded in Equity-regarding Welfare Maximization (EqWM) principles, which incorporate Rawlsian fairness to prioritize the welfare of the least advantaged participants. We introduce a Proximal Policy Optimization (PPO) agent that dynamically adjusts socio-economic weights in the optimization objective based on observed inequities in cost and renewable energy access. This RL-powered feedback loop enables the system to learn and adapt, continuously striving for a more equitable state. To ensure transparency, Explainable AI (XAI) is used to interpret the benefit allocations derived from a weighted Shapley value. Validated across six realistic scenarios, the framework demonstrates peak demand reductions of up to 72.6%, and significant cooperative gains. The adaptive RL mechanism further reduces the Gini coefficient over time, showcasing a pathway to truly sustainable and fair energy communities.},
 author = {Abhijan Theja and Mayukha Pal},
 comment = {},
 doi = {},
 eprint = {2507.18738v1},
 journal = {arXiv preprint},
 title = {An Explainable Equity-Aware P2P Energy Trading Framework for Socio-Economically Diverse Microgrid},
 url = {http://arxiv.org/abs/2507.18738v1},
 year = {2025}
}

@article{2507.19168v1,
 abstract = {Commercial high-voltage circuit breaker (CB) condition monitoring systems rely on directly observable physical parameters such as gas filling pressure with pre-defined thresholds. While these parameters are crucial, they only cover a small subset of malfunctioning mechanisms and usually can be monitored only if the CB is disconnected from the grid. To facilitate online condition monitoring while CBs remain connected, non-intrusive measurement techniques such as vibration or acoustic signals are necessary. Currently, CB condition monitoring studies using these signals typically utilize supervised methods for fault diagnostics, where ground-truth fault types are known due to artificially introduced faults in laboratory settings. This supervised approach is however not feasible in real-world applications, where fault labels are unavailable. In this work, we propose a novel unsupervised fault detection and segmentation framework for CBs based on vibration and acoustic signals. This framework can detect deviations from the healthy state. The explainable artificial intelligence (XAI) approach is applied to the detected faults for fault diagnostics. The specific contributions are: (1) we propose an integrated unsupervised fault detection and segmentation framework that is capable of detecting faults and clustering different faults with only healthy data required during training (2) we provide an unsupervised explainability-guided fault diagnostics approach using XAI to offer domain experts potential indications of the aged or faulty components, achieving fault diagnostics without the prerequisite of ground-truth fault labels. These contributions are validated using an experimental dataset from a high-voltage CB under healthy and artificially introduced fault conditions, contributing to more reliable CB system operation.},
 author = {Chi-Ching Hsu and Gaëtan Frusque and Florent Forest and Felipe Macedo and Christian M. Franck and Olga Fink},
 comment = {},
 doi = {10.1016/j.ress.2025.111199},
 eprint = {2507.19168v1},
 journal = {arXiv preprint},
 title = {Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers},
 url = {http://arxiv.org/abs/2507.19168v1},
 year = {2025}
}

@article{2507.19898v2,
 abstract = {Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit algorithms used to balance exploration and exploitation strategies in active learning. Yet, their probabilistic nature often turns them into a "black box", hindering debugging and trust. We introduce TS-Insight, a visual analytics tool explicitly designed to shed light on the internal decision mechanisms of Thompson Sampling-based algorithms, for model developers. It comprises multiple plots, tracing for each arm the evolving posteriors, evidence counts, and sampling outcomes, enabling the verification, diagnosis, and explainability of exploration/exploitation dynamics. This tool aims at fostering trust and facilitating effective debugging and deployment in complex binary decision-making scenarios especially in sensitive domains requiring interpretable decision-making.},
 author = {Parsa Vares and Éloi Durant and Jun Pang and Nicolas Médoc and Mohammad Ghoniem},
 comment = {Accepted as a poster at IEEE VIS 2025 ("TS-Insight: Visual Fingerprinting of Multi-Armed Bandits"). Open-source tool available at https://github.com/LIST-LUXEMBOURG/ts-insight},
 doi = {},
 eprint = {2507.19898v2},
 journal = {arXiv preprint},
 title = {TS-Insight: Visualizing Thompson Sampling for Verification and XAI},
 url = {http://arxiv.org/abs/2507.19898v2},
 year = {2025}
}

@article{2507.20714v1,
 abstract = {Prostate cancer, the second most prevalent male malignancy, requires advanced diagnostic tools. We propose an explainable AI system combining BERT (for textual clinical notes) and Random Forest (for numerical lab data) through a novel multimodal fusion strategy, achieving superior classification performance on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is established, our work demonstrates that a simple yet interpretable BERT+RF pipeline delivers clinically significant improvements - particularly for intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824 numerical/0.725 textual). SHAP analysis provides transparent feature importance rankings, while ablation studies prove textual features' complementary value. This accessible approach offers hospitals a balance of high performance (F1=89%), computational efficiency, and clinical interpretability - addressing critical needs in prostate cancer diagnostics.},
 author = {Asma Sadia Khan and Fariba Tasnia Khan and Tanjim Mahmud and Salman Karim Khan and Rishita Chakma and Nahed Sharmen and Mohammad Shahadat Hossain and Karl Andersson},
 comment = {},
 doi = {},
 eprint = {2507.20714v1},
 journal = {arXiv preprint},
 title = {Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI},
 url = {http://arxiv.org/abs/2507.20714v1},
 year = {2025}
}

@article{2507.21187v1,
 abstract = {Consumption of YouTube news videos significantly shapes public opinion and political narratives. While prior works have studied the longitudinal dissemination dynamics of YouTube News videos across extended periods, limited attention has been paid to the short-term trends. In this paper, we investigate the early-stage diffusion patterns and dispersion rate of news videos on YouTube, focusing on the first 24 hours. To this end, we introduce and analyze a rich dataset of over 50,000 videos across 75 countries and six continents. We provide the first quantitative evaluation of the 24-hour half-life of YouTube news videos as well as identify their distinct diffusion patterns. According to the findings, the average 24-hour half-life is approximately 7 hours, with substantial variance both within and across countries, ranging from as short as 2 hours to as long as 15 hours. Additionally, we explore the problem of predicting the latency of news videos' 24-hour half-lives. Leveraging the presented datasets, we train and contrast the performance of 6 different models based on statistical as well as Deep Learning techniques. The difference in prediction results across the models is traced and analyzed. Lastly, we investigate the importance of video- and channel-related predictors through Explainable AI (XAI) techniques. The dataset, analysis codebase and the trained models are released at http://bit.ly/3ILvTLU to facilitate further research in this area.},
 author = {Anahit Sargsyan and Hridoy Sankar Dutta and Juergen Pfeffer},
 comment = {To be published in International Conference on Machine Learning, Optimization, and Data Science (LOD 2025)},
 doi = {},
 eprint = {2507.21187v1},
 journal = {arXiv preprint},
 title = {Half-life of Youtube News Videos: Diffusion Dynamics and Predictive Factors},
 url = {http://arxiv.org/abs/2507.21187v1},
 year = {2025}
}

@article{2507.21193v1,
 abstract = {Next generation Radio Access Networks (RANs) introduce programmability, intelligence, and near real-time control through intelligent controllers, enabling enhanced security within the RAN and across broader 5G/6G infrastructures. This paper presents a comprehensive survey highlighting opportunities, challenges, and research gaps for Large Language Models (LLMs)-assisted explainable (XAI) intrusion detection (IDS) for secure future RAN environments. Motivated by this, we propose an LLM interpretable anomaly-based detection system for distributed denial-of-service (DDoS) attacks using multivariate time series key performance measures (KPMs), extracted from E2 nodes, within the Near Real-Time RAN Intelligent Controller (Near-RT RIC). An LSTM-based model is trained to identify malicious User Equipment (UE) behavior based on these KPMs. To enhance transparency, we apply post-hoc local explainability methods such as LIME and SHAP to interpret individual predictions. Furthermore, LLMs are employed to convert technical explanations into natural-language insights accessible to non-expert users. Experimental results on real 5G network KPMs demonstrate that our framework achieves high detection accuracy (F1-score > 0.96) while delivering actionable and interpretable outputs.},
 author = {Sotiris Chatzimiltis and Mohammad Shojafar and Mahdi Boloursaz Mashhadi and Rahim Tafazolli},
 comment = {},
 doi = {},
 eprint = {2507.21193v1},
 journal = {arXiv preprint},
 title = {Interpretable Anomaly-Based DDoS Detection in AI-RAN with XAI and LLMs},
 url = {http://arxiv.org/abs/2507.21193v1},
 year = {2025}
}

@article{2507.21242v1,
 abstract = {In the current digital landscape, misinformation circulates rapidly, shaping public perception and causing societal divisions. It is difficult to identify hyperpartisan news in Bangla since there aren't many sophisticated natural language processing methods available for this low-resource language. Without effective detection methods, biased content can spread unchecked, posing serious risks to informed discourse. To address this gap, our research fine-tunes Bangla BERT. This is a state-of-the-art transformer-based model, designed to enhance classification accuracy for hyperpartisan news. We evaluate its performance against traditional machine learning models and implement semi-supervised learning to enhance predictions further. Not only that, we use LIME to provide transparent explanations of the model's decision-making process, which helps to build trust in its outcomes. With a remarkable accuracy score of 95.65%, Bangla BERT outperforms conventional approaches, according to our trial data. The findings of this study demonstrate the usefulness of transformer models even in environments with limited resources, which opens the door to further improvements in this area.},
 author = {Mohammad Mehadi Hasan and Fatema Binte Hassan and Md Al Jubair and Zobayer Ahmed and Sazzatul Yeakin and Md Masum Billah},
 comment = {},
 doi = {},
 eprint = {2507.21242v1},
 journal = {arXiv preprint},
 title = {Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach},
 url = {http://arxiv.org/abs/2507.21242v1},
 year = {2025}
}

@article{2507.21571v1,
 abstract = {The need for explanations in AI has, by and large, been driven by the desire to increase the transparency of black-box machine learning models. However, such explanations, which focus on the internal mechanisms that lead to a specific output, are often unsuitable for non-experts. To facilitate a human-centered perspective on AI explanations, agents need to focus on individuals and their preferences as well as the context in which the explanations are given. This paper proposes a personalized approach to explanation, where the agent tailors the information provided to the user based on what is most likely pertinent to them. We propose a model of the agent's worldview that also serves as a personal and dynamic memory of its previous interactions with the same user, based on which the artificial agent can estimate what part of its knowledge is most likely new information to the user.},
 author = {Laura Spillner and Nima Zargham and Mihai Pomarlan and Robert Porzel and Rainer Malaka},
 comment = {Presented at the IJCAI 2023 Workshop on Explainable Artificial Intelligence (XAI)},
 doi = {},
 eprint = {2507.21571v1},
 journal = {arXiv preprint},
 title = {Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations},
 url = {http://arxiv.org/abs/2507.21571v1},
 year = {2025}
}

@article{2507.22570v1,
 abstract = {This work demonstrates a methodology for using deep learning to discover simple, practical criteria for classifying matrices based on abstract algebraic properties. By combining a high-performance neural network with explainable AI (XAI) techniques, we can distill a model's learned strategy into human-interpretable rules. We apply this approach to the challenging case of monotone matrices, defined by the condition that their inverses are entrywise nonnegative. Despite their simple definition, an easy characterization in terms of the matrix elements or the derived parameters is not known. Here, we present, to the best of our knowledge, the first systematic machine-learning approach for deriving a practical criterion that distinguishes monotone from non-monotone matrices. After establishing a labelled dataset by randomly generated monotone and non-monotone matrices uniformly on $(-1,1)$, we employ deep neural network algorithms for classifying the matrices as monotone or non-monotone, using both their entries and a comprehensive set of matrix features. By saliency methods, such as integrated gradients, we identify among all features, two matrix parameters which alone provide sufficient information for the matrix classification, with $95\%$ accuracy, namely the absolute values of the two lowest-order coefficients, $c_0$ and $c_1$ of the matrix's characteristic polynomial. A data-driven study of 18,000 random $7\times7$ matrices shows that the monotone class obeys $\lvert c_{0}/c_{1}\rvert\le0.18$ with probability $>99.98\%$; because $\lvert c_{0}/c_{1}\rvert = 1/\mathrm{tr}(A^{-1})$ for monotone $A$, this is equivalent to the simple bound $\mathrm{tr}(A^{-1})\ge5.7$.},
 author = {Leandro Farina and Sergey Korotov},
 comment = {22 pages, 11 figures. To be submitted to a journal},
 doi = {},
 eprint = {2507.22570v1},
 journal = {arXiv preprint},
 title = {Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity},
 url = {http://arxiv.org/abs/2507.22570v1},
 year = {2025}
}

@article{2507.22962v1,
 abstract = {Climate extremes present escalating risks to agriculture intensifying the need for reliable multi-hazard early warning systems (EWS). The situation is evolving due to climate change and hence such systems should have the intelligent to continue to learn from recent climate behaviours. However, traditional single-hazard forecasting methods fall short in capturing complex interactions among concurrent climatic events. To address this deficiency, in this paper, we combine sequential deep learning models and advanced Explainable Artificial Intelligence (XAI) techniques to introduce a multi-hazard forecasting framework for agriculture. In our experiments, we utilize meteorological data from four prominent agricultural regions in the United States (between 2010 and 2023) to validate the predictive accuracy of our framework on multiple severe event types, which are extreme cold, floods, frost, hail, heatwaves, and heavy rainfall, with tailored models for each area. The framework uniquely integrates attention mechanisms with TimeSHAP (a recurrent XAI explainer for time series) to provide comprehensive temporal explanations revealing not only which climatic features are influential but precisely when their impacts occur. Our results demonstrate strong predictive accuracy, particularly with the BiLSTM architecture, and highlight the system's capacity to inform nuanced, proactive risk management strategies. This research significantly advances the explainability and applicability of multi-hazard EWS, fostering interdisciplinary trust and effective decision-making process for climate risk management in the agricultural industry.},
 author = {Boyuan Zheng and Victor W. Chu},
 comment = {Pre-print v0.8 2025-07-30},
 doi = {},
 eprint = {2507.22962v1},
 journal = {arXiv preprint},
 title = {Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations},
 url = {http://arxiv.org/abs/2507.22962v1},
 year = {2025}
}

@article{2508.00381v2,
 abstract = {Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.},
 author = {Kamal Basha S and Athira Nambiar},
 comment = {},
 doi = {},
 eprint = {2508.00381v2},
 journal = {arXiv preprint},
 title = {Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis},
 url = {http://arxiv.org/abs/2508.00381v2},
 year = {2025}
}

@article{2508.00665v1,
 abstract = {Artificial intelligence-driven adaptive learning systems are reshaping education through data-driven adaptation of learning experiences. Yet many of these systems lack transparency, offering limited insight into how decisions are made. Most explainable AI (XAI) techniques focus on technical outputs but neglect user roles and comprehension. This paper proposes a hybrid framework that integrates traditional XAI techniques with generative AI models and user personalisation to generate multimodal, personalised explanations tailored to user needs. We redefine explainability as a dynamic communication process tailored to user roles and learning goals. We outline the framework's design, key XAI limitations in education, and research directions on accuracy, fairness, and personalisation. Our aim is to move towards explainable AI that enhances transparency while supporting user-centred experiences.},
 author = {Maryam Mosleh and Marie Devlin and Ellis Solaiman},
 comment = {},
 doi = {},
 eprint = {2508.00665v1},
 journal = {arXiv preprint},
 title = {Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI},
 url = {http://arxiv.org/abs/2508.00665v1},
 year = {2025}
}

@article{2508.00674v1,
 abstract = {Social media platforms today strive to improve user experience through AI recommendations, yet the value of such recommendations vanishes as users do not understand the reasons behind them. This issue arises because explainability in social media is general and lacks alignment with user-specific needs. In this vision paper, we outline a user-segmented and context-aware explanation layer by proposing a visual explanation system with diverse explanation methods. The proposed system is framed by the variety of user needs and contexts, showing explanations in different visualized forms, including a technically detailed version for AI experts and a simplified one for lay users. Our framework is the first to jointly adapt explanation style (visual vs. numeric) and granularity (expert vs. lay) inside a single pipeline. A public pilot with 30 X users will validate its impact on decision-making and trust.},
 author = {Banan Alkhateeb and Ellis Solaiman},
 comment = {},
 doi = {},
 eprint = {2508.00674v1},
 journal = {arXiv preprint},
 title = {Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations},
 url = {http://arxiv.org/abs/2508.00674v1},
 year = {2025}
}

@article{2508.00785v1,
 abstract = {Academic performance depends on a multivariable nexus of socio-academic and financial factors. This study investigates these influences to develop effective strategies for optimizing students' CGPA. To achieve this, we reviewed various literature to identify key influencing factors and constructed an initial hypothetical causal graph based on the findings. Additionally, an online survey was conducted, where 1,050 students participated, providing comprehensive data for analysis. Rigorous data preprocessing techniques, including cleaning and visualization, ensured data quality before analysis. Causal analysis validated the relationships among variables, offering deeper insights into their direct and indirect effects on CGPA. Regression models were implemented for CGPA prediction, while classification models categorized students based on performance levels. Ridge Regression demonstrated strong predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared Error of 0.023. Random Forest outperformed in classification, attaining an F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting critical factors such as study hours, scholarships, parental education, and prior academic performance. The study culminated in the development of a web-based application that provides students with personalized insights, allowing them to predict academic performance, identify areas for improvement, and make informed decisions to enhance their outcomes.},
 author = {Bushra Akter and Md Biplob Hosen and Sabbir Ahmed and Mehrin Anannya and Md. Farhad Hossain},
 comment = {},
 doi = {},
 eprint = {2508.00785v1},
 journal = {arXiv preprint},
 title = {Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors},
 url = {http://arxiv.org/abs/2508.00785v1},
 year = {2025}
}

@article{2508.00930v1,
 abstract = {Leveraging recent advances in the analysis of synergy and redundancy in systems of random variables, an adaptive version of the widely used metric Leave One Covariate Out (LOCO) has been recently proposed to quantify cooperative effects in feature importance (Hi-Fi), a key technique in explainable artificial intelligence (XAI), so as to disentangle high-order effects involving a particular input feature in regression problems. Differently from standard feature importance tools, where a single score measures the relevance of each feature, each feature is here characterized by three scores, a two-body (unique) score and higher-order scores (redundant and synergistic). This paper presents a framework to assign those three scores (unique, redundant, and synergistic) to each individual pattern of the data set, while comparing it with the well-known measure of feature importance named {\it Shapley effect}. To illustrate the potential of the proposed framework, we focus on a One-Health application: the relation between air pollutants and Alzheimer's disease mortality rate. Our main result is the synergistic association between features related to $O_3$ and $NO_2$ with mortality, especially in the provinces of Bergamo e Brescia; notably also the density of urban green areas displays synergistic influence with pollutants for the prediction of AD mortality. Our results place local Hi-Fi as a promising tool of wide applicability, which opens new perspectives for XAI as well as to analyze high-order relationships in complex systems.},
 author = {M. Ontivero-Ortega and A. Fania and A. Lacalamita and R. Bellotti and A. Monaco and S. Stramaglia},
 comment = {},
 doi = {},
 eprint = {2508.00930v1},
 journal = {arXiv preprint},
 title = {Cooperative effects in feature importance of individual patterns: application to air pollutants and Alzheimer disease},
 url = {http://arxiv.org/abs/2508.00930v1},
 year = {2025}
}

@article{2508.01228v2,
 abstract = {Modeling forest dynamics under novel climatic conditions requires a careful balance between process-based understanding and empirical flexibility. Dynamic Vegetation Models (DVM) represent ecological processes mechanistically, but their performance is prone to misspecified assumptions about functional forms. Inferring the structure of these processes and their functional forms correctly from data remains a major challenge because current approaches, such as plug-in estimators, have proven ineffective. We introduce Forest Informed Neural Networks (FINN), a hybrid modeling approach that combines a forest gap model with deep neural networks (DNN). FINN replaces processes with DNNs, which are then calibrated alongside the other mechanistic components in one unified step. In a case study on the Barro Colorado Island 50-ha plot we demonstrate that replacing the growth process with a DNN improves predictive performance and succession trajectories compared to a mechanistic version of FINN. Furthermore, we discovered that the DNN learned an ecologically plausible, improved functional form of the growth process, which we extracted from the DNN using explainable AI. In conclusion, our new hybrid modeling approach offers a versatile opportunity to infer forest dynamics from data and to improve forecasts of ecosystem trajectories under unprecedented environmental change.},
 author = {Maximilian Pichler and Yannek Käber},
 comment = {29 pages, 17 figures},
 doi = {},
 eprint = {2508.01228v2},
 journal = {arXiv preprint},
 title = {Inferring processes within dynamic forest models using hybrid modeling},
 url = {http://arxiv.org/abs/2508.01228v2},
 year = {2025}
}

@article{2508.01851v1,
 abstract = {The increasing development in the consumer credit card market brings substantial regulatory and risk management challenges. The advanced machine learning models applications bring concerns about model transparency and fairness for both financial institutions and regulatory departments. In this study, we evaluate the consistency of one commonly used Explainable AI (XAI) technology, SHAP, for variable explanation in credit card probability of default models via a case study about credit card default prediction. The study shows the consistency is related to the variable importance level and hence provides practical recommendation for credit risk management},
 author = {Luyun Lin and Yiqing Wang},
 comment = {17 Pages, 6 Figures, and 7 Tables},
 doi = {10.3390/risks13120238},
 eprint = {2508.01851v1},
 journal = {arXiv preprint},
 title = {SHAP Stability in Credit Risk Management: A Case Study in Credit Card Default Model},
 url = {http://arxiv.org/abs/2508.01851v1},
 year = {2025}
}

@article{2508.02283v1,
 abstract = {In insurance fraud prediction, handling class imbalance remains a critical challenge. This paper presents a novel multistage focal loss function designed to enhance the performance of machine learning models in such imbalanced settings by helping to escape local minima and converge to a good solution. Building upon the foundation of the standard focal loss, our proposed approach introduces a dynamic, multi-stage convex and nonconvex mechanism that progressively adjusts the focus on hard-to-classify samples across training epochs. This strategic refinement facilitates more stable learning and improved discrimination between fraudulent and legitimate cases. Through extensive experimentation on a real-world insurance dataset, our method achieved better performance than the traditional focal loss, as measured by accuracy, precision, F1-score, recall and Area Under the Curve (AUC) metrics on the auto insurance dataset. These results demonstrate the efficacy of the multistage focal loss in boosting model robustness and predictive accuracy in highly skewed classification tasks, offering significant implications for fraud detection systems in the insurance industry. An explainable model is included to interpret the results.},
 author = {Francis Boabang and Samuel Asante Gyamerah},
 comment = {28 pages, 4 figures, 2 tables},
 doi = {},
 eprint = {2508.02283v1},
 journal = {arXiv preprint},
 title = {An Enhanced Focal Loss Function to Mitigate Class Imbalance in Auto Insurance Fraud Detection with Explainable AI},
 url = {http://arxiv.org/abs/2508.02283v1},
 year = {2025}
}

@article{2508.02560v1,
 abstract = {Trustworthy interpretation of deep learning models is critical for neuroimaging applications, yet commonly used Explainable AI (XAI) methods lack rigorous validation, risking misinterpretation. We performed the first large-scale, systematic comparison of XAI methods on ~45,000 structural brain MRIs using a novel XAI validation framework. This framework establishes verifiable ground truth by constructing prediction tasks with known signal sources - from localized anatomical features to subject-specific clinical lesions - without artificially altering input images. Our analysis reveals systematic failures in two of the most widely used methods: GradCAM consistently failed to localize predictive features, while Layer-wise Relevance Propagation generated extensive, artifactual explanations that suggest incompatibility with neuroimaging data characteristics. Our results indicate that these failures stem from a domain mismatch, where methods with design principles tailored to natural images require substantial adaptation for neuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad, which makes fewer assumptions about data structure, proved consistently accurate, suggesting its conceptual simplicity makes it more robust to this domain shift. These findings highlight the need for domain-specific adaptation and validation of XAI methods, suggest that interpretations from prior neuroimaging studies using standard XAI methodology warrant re-evaluation, and provide urgent guidance for practical application of XAI in neuroimaging.},
 author = {Nys Tjade Siegel and James H. Cole and Mohamad Habes and Stefan Haufe and Kerstin Ritter and Marc-André Schulz},
 comment = {},
 doi = {},
 eprint = {2508.02560v1},
 journal = {arXiv preprint},
 title = {Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application},
 url = {http://arxiv.org/abs/2508.02560v1},
 year = {2025}
}

@article{2508.02593v1,
 abstract = {Traditional surgical skill acquisition relies heavily on expert feedback, yet direct access is limited by faculty availability and variability in subjective assessments. While trainees can practice independently, the lack of personalized, objective, and quantitative feedback reduces the effectiveness of self-directed learning. Recent advances in computer vision and machine learning have enabled automated surgical skill assessment, demonstrating the feasibility of automatic competency evaluation. However, it is unclear whether such Artificial Intelligence (AI)-driven feedback can contribute to skill acquisition. Here, we examine the effectiveness of explainable AI (XAI)-generated feedback in surgical training through a human-AI study. We create a simulation-based training framework that utilizes XAI to analyze videos and extract surgical skill proxies related to primitive actions. Our intervention provides automated, user-specific feedback by comparing trainee performance to expert benchmarks and highlighting deviations from optimal execution through understandable proxies for actionable guidance. In a prospective user study with medical students, we compare the impact of XAI-guided feedback against traditional video-based coaching on task outcomes, cognitive load, and trainees' perceptions of AI-assisted learning. Results showed improved cognitive load and confidence post-intervention. While no differences emerged between the two feedback types in reducing performance gaps or practice adjustments, trends in the XAI group revealed desirable effects where participants more closely mimicked expert practice. This work encourages the study of explainable AI in surgical education and the development of data-driven, adaptive feedback mechanisms that could transform learning experiences and competency assessment.},
 author = {Catalina Gomez and Lalithkumar Seenivasan and Xinrui Zou and Jeewoo Yoon and Sirui Chu and Ariel Leong and Patrick Kramer and Yu-Chun Ku and Jose L. Porras and Alejandro Martin-Gomez and Masaru Ishii and Mathias Unberath},
 comment = {10 pages, 4 figures},
 doi = {},
 eprint = {2508.02593v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition},
 url = {http://arxiv.org/abs/2508.02593v1},
 year = {2025}
}

@article{2508.03586v1,
 abstract = {Explainable AI (XAI) builds trust in complex systems through model attribution methods that reveal the decision rationale. However, due to the absence of a unified optimal explanation, existing XAI methods lack a ground truth for objective evaluation and optimization. To address this issue, we propose Deep architecture-based Faith explainer (DeepFaith), a domain-free and model-agnostic unified explanation framework under the lens of faithfulness. By establishing a unified formulation for multiple widely used and well-validated faithfulness metrics, we derive an optimal explanation objective whose solution simultaneously achieves optimal faithfulness across these metrics, thereby providing a ground truth from a theoretical perspective. We design an explainer learning framework that leverages multiple existing explanation methods, applies deduplicating and filtering to construct high-quality supervised explanation signals, and optimizes both pattern consistency loss and local correlation to train a faithful explainer. Once trained, DeepFaith can generate highly faithful explanations through a single forward pass without accessing the model being explained. On 12 diverse explanation tasks spanning 6 models and 6 datasets, DeepFaith achieves the highest overall faithfulness across 10 metrics compared to all baseline methods, highlighting its effectiveness and cross-domain generalizability.},
 author = {Yuhan Guo and Lizhong Ding and Shihan Jia and Yanyu Ren and Pengqi Li and Jiarun Fu and Changsheng Li and Ye yuan and Guoren Wang},
 comment = {22 pages},
 doi = {},
 eprint = {2508.03586v1},
 journal = {arXiv preprint},
 title = {DeepFaith: A Domain-Free and Model-Agnostic Unified Framework for Highly Faithful Explanations},
 url = {http://arxiv.org/abs/2508.03586v1},
 year = {2025}
}

@article{2508.03769v1,
 abstract = {The study addresses the paradigm shift in corporate management, where AI is moving from a decision support tool to an autonomous decision-maker, with some AI systems already appointed to leadership roles in companies. A central problem identified is that the development of AI technologies is far outpacing the creation of adequate legal and ethical guidelines.
  The research proposes a "reference model" for the development and implementation of autonomous AI systems in corporate management. This model is based on a synthesis of several key components to ensure legitimate and ethical decision-making. The model introduces the concept of "computational law" or "algorithmic law". This involves creating a separate legal framework for AI systems, with rules and regulations translated into a machine-readable, algorithmic format to avoid the ambiguity of natural language. The paper emphasises the need for a "dedicated operational context" for autonomous AI systems, analogous to the "operational design domain" for autonomous vehicles. This means creating a specific, clearly defined environment and set of rules within which the AI can operate safely and effectively. The model advocates for training AI systems on controlled, synthetically generated data to ensure fairness and ethical considerations are embedded from the start. Game theory is also proposed as a method for calculating the optimal strategy for the AI to achieve its goals within these ethical and legal constraints. The provided analysis highlights the importance of explainable AI (XAI) to ensure the transparency and accountability of decisions made by autonomous systems. This is crucial for building trust and for complying with the "right to explanation".},
 author = {Anna Romanova},
 comment = {},
 doi = {10.13140/RG.2.2.34004.10888},
 eprint = {2508.03769v1},
 journal = {arXiv preprint},
 title = {Development of management systems using artificial intelligence systems and machine learning methods for boards of directors (preprint, unofficial translation)},
 url = {http://arxiv.org/abs/2508.03769v1},
 year = {2025}
}

@article{2508.03913v1,
 abstract = {Distance-based classifiers, such as k-nearest neighbors and support vector machines, continue to be a workhorse of machine learning, widely used in science and industry. In practice, to derive insights from these models, it is also important to ensure that their predictions are explainable. While the field of Explainable AI has supplied methods that are in principle applicable to any model, it has also emphasized the usefulness of latent structures (e.g. the sequence of layers in a neural network) to produce explanations. In this paper, we contribute by uncovering a hidden neural network structure in distance-based classifiers (consisting of linear detection units combined with nonlinear pooling layers) upon which Explainable AI techniques such as layer-wise relevance propagation (LRP) become applicable. Through quantitative evaluations, we demonstrate the advantage of our novel explanation approach over several baselines. We also show the overall usefulness of explaining distance-based models through two practical use cases.},
 author = {Florian Bley and Jacob Kauffmann and Simon León Krug and Klaus-Robert Müller and Grégoire Montavon},
 comment = {},
 doi = {},
 eprint = {2508.03913v1},
 journal = {arXiv preprint},
 title = {Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures},
 url = {http://arxiv.org/abs/2508.03913v1},
 year = {2025}
}

@article{2508.04427v1,
 abstract = {Multimodal learning has witnessed remarkable advancements in recent years, particularly with the integration of attention-based models, leading to significant performance gains across a variety of tasks. Parallel to this progress, the demand for explainable artificial intelligence (XAI) has spurred a growing body of research aimed at interpreting the complex decision-making processes of these models. This systematic literature review analyzes research published between January 2020 and early 2024 that focuses on the explainability of multimodal models. Framed within the broader goals of XAI, we examine the literature across multiple dimensions, including model architecture, modalities involved, explanation algorithms and evaluation methodologies. Our analysis reveals that the majority of studies are concentrated on vision-language and language-only models, with attention-based techniques being the most commonly employed for explanation. However, these methods often fall short in capturing the full spectrum of interactions between modalities, a challenge further compounded by the architectural heterogeneity across domains. Importantly, we find that evaluation methods for XAI in multimodal settings are largely non-systematic, lacking consistency, robustness, and consideration for modality-specific cognitive and contextual factors. Based on these findings, we provide a comprehensive set of recommendations aimed at promoting rigorous, transparent, and standardized evaluation and reporting practices in multimodal XAI research. Our goal is to support future research in more interpretable, accountable, and responsible mulitmodal AI systems, with explainability at their core.},
 author = {Md Raisul Kibria and Sébastien Lafond and Janan Arslan},
 comment = {},
 doi = {},
 eprint = {2508.04427v1},
 journal = {arXiv preprint},
 title = {Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models},
 url = {http://arxiv.org/abs/2508.04427v1},
 year = {2025}
}

@article{2508.05846v1,
 abstract = {As artificial intelligence (AI) and robotics increasingly permeate society, ensuring the ethical behavior of these systems has become paramount. This paper contends that transparency in AI decision-making processes is fundamental to developing trustworthy and ethically aligned robotic systems. We explore how transparency facilitates accountability, enables informed consent, and supports the debugging of ethical algorithms. The paper outlines technical, ethical, and practical challenges in implementing transparency and proposes novel approaches to enhance it, including standardized metrics, explainable AI techniques, and user-friendly interfaces. This paper introduces a framework that connects technical implementation with ethical considerations in robotic systems, focusing on the specific challenges of achieving transparency in dynamic, real-world contexts. We analyze how prioritizing transparency can impact public trust, regulatory policies, and avenues for future research. By positioning transparency as a fundamental element in ethical AI system design, we aim to add to the ongoing discussion on responsible AI and robotics, providing direction for future advancements in this vital field.},
 author = {Ahmad Farooq and Kamran Iqbal},
 comment = {Published in the Proceedings of the 2025 3rd International Conference on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 tables},
 doi = {10.1145/3747393.374740},
 eprint = {2508.05846v1},
 journal = {arXiv preprint},
 title = {Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems},
 url = {http://arxiv.org/abs/2508.05846v1},
 year = {2025}
}

@article{2508.06129v1,
 abstract = {The Vehicle Routing Problem (VRP) is a complex optimization problem with numerous real-world applications, mostly solved using metaheuristic algorithms due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely on human-crafted designs developed through empirical studies. However, recent research shows that machine learning methods can be used the structural characteristics of solutions in combinatorial optimization, thereby aiding in designing more efficient algorithms, particularly for solving VRP. Building on this advancement, this study extends the previous research by conducting a sensitivity analysis using multiple classifier models that are capable of predicting the quality of VRP solutions. Hence, by leveraging explainable AI, this research is able to extend the understanding of how these models make decisions. Finally, our findings indicate that while feature importance varies, certain features consistently emerge as strong predictors. Furthermore, we propose a unified framework able of ranking feature impact across different scenarios to illustrate this finding. These insights highlight the potential of feature importance analysis as a foundation for developing a guidance mechanism of metaheuristic algorithms for solving the VRP.},
 author = {Bachtiar Herdianto and Romain Billot and Flavien Lucas and Marc Sevaux},
 comment = {22 pages, 14 figures},
 doi = {},
 eprint = {2508.06129v1},
 journal = {arXiv preprint},
 title = {Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem},
 url = {http://arxiv.org/abs/2508.06129v1},
 year = {2025}
}

@article{2508.06996v3,
 abstract = {We explore machine learning techniques for predicting Curie temperatures of magnetic materials using the NEMAD database. By augmenting the dataset with composition-based and domain-aware descriptors, we evaluate the performance of several machine learning models. We find that the Extra Trees Regressor delivers the best performance reaching an R^2 score of up to 0.85 $\pm$ 0.01 (cross-validated) for a balanced dataset. We employ the k-means clustering algorithm to gain insights into the performance of chemically distinct material groups. Furthermore, we perform the SHAP analysis to identify key physicochemical drivers of Curie behavior, such as average atomic number and magnetic moment. By employing explainable AI techniques, this analysis offers insights into the model's predictive behavior, thereby advancing scientific interpretability.},
 author = {M. Adeel Ajaib and Fariha Nasir and Abdul Rehman},
 comment = {7 pages, 6 figures, minor corrections},
 doi = {},
 eprint = {2508.06996v3},
 journal = {arXiv preprint},
 title = {Explainable AI for Curie Temperature Prediction in Magnetic Materials},
 url = {http://arxiv.org/abs/2508.06996v3},
 year = {2025}
}

@article{2508.07027v1,
 abstract = {This paper considers Ecogame, an innovative art project of 1970, whose creators believed in a positive vision of a technological future; an understanding, posited on cybernetics, of a future that could be participatory via digital means, and therefore more democratised. Using simulation and early machine learning techniques over a live network, Ecogame combined the power of visual art with cybernetic concepts of adaptation, feedback, and control to propose that behaviour had implications for the total system. It provides an historical precedent for contemporary AI-driven art about using AI in a more human-centred way.},
 author = {Catherine Mason},
 comment = {In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts 2025) arXiv:2406.14485},
 doi = {},
 eprint = {2508.07027v1},
 journal = {arXiv preprint},
 title = {Making Effective Decisions: Machine Learning and the Ecogame in 1970},
 url = {http://arxiv.org/abs/2508.07027v1},
 year = {2025}
}

@article{2508.07183v1,
 abstract = {Explainable AI (XAI) in creative contexts can go beyond transparency to support artistic engagement, modifiability, and sustained practice. While curated datasets and training human-scale models can offer artists greater agency and control, large-scale generative models like text-to-image diffusion systems often obscure these possibilities. We suggest that even large models can be treated as creative materials if their internal structure is exposed and manipulable. We propose a craft-based approach to explainability rooted in long-term, hands-on engagement akin to Schön's "reflection-in-action" and demonstrate its application through a model-bending and inspection plugin integrated into the node-based interface of ComfyUI. We demonstrate that by interactively manipulating different parts of a generative model, artists can develop an intuition about how each component influences the output.},
 author = {Ahmed M. Abuzuraiq and Philippe Pasquier},
 comment = {In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts 2025) arXiv:2406.14485},
 doi = {},
 eprint = {2508.07183v1},
 journal = {arXiv preprint},
 title = {Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI},
 url = {http://arxiv.org/abs/2508.07183v1},
 year = {2025}
}

@article{2508.08273v1,
 abstract = {Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.},
 author = {Kristian Miok and Blaz Škrlj and Daniela Zaharie and Marko Robnik Šikonja},
 comment = {},
 doi = {},
 eprint = {2508.08273v1},
 journal = {arXiv preprint},
 title = {TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning},
 url = {http://arxiv.org/abs/2508.08273v1},
 year = {2025}
}

@article{2508.08966v1,
 abstract = {The attention mechanism lies at the core of the transformer architecture, providing an interpretable model-internal signal that has motivated a growing interest in attention-based model explanations. Although attention weights do not directly determine model outputs, they reflect patterns of token influence that can inform and complement established explainability techniques. This work studies the potential of utilising the information encoded in attention weights to provide meaningful model explanations by integrating them into explainable AI (XAI) frameworks that target fundamentally different aspects of model behaviour. To this end, we develop two novel explanation methods applicable to both natural language processing and computer vision tasks. The first integrates attention weights into the Shapley value decomposition by redefining the characteristic function in terms of pairwise token interactions via attention weights, thus adapting this widely used game-theoretic solution concept to provide attention-driven attributions for local explanations. The second incorporates attention weights into token-level directional derivatives defined through concept activation vectors to measure concept sensitivity for global explanations. Our empirical evaluations on standard benchmarks and in a comparison study with widely used explanation methods show that attention weights can be meaningfully incorporated into the studied XAI frameworks, highlighting their value in enriching transformer explainability.},
 author = {Marte Eggen and Jacob Lysnæs-Larsen and Inga Strümke},
 comment = {},
 doi = {},
 eprint = {2508.08966v1},
 journal = {arXiv preprint},
 title = {Integrating attention into explanation frameworks for language and vision transformers},
 url = {http://arxiv.org/abs/2508.08966v1},
 year = {2025}
}

@article{2508.09162v3,
 abstract = {Next generation advanced nuclear reactors are expected to be smaller both in size and power output, relying extensively on fully digital instrumentation and control systems. These reactors will generate a large flow of information in the form of multivariate time series data, conveying simultaneously various non linear cyber physical, process, control, sensor, and operational states. Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data. Here, we propose an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm to fully characterize real-time replay attacks, i.e., detection, source identification, timing and type, of increasing complexity during a dynamic time evolving reactor process. The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.},
 author = {Konstantinos Vasili and Zachery T. Dahm and Stylianos Chatzidakis},
 comment = {Added references, corrected typos, grammar check, authors updated},
 doi = {},
 eprint = {2508.09162v3},
 journal = {arXiv preprint},
 title = {An Unsupervised Deep Explainable AI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals},
 url = {http://arxiv.org/abs/2508.09162v3},
 year = {2025}
}

@article{2508.09187v1,
 abstract = {Breath analysis has emerged as a critical tool in health monitoring, offering insights into respiratory function, disease detection, and continuous health assessment. While traditional contact-based methods are reliable, they often pose challenges in comfort and practicality, particularly for long-term monitoring. This survey comprehensively examines contact-based and contactless approaches, emphasizing recent advances in machine learning and deep learning techniques applied to breath analysis. Contactless methods, including Wi-Fi Channel State Information and acoustic sensing, are analyzed for their ability to provide accurate, noninvasive respiratory monitoring. We explore a broad range of applications, from single-user respiratory rate detection to multi-user scenarios, user identification, and respiratory disease detection. Furthermore, this survey details essential data preprocessing, feature extraction, and classification techniques, offering comparative insights into machine learning/deep learning models suited to each approach. Key challenges like dataset scarcity, multi-user interference, and data privacy are also discussed, along with emerging trends like Explainable AI, federated learning, transfer learning, and hybrid modeling. By synthesizing current methodologies and identifying open research directions, this survey offers a comprehensive framework to guide future innovations in breath analysis, bridging advanced technological capabilities with practical healthcare applications.},
 author = {Almustapha A. Wakili and Babajide J. Asaju and Woosub Jung},
 comment = {},
 doi = {10.1016/j.smhl.2025.100579},
 eprint = {2508.09187v1},
 journal = {arXiv preprint},
 title = {Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring},
 url = {http://arxiv.org/abs/2508.09187v1},
 year = {2025}
}

@article{2508.09810v1,
 abstract = {Biomechanical features have become important indicators for evaluating athletes' techniques. Traditionally, experts propose significant features and evaluate them using physics equations. However, the complexity of the human body and its movements makes it challenging to explicitly analyze the relationships between some features and athletes' final performance. With advancements in modern machine learning and statistics, data analytics methods have gained increasing importance in sports analytics. In this study, we leverage machine learning models to analyze expert-proposed biomechanical features from the finals of long jump competitions in the World Championships. The objectives of the analysis include identifying the most important features contributing to top-performing jumps and exploring the combined effects of these key features. Using quantile regression, we model the relationship between the biomechanical feature set and the target variable (effective distance), with a particular focus on elite-level jumps. To interpret the model, we apply SHapley Additive exPlanations (SHAP) alongside Partial Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The findings reveal that, beyond the well-documented velocity-related features, specific technical aspects also play a pivotal role. For male athletes, the angle of the knee of the supporting leg before take-off is identified as a key factor for achieving top 10% performance in our dataset, with angles greater than 169°contributing significantly to jump performance. In contrast, for female athletes, the landing pose and approach step technique emerge as the most critical features influencing top 10% performances, alongside velocity. This study establishes a framework for analyzing the impact of various features on athletic performance, with a particular emphasis on top-performing events.},
 author = {Qi Gan and Stephan Clémençon and Mounîm A. El-Yacoubi and Sao Mai Nguyen and Eric Fenaux and Ons Jelassi},
 comment = {15 pages, 6 figures},
 doi = {},
 eprint = {2508.09810v1},
 journal = {arXiv preprint},
 title = {Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques},
 url = {http://arxiv.org/abs/2508.09810v1},
 year = {2025}
}

@article{2508.10210v3,
 abstract = {Monitoring cattle health and optimizing yield are key challenges faced by dairy farmers due to difficulties in tracking all animals on the farm. This work aims to showcase modern data-driven farming practices based on explainable machine learning(ML) methods that explain the activity and behaviour of dairy cattle (cows). Continuous data collection of 3-axis accelerometer sensors and usage of robust ML methodologies and algorithms, provide farmers and researchers with actionable information on cattle activity, allowing farmers to make informed decisions and incorporate sustainable practices. This study utilizes Bluetooth-based Internet of Things (IoT) devices and 4G networks for seamless data transmission, immediate analysis, inference generation, and explains the models performance with explainability frameworks. Special emphasis is put on the pre-processing of the accelerometers time series data, including the extraction of statistical characteristics, signal processing techniques, and lag-based features using the sliding window technique. Various hyperparameter-optimized ML models are evaluated across varying window lengths for activity classification. The k-nearest neighbour Classifier achieved the best performance, with AUC of mean 0.98 and standard deviation of 0.0026 on the training set and 0.99 on testing set). In order to ensure transparency, Explainable AI based frameworks such as SHAP is used to interpret feature importance that can be understood and used by practitioners. A detailed comparison of the important features, along with the stability analysis of selected features, supports development of explainable and practical ML models for sustainable livestock management.},
 author = {Rahul Jana and Shubham Dixit and Mrityunjay Sharma and Ritesh Kumar},
 comment = {},
 doi = {},
 eprint = {2508.10210v3},
 journal = {arXiv preprint},
 title = {An Explainable AI based approach for Monitoring Animal Health},
 url = {http://arxiv.org/abs/2508.10210v3},
 year = {2025}
}

@article{2508.10241v1,
 abstract = {This work demonstrates how the concept of the entropic potential of events -- a parameter quantifying the influence of discrete events on the expected future entropy of a system -- can enhance uncertainty quantification, decision-making, and interpretability in artificial intelligence (AI). Building on its original formulation in physics, the framework is adapted for AI by introducing an event-centric measure that captures how actions, observations, or other discrete occurrences impact uncertainty at future time horizons. Both the original and AI-adjusted definitions of entropic potential are formalized, with the latter emphasizing conditional expectations to account for counterfactual scenarios. Applications are explored in policy evaluation, intrinsic reward design, explainable AI, and anomaly detection, highlighting the metric's potential to unify and strengthen uncertainty modeling in intelligent systems. Conceptual examples illustrate its use in reinforcement learning, Bayesian inference, and anomaly detection, while practical considerations for computation in complex AI models are discussed. The entropic potential framework offers a theoretically grounded, interpretable, and versatile approach to managing uncertainty in AI, bridging principles from thermodynamics, information theory, and machine learning.},
 author = {Mark Zilberman},
 comment = {10 pages},
 doi = {},
 eprint = {2508.10241v1},
 journal = {arXiv preprint},
 title = {Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence},
 url = {http://arxiv.org/abs/2508.10241v1},
 year = {2025}
}

@article{2508.10860v1,
 abstract = {Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.},
 author = {Zhaokun Jiang and Ziyin Zhang},
 comment = {},
 doi = {},
 eprint = {2508.10860v1},
 journal = {arXiv preprint},
 title = {From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms},
 url = {http://arxiv.org/abs/2508.10860v1},
 year = {2025}
}

@article{2508.11529v1,
 abstract = {Artificial intelligence is reshaping science and industry, yet many users still regard its models as opaque "black boxes". Conventional explainable artificial-intelligence methods clarify individual predictions but overlook the upstream decisions and downstream quality checks that determine whether insights can be trusted. In this work, we present Holistic Explainable Artificial Intelligence (HXAI), a user-centric framework that embeds explanation into every stage of the data-analysis workflow and tailors those explanations to users. HXAI unifies six components (data, analysis set-up, learning process, model output, model quality, communication channel) into a single taxonomy and aligns each component with the needs of domain experts, data analysts and data scientists. A 112-item question bank covers these needs; our survey of contemporary tools highlights critical coverage gaps. Grounded in theories of human explanation, principles from human-computer interaction and findings from empirical user studies, HXAI identifies the characteristics that make explanations clear, actionable and cognitively manageable. A comprehensive taxonomy operationalises these insights, reducing terminological ambiguity and enabling rigorous coverage analysis of existing toolchains. We further demonstrate how AI agents that embed large-language models can orchestrate diverse explanation techniques, translating technical artifacts into stakeholder-specific narratives that bridge the gap between AI developers and domain experts. Departing from traditional surveys or perspective articles, this work melds concepts from multiple disciplines, lessons from real-world projects and a critical synthesis of the literature to advance a novel, end-to-end viewpoint on transparency, trustworthiness and responsible AI deployment.},
 author = {George Paterakis and Andrea Castellani and George Papoutsoglou and Tobias Rodemann and Ioannis Tsamardinos},
 comment = {Preprint. Currently under review at "Artificial Intelligence Review" journal},
 doi = {},
 eprint = {2508.11529v1},
 journal = {arXiv preprint},
 title = {A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow},
 url = {http://arxiv.org/abs/2508.11529v1},
 year = {2025}
}

@article{2508.11710v1,
 abstract = {Security vulnerabilities present in a code that has been written in diverse programming languages are among the most critical yet complicated aspects of source code to detect. Static analysis tools based on rule-based patterns usually do not work well at detecting the context-dependent bugs and lead to high false positive rates. Recent developments in artificial intelligence, specifically the use of transformer-based models like CodeBERT and CodeLlama, provide light to this problem, as they show potential in finding such flaws better. This paper presents the implementations of these models on various datasets of code vulnerability, showing how off-the-shelf models can successfully produce predictive capacity in models through dynamic fine-tuning of the models on vulnerable and safe code fragments. The methodology comprises the gathering of the dataset, normalization of the language, fine-tuning of the model, and incorporation of ensemble learning and explainable AI. Experiments show that a well-trained CodeBERT can be as good as or even better than some existing static analyzers in terms of accuracy greater than 97%. Further study has indicated that although language models can achieve close-to-perfect recall, the precision can decrease. A solution to this is given by hybrid models and validation procedures, which will reduce false positives. According to the results, the AI-based solutions generalize to different programming languages and classes of vulnerability. Nevertheless, robustness, interpretability, and deployment readiness are still being developed. The results illustrate the probabilities that AI will enhance the trustworthiness in the usability and scalability of machine-learning-based detectors of vulnerabilities.},
 author = {Hael Abdulhakim Ali Humran and Ferdi Sonmez},
 comment = {},
 doi = {},
 eprint = {2508.11710v1},
 journal = {arXiv preprint},
 title = {Code Vulnerability Detection Across Different Programming Languages with AI Models},
 url = {http://arxiv.org/abs/2508.11710v1},
 year = {2025}
}

@article{2508.11959v1,
 abstract = {Feature attribution methods based on game theory are ubiquitous in the field of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous feature attribution using logic-based explanations, specifically targeting high-stakes uses of machine learning (ML) models. Typically, such works exploit weak abductive explanation (WAXp) as the characteristic function to assign importance to features. However, one possible downside is that the contribution of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important information, because of the relationship between formal explanations (XPs) and adversarial examples (AExs). Accordingly, this paper leverages Shapley value and Banzhaf index to devise two novel feature importance scores. We take into account non-WAXp sets when computing feature contribution, and the novel scores quantify how effective each feature is at excluding AExs. Furthermore, the paper identifies properties and studies the computational complexity of the proposed scores.},
 author = {Xuanxiang Huang and Olivier Létoffé and Joao Marques-Silva},
 comment = {},
 doi = {},
 eprint = {2508.11959v1},
 journal = {arXiv preprint},
 title = {Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index},
 url = {http://arxiv.org/abs/2508.11959v1},
 year = {2025}
}

@article{2508.12623v1,
 abstract = {Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in scientific research. However, the price we pay for their impressively accurate predictions is significant: their inner workings are notoriously opaque - it is unknown to laypeople and researchers alike what features of the data a DL system focuses on and how it ultimately succeeds in predicting correct outputs. A necessary criterion for trustworthy explanations is that they should reflect the relevant processes the algorithms' predictions are based on. The field of eXplainable Artificial Intelligence (XAI) presents promising methods to create such explanations. But recent reviews about their performance offer reasons for skepticism. As we will argue, a good criterion for trustworthiness is explanatory robustness: different XAI methods produce the same explanations in comparable contexts. However, in some instances, all methods may give the same, but still wrong, explanation. We therefore argue that in addition to explanatory robustness (ER), a prior requirement of explanation method robustness (EMR) has to be fulfilled by every XAI method. Conversely, the robustness of an individual method is in itself insufficient for trustworthiness. In what follows, we develop and formalize criteria for ER as well as EMR, providing a framework for explaining and establishing trust in DL algorithms. We also highlight interesting application cases and outline directions for future work.},
 author = {Florian J. Boge and Annika Schuster},
 comment = {8 pages, 1 figure},
 doi = {},
 eprint = {2508.12623v1},
 journal = {arXiv preprint},
 title = {How can we trust opaque systems? Criteria for robust explanations in XAI},
 url = {http://arxiv.org/abs/2508.12623v1},
 year = {2025}
}

@article{2508.13529v1,
 abstract = {The opaqueness of many complex machine learning algorithms is often mentioned as one of the main obstacles to the ethical development of artificial intelligence (AI). But what does it mean for an algorithm to be opaque? Highly complex algorithms such as artificial neural networks process enormous volumes of data in parallel along multiple hidden layers of interconnected nodes, rendering their inner workings epistemically inaccessible to any human being, including their designers and developers; they are "black boxes" for all their stakeholders. But opaqueness is not always the inevitable result of technical complexity. Sometimes, the way an algorithm works is intentionally hidden from view for proprietary reasons, especially in commercial automated decision systems, creating an entirely different type of opaqueness. In the first part of the chapter, we will examine these two ways of understanding opacity and the ethical implications that stem from each of them. In the second part, we explore the different explanatory methods that have been developed in computer science to overcome an AI system's technical opaqueness. As the analysis shows, explainable AI (XAI) still faces numerous challenges.},
 author = {Andrés Páez},
 comment = {},
 doi = {10.1002/9781394240821.ch11},
 eprint = {2508.13529v1},
 journal = {arXiv preprint},
 title = {Explainability of Algorithms},
 url = {http://arxiv.org/abs/2508.13529v1},
 year = {2025}
}

@article{2508.13672v2,
 abstract = {Explainable Artificial Intelligence (XAI) methods, such as Local Interpretable Model-Agnostic Explanations (LIME), have advanced the interpretability of black-box machine learning models by approximating their behavior locally using interpretable surrogate models. However, LIME's inherent randomness in perturbation and sampling can lead to locality and instability issues, especially in scenarios with limited training data. In such cases, data scarcity can result in the generation of unrealistic variations and samples that deviate from the true data manifold. Consequently, the surrogate model may fail to accurately approximate the complex decision boundary of the original model. To address these challenges, we propose a novel Instance-based Transfer Learning LIME framework (ITL-LIME) that enhances explanation fidelity and stability in data-constrained environments. ITL-LIME introduces instance transfer learning into the LIME framework by leveraging relevant real instances from a related source domain to aid the explanation process in the target domain. Specifically, we employ clustering to partition the source domain into clusters with representative prototypes. Instead of generating random perturbations, our method retrieves pertinent real source instances from the source cluster whose prototype is most similar to the target instance. These are then combined with the target instance's neighboring real instances. To define a compact locality, we further construct a contrastive learning-based encoder as a weighting mechanism to assign weights to the instances from the combined set based on their proximity to the target instance. Finally, these weighted source and target instances are used to train the surrogate model for explanation purposes.},
 author = {Rehan Raza and Guanjin Wang and Kok Wai Wong and Hamid Laga and Marco Fisichella},
 comment = {Accepted at the 34th ACM International Conference on Information and Knowledge Management (CIKM 2025)},
 doi = {},
 eprint = {2508.13672v2},
 journal = {arXiv preprint},
 title = {ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings},
 url = {http://arxiv.org/abs/2508.13672v2},
 year = {2025}
}

@article{2508.13715v1,
 abstract = {This paper proposes a Trans-XFed architecture that combines federated learning with explainable AI techniques for supply chain credit assessment. The proposed model aims to address several key challenges, including privacy, information silos, class imbalance, non-identically and independently distributed (Non-IID) data, and model interpretability in supply chain credit assessment. We introduce a performance-based client selection strategy (PBCS) to tackle class imbalance and Non-IID problems. This strategy achieves faster convergence by selecting clients with higher local F1 scores. The FedProx architecture, enhanced with homomorphic encryption, is used as the core model, and further incorporates a transformer encoder. The transformer encoder block provides insights into the learned features. Additionally, we employ the integrated gradient explainable AI technique to offer insights into decision-making. We demonstrate the effectiveness of Trans-XFed through experimental evaluations on real-world supply chain datasets. The obtained results show its ability to deliver accurate credit assessments compared to several baselines, while maintaining transparency and privacy.},
 author = {Jie Shi and Arno P. J. M. Siebes and Siamak Mehrkanoon},
 comment = {Accepted by FLTA 2025},
 doi = {},
 eprint = {2508.13715v1},
 journal = {arXiv preprint},
 title = {Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment},
 url = {http://arxiv.org/abs/2508.13715v1},
 year = {2025}
}

@article{2508.14499v1,
 abstract = {Shapley values are widely recognized as a principled method for attributing importance to input features in machine learning. However, the exact computation of Shapley values scales exponentially with the number of features, severely limiting the practical application of this powerful approach. The challenge is further compounded when the predictive model is probabilistic - as in Gaussian processes (GPs) - where the outputs are random variables rather than point estimates, necessitating additional computational effort in modeling higher-order moments. In this work, we demonstrate that for an important class of GPs known as FANOVA GP, which explicitly models all main effects and interactions, *exact* Shapley attributions for both local and global explanations can be computed in *quadratic time*. For local, instance-wise explanations, we define a stochastic cooperative game over function components and compute the exact stochastic Shapley value in quadratic time only, capturing both the expected contribution and uncertainty. For global explanations, we introduce a deterministic, variance-based value function and compute exact Shapley values that quantify each feature's contribution to the model's overall sensitivity. Our methods leverage a closed-form (stochastic) Möbius representation of the FANOVA decomposition and introduce recursive algorithms, inspired by Newton's identities, to efficiently compute the mean and variance of Shapley values. Our work enhances the utility of explainable AI, as demonstrated by empirical studies, by providing more scalable, axiomatically sound, and uncertainty-aware explanations for predictions generated by structured probabilistic models.},
 author = {Majid Mohammadi and Krikamol Muandet and Ilaria Tiddi and Annette Ten Teije and Siu Lun Chau},
 comment = {},
 doi = {},
 eprint = {2508.14499v1},
 journal = {arXiv preprint},
 title = {Exact Shapley Attributions in Quadratic-time for FANOVA Gaussian Processes},
 url = {http://arxiv.org/abs/2508.14499v1},
 year = {2025}
}

@article{2508.14618v1,
 abstract = {Continuous Descent Operations (CDO) involve smooth, idle-thrust descents that avoid level-offs, reducing fuel burn, emissions, and noise while improving efficiency and passenger comfort. Despite its operational and environmental benefits, limited research has systematically examined the factors influencing CDO performance. Moreover, many existing methods in related areas, such as trajectory optimization, lack the transparency required in aviation, where explainability is critical for safety and stakeholder trust. This study addresses these gaps by proposing a Fuzzy-Enhanced Explainable AI (FEXAI) framework that integrates fuzzy logic with machine learning and SHapley Additive exPlanations (SHAP) analysis. For this purpose, a comprehensive dataset of 29 features, including 11 operational and 18 weather-related features, was collected from 1,094 flights using Automatic Dependent Surveillance-Broadcast (ADS-B) data. Machine learning models and SHAP were then applied to classify flights' CDO adherence levels and rank features by importance. The three most influential features, as identified by SHAP scores, were then used to construct a fuzzy rule-based classifier, enabling the extraction of interpretable fuzzy rules. All models achieved classification accuracies above 90%, with FEXAI providing meaningful, human-readable rules for operational users. Results indicated that the average descent rate within the arrival route, the number of descent segments, and the average change in directional heading during descent were the strongest predictors of CDO performance. The FEXAI method proposed in this study presents a novel pathway for operational decision support and could be integrated into aviation tools to enable real-time advisories that maintain CDO adherence under varying operational conditions.},
 author = {Amin Noroozi and Sandaruwan K. Sethunge and Elham Norouzi and Phat T. Phan and Kavinda U. Waduge and Md. Arafatur Rahman},
 comment = {},
 doi = {},
 eprint = {2508.14618v1},
 journal = {arXiv preprint},
 title = {A Fuzzy-Enhanced Explainable AI Framework for Flight Continuous Descent Operations Classification},
 url = {http://arxiv.org/abs/2508.14618v1},
 year = {2025}
}

@article{2508.14949v1,
 abstract = {This paper proposes an eXplainable Artificial Intelligence (XAI)-driven methodology to enhance the understanding of cough sound analysis for respiratory disease management. We employ occlusion maps to highlight relevant spectral regions in cough spectrograms processed by a Convolutional Neural Network (CNN). Subsequently, spectral analysis of spectrograms weighted by these occlusion maps reveals significant differences between disease groups, particularly in patients with COPD, where cough patterns appear more variable in the identified spectral regions of interest. This contrasts with the lack of significant differences observed when analyzing raw spectrograms. The proposed approach extracts and analyzes several spectral features, demonstrating the potential of XAI techniques to uncover disease-specific acoustic signatures and improve the diagnostic capabilities of cough sound analysis by providing more interpretable results.},
 author = {Patricia Amado-Caballero and Luis Miguel San-José-Revuelta and María Dolores Aguilar-García and José Ramón Garmendia-Leiza and Carlos Alberola-López and Pablo Casaseca-de-la-Higuera},
 comment = {},
 doi = {},
 eprint = {2508.14949v1},
 journal = {arXiv preprint},
 title = {XAI-Driven Spectral Analysis of Cough Sounds for Respiratory Disease Characterization},
 url = {http://arxiv.org/abs/2508.14949v1},
 year = {2025}
}

@article{2508.15364v1,
 abstract = {In user-centric design, persona development plays a vital role in understanding user behaviour, capturing needs, segmenting audiences, and guiding design decisions. However, the growing complexity of user interactions calls for a more contextualized approach to ensure designs align with real user needs. While earlier studies have advanced persona classification by modelling user behaviour, capturing contextual information, especially by integrating textual and tabular data, remains a key challenge. These models also often lack explainability, leaving their predictions difficult to interpret or justify. To address these limitations, we present ExBigBang (Explainable BigBang), a hybrid text-tabular approach that uses transformer-based architectures to model rich contextual features for persona classification. ExBigBang incorporates metadata, domain knowledge, and user profiling to embed deeper context into predictions. Through a cyclical process of user profiling and classification, our approach dynamically updates to reflect evolving user behaviours. Experiments on a benchmark persona classification dataset demonstrate the robustness of our model. An ablation study confirms the benefits of combining text and tabular data, while Explainable AI techniques shed light on the rationale behind the model's predictions.},
 author = {Saleh Afzoon and Amin Beheshti and Nabi Rezvani and Farshad Khunjush and Usman Naseem and John McMahon and Zahra Fathollahi and Mahdieh Labani and Wathiq Mansoor and Xuyun Zhang},
 comment = {},
 doi = {},
 eprint = {2508.15364v1},
 journal = {arXiv preprint},
 title = {ExBigBang: A Dynamic Approach for Explainable Persona Classification through Contextualized Hybrid Transformer Analysis},
 url = {http://arxiv.org/abs/2508.15364v1},
 year = {2025}
}

@article{2508.15569v1,
 abstract = {Understanding the nuanced performance of machine learning models is essential for responsible deployment, especially in high-stakes domains like healthcare and finance. This paper introduces a novel framework, Conformalized Exceptional Model Mining, which combines the rigor of Conformal Prediction with the explanatory power of Exceptional Model Mining (EMM). The proposed framework identifies cohesive subgroups within data where model performance deviates exceptionally, highlighting regions of both high confidence and high uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model Performance Evaluation), which quantifies uncertainty through conformal prediction's rigorous coverage guarantees. By defining a new quality measure, Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with exceptional performance patterns in multi-class classification and regression tasks. Experimental results across diverse datasets demonstrate the framework's effectiveness in uncovering interpretable subgroups that provide critical insights into model behavior. This work lays the groundwork for enhancing model interpretability and reliability, advancing the state-of-the-art in explainable AI and uncertainty quantification.},
 author = {Xin Du and Sikun Yang and Wouter Duivesteijn and Mykola Pechenizkiy},
 comment = {Accepted by ECML-PKDD},
 doi = {},
 eprint = {2508.15569v1},
 journal = {arXiv preprint},
 title = {Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well},
 url = {http://arxiv.org/abs/2508.15569v1},
 year = {2025}
}

@article{2508.15872v1,
 abstract = {The heart's electrical activity, recorded through Electrocardiography (ECG), is essential for diagnosing various cardiovascular conditions. However, many existing ECG segmentation models rely on complex, multi-layered architectures such as BiLSTM, which are computationally intensive and inefficient. This study introduces a streamlined architecture that combines spectral analysis with probabilistic predictions for ECG signal segmentation. By replacing complex layers with simpler ones, the model effectively captures both temporal and spectral features of the P, QRS, and T waves. Additionally, an Explainable AI (XAI) approach is applied to enhance model interpretability by explaining how temporal and frequency-based features contribute to ECG segmentation. By incorporating principles from physics-based AI, this method provides a clear understanding of the decision-making process, ensuring reliability and transparency in ECG analysis. This approach achieves high segmentation accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P wave. These results indicate that the simplified architecture not only improves computational efficiency but also provides precise segmentation, making it a practical and effective solution for heart signal monitoring.},
 author = {Muhammad Fathur Rohman Sidiq and Abdurrouf and Didik Rahadi Santoso},
 comment = {16 pages},
 doi = {},
 eprint = {2508.15872v1},
 journal = {arXiv preprint},
 title = {Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model},
 url = {http://arxiv.org/abs/2508.15872v1},
 year = {2025}
}

@article{2508.16000v1,
 abstract = {A precise assessment of the risk of breast lesions can greatly lower it and assist physicians in choosing the best course of action. To categorise breast lesions, the majority of current computer-aided systems only use characteristics from mammograms. Although this method is practical, it does not completely utilise clinical reports' valuable information to attain the best results. When compared to utilising mammography alone, will clinical features greatly enhance the categorisation of breast lesions? How may clinical features and mammograms be combined most effectively? In what ways may explainable AI approaches improve the interpretability and reliability of models used to diagnose breast cancer? To answer these basic problems, a comprehensive investigation is desperately needed. In order to integrate mammography and categorical clinical characteristics, this study examines a number of multimodal deep networks grounded on feature concatenation, co-attention, and cross-attention. The model achieved an AUC-ROC of 0.98, accuracy of 0.96, F1-score of 0.94, precision of 0.92, and recall of 0.95 when tested on publicly accessible datasets (TCGA and CBIS-DDSM).},
 author = {Muhaisin Tiyumba Nantogmah and Abdul-Barik Alhassan and Salamudeen Alhassan},
 comment = {11 pages, 9 figures},
 doi = {},
 eprint = {2508.16000v1},
 journal = {arXiv preprint},
 title = {Cross-Attention Multimodal Fusion for Breast Cancer Diagnosis: Integrating Mammography and Clinical Data with Explainability},
 url = {http://arxiv.org/abs/2508.16000v1},
 year = {2025}
}

@article{2508.16237v1,
 abstract = {This paper presents an explainable artificial intelligence (XAI)-based framework for the spectral analysis of cough sounds associated with chronic respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary Disease (COPD). A Convolutional Neural Network (CNN) is trained on time-frequency representations of cough signals, and occlusion maps are used to identify diagnostically relevant regions within the spectrograms. These highlighted areas are subsequently decomposed into five frequency subbands, enabling targeted spectral feature extraction and analysis. The results reveal that spectral patterns differ across subbands and disease groups, uncovering complementary and compensatory trends across the frequency spectrum. Noteworthy, the approach distinguishes COPD from other respiratory conditions, and chronic from non-chronic patient groups, based on interpretable spectral markers. These findings provide insight into the underlying pathophysiological characteristics of cough acoustics and demonstrate the value of frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation and translational respiratory disease diagnostics.},
 author = {Patricia Amado-Caballero and Luis M. San-José-Revuelta and Xinheng Wang and José Ramón Garmendia-Leiza and Carlos Alberola-López and Pablo Casaseca-de-la-Higuera},
 comment = {},
 doi = {},
 eprint = {2508.16237v1},
 journal = {arXiv preprint},
 title = {A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease},
 url = {http://arxiv.org/abs/2508.16237v1},
 year = {2025}
}

@article{2508.16543v1,
 abstract = {A deep learning model is often considered a black-box model, as its internal workings tend to be opaque to the user. Because of the lack of transparency, it is challenging to understand the reasoning behind the model's predictions. Here, we present an approach to making a deep learning-based solar storm prediction model interpretable, where solar storms include solar flares and coronal mass ejections (CMEs). This deep learning model, built based on a long short-term memory (LSTM) network with an attention mechanism, aims to predict whether an active region (AR) on the Sun's surface that produces a flare within 24 hours will also produce a CME associated with the flare. The crux of our approach is to model data samples in an AR as time series and use the LSTM network to capture the temporal dynamics of the data samples. To make the model's predictions accountable and reliable, we leverage post hoc model-agnostic techniques, which help elucidate the factors contributing to the predicted output for an input sequence and provide insights into the model's behavior across multiple sequences within an AR. To our knowledge, this is the first time that interpretability has been added to an LSTM-based solar storm prediction model.},
 author = {Adam O. Rawashdeh and Jason T. L. Wang and Katherine G. Herbert},
 comment = {6 pages, 8 figures},
 doi = {},
 eprint = {2508.16543v1},
 journal = {arXiv preprint},
 title = {Explainable AI in Deep Learning-Based Prediction of Solar Storms},
 url = {http://arxiv.org/abs/2508.16543v1},
 year = {2025}
}

@article{2508.16747v1,
 abstract = {Understanding the factors that shape students' mathematics performance is vital for designing effective educational policies. This study applies explainable artificial intelligence (XAI) techniques to PISA 2018 data to predict math achievement and identify key predictors across ten countries (67,329 students). We tested four models: Multiple Linear Regression (MLR), Random Forest (RF), CATBoost, and Artificial Neural Networks (ANN), using student, family, and school variables. Models were trained on 70% of the data (with 5-fold cross-validation) and tested on 30%, stratified by country. Performance was assessed with R^2 and Mean Absolute Error (MAE). To ensure interpretability, we used feature importance, SHAP values, and decision tree visualizations. Non-linear models, especially RF and ANN, outperformed MLR, with RF balancing accuracy and generalizability. Key predictors included socio-economic status, study time, teacher motivation, and students' attitudes toward mathematics, though their impact varied across countries. Visual diagnostics such as scatterplots of predicted vs actual scores showed RF and CATBoost aligned closely with actual performance. Findings highlight the non-linear and context-dependent nature of achievement and the value of XAI in educational research. This study uncovers cross-national patterns, informs equity-focused reforms, and supports the development of personalized learning strategies.},
 author = {Liu Liu and Rui Dai},
 comment = {},
 doi = {},
 eprint = {2508.16747v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Predicting and Understanding Mathematics Achievement: A Cross-National Analysis of PISA 2018},
 url = {http://arxiv.org/abs/2508.16747v1},
 year = {2025}
}

@article{2508.17244v1,
 abstract = {Recent developments in Artificial Intelligence (AI) and their applications in critical industries such as healthcare, fin-tech and cybersecurity have led to a surge in research in explainability in AI. Innovative research methods are being explored to extract meaningful insight from blackbox AI systems to make the decision-making technology transparent and interpretable. Explainability becomes all the more critical when AI is used in decision making in domains like fintech, healthcare and safety critical systems such as cybersecurity and autonomous vehicles. However, there is still ambiguity lingering on the reliable evaluations for the users and nature of transparency in the explanations provided for the decisions made by black-boxed AI. To solve the blackbox nature of Machine Learning based Intrusion Detection Systems, a framework is proposed in this paper to give an explanation for IDSs decision making. This framework uses Local Interpretable Model-Agnostic Explanations (LIME) coupled with Explain Like I'm five (ELI5) and Decision Tree algorithms to provide local and global explanations and improve the interpretation of IDSs. The local explanations provide the justification for the decision made on a specific input. Whereas, the global explanations provides the list of significant features and their relationship with attack traffic. In addition, this framework brings transparency in the field of ML driven IDS that might be highly significant for wide scale adoption of eXplainable AI in cyber-critical systems. Our framework is able to achieve 85 percent accuracy in classifying attack behaviour on UNSW-NB15 dataset, while at the same time displaying the feature significance ranking of the top 10 features used in the classification.},
 author = {Aoun E Muhammad and Kin-Choong Yow and Nebojsa Bacanin-Dzakula and Muhammad Attique Khan},
 comment = {This is the authors accepted manuscript of an article accepted for publication in Cluster Computing. The final published version is available at: 10.1007/s10586-025-05326-9},
 doi = {10.1007/s10586-025-05326-9},
 eprint = {2508.17244v1},
 journal = {arXiv preprint},
 title = {L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems},
 url = {http://arxiv.org/abs/2508.17244v1},
 year = {2025}
}

@article{2508.17294v1,
 abstract = {Advancements in deep learning have enabled highly accurate arrhythmia detection from electrocardiogram (ECG) signals, but limited interpretability remains a barrier to clinical adoption. This study investigates the application of Explainable AI (XAI) techniques specifically adapted for time-series ECG analysis. Using the MIT-BIH arrhythmia dataset, a convolutional neural network-based model was developed for arrhythmia classification, with R-peak-based segmentation via the Pan-Tompkins algorithm. To increase the dataset size and to reduce class imbalance, an additional 12-lead ECG dataset was incorporated. A user needs assessment was carried out to identify what kind of explanation would be preferred by medical professionals. Medical professionals indicated a preference for saliency map-based explanations over counterfactual visualisations, citing clearer correspondence with ECG interpretation workflows. Four SHapley Additive exPlanations (SHAP)-based approaches: permutation importance, KernelSHAP, gradient-based methods, and Deep Learning Important FeaTures (DeepLIFT), were implemented and compared. The model achieved 98.3% validation accuracy on MIT-BIH but showed performance degradation on the combined dataset, underscoring dataset variability challenges. Permutation importance and KernelSHAP produced cluttered visual outputs, while gradient-based and DeepLIFT methods highlighted waveform regions consistent with clinical reasoning, but with variability across samples. Findings emphasize the need for domain-specific XAI adaptations in ECG analysis and highlight saliency mapping as a more clinically intuitive approach},
 author = {Joschka Beck and Arlene John},
 comment = {},
 doi = {},
 eprint = {2508.17294v1},
 journal = {arXiv preprint},
 title = {Explainable AI (XAI) for Arrhythmia detection from electrocardiograms},
 url = {http://arxiv.org/abs/2508.17294v1},
 year = {2025}
}

@article{2508.18188v1,
 abstract = {Deep learning has transformed computer vision (CV), achieving outstanding performance in classification, segmentation, and related tasks. Such AI-based CV systems are becoming prevalent, with applications spanning from medical imaging to surveillance. State of the art models such as convolutional neural networks (CNNs) and vision transformers (ViTs) are often regarded as ``black boxes,'' offering limited transparency into their decision-making processes. Despite a recent advancement in explainable AI (XAI), explainability remains underutilized in practical CV deployments. A primary obstacle is the absence of integrated software solutions that connect XAI techniques with robust knowledge management and monitoring frameworks. To close this gap, we have developed Obz AI, a comprehensive software ecosystem designed to facilitate state-of-the-art explainability and observability for vision AI systems. Obz AI provides a seamless integration pipeline, from a Python client library to a full-stack analytics dashboard. With Obz AI, a machine learning engineer can easily incorporate advanced XAI methodologies, extract and analyze features for outlier detection, and continuously monitor AI models in real time. By making the decision-making mechanisms of deep models interpretable, Obz AI promotes observability and responsible deployment of computer vision systems.},
 author = {Neo Christopher Chung and Jakub Binda},
 comment = {},
 doi = {},
 eprint = {2508.18188v1},
 journal = {arXiv preprint},
 title = {Explain and Monitor Deep Learning Models for Computer Vision using Obz AI},
 url = {http://arxiv.org/abs/2508.18188v1},
 year = {2025}
}

@article{2508.20130v1,
 abstract = {CRISPR-based genome editing has revolutionized biotechnology, yet optimizing guide RNA (gRNA) design for efficiency and safety remains a critical challenge. Recent advances (2020--2025, updated to reflect current year if needed) demonstrate that artificial intelligence (AI), especially deep learning, can markedly improve the prediction of gRNA on-target activity and identify off-target risks. In parallel, emerging explainable AI (XAI) techniques are beginning to illuminate the black-box nature of these models, offering insights into sequence features and genomic contexts that drive Cas enzyme performance. Here we review how state-of-the-art machine learning models are enhancing gRNA design for CRISPR systems, highlight strategies for interpreting model predictions, and discuss new developments in off-target prediction and safety assessment. We emphasize breakthroughs from top-tier journals that underscore an interdisciplinary convergence of AI and genome editing to enable more efficient, specific, and clinically viable CRISPR applications.},
 author = {Alireza Abbaszadeh and Armita Shahlai},
 comment = {29 pages, 5 figures, 2 tables, 42 cited references},
 doi = {},
 eprint = {2508.20130v1},
 journal = {arXiv preprint},
 title = {Artificial Intelligence for CRISPR Guide RNA Design: Explainable Models and Off-Target Safety},
 url = {http://arxiv.org/abs/2508.20130v1},
 year = {2025}
}

@article{2508.20227v1,
 abstract = {The development of many vision models mainly focuses on improving their performance using metrics such as accuracy, IoU, and mAP, with less attention to explainability due to the complexity of applying xAI methods to provide a meaningful explanation of trained models. Although many existing xAI methods aim to explain vision models sample-by-sample, methods explaining the general behavior of vision models, which can only be captured after running on a large dataset, are still underexplored. Furthermore, understanding the behavior of vision models on general images can be very important to prevent biased judgments and help identify the model's trends and patterns. With the application of Vision-Language Models, this paper proposes a pipeline to explain vision models at both the sample and dataset levels. The proposed pipeline can be used to discover failure cases and gain insights into vision models with minimal effort, thereby integrating vision model development with xAI analysis to advance image analysis.},
 author = {Phu-Vinh Nguyen and Tan-Hanh Pham and Chris Ngo and Truong Son Hy},
 comment = {},
 doi = {},
 eprint = {2508.20227v1},
 journal = {arXiv preprint},
 title = {A Novel Framework for Automated Explain Vision Model Using Vision-Language Models},
 url = {http://arxiv.org/abs/2508.20227v1},
 year = {2025}
}

@article{2508.20437v1,
 abstract = {Time-series forecasting models (TSFM) have evolved from classical statistical methods to sophisticated foundation models, yet understanding why and when these models succeed or fail remains challenging. Despite this known limitation, time series forecasting models are increasingly used to generate information that informs real-world actions with equally real consequences. Understanding the complexity, performance variability, and opaque nature of these models then becomes a valuable endeavor to combat serious concerns about how users should interact with and rely on these models' outputs. This work addresses these concerns by combining traditional explainable AI (XAI) methods with Rating Driven Explanations (RDE) to assess TSFM performance and interpretability across diverse domains and use cases. We evaluate four distinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series specific foundation model), Llama (general-purpose; both fine-tuned and base models) on four heterogeneous datasets spanning finance, energy, transportation, and automotive sales domains. In doing so, we demonstrate that feature-engineered models (e.g., Gradient Boosting) consistently outperform foundation models (e.g., Chronos) in volatile or sparse domains (e.g., power, car parts) while providing more interpretable explanations, whereas foundation models excel only in stable or trend-driven contexts (e.g., finance).},
 author = {Michael Widener and Kausik Lakkaraju and John Aydin and Biplav Srivastava},
 comment = {8 pages, 5 Tables, 5 Figures, AI Trustworthiness and Risk Assessment for Challenged Contexts (ATRACC), Appendix},
 doi = {},
 eprint = {2508.20437v1},
 journal = {arXiv preprint},
 title = {On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating},
 url = {http://arxiv.org/abs/2508.20437v1},
 year = {2025}
}

@article{2508.20519v3,
 abstract = {Khiops is an open source machine learning tool designed for mining large multi-table databases. Khiops is based on a unique Bayesian approach that has attracted academic interest with more than 20 publications on topics such as variable selection, classification, decision trees and co-clustering. It provides a predictive measure of variable importance using discretisation models for numerical data and value clustering for categorical data. The proposed classification/regression model is a naive Bayesian classifier incorporating variable selection and weight learning. In the case of multi-table databases, it provides propositionalisation by automatically constructing aggregates. Khiops is adapted to the analysis of large databases with millions of individuals, tens of thousands of variables and hundreds of millions of records in secondary tables. It is available on many environments, both from a Python library and via a user interface.},
 author = {Marc Boullé and Nicolas Voisine and Bruno Guerraz and Carine Hue and Felipe Olmos and Vladimir Popescu and Stéphane Gouache and Stéphane Bouget and Alexis Bondu and Luc Aurelien Gauthier and Yassine Nair Benrekia and Fabrice Clérot and Vincent Lemaire},
 comment = {},
 doi = {},
 eprint = {2508.20519v3},
 journal = {arXiv preprint},
 title = {Khiops: An End-to-End, Frugal AutoML and XAI Machine Learning Solution for Large, Multi-Table Databases},
 url = {http://arxiv.org/abs/2508.20519v3},
 year = {2025}
}

@article{2509.00069v1,
 abstract = {Conversational AI and Large Language Models (LLMs) have become powerful tools across domains, including cybersecurity, where they help detect threats early and improve response times. However, challenges such as false positives and complex model management still limit trust. Although Explainable AI (XAI) aims to make AI decisions more transparent, many security analysts remain uncertain about its usefulness. This study presents a framework that detects anomalies and provides high-quality explanations through visual tools BERTViz and Captum, combined with natural language reports based on attention outputs. This reduces manual effort and speeds up remediation. Our comparative analysis showed that RoBERTa offers high accuracy (99.6 %) and strong anomaly detection, outperforming Falcon-7B and DeBERTa, as well as exhibiting better flexibility than large-scale Mistral-7B on the HDFS dataset from LogHub. User feedback confirms the chatbot's ease of use and improved understanding of anomalies, demonstrating the ability of the developed framework to strengthen cybersecurity workflows.},
 author = {Prasasthy Balasubramanian and Dumindu Kankanamge and Ekaterina Gilman and Mourad Oussalah},
 comment = {},
 doi = {},
 eprint = {2509.00069v1},
 journal = {arXiv preprint},
 title = {AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum},
 url = {http://arxiv.org/abs/2509.00069v1},
 year = {2025}
}

@article{2509.00744v1,
 abstract = {Distinguishing correlation from causation is a fundamental challenge in machine intelligence, often representing a critical barrier to building robust and trustworthy systems. While Pearl's $\mathcal{DO}$-calculus provides a rigorous framework for causal inference, a parallel challenge lies in its physical implementation. Here, we apply and experimentally validate a quantum algorithmic framework for performing causal interventions. Our approach maps causal networks onto quantum circuits where probabilistic links are encoded by controlled-rotation gates, and interventions are realized by a structural remodeling of the circuit -- a physical analogue to Pearl's ``graph surgery''. We demonstrate the method's efficacy by resolving Simpson's Paradox in a 3-qubit model, and show its scalability by quantifying confounding bias in a 10-qubit healthcare simulation. Critically, we provide a proof-of-principle experimental validation on an IonQ Aria quantum computer, successfully reproducing the paradox and its resolution in the presence of real-world noise. This work establishes a practical pathway for quantum causal inference, offering a new computational tool to address deep-rooted challenges in algorithmic fairness and explainable AI (XAI).},
 author = {Pilsung Kang},
 comment = {},
 doi = {},
 eprint = {2509.00744v1},
 journal = {arXiv preprint},
 title = {Quantum Causality: Resolving Simpson's Paradox with $\mathcal{DO}$-Calculus},
 url = {http://arxiv.org/abs/2509.00744v1},
 year = {2025}
}

@article{2509.00802v1,
 abstract = {Artificial intelligence (AI) is increasingly used in the automotive industry for applications such as driving style classification, which aims to improve road safety, efficiency, and personalize user experiences. While deep learning (DL) models, such as Long Short-Term Memory (LSTM) networks, excel at this task, their black-box nature limits interpretability and trust. This paper proposes a machine learning (ML)-based method that balances high accuracy with interpretability. We introduce a high-quality dataset, CARLA-Drive, and leverage ML techniques like Random Forest (RF), Gradient Boosting (XGBoost), and Support Vector Machine (SVM), which are efficient, lightweight, and interpretable. In addition, we apply the SHAP (Shapley Additive Explanations) explainability technique to provide personalized recommendations for safer driving. Achieving an accuracy of 0.92 on a three-class classification task with both RF and XGBoost classifiers, our approach matches DL models in performance while offering transparency and practicality for real-world deployment in intelligent transportation systems.},
 author = {Feriel Amel Sellal and Ahmed Ayoub Bellachia and Meryem Malak Dif and Enguerrand De Rautlin De La Roy and Mouhamed Amine Bouchiha and Yacine Ghamri-Doudane},
 comment = {},
 doi = {},
 eprint = {2509.00802v1},
 journal = {arXiv preprint},
 title = {XAI-Driven Machine Learning System for Driving Style Recognition and Personalized Recommendations},
 url = {http://arxiv.org/abs/2509.00802v1},
 year = {2025}
}

@article{2509.00846v1,
 abstract = {Explaining machine learning (ML) predictions has become crucial as ML models are increasingly deployed in high-stakes domains such as healthcare. While SHapley Additive exPlanations (SHAP) is widely used for model interpretability, it fails to differentiate between causality and correlation, often misattributing feature importance when features are highly correlated. We propose Causal SHAP, a novel framework that integrates causal relationships into feature attribution while preserving many desirable properties of SHAP. By combining the Peter-Clark (PC) algorithm for causal discovery and the Intervention Calculus when the DAG is Absent (IDA) algorithm for causal strength quantification, our approach addresses the weakness of SHAP. Specifically, Causal SHAP reduces attribution scores for features that are merely correlated with the target, as validated through experiments on both synthetic and real-world datasets. This study contributes to the field of Explainable AI (XAI) by providing a practical framework for causal-aware model explanations. Our approach is particularly valuable in domains such as healthcare, where understanding true causal relationships is critical for informed decision-making.},
 author = {Woon Yee Ng and Li Rong Wang and Siyuan Liu and Xiuyi Fan},
 comment = {Published in 2025 International Joint Conference on Neural Networks (IJCNN). IEEE, 2025},
 doi = {},
 eprint = {2509.00846v1},
 journal = {arXiv preprint},
 title = {Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery},
 url = {http://arxiv.org/abs/2509.00846v1},
 year = {2025}
}

@article{2509.02127v2,
 abstract = {Explainable artificial intelligence (XAI) methods have been applied to interpret deep learning model results. However, applications that integrate XAI with established hydrologic knowledge for process understanding remain limited. Here we show that XAI method, applied at point-scale, could be used for cross-scale aggregation of hydrologic responses, a fundamental question in scaling problems, using hydrologic connectivity as a demonstration. Soil moisture and its movement generated by physically based hydrologic model were used to train a long short-term memory (LSTM) network, whose impacts of inputs were evaluated by XAI methods. Our results suggest that XAI-based classification can effectively identify the differences in the functional roles of various sub-regions at watershed scale. The aggregated XAI results could be considered as an explicit and quantitative indicator of hydrologic connectivity development, offering insights to hydrological organization. This framework could be used to facilitate aggregation of other geophysical responses to advance process understandings.},
 author = {Sheng Ye and Jiyu Li and Yifan Chai and Lin Liu and Murugesu Sivapalan and Qihua Ran},
 comment = {27 pages, 12 figures},
 doi = {},
 eprint = {2509.02127v2},
 journal = {arXiv preprint},
 title = {Explainable artificial intelligence (XAI) for scaling: An application for deducing hydrologic connectivity at watershed scale},
 url = {http://arxiv.org/abs/2509.02127v2},
 year = {2025}
}

@article{2509.02920v1,
 abstract = {Detecting elephants through seismic signals is an emerging research topic aimed at developing solutions for Human-Elephant Conflict (HEC). Despite the promising results, such solutions heavily rely on manual classification of elephant footfalls, which limits their applicability for real-time classification in natural settings. To address this limitation and build on our previous work, this study introduces a classification framework targeting resource-constrained implementations, prioritizing both accuracy and computational efficiency. As part of this framework, a novel event detection technique named Contextually Customized Windowing (CCW), tailored specifically for detecting elephant footfalls, was introduced, and evaluations were conducted by comparing it with the Short-Term Average/Long-Term Average (STA/LTA) method. The yielded results show that the maximum validated detection range was 155.6 m in controlled conditions and 140 m in natural environments. Elephant footfall classification using Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel demonstrated superior performance across multiple settings, achieving an accuracy of 99% in controlled environments, 73% in natural elephant habitats, and 70% in HEC-prone human habitats, the most challenging scenario. Furthermore, feature impact analysis using explainable AI identified the number of Zero Crossings and Dynamic Time Warping (DTW) Alignment Cost as the most influential factors in all experiments, while Predominant Frequency exhibited significant influence in controlled settings.},
 author = {Jaliya L. Wijayaraja and Janaka L. Wijekoon and Malitha Wijesundara},
 comment = {This article has been accepted for publication in IEEE Access},
 doi = {10.1109/ACCESS.2025.3607383},
 eprint = {2509.02920v1},
 journal = {arXiv preprint},
 title = {Event Detection and Classification for Long Range Sensing of Elephants Using Seismic Signal},
 url = {http://arxiv.org/abs/2509.02920v1},
 year = {2025}
}

@article{2509.03169v1,
 abstract = {Explainable AI (XAI) is essential for validating and trusting models in safety-critical applications like autonomous driving. However, the reliability of XAI is challenged by the Rashomon effect, where multiple, equally accurate models can offer divergent explanations for the same prediction. This paper provides the first empirical quantification of this effect for the task of action prediction in real-world driving scenes. Using Qualitative Explainable Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two distinct model classes: interpretable, pair-based gradient boosting models and complex, graph-based Graph Neural Networks (GNNs). Using feature attribution methods, we measure the agreement of explanations both within and between these classes. Our results reveal significant explanation disagreement. Our findings suggest that explanation ambiguity is an inherent property of the problem, not just a modeling artifact.},
 author = {Helge Spieker and Jørn Eirik Betten and Arnaud Gotlieb and Nadjib Lazaar and Nassim Belmecheri},
 comment = {AAAI 2025 Fall Symposium: AI Trustworthiness and Risk Assessment for Challenged Contexts (ATRACC)},
 doi = {},
 eprint = {2509.03169v1},
 journal = {arXiv preprint},
 title = {Rashomon in the Streets: Explanation Ambiguity in Scene Understanding},
 url = {http://arxiv.org/abs/2509.03169v1},
 year = {2025}
}

@article{2509.03547v2,
 abstract = {This study introduces MatterVial, an innovative hybrid framework for feature-based machine learning in materials science. MatterVial expands the feature space by integrating latent representations from a diverse suite of pretrained graph neural network (GNN) models including: structure-based (MEGNet), composition-based (ROOST), and equivariant (ORB) graph networks, with computationally efficient, GNN-approximated descriptors and novel features from symbolic regression. Our approach combines the chemical transparency of traditional feature-based models with the predictive power of deep learning architectures. When augmenting the feature-based model MODNet on Matbench tasks, this method yields significant error reductions and elevates its performance to be competitive with, and in several cases superior to, state-of-the-art end-to-end GNNs, with accuracy increases exceeding 40% for multiple tasks. An integrated interpretability module, employing surrogate models and symbolic regression, decodes the latent GNN-derived descriptors into explicit, physically meaningful formulas. This unified framework advances materials informatics by providing a high-performance, transparent tool that aligns with the principles of explainable AI, paving the way for more targeted and autonomous materials discovery.},
 author = {Rogério Almeida Gouvêa and Pierre-Paul De Breuck and Tatiane Pretto and Gian-Marco Rignanese and Marcos José Leite Santos},
 comment = {},
 doi = {},
 eprint = {2509.03547v2},
 journal = {arXiv preprint},
 title = {Combining feature-based approaches with graph neural networks and symbolic regression for synergistic performance and interpretability},
 url = {http://arxiv.org/abs/2509.03547v2},
 year = {2025}
}

@article{2509.05370v1,
 abstract = {This study explores the application of quantum machine learning (QML) algorithms to enhance cybersecurity threat detection, particularly in the classification of malware and intrusion detection within high-dimensional datasets. Classical machine learning approaches encounter limitations when dealing with intricate, obfuscated malware patterns and extensive network intrusion data. To address these challenges, we implement and evaluate various QML algorithms, including Quantum Neural Networks (QNN), Quantum Support Vector Machines (QSVM), and hybrid Quantum Convolutional Neural Networks (QCNN) for malware detection tasks. Our experimental analysis utilized two datasets: the Intrusion dataset, comprising 150 samples with 56 memory-based features derived from Volatility framework analysis, and the ObfuscatedMalMem2022 dataset, containing 58,596 samples with 57 features representing benign and malicious software. Remarkably, our QML methods demonstrated superior performance compared to classical approaches, achieving accuracies of 95% for QNN and 94% for QSVM. These quantum-enhanced methods leveraged quantum superposition and entanglement principles to accurately identify complex patterns within highly obfuscated malware samples that were imperceptible to classical methods. To further advance malware analysis, we propose a novel real-time malware analysis framework that incorporates Quantum Feature Extraction using Quantum Fourier Transform, Quantum Feature Maps, and Classification using Variational Quantum Circuits. This system integrates explainable AI methods, including GradCAM++ and ScoreCAM algorithms, to provide interpretable insights into the quantum decision-making processes.},
 author = {Tanya Joshi and Krishnendu Guha},
 comment = {10 pages},
 doi = {},
 eprint = {2509.05370v1},
 journal = {arXiv preprint},
 title = {Quantum AI Algorithm Development for Enhanced Cybersecurity: A Hybrid Approach to Malware Detection},
 url = {http://arxiv.org/abs/2509.05370v1},
 year = {2025}
}

@article{2509.07039v1,
 abstract = {Artificial intelligence deployment for automated photovoltaic (PV) monitoring faces interpretability barriers that limit adoption in energy infrastructure applications. While deep learning achieves high accuracy in thermal fault detection, validation that model decisions align with thermal physics principles remains lacking, creating deployment hesitancy where understanding model reasoning is critical. This study provides a systematic comparison of convolutional neural networks (ResNet-18, EfficientNet-B0) and vision transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI saliency analysis to assess alignment with thermal physics principles. This represents the first systematic comparison of CNNs and vision transformers for thermal PV fault detection with physics-validated interpretability. Evaluation on 20,000 infrared images spanning normal operation and 11 fault categories shows that Swin Transformer achieves the highest performance (94% binary accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis reveals that models learn physically meaningful features, such as localized hotspots for cell defects, linear thermal paths for diode failures, and thermal boundaries for vegetation shading, consistent with expected thermal signatures. However, performance varies significantly across fault types: electrical faults achieve strong detection (F1-scores >0.90) while environmental factors like soiling remain challenging (F1-scores 0.20-0.33), indicating limitations imposed by thermal imaging resolution. The thermal physics-guided interpretability approach provides methodology for validating AI decision-making in energy monitoring applications, addressing deployment barriers in renewable energy infrastructure.},
 author = {Serra Aksoy},
 comment = {28 Pages, 4 Figures},
 doi = {},
 eprint = {2509.07039v1},
 journal = {arXiv preprint},
 title = {Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation},
 url = {http://arxiv.org/abs/2509.07039v1},
 year = {2025}
}

@article{2509.07477v1,
 abstract = {Deep neural networks excel in radiological image classification but frequently suffer from poor interpretability, limiting clinical acceptance. We present MedicalPatchNet, an inherently self-explainable architecture for chest X-ray classification that transparently attributes decisions to distinct image regions. MedicalPatchNet splits images into non-overlapping patches, independently classifies each patch, and aggregates predictions, enabling intuitive visualization of each patch's diagnostic contribution without post-hoc techniques. Trained on the CheXpert dataset (223,414 images), MedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908) of EfficientNet-B0, while substantially improving interpretability: MedicalPatchNet demonstrates substantially improved interpretability with higher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with Grad-CAM) on the CheXlocalize dataset. By providing explicit, reliable explanations accessible even to non-AI experts, MedicalPatchNet mitigates risks associated with shortcut learning, thus improving clinical trust. Our model is publicly available with reproducible training and inference scripts and contributes to safer, explainable AI-assisted diagnostics across medical imaging domains. We make the code publicly available: https://github.com/TruhnLab/MedicalPatchNet},
 author = {Patrick Wienholt and Christiane Kuhl and Jakob Nikolas Kather and Sven Nebelung and Daniel Truhn},
 comment = {},
 doi = {},
 eprint = {2509.07477v1},
 journal = {arXiv preprint},
 title = {MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification},
 url = {http://arxiv.org/abs/2509.07477v1},
 year = {2025}
}

@article{2509.08717v1,
 abstract = {Explainable Artificial Intelligence (XAI) has emerged as a critical tool for interpreting the predictions of complex deep learning models. While XAI has been increasingly applied in various domains within acoustics, its use in bioacoustics, which involves analyzing audio signals from living organisms, remains relatively underexplored. In this paper, we investigate the vocalizations of a bird species with strong geographic variation throughout its range in North America. Audio recordings were converted into spectrogram images and used to train a deep Convolutional Neural Network (CNN) for classification, achieving an accuracy of 94.8\%. To interpret the model's predictions, we applied both model-agnostic (LIME, SHAP) and model-specific (DeepLIFT, Grad-CAM) XAI techniques. These techniques produced different but complementary explanations, and when their explanations were considered together, they provided more complete and interpretable insights into the model's decision-making. This work highlights the importance of using a combination of XAI techniques to improve trust and interoperability, not only in broader acoustics signal analysis but also argues for broader applicability in different domain specific tasks.},
 author = {Zubair Faruqui and Mackenzie S. McIntire and Rahul Dubey and Jay McEntee},
 comment = {Accepted in IEEE ICTAI 2025},
 doi = {},
 eprint = {2509.08717v1},
 journal = {arXiv preprint},
 title = {Explainability of CNN Based Classification Models for Acoustic Signal},
 url = {http://arxiv.org/abs/2509.08717v1},
 year = {2025}
}

@article{2509.08988v2,
 abstract = {Spin coating polymer thin films to achieve specific mechanical properties is inherently a multi-objective optimization problem. We present a framework that integrates an active Pareto front learning algorithm (PyePAL) with visualization and explainable AI techniques to optimize processing parameters. PyePAL uses Gaussian process models to predict objective values (hardness and elasticity) from the design variables (spin speed, dilution, and polymer mixture), guiding the adaptive selection of samples toward promising regions of the design space. To enable interpretable insights into the high-dimensional design space, we utilize UMAP (Uniform Manifold Approximation and Projection) for two-dimensional visualization of the Pareto front exploration. Additionally, we incorporate fuzzy linguistic summaries, which translate the learned relationships between process parameters and performance objectives into linguistic statements, thus enhancing the explainability and understanding of the optimization results. Experimental results demonstrate that our method efficiently identifies promising polymer designs, while the visual and linguistic explanations facilitate expert-driven analysis and knowledge discovery.},
 author = {Brendan Young and Brendan Alvey and Andreas Werbrouck and Will Murphy and James Keller and Matthias J. Young and Matthew Maschmann},
 comment = {8 pages, 7 figures, Presented at 2025 AAAI Spring Symposium Series},
 doi = {},
 eprint = {2509.08988v2},
 journal = {arXiv preprint},
 title = {Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers},
 url = {http://arxiv.org/abs/2509.08988v2},
 year = {2025}
}

@article{2509.09070v2,
 abstract = {Most explainable AI (XAI) frameworks are limited in their expressiveness, summarizing complex feature effects as single scalar values φ_i. This approach answers "what" features are important but fails to reveal "how" they interact. Furthermore, methods that attempt to capture interactions, like those based on Shapley values, often face an exponential computational cost. We present STRIDE, a scalable framework that addresses both limitations by reframing explanation as a subset-enumeration-free, orthogonal "functional decomposition" in a Reproducing Kernel Hilbert Space (RKHS). In the tabular setups we study, STRIDE analytically computes functional components f_S(x_S) via a recursive kernel-centering procedure. The approach is model-agnostic and theoretically grounded with results on orthogonality and L^2 convergence. In tabular benchmarks (10 datasets, median over 10 seeds), STRIDE attains a 3.0 times median speedup over TreeSHAP and a mean R^2=0.93 for reconstruction. We also introduce "component surgery", a diagnostic that isolates a learned interaction and quantifies its contribution; on California Housing, removing a single interaction reduces test R^2 from 0.019 to 0.027.},
 author = {Chaeyun Ko},
 comment = {Major revision for submission to ICLR 2026. Substantially revised abstract, introduction, and discussion. Added new 'component surgery' analysis and updated benchmark results for clarity. (12 pages, 2 figures)},
 doi = {},
 eprint = {2509.09070v2},
 journal = {arXiv preprint},
 title = {STRIDE: Subset-Free Functional Decomposition for XAI in Tabular Settings},
 url = {http://arxiv.org/abs/2509.09070v2},
 year = {2025}
}

@article{2509.09127v1,
 abstract = {Anti-money laundering (AML) actions and measurements are among the priorities of financial institutions, for which machine learning (ML) has shown to have a high potential. In this paper, we propose a comprehensive and systematic approach for developing ML pipelines to identify high-risk bank clients in a dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for Management and Innovation (IMI) Big Data and Artificial Intelligence Competition. The dataset included 195,789 customer IDs, and we employed a 16-step design and statistical analysis to ensure the final pipeline was robust. We also framed the data in a SQLite database, developed SQL-based feature engineering algorithms, connected our pre-trained model to the database, and made it inference-ready, and provided explainable artificial intelligence (XAI) modules to derive feature importance. Our pipeline achieved a mean area under the receiver operating characteristic curve (AUROC) of 0.961 with a standard deviation (SD) of 0.005. The proposed pipeline achieved second place in the competition.},
 author = {Khashayar Namdar and Pin-Chien Wang and Tushar Raju and Steven Zheng and Fiona Li and Safwat Tahmin Khan},
 comment = {},
 doi = {},
 eprint = {2509.09127v1},
 journal = {arXiv preprint},
 title = {Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning},
 url = {http://arxiv.org/abs/2509.09127v1},
 year = {2025}
}

@article{2509.09387v3,
 abstract = {Effective model and hyperparameter selection remains a major challenge in deep learning, often requiring extensive expertise and computation. While AutoML and large language models (LLMs) promise automation, current LLM-based approaches rely on trial and error and expensive APIs, which provide limited interpretability and generalizability. We propose MetaLLMiX, a zero-shot hyperparameter optimization framework combining meta-learning, explainable AI, and efficient LLM reasoning. By leveraging historical experiment outcomes with SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained models without additional trials. We further employ an LLM-as-judge evaluation to control output format, accuracy, and completeness. Experiments on eight medical imaging datasets using nine open-source lightweight LLMs show that MetaLLMiX achieves competitive or superior performance to traditional HPO methods while drastically reducing computational cost. Our local deployment outperforms prior API-based approaches, achieving optimal results on 5 of 8 tasks, response time reductions of 99.6-99.9%, and the fastest training times on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of best-performing baselines.},
 author = {Mohamed Bal-Ghaoui and Mohammed Tiouti},
 comment = {},
 doi = {},
 eprint = {2509.09387v3},
 journal = {arXiv preprint},
 title = {MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization},
 url = {http://arxiv.org/abs/2509.09387v3},
 year = {2025}
}

@article{2509.09513v1,
 abstract = {The diffusion MRI Neurite Exchange Imaging model offers a promising framework for probing gray matter microstructure by estimating parameters such as compartment sizes, diffusivities, and inter-compartmental water exchange time. However, existing protocols require long scan times. This study proposes a reduced acquisition scheme for the Connectome 2.0 scanner that preserves model accuracy while substantially shortening scan duration. We developed a data-driven framework using explainable artificial intelligence with a guided recursive feature elimination strategy to identify an optimal 8-feature subset from a 15-feature protocol. The performance of this optimized protocol was validated in vivo and benchmarked against the full acquisition and alternative reduction strategies. Parameter accuracy, preservation of anatomical contrast, and test-retest reproducibility were assessed. The reduced protocol yielded parameter estimates and cortical maps comparable to the full protocol, with low estimation errors in synthetic data and minimal impact on test-retest variability. Compared to theory-driven and heuristic reduction schemes, the optimized protocol demonstrated superior robustness, reducing the deviation in water exchange time estimates by over two-fold. In conclusion, this hybrid optimization framework enables viable imaging of neurite exchange in 14 minutes without loss of parameter fidelity. This approach supports the broader application of exchange-sensitive diffusion magnetic resonance imaging in neuroscience and clinical research, and offers a generalizable method for designing efficient acquisition protocols in biophysical parameter mapping.},
 author = {Quentin Uhl and Tommaso Pavan and Julianna Gerold and Kwok-Shing Chan and Yohan Jun and Shohei Fujita and Aneri Bhatt and Yixin Ma and Qiaochu Wang and Hong-Hsi Lee and Susie Y. Huang and Berkin Bilgic and Ileana Jelescu},
 comment = {Submitted to IEEE Transactions on Medical Imaging (TMI). This all-in-one version includes supplementary materials. 18 pages, 14 figures, 2 tables},
 doi = {},
 eprint = {2509.09513v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Accelerated Microstructure Imaging: A SHAP-Guided Protocol on the Connectome 2.0 scanner},
 url = {http://arxiv.org/abs/2509.09513v1},
 year = {2025}
}

@article{2509.09989v1,
 abstract = {The classification of network traffic using machine learning (ML) models is one of the primary mechanisms to address the security issues in IoT networks and/or IoT devices. However, the ML models often act as black-boxes that create a roadblock to take critical decision based on the model output. To address this problem, we design and develop a system, called rCamInspector, that employs Explainable AI (XAI) to provide reliable and trustworthy explanations to model output. rCamInspector adopts two classifiers, Flow Classifier - categorizes a flow into one of four classes, IoTCam, Conf, Share and Others, and SmartCam Classifier - classifies an IoTCam flow into one of six classes, Netatmo, Spy Clock, Canary, D3D, Ezviz, V380 Spy Bulb; both are IP address and transport port agnostic. rCamInspector is evaluated using 38GB of network traffic and our results show that XGB achieves the highest accuracy of 92% and 99% in the Flow and SmartCam classifiers respectively among eight supervised ML models. We analytically show that the traditional mutual information (MI) based feature importance cannot provide enough reliability on the model output of XGB in either classifiers. Using SHAP and LIME, we show that a separate set of features can be picked up to explain a correct prediction of XGB. For example, the feature Init Bwd Win Byts turns out to have the highest SHAP values to support the correct prediction of both IoTCam in Flow Classifier and Netatmo class in SmartCam Classifier. To evaluate the faithfulness of the explainers on our dataset, we show that both SHAP and LIME have a consistency of more than 0.7 and a sufficiency of 1.0. Comparing with existing works, we show that rCamInspector achieves a better accuracy (99%), precision (99%), and false negative rate (0.7%).},
 author = {Priyanka Rushikesh Chaudhary and Manan Gupta and Jabez Christopher and Putrevu Venkata Sai Charan and Rajib Ranjan Maiti},
 comment = {},
 doi = {},
 eprint = {2509.09989v1},
 journal = {arXiv preprint},
 title = {rCamInspector: Building Reliability and Trust on IoT (Spy) Camera Detection using XAI},
 url = {http://arxiv.org/abs/2509.09989v1},
 year = {2025}
}

@article{2509.10206v1,
 abstract = {With the rise of fifth-generation (5G) networks in critical applications, it is urgent to move from detection of malicious activity to systems capable of providing a reliable verdict suitable for mitigation. In this regard, understanding and interpreting machine learning (ML) models' security alerts is crucial for enabling actionable incident response orchestration. Explainable Artificial Intelligence (XAI) techniques are expected to enhance trust by providing insights into why alerts are raised. A dominant approach statistically associates feature sets that can be correlated to a given alert. This paper starts by questioning whether such attribution is relevant for future generation communication systems, and investigates its merits in comparison with an approach based on logical explanations. We extensively study two methods, SHAP and VoTE-XAI, by analyzing their interpretations of alerts generated by an XGBoost model in three different use cases with several 5G communication attacks. We identify three metrics for assessing explanations: sparsity, how concise they are; stability, how consistent they are across samples from the same attack type; and efficiency, how fast an explanation is generated. As an example, in a 5G network with 92 features, 6 were deemed important by VoTE-XAI for a Denial of Service (DoS) variant, ICMPFlood, while SHAP identified over 20. More importantly, we found a significant divergence between features selected by SHAP and VoTE-XAI. However, none of the top-ranked features selected by SHAP were missed by VoTE-XAI. When it comes to efficiency of providing interpretations, we found that VoTE-XAI is significantly more responsive, e.g. it provides a single explanation in under 0.002 seconds, in a high-dimensional setting (478 features).},
 author = {Federica Uccello and Simin Nadjm-Tehrani},
 comment = {},
 doi = {},
 eprint = {2509.10206v1},
 journal = {arXiv preprint},
 title = {Investigating Feature Attribution for 5G Network Intrusion Detection},
 url = {http://arxiv.org/abs/2509.10206v1},
 year = {2025}
}

@article{2509.10523v1,
 abstract = {Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by atypical brain maturation. However, the adaptation of transfer learning paradigms in machine learning for ASD research remains notably limited. In this study, we propose a computer-aided diagnostic framework with two modules. This chapter presents a two-module framework combining deep learning and explainable AI for ASD diagnosis. The first module leverages a deep learning model fine-tuned through cross-domain transfer learning for ASD classification. The second module focuses on interpreting the model decisions and identifying critical brain regions. To achieve this, we employed three explainable AI (XAI) techniques: saliency mapping, Gradient-weighted Class Activation Mapping, and SHapley Additive exPlanations (SHAP) analysis. This framework demonstrates that cross-domain transfer learning can effectively address data scarcity in ASD research. In addition, by applying three established explainability techniques, the approach reveals how the model makes diagnostic decisions and identifies brain regions most associated with ASD. These findings were compared against established neurobiological evidence, highlighting strong alignment and reinforcing the clinical relevance of the proposed approach.},
 author = {Kush Gupta and Amir Aly and Emmanuel Ifeachor and Rohit Shankar},
 comment = {},
 doi = {},
 eprint = {2509.10523v1},
 journal = {arXiv preprint},
 title = {From Predictions to Explanations: Explainable AI for Autism Diagnosis and Identification of Critical Brain Regions},
 url = {http://arxiv.org/abs/2509.10523v1},
 year = {2025}
}

@article{2509.10563v1,
 abstract = {Explainable Artificial Intelligence (XAI) enhances the transparency and interpretability of AI models, addressing their inherent opacity. In cybersecurity, particularly within the Internet of Medical Things (IoMT), the black-box nature of AI-driven threat detection poses a significant challenge. Cybersecurity professionals must not only detect attacks but also understand the reasoning behind AI decisions to ensure trust and accountability. The rapid increase in cyberattacks targeting connected medical devices threatens patient safety and data privacy, necessitating advanced AI-driven solutions. This study compares two ensemble learning techniques, bagging and boosting, for cyber-attack classification in IoMT environments. We selected Random Forest for bagging and CatBoost for boosting. Random Forest helps reduce variance, while CatBoost improves bias by combining weak classifiers into a strong ensemble model, making them effective for detecting sophisticated attacks. However, their complexity often reduces transparency, making it difficult for cybersecurity professionals to interpret and trust their decisions. To address this issue, we apply XAI models to generate local and global explanations, providing insights into AI decision-making. Using techniques like SHAP (Shapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations), we highlight feature importance to help stakeholders understand the key factors driving cyber threat detection.},
 author = {Mohammed Yacoubi and Omar Moussaoui and C. Drocourt},
 comment = {},
 doi = {},
 eprint = {2509.10563v1},
 journal = {arXiv preprint},
 title = {Enhancing IoMT Security with Explainable Machine Learning: A Case Study on the CICIOMT2024 Dataset},
 url = {http://arxiv.org/abs/2509.10563v1},
 year = {2025}
}

@article{2509.10929v1,
 abstract = {The impressive capabilities of deep learning models are often counterbalanced by their inherent opacity, commonly termed the "black box" problem, which impedes their widespread acceptance in high-trust domains. In response, the intersecting disciplines of interpretability and explainability, collectively falling under the Explainable AI (XAI) umbrella, have become focal points of research. Although these terms are frequently used as synonyms, they carry distinct conceptual weights. This document offers a comparative exploration of interpretability and explainability within the deep learning paradigm, carefully outlining their respective definitions, objectives, prevalent methodologies, and inherent difficulties. Through illustrative examinations of the MNIST digit classification task and IMDB sentiment analysis, we substantiate a key argument: interpretability generally pertains to a model's inherent capacity for human comprehension of its operational mechanisms (global understanding), whereas explainability is more commonly associated with post-hoc techniques designed to illuminate the basis for a model's individual predictions or behaviors (local explanations). For example, feature attribution methods can reveal why a specific MNIST image is recognized as a '7', and word-level importance can clarify an IMDB sentiment outcome. However, these local insights do not render the complex underlying model globally transparent. A clear grasp of this differentiation, as demonstrated by these standard datasets, is vital for fostering dependable and sound artificial intelligence.},
 author = {Mitali Raj},
 comment = {5 pages, 2 figures, Accepted at ICICC 2026},
 doi = {},
 eprint = {2509.10929v1},
 journal = {arXiv preprint},
 title = {Clarifying Model Transparency: Interpretability versus Explainability in Deep Learning with MNIST and IMDB Examples},
 url = {http://arxiv.org/abs/2509.10929v1},
 year = {2025}
}

@article{2509.11389v1,
 abstract = {Predicting default is essential for banks to ensure profitability and financial stability. While modern machine learning methods often outperform traditional regression techniques, their lack of transparency limits their use in regulated environments. Explainable artificial intelligence (XAI) has emerged as a solution in domains like credit scoring. However, most XAI research focuses on post-hoc interpretation of black-box models, which does not produce models lightweight or transparent enough to meet regulatory requirements, such as those for Internal Ratings-Based (IRB) models.
  This paper proposes a hybrid approach: post-hoc interpretations of black-box models guide feature selection, followed by training glass-box models that maintain both predictive power and transparency.
  Using the Lending Club dataset, we demonstrate that this approach achieves performance comparable to a benchmark black-box model while using only 10 features - an 88.5% reduction. In our example, SHapley Additive exPlanations (SHAP) is used for feature selection, eXtreme Gradient Boosting (XGBoost) serves as the benchmark and the base black-box model, and Explainable Boosting Machine (EBM) and Penalized Logistic Tree Regression (PLTR) are the investigated glass-box models.
  We also show that model refinement using feature interaction analysis, correlation checks, and expert input can further enhance model interpretability and robustness.},
 author = {Sagi Schwartz and Qinling Wang and Fang Fang},
 comment = {},
 doi = {},
 eprint = {2509.11389v1},
 journal = {arXiv preprint},
 title = {Enhancing ML Models Interpretability for Credit Scoring},
 url = {http://arxiv.org/abs/2509.11389v1},
 year = {2025}
}

@article{2509.13331v1,
 abstract = {We use artificial intelligence (AI) and supervisory adaptive control systems to plan and optimize the mission of precise spacecraft formation. Machine learning and robust control enhance the efficiency of spacecraft precision formation of the Virtual Telescope for X-ray Observation (VTXO) space mission. VTXO is a precise formation of two separate spacecraft making a virtual telescope with a one-kilometer focal length. One spacecraft carries the lens and the other spacecraft holds the camera to observe high-energy space objects in the X-ray domain with 55 milli-arcsecond angular resolution accuracy. Timed automata for supervisory control, Monte Carlo simulations for stability and robustness evaluation, and integration of deep neural networks for optimal estimation of mission parameters, satisfy the high precision mission criteria. We integrate deep neural networks with a constrained, non-convex dynamic optimization pipeline to predict optimal mission parameters, ensuring precision mission criteria are met. AI framework provides explainability by predicting the resulting energy consumption and mission error for a given set of mission parameters. It allows for transparent, justifiable, and real-time trade-offs, a capability not present in traditional adaptive controllers. The results show reductions in energy consumption and improved mission accuracy, demonstrating the capability of the system to address dynamic uncertainties and disturbances.},
 author = {Reza Pirayeshshirazinezhad},
 comment = {},
 doi = {},
 eprint = {2509.13331v1},
 journal = {arXiv preprint},
 title = {Explainable AI-Enhanced Supervisory Control for High-Precision Spacecraft Formation},
 url = {http://arxiv.org/abs/2509.13331v1},
 year = {2025}
}

@article{2509.13527v1,
 abstract = {Chemists in search of structure-property relationships face great challenges due to limited high quality, concordant datasets. Machine learning (ML) has significantly advanced predictive capabilities in chemical sciences, but these modern data-driven approaches have increased the demand for data. In response to the growing demand for explainable AI (XAI) and to bridge the gap between predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear Algorithm for Meta-Learning that preserves interpretability while improving the prediction accuracy across multiple properties. While most approaches treat each chemical prediction task in isolation, LAMeL leverages a meta-learning framework to identify shared model parameters across related tasks, even if those tasks do not share data, allowing it to learn a common functional manifold that serves as a more informed starting point for new unseen tasks. Our method delivers performance improvements ranging from 1.1- to 25-fold over standard ridge regression, depending on the domain of the dataset. While the degree of performance enhancement varies across tasks, LAMeL consistently outperforms or matches traditional linear methods, making it a reliable tool for chemical property prediction where both accuracy and interpretability are critical.},
 author = {Yulia Pimonova and Michael G. Taylor and Alice Allen and Ping Yang and Nicholas Lubbers},
 comment = {26 pages, 16 figures},
 doi = {},
 eprint = {2509.13527v1},
 journal = {arXiv preprint},
 title = {Meta-Learning Linear Models for Molecular Property Prediction},
 url = {http://arxiv.org/abs/2509.13527v1},
 year = {2025}
}

@article{2509.14830v2,
 abstract = {Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal (multimodal) model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX's prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.},
 author = {Alvaro Lopez Pellicer and Andre Mariucci and Plamen Angelov and Marwan Bukhari and Jemma G. Kerns},
 comment = {ICCV 2025 (PHAROS-AFE-AIMI: Adaptation, Fairness, and Explainability in Medical Imaging). 8 pages, 5 figures, 4 tables. Keywords: multi-modal, multimodal, prototype learning, explainable AI, interpretable models, case-based reasoning, medical imaging, DEXA, bone health, osteoporosis, osteopenia, diagnosis, classification, clustering},
 doi = {},
 eprint = {2509.14830v2},
 journal = {arXiv preprint},
 title = {ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification},
 url = {http://arxiv.org/abs/2509.14830v2},
 year = {2025}
}

@article{2509.14942v1,
 abstract = {Carbapenemase-Producing Enterobacteriace poses a critical concern for infection prevention and control in hospitals. However, predictive modeling of previously highlighted CPE-associated risks such as readmission, mortality, and extended length of stay (LOS) remains underexplored, particularly with modern deep learning approaches. This study introduces an eXplainable AI modeling framework to investigate CPE impact on patient outcomes from Electronic Medical Records data of an Irish hospital. We analyzed an inpatient dataset from an Irish acute hospital, incorporating diagnostic codes, ward transitions, patient demographics, infection-related variables and contact network features. Several Transformer-based architectures were benchmarked alongside traditional machine learning models. Clinical outcomes were predicted, and XAI techniques were applied to interpret model decisions. Our framework successfully demonstrated the utility of Transformer-based models, with TabTransformer consistently outperforming baselines across multiple clinical prediction tasks, especially for CPE acquisition (AUROC and sensitivity). We found infection-related features, including historical hospital exposure, admission context, and network centrality measures, to be highly influential in predicting patient outcomes and CPE acquisition risk. Explainability analyses revealed that features like "Area of Residence", "Admission Ward" and prior admissions are key risk factors. Network variables like "Ward PageRank" also ranked highly, reflecting the potential value of structural exposure information. This study presents a robust and explainable AI framework for analyzing complex EMR data to identify key risk factors and predict CPE-related outcomes. Our findings underscore the superior performance of the Transformer models and highlight the importance of diverse clinical and network features.},
 author = {Minh-Khoi Pham and Tai Tan Mai and Martin Crane and Rob Brennan and Marie E. Ward and Una Geary and Declan Byrne and Brian O Connell and Colm Bergin and Donncha Creagh and Nick McDonald and Marija Bezbradica},
 comment = {Accepted to BMC Medical Informatics and Decision Making on September 18th 2025},
 doi = {10.1186/s12911-025-03214-1},
 eprint = {2509.14942v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers},
 url = {http://arxiv.org/abs/2509.14942v1},
 year = {2025}
}

@article{2509.14987v1,
 abstract = {This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF) for healthcare systems to tackle two essential challenges confronting health information networks: safe data exchange and comprehensible AI-driven clinical decision-making. Our architecture incorporates blockchain, ensuring patient records are immutable, auditable, and tamper-proof, alongside Explainable AI (XAI) methodologies that yield transparent and clinically relevant model predictions. By incorporating security assurances and interpretability requirements into a unified optimization pipeline, BXHF ensures both data-level trust (by verified and encrypted record sharing) and decision-level trust (with auditable and clinically aligned explanations). Its hybrid edge-cloud architecture allows for federated computation across different institutions, enabling collaborative analytics while protecting patient privacy. We demonstrate the framework's applicability through use cases such as cross-border clinical research networks, uncommon illness detection and high-risk intervention decision support. By ensuring transparency, auditability, and regulatory compliance, BXHF improves the credibility, uptake, and effectiveness of AI in healthcare, laying the groundwork for safer and more reliable clinical decision-making.},
 author = {Md Talha Mohsin},
 comment = {6 Pages, 4 Figures},
 doi = {},
 eprint = {2509.14987v1},
 journal = {arXiv preprint},
 title = {Blockchain-Enabled Explainable AI for Trusted Healthcare Systems},
 url = {http://arxiv.org/abs/2509.14987v1},
 year = {2025}
}

@article{2509.16685v1,
 abstract = {The integration of artificial intelligence (AI) into medicine is remarkable, offering advanced diagnostic and therapeutic possibilities. However, the inherent opacity of complex AI models presents significant challenges to their clinical practicality. This paper focuses primarily on investigating the application of explainable artificial intelligence (XAI) methods, with the aim of making AI decisions transparent and interpretable. Our research focuses on implementing simulations using various medical datasets to elucidate the internal workings of the XAI model. These dataset-driven simulations demonstrate how XAI effectively interprets AI predictions, thus improving the decision-making process for healthcare professionals. In addition to a survey of the main XAI methods and simulations, ongoing challenges in the XAI field are discussed. The study highlights the need for the continuous development and exploration of XAI, particularly from the perspective of diverse medical datasets, to promote its adoption and effectiveness in the healthcare domain.},
 author = {Binbin Wen and Yihang Wu and Tareef Daqqaq and Ahmad Chaddad},
 comment = {Published in Cognitive Neurodynamics},
 doi = {10.1007/s11571-025-10343-w},
 eprint = {2509.16685v1},
 journal = {arXiv preprint},
 title = {Towards a Transparent and Interpretable AI Model for Medical Image Classifications},
 url = {http://arxiv.org/abs/2509.16685v1},
 year = {2025}
}

@article{2509.17491v1,
 abstract = {Integrated Gradients (IG) is a widely used attribution method in explainable artificial intelligence (XAI). In this paper, we introduce Path-Weighted Integrated Gradients (PWIG), a generalization of IG that incorporates a customizable weighting function into the attribution integral. This modification allows for targeted emphasis along different segments of the path between a baseline and the input, enabling improved interpretability, noise mitigation, and the detection of path-dependent feature relevance. We establish its theoretical properties and illustrate its utility through experiments on a dementia classification task using the OASIS-1 MRI dataset. Attribution maps generated by PWIG highlight clinically meaningful brain regions associated with various stages of dementia, providing users with sharp and stable explanations. The results suggest that PWIG offers a flexible and theoretically grounded approach for enhancing attribution quality in complex predictive models.},
 author = {Firuz Kamalov and Mohmad Al Falasi and Fadi Thabtah},
 comment = {},
 doi = {},
 eprint = {2509.17491v1},
 journal = {arXiv preprint},
 title = {Path-Weighted Integrated Gradients for Interpretable Dementia Classification},
 url = {http://arxiv.org/abs/2509.17491v1},
 year = {2025}
}

@article{2509.17924v1,
 abstract = {Clinical machine learning faces a critical dilemma in high-stakes medical applications: algorithms achieving optimal diagnostic performance typically sacrifice the interpretability essential for physician decision-making, while interpretable methods compromise sensitivity in complex scenarios. This paradox becomes particularly acute in non-invasive prenatal testing (NIPT), where missed chromosomal abnormalities carry profound clinical consequences yet regulatory frameworks mandate explainable AI systems. We introduce Medical Priority Fusion (MPF), a constrained multi-objective optimization framework that resolves this fundamental trade-off by systematically integrating Naive Bayes probabilistic reasoning with Decision Tree rule-based logic through mathematically-principled weighted fusion under explicit medical constraints. Rigorous validation on 1,687 real-world NIPT samples characterized by extreme class imbalance (43.4:1 normal-to-abnormal ratio) employed stratified 5-fold cross-validation with comprehensive ablation studies and statistical hypothesis testing using McNemar's paired comparisons. MPF achieved simultaneous optimization of dual objectives: 89.3% sensitivity (95% CI: 83.9-94.7%) with 80% interpretability score, significantly outperforming individual algorithms (McNemar's test, p < 0.001). The optimal fusion configuration achieved Grade A clinical deployment criteria with large effect size (d = 1.24), establishing the first clinically-deployable solution that maintains both diagnostic accuracy and decision transparency essential for prenatal care. This work demonstrates that medical-constrained algorithm fusion can resolve the interpretability-performance trade-off, providing a mathematical framework for developing high-stakes medical decision support systems that meet both clinical efficacy and explainability requirements.},
 author = {Xiuqi Ge and Zhibo Yao and Yaosong Du},
 comment = {24 pages, 47 figures, publish to BIBM},
 doi = {},
 eprint = {2509.17924v1},
 journal = {arXiv preprint},
 title = {Medical priority fusion: achieving dual optimization of sensitivity and interpretability in nipt anomaly detection},
 url = {http://arxiv.org/abs/2509.17924v1},
 year = {2025}
}

@article{2509.19202v1,
 abstract = {This entry description proposes AlloyInter, a novel system to enable joint exploration of input mixtures and output parameters space in the context of the SciVis Contest 2025. We propose an interpolation approach, guided by eXplainable Artificial Intelligence (XAI) based on a learned model ensemble that allows users to discover input mixture ratios by specifying output parameter goals that can be iteratively adjusted and improved towards a goal. We strengthen the capabilities of our system by building upon prior research within the robustness of XAI, as well as combining well-established techniques like manifold learning with interpolation approaches.},
 author = {Benedikt Kantz and Peter Waldert and Stefan Lengauer and Tobias Schreck},
 comment = {6 pages, 5 figures, Submitted to the IEEE SciVis 2025 contest},
 doi = {},
 eprint = {2509.19202v1},
 journal = {arXiv preprint},
 title = {AlloyInter: Visualising Alloy Mixture Interpolations in t-SNE Representations},
 url = {http://arxiv.org/abs/2509.19202v1},
 year = {2025}
}

@article{2509.20391v1,
 abstract = {The growing integration of drones into civilian, commercial, and defense sectors introduces significant cybersecurity concerns, particularly with the increased risk of network-based intrusions targeting drone communication protocols. Detecting and classifying these intrusions is inherently challenging due to the dynamic nature of drone traffic and the presence of multiple sophisticated attack vectors such as spoofing, injection, replay, and man-in-the-middle (MITM) attacks. This research aims to develop a robust and interpretable intrusion detection framework tailored for drone networks, with a focus on handling multi-class classification and model explainability. We present a comparative analysis of ensemble-based machine learning models, namely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on a labeled dataset comprising benign traffic and nine distinct intrusion types. Comprehensive data preprocessing was performed, including missing value imputation, scaling, and categorical encoding, followed by model training and extensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews Correlation Coefficient, and Log Loss. Random Forest achieved the highest performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate the superiority of the models, statistical tests, including Friedmans test, the Wilcoxon signed-rank test with Holm correction, and bootstrapped confidence intervals, were applied. Furthermore, explainable AI methods, SHAP and LIME, were integrated to interpret both global and local feature importance, enhancing model transparency and decision trustworthiness. The proposed approach not only delivers near-perfect accuracy but also ensures interpretability, making it highly suitable for real-time and safety-critical drone operations.},
 author = {Md. Alamgir Hossain and Waqas Ishtiaq and Md. Samiul Islam},
 comment = {27 pages, 18 figures, 10 tables},
 doi = {},
 eprint = {2509.20391v1},
 journal = {arXiv preprint},
 title = {A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks},
 url = {http://arxiv.org/abs/2509.20391v1},
 year = {2025}
}

@article{2509.20641v1,
 abstract = {Audio Large Language Models (Audio LLMs) enable human-like conversation about music, yet it is unclear if they are truly listening to the audio or just using textual reasoning, as recent benchmarks suggest. This paper investigates this issue by quantifying the contribution of each modality to a model's output. We adapt the MM-SHAP framework, a performance-agnostic score based on Shapley values that quantifies the relative contribution of each modality to a model's prediction. We evaluate two models on the MuChoMusic benchmark and find that the model with higher accuracy relies more on text to answer questions, but further inspection shows that even if the overall audio contribution is low, models can successfully localize key sound events, suggesting that audio is not entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs and we hope it will serve as a foundational step for future research in explainable AI and audio.},
 author = {Giovana Morais and Magdalena Fuentes},
 comment = {},
 doi = {},
 eprint = {2509.20641v1},
 journal = {arXiv preprint},
 title = {Investigating Modality Contribution in Audio LLMs for Music},
 url = {http://arxiv.org/abs/2509.20641v1},
 year = {2025}
}

@article{2509.21068v1,
 abstract = {Quantum Software Engineering (QSE) is a research area practiced by tech firms. Quantum developers face challenges in optimizing quantum computing and QSE concepts. They use Stack Overflow (SO) to discuss challenges and label posts with specialized quantum tags, which often refer to technical aspects rather than developer posts. Categorizing questions based on quantum concepts can help identify frequent QSE challenges. We conducted studies to classify questions into various challenges. We extracted 2829 questions from Q&A platforms using quantum-related tags. Posts were analyzed to identify frequent challenges and develop a novel grounded theory. Challenges include Tooling, Theoretical, Learning, Conceptual, Errors, and API Usage. Through content analysis and grounded theory, discussions were annotated with common challenges to develop a ground truth dataset. ChatGPT validated human annotations and resolved disagreements. Fine-tuned transformer algorithms, including BERT, DistilBERT, and RoBERTa, classified discussions into common challenges. We achieved an average accuracy of 95% with BERT DistilBERT, compared to fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%, respectively. The Transformer-based approach outperforms the D&ML-based approach with a 6\% increase in accuracy by processing actual discussions, i.e., without data augmentation. We applied SHAP (SHapley Additive exPlanations) for model interpretability, revealing how linguistic features drive predictions and enhancing transparency in classification. These findings can help quantum vendors and forums better organize discussions for improved access and readability. However,empirical evaluation studies with actual developers and vendors are needed.},
 author = {Nek Dil Khan and Javed Ali Khan and Mobashir Husain and Muhammad Sohail Khan and Arif Ali Khan and Muhammad Azeem Akbar and Shahid Hussain},
 comment = {},
 doi = {},
 eprint = {2509.21068v1},
 journal = {arXiv preprint},
 title = {An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI},
 url = {http://arxiv.org/abs/2509.21068v1},
 year = {2025}
}

@article{2509.21327v2,
 abstract = {Predicting the spread of wildfires is essential for effective fire management and risk assessment. With the fast advancements of artificial intelligence (AI), various deep learning models have been developed and utilized for wildfire spread prediction. However, there is limited understanding of the advantages and limitations of these models, and it is also unclear how deep learning-based fire spread models can be compared with existing non-AI fire models. In this work, we assess the ability of five typical deep learning models integrated with weather and environmental variables for wildfire spread prediction based on over ten years of wildfire data in the state of Hawaii. We further use the 2023 Maui fires as a case study to compare the best deep learning models with a widely-used fire spread model, FARSITE. The results show that two deep learning models, i.e., ConvLSTM and ConvLSTM with attention, perform the best among the five tested AI models. FARSITE shows higher precision, lower recall, and higher F1-score than the best AI models, while the AI models offer higher flexibility for the input data. By integrating AI models with an explainable AI method, we further identify important weather and environmental factors associated with the 2023 Maui wildfires.},
 author = {Jiyeon Kim and Yingjie Hu and Negar Elhami-Khorasani and Kai Sun and Ryan Zhenqi Zhou},
 comment = {},
 doi = {},
 eprint = {2509.21327v2},
 journal = {arXiv preprint},
 title = {Assessment of deep learning models integrated with weather and environmental variables for wildfire spread prediction and a case study of the 2023 Maui fires},
 url = {http://arxiv.org/abs/2509.21327v2},
 year = {2025}
}

@article{2509.21770v1,
 abstract = {People with Multiple Sclerosis (MS) complain of problems with hand dexterity and cognitive fatigue. However, in many cases, impairments are subtle and difficult to detect. Functional near-infrared spectroscopy (fNIRS) is a non-invasive neuroimaging technique that measures brain hemodynamic responses during cognitive or motor tasks. We aimed to detect brain activity biomarkers that could explain subjective reports of cognitive fatigue while completing dexterous tasks and provide targets for future brain stimulation treatments. We recruited 15 people with MS who did not have a hand (Nine Hole Peg Test [NHPT]), mobility, or cognitive impairment, and 12 age- and sex-matched controls. Participants completed two types of hand dexterity tasks with their dominant hand, single task and dual task (NHPT while holding a ball between the fifth finger and hypothenar eminence of the same hand). We analyzed fNIRS data (oxygenated and deoxygenated hemoglobin levels) using a machine learning framework to classify MS patients from controls based on their brain activation patterns in bilateral prefrontal and sensorimotor cortices. The K-Nearest Neighbor classifier achieved an accuracy of 75.0% for single manual dexterity tasks and 66.7% for the more complex dual manual dexterity tasks. Using XAI, we found that the most important brain regions contributing to the machine learning model were the supramarginal/angular gyri and the precentral gyrus (sensory integration and motor regions) of the ipsilateral hemisphere, with suppressed activity and slower neurovascular response in the MS group. During both tasks, deoxygenated hemoglobin levels were better predictors than the conventional measure of oxygenated hemoglobin. This nonconventional method of fNIRS data analysis revealed novel brain activity biomarkers that can help develop personalized brain stimulation targets.},
 author = {Sadman Saumik Islam and Bruna Dalcin Baldasso and Davide Cattaneo and Xianta Jiang and Michelle Ploughman},
 comment = {},
 doi = {},
 eprint = {2509.21770v1},
 journal = {arXiv preprint},
 title = {Machine Learning and AI Applied to fNIRS Data Reveals Novel Brain Activity Biomarkers in Stable Subclinical Multiple Sclerosis},
 url = {http://arxiv.org/abs/2509.21770v1},
 year = {2025}
}

@article{2509.22484v1,
 abstract = {We present a machine learning pipeline for biomarker discovery in Multiple Sclerosis (MS), integrating eight publicly available microarray datasets from Peripheral Blood Mononuclear Cells (PBMC). After robust preprocessing we trained an XGBoost classifier optimized via Bayesian search. SHapley Additive exPlanations (SHAP) were used to identify key features for model prediction, indicating thus possible biomarkers. These were compared with genes identified through classical Differential Expression Analysis (DEA). Our comparison revealed both overlapping and unique biomarkers between SHAP and DEA, suggesting complementary strengths. Enrichment analysis confirmed the biological relevance of SHAP-selected genes, linking them to pathways such as sphingolipid signaling, Th1/Th2/Th17 cell differentiation, and Epstein-Barr virus infection all known to be associated with MS. This study highlights the value of combining explainable AI (xAI) with traditional statistical methods to gain deeper insights into disease mechanism.},
 author = {Samuele Punzo and Silvia Giulia Galfrè and Francesco Massafra and Alessandro Maglione and Corrado Priami and Alina Sîrbu},
 comment = {Short paper presented at the 20th conference on Computational Intelligence methods for Bioinformatics and Biostatistics (CIBB2025)},
 doi = {},
 eprint = {2509.22484v1},
 journal = {arXiv preprint},
 title = {A Machine Learning Pipeline for Multiple Sclerosis Biomarker Discovery: Comparing explainable AI and Traditional Statistical Approaches},
 url = {http://arxiv.org/abs/2509.22484v1},
 year = {2025}
}

@article{2509.22755v1,
 abstract = {Concept Activation Vectors (CAVs) are a tool from explainable AI, offering a promising approach for understanding how human-understandable concepts are encoded in a model's latent spaces. They are computed from hidden-layer activations of inputs belonging either to a concept class or to non-concept examples. Adopting a probabilistic perspective, the distribution of the (non-)concept inputs induces a distribution over the CAV, making it a random vector in the latent space. This enables us to derive mean and covariance for different types of CAVs, leading to a unified theoretical view. This probabilistic perspective also reveals a potential vulnerability: CAVs can strongly depend on the rather arbitrary non-concept distribution, a factor largely overlooked in prior work. We illustrate this with a simple yet effective adversarial attack, underscoring the need for a more systematic study.},
 author = {Ekkehard Schnoor and Malik Tiomoko and Jawher Said and Alex Jung and Wojciech Samek},
 comment = {5 pages, 4 figures},
 doi = {},
 eprint = {2509.22755v1},
 journal = {arXiv preprint},
 title = {Concept activation vectors: a unifying view and adversarial attacks},
 url = {http://arxiv.org/abs/2509.22755v1},
 year = {2025}
}

@article{2509.23585v2,
 abstract = {Explainable AI (XAI) methods help identify which image regions influence a model's prediction, but often face a trade-off between detail and interpretability. Layer-wise Relevance Propagation (LRP) offers a model-aware alternative. However, LRP implementations commonly rely on heuristic rule sets that are not optimized for clarity or alignment with model behavior. We introduce EVO-LRP, a method that applies Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to tune LRP hyperparameters based on quantitative interpretability metrics, such as faithfulness or sparseness. EVO-LRP outperforms traditional XAI approaches in both interpretability metric performance and visual coherence, with strong sensitivity to class-specific features. These findings demonstrate that attribution quality can be systematically improved through principled, task-specific optimization.},
 author = {Emerald Zhang and Julian Weaver and Samantha R Santacruz and Edward Castillo},
 comment = {15 pages},
 doi = {},
 eprint = {2509.23585v2},
 journal = {arXiv preprint},
 title = {EVO-LRP: Evolutionary Optimization of LRP for Interpretable Model Explanations},
 url = {http://arxiv.org/abs/2509.23585v2},
 year = {2025}
}

@article{2509.23811v1,
 abstract = {We propose AnveshanaAI, an application-based learning platform for artificial intelligence. With AnveshanaAI, learners are presented with a personalized dashboard featuring streaks, levels, badges, and structured navigation across domains such as data science, machine learning, deep learning, transformers, generative AI, large language models, and multimodal AI, with scope to include more in the future. The platform incorporates gamified tracking with points and achievements to enhance engagement and learning, while switching between Playground, Challenges, Simulator, Dashboard, and Community supports exploration and collaboration. Unlike static question repositories used in existing platforms, AnveshanaAI ensures balanced learning progression through a dataset grounded in Bloom's taxonomy, with semantic similarity checks and explainable AI techniques improving transparency and reliability. Adaptive, automated, and domain-aware assessment methods are also employed. Experiments demonstrate broad dataset coverage, stable fine-tuning with reduced perplexity, and measurable gains in learner engagement. Together, these features illustrate how AnveshanaAI integrates adaptivity, gamification, interactivity, and explainability to support next-generation AI education.},
 author = {Rakesh Thakur and Diksha Khandelwal and Shreya Tiwari},
 comment = {11 pages, 12 figures. Under review as a conference paper at ICLR 2026. Preprint version posted on arXiv},
 doi = {},
 eprint = {2509.23811v1},
 journal = {arXiv preprint},
 title = {AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through Automated Question Generation and Interactive Assessment},
 url = {http://arxiv.org/abs/2509.23811v1},
 year = {2025}
}

@article{2509.24149v1,
 abstract = {The early and accurate classification of brain tumors is crucial for guiding effective treatment strategies and improving patient outcomes. This study presents BrainFusion, a significant advancement in brain tumor analysis using magnetic resonance imaging (MRI) by combining fine-tuned convolutional neural networks (CNNs) for tumor classification--including VGG16, ResNet50, and Xception--with YOLOv8 for precise tumor localization with bounding boxes. Leveraging the Brain Tumor MRI Dataset, our experiments reveal that the fine-tuned VGG16 model achieves test accuracy of 99.86%, substantially exceeding previous benchmarks. Beyond setting a new accuracy standard, the integration of bounding-box localization and explainable AI techniques further enhances both the clinical interpretability and trustworthiness of the system's outputs. Overall, this approach underscores the transformative potential of deep learning in delivering faster, more reliable diagnoses, ultimately contributing to improved patient care and survival rates.},
 author = {Walid Houmaidi and Youssef Sabiri and Salmane El Mansour Billah and Amine Abouaomar},
 comment = {6 pages, 4 figures, 3 tables. Accepted at the 12th International Conference on Wireless Networks and Mobile Communications 2025 (WINCOM 2025)},
 doi = {},
 eprint = {2509.24149v1},
 journal = {arXiv preprint},
 title = {Accelerating Cerebral Diagnostics with BrainFusion: A Comprehensive MRI Tumor Framework},
 url = {http://arxiv.org/abs/2509.24149v1},
 year = {2025}
}

@article{2509.25804v2,
 abstract = {This study aims to develop and evaluate an ensemble machine learning-based framework for the automatic detection of Wide QRS Complex Tachycardia (WCT) from ECG signals, emphasizing diagnostic accuracy and interpretability using Explainable AI. The proposed system integrates ensemble learning techniques, i.e., an optimized Random Forest known as CardioForest, and models like XGBoost and LightGBM. The models were trained and tested on ECG data from the publicly available MIMIC-IV dataset. The testing was carried out with the assistance of accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations) was used to ascertain model explainability and clinical relevance. The CardioForest model performed best on all metrics, achieving a test accuracy of 95.19%, a balanced accuracy of 88.76%, a precision of 95.26%, a recall of 78.42%, and an ROC-AUC of 0.8886. SHAP analysis confirmed the model's ability to rank the most relevant ECG features, such as QRS duration, in accordance with clinical intuitions, thereby fostering trust and usability in clinical practice. The findings recognize CardioForest as an extremely dependable and interpretable WCT detection model. Being able to offer accurate predictions and transparency through explainability makes it a valuable tool to help cardiologists make timely and well-informed diagnoses, especially for high-stakes and emergency scenarios.},
 author = {Vaskar Chakma and Ju Xiaolin and Heling Cao and Xue Feng and Ji Xiaodong and Pan Haiyan and Gao Zhan},
 comment = {},
 doi = {10.1101/2025.09.15.25335837},
 eprint = {2509.25804v2},
 journal = {arXiv preprint},
 title = {CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG},
 url = {http://arxiv.org/abs/2509.25804v2},
 year = {2025}
}

@article{2510.00048v2,
 abstract = {Early and accurate diagnosis of Alzheimer Disease is critical for effective clinical intervention, particularly in distinguishing it from Mild Cognitive Impairment, a prodromal stage marked by subtle structural changes. In this study, we propose a hybrid deep learning ensemble framework for Alzheimer Disease classification using structural magnetic resonance imaging. Gray and white matter slices are used as inputs to three pretrained convolutional neural networks such as ResNet50, NASNet, and MobileNet, each fine tuned through an end to end process. To further enhance performance, we incorporate a stacked ensemble learning strategy with a meta learner and weighted averaging to optimally combine the base models. Evaluated on the Alzheimer Disease Neuroimaging Initiative dataset, the proposed method achieves state of the art accuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and 91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming conventional transfer learning and baseline ensemble methods. To improve interpretability in image based diagnostics, we integrate Explainable AI techniques by Gradient weighted Class Activation, which generates heatmaps and attribution maps that highlight critical regions in gray and white matter slices, revealing structural biomarkers that influence model decisions. These results highlight the frameworks potential for robust and scalable clinical decision support in neurodegenerative disease diagnostics.},
 author = {Fahad Mostafa and Kannon Hossain and Hafiz Khan},
 comment = {18 pages, 4 figures},
 doi = {},
 eprint = {2510.00048v2},
 journal = {arXiv preprint},
 title = {Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment},
 url = {http://arxiv.org/abs/2510.00048v2},
 year = {2025}
}

@article{2510.00061v1,
 abstract = {Osteoporosis silently erodes skeletal integrity worldwide; however, early detection through imaging can prevent most fragility fractures. Artificial intelligence (AI) methods now mine routine Dual-energy X-ray Absorptiometry (DXA), X-ray, Computed Tomography (CT), and Magnetic Resonance Imaging (MRI) scans for subtle, clinically actionable markers, but the literature is fragmented. This survey unifies the field through a tri-axial framework that couples imaging modalities with clinical tasks and AI methodologies (classical machine learning, convolutional neural networks (CNNs), transformers, self-supervised learning, and explainable AI). Following a concise clinical and technical primer, we detail our Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)-guided search strategy, introduce the taxonomy via a roadmap figure, and synthesize cross-study insights on data scarcity, external validation, and interpretability. By identifying emerging trends, open challenges, and actionable research directions, this review provides AI scientists, medical imaging researchers, and musculoskeletal clinicians with a clear compass to accelerate rigorous, patient-centered innovation in osteoporosis care. The project page of this survey can also be found on Github.},
 author = {Abdul Rahman and Bumshik Lee},
 comment = {56 pages, 18 figures},
 doi = {},
 eprint = {2510.00061v1},
 journal = {arXiv preprint},
 title = {Survey of AI-Powered Approaches for Osteoporosis Diagnosis in Medical Imaging},
 url = {http://arxiv.org/abs/2510.00061v1},
 year = {2025}
}

@article{2510.00244v1,
 abstract = {With the European Union introducing gender quotas on corporate boards, this study investigates the impact of board gender diversity (BGD) on firms' carbon emission performance (CEP). Using panel regressions and advanced machine learning algorithms on data from European firms between 2016 and 2022, the analyses reveal a significant non-linear relationship. Specifically, CEP improves with BGD up to an optimal level of approximately 35 percent, beyond which further increases in BGD yield no additional improvement in CEP. A minimum threshold of 22 percent BGD is necessary for meaningful improvements in CEP. To assess the legitimacy of CEP outcomes, this study examines whether ESG controversies affect the relationship between BGD and CEP. The results show no significant effect, suggesting that the effect of BGD is driven by governance mechanisms rather than symbolic actions. Additionally, structural equation modelling (SEM) indicates that while environmental innovation contributes to CEP, it is not the mediating channel through which BGD promotes CEP. The results have implications for academics, businesses, and regulators.},
 author = {Mohammad Hassan Shakil and Arne Johan Pollestad and Khine Kyaw and Ziaul Haque Munim},
 comment = {34 pages and 3 figures},
 doi = {},
 eprint = {2510.00244v1},
 journal = {arXiv preprint},
 title = {Board Gender Diversity and Carbon Emissions Performance: Insights from Panel Regressions, Machine Learning and Explainable AI},
 url = {http://arxiv.org/abs/2510.00244v1},
 year = {2025}
}

@article{2510.01038v1,
 abstract = {Black-box explainability methods are popular tools for explaining the decisions of image classifiers. A major drawback of these tools is their reliance on mutants obtained by occluding parts of the input, leading to out-of-distribution images. This raises doubts about the quality of the explanations. Moreover, choosing an appropriate occlusion value often requires domain knowledge. In this paper we introduce a novel forward-pass paradigm Activation-Deactivation (AD), which removes the effects of occluded input features from the model's decision-making by switching off the parts of the model that correspond to the occlusions. We introduce ConvAD, a drop-in mechanism that can be easily added to any trained Convolutional Neural Network (CNN), and which implements the AD paradigm. This leads to more robust explanations without any additional training or fine-tuning. We prove that the ConvAD mechanism does not change the decision-making process of the network. We provide experimental evaluation across several datasets and model architectures. We compare the quality of AD-explanations with explanations achieved using a set of masking values, using the proxies of robustness, size, and confidence drop-off. We observe a consistent improvement in robustness of AD explanations (up to 62.5%) compared to explanations obtained with occlusions, demonstrating that ConvAD extracts more robust explanations without the need for domain knowledge.},
 author = {Akchunya Chanchal and David A. Kelly and Hana Chockler},
 comment = {Preprint: Under Review},
 doi = {},
 eprint = {2510.01038v1},
 journal = {arXiv preprint},
 title = {Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI},
 url = {http://arxiv.org/abs/2510.01038v1},
 year = {2025}
}

@article{2510.01520v1,
 abstract = {The safe use of pharmaceuticals in food-producing animals is vital to protect animal welfare and human food safety. Adverse events (AEs) may signal unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of violative residues in the food chain. This study introduces a predictive framework for classifying outcomes (Death vs. Recovery) using ~1.28 million reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary Medicine. A preprocessing pipeline merged relational tables and standardized AEs through VeDDRA ontologies. Data were normalized, missing values imputed, and high-cardinality features reduced; physicochemical drug properties were integrated to capture chemical-residue links. We evaluated supervised models, including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as undersampling and oversampling, with a focus on prioritizing recall for fatal outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best, achieving precision, recall, and F1-scores of 0.95. Incorporating Average Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved minority-class detection, particularly in ExcelFormer and XGBoost. Interpretability via SHAP identified biologically plausible predictors, including lung, heart, and bronchial disorders, animal demographics, and drug physicochemical properties. These features were strongly linked to fatal outcomes. Overall, the framework shows that combining rigorous data engineering, advanced machine learning, and explainable AI enables accurate, interpretable predictions of veterinary safety outcomes. The approach supports FARAD's mission by enabling early detection of high-risk drug-event profiles, strengthening residue risk assessment, and informing regulatory and clinical decision-making.},
 author = {Hossein Sholehrasa and Xuan Xu and Doina Caragea and Jim E. Riviere and Majid Jaberi-Douraki},
 comment = {},
 doi = {},
 eprint = {2510.01520v1},
 journal = {arXiv preprint},
 title = {Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties},
 url = {http://arxiv.org/abs/2510.01520v1},
 year = {2025}
}

@article{2510.03134v2,
 abstract = {Explainable Artificial Intelligence has become a crucial area of research, aiming to demystify the decision-making processes of deep learning models. Among various explainability techniques, counterfactual explanations have been proven particularly promising, as they offer insights into model behavior by highlighting minimal changes that would alter a prediction. Despite their potential, these explanations are often complex and technical, making them difficult for non-experts to interpret. To address this challenge, we propose a novel pipeline that leverages Language Models, large and small, to compose narratives for counterfactual explanations. We employ knowledge distillation techniques along with a refining mechanism to enable Small Language Models to perform comparably to their larger counterparts while maintaining robust reasoning abilities. In addition, we introduce a simple but effective evaluation method to assess natural language narratives, designed to verify whether the models' responses are in line with the factual, counterfactual ground truth. As a result, our proposed pipeline enhances both the reasoning capabilities and practical performance of student models, making them more suitable for real-world use cases.},
 author = {Flavio Giorgi and Matteo Silvestri and Cesare Campagnano and Fabrizio Silvestri and Gabriele Tolomei},
 comment = {},
 doi = {},
 eprint = {2510.03134v2},
 journal = {arXiv preprint},
 title = {Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation},
 url = {http://arxiv.org/abs/2510.03134v2},
 year = {2025}
}

@article{2510.03623v1,
 abstract = {Explainable Artificial Intelligence (XAI) has aided machine learning (ML) researchers with the power of scrutinizing the decisions of the black-box models. XAI methods enable looking deep inside the models' behavior, eventually generating explanations along with a perceived trust and transparency. However, depending on any specific XAI method, the level of trust can vary. It is evident that XAI methods can themselves be a victim of post-adversarial attacks that manipulate the expected outcome from the explanation module. Among such attack tactics, fairwashing explanation (FE), manipulation explanation (ME), and backdoor-enabled manipulation attacks (BD) are the notable ones. In this paper, we try to understand these adversarial attack techniques, tactics, and procedures (TTPs) on explanation alteration and thus the effect on the model's decisions. We have explored a total of six different individual attack procedures on post-hoc explanation methods such as SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG (Integrated Gradients), and investigated those adversarial attacks in cybersecurity applications scenarios such as phishing, malware, intrusion, and fraudulent website detection. Our experimental study reveals the actual effectiveness of these attacks, thus providing an urgency for immediate attention to enhance the resiliency of XAI methods and their applications.},
 author = {Maraz Mia and Mir Mehedi A. Pritom},
 comment = {10 pages, 9 figures, 4 tables},
 doi = {},
 eprint = {2510.03623v1},
 journal = {arXiv preprint},
 title = {Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications},
 url = {http://arxiv.org/abs/2510.03623v1},
 year = {2025}
}

@article{2510.03814v3,
 abstract = {Recurrent Neural Networks (RNNs) have found widespread applications in machine learning for time series prediction and dynamical systems reconstruction, and experienced a recent renaissance with improved training algorithms and architectural designs. Understanding why and how trained RNNs produce their behavior is important for scientific and medical applications, and explainable AI more generally. An RNN's dynamical repertoire depends on the topological and geometrical properties of its state space. Stable and unstable manifolds of periodic points play a particularly important role: They dissect a dynamical system's state space into different basins of attraction, and their intersections lead to chaotic dynamics with fractal geometry. Here we introduce a novel algorithm for detecting these manifolds, with a focus on piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as their activation function. We demonstrate how the algorithm can be used to trace the boundaries between different basins of attraction, and hence to characterize multistability, a computationally important property. We further show its utility in finding so-called homoclinic points, the intersections between stable and unstable manifolds, and thus establish the existence of chaos in PLRNNs. Finally we show for an empirical example, electrophysiological recordings from a cortical neuron, how insights into the underlying dynamics could be gained through our method.},
 author = {Lukas Eisenmann and Alena Brändle and Zahra Monfared and Daniel Durstewitz},
 comment = {},
 doi = {},
 eprint = {2510.03814v3},
 journal = {arXiv preprint},
 title = {Detecting Invariant Manifolds in ReLU-Based RNNs},
 url = {http://arxiv.org/abs/2510.03814v3},
 year = {2025}
}

@article{2510.03815v1,
 abstract = {There are limitations of traditional methods and deep learning methods in terms of interpretability, generalization, and quantification of uncertainty in industrial fault diagnosis, and there are core problems of insufficient credibility in industrial fault diagnosis. The architecture performs preliminary analysis through a Bayesian network-based diagnostic engine and features an LLM-driven cognitive quorum module with multimodal input capabilities. The module conducts expert-level arbitration of initial diagnoses by analyzing structured features and diagnostic charts, prioritizing final decisions after conflicts are identified. To ensure the reliability of the system output, the architecture integrates a confidence calibration module based on temperature calibration and a risk assessment module, which objectively quantifies the reliability of the system using metrics such as expected calibration error (ECE). Experimental results on a dataset containing multiple fault types showed that the proposed framework improved diagnostic accuracy by more than 28 percentage points compared to the baseline model, while the calibrated ECE was reduced by more than 75%. Case studies have confirmed that HCAA effectively corrects misjudgments caused by complex feature patterns or knowledge gaps in traditional models, providing novel and practical engineering solutions for building high-trust, explainable AI diagnostic systems for industrial applications.},
 author = {Yue wu},
 comment = {1tables,6 figs,11pages},
 doi = {},
 eprint = {2510.03815v1},
 journal = {arXiv preprint},
 title = {A Trustworthy Industrial Fault Diagnosis Architecture Integrating Probabilistic Models and Large Language Models},
 url = {http://arxiv.org/abs/2510.03815v1},
 year = {2025}
}

@article{2510.03859v1,
 abstract = {Ensuring that critical IoT systems function safely and smoothly depends a lot on finding anomalies quickly. As more complex systems, like smart healthcare, energy grids and industrial automation, appear, it is easier to see the shortcomings of older methods of detection. Monitoring failures usually happen in dynamic, high dimensional situations, especially when data is incomplete, messy or always evolving. Such limits point out the requirement for adaptive, intelligent systems that always improve and think. LLMs are now capable of significantly changing how context is understood and semantic inference is done across all types of data. This proposal suggests using an LLM supported contextual reasoning method along with XAI agents to improve how anomalies are found in significant IoT environments. To discover hidden patterns and notice inconsistencies in data streams, it uses attention methods, avoids dealing with details from every time step and uses memory buffers with meaning. Because no code AI stresses transparency and interpretability, people can check and accept the AI's decisions, helping ensure AI follows company policies. The two architectures are put together in a test that compares the results of the traditional model with those of the suggested LLM enhanced model. Important measures to check are the accuracy of detection, how much inaccurate information is included in the results, how clearly the findings can be read and how fast the system responds under different test situations. The metaheuristic is tested in simulations of real world smart grid and healthcare contexts to check its adaptability and reliability. From the study, we see that the new approach performs much better than most existing models in both accuracy and interpretation, so it could be a good fit for future anomaly detection tasks in IoT},
 author = {Raghav Sharma and Manan Mehta},
 comment = {22 pages},
 doi = {},
 eprint = {2510.03859v1},
 journal = {arXiv preprint},
 title = {Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning},
 url = {http://arxiv.org/abs/2510.03859v1},
 year = {2025}
}

@article{2510.04377v1,
 abstract = {T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is a central component of adaptive immunity, with implications for vaccine design, cancer immunotherapy, and autoimmune disease. While recent advances in machine learning have improved prediction of TCR-pMHC binding, the most effective approaches are black-box transformer models that cannot provide a rationale for predictions. Post-hoc explanation methods can provide insight with respect to the input but do not explicitly model biochemical mechanisms (e.g. known binding regions), as in TCR-pMHC binding. ``Explain-by-design'' models (i.e., with architectural components that can be examined directly after training) have been explored in other domains, but have not been used for TCR-pMHC binding. We propose explainable model layers (TCR-EML) that can be incorporated into protein-language model backbones for TCR-pMHC modeling. Our approach uses prototype layers for amino acid residue contacts drawn from known TCR-pMHC binding mechanisms, enabling high-quality explanations for predicted TCR-pMHC binding. Experiments of our proposed method on large-scale datasets demonstrate competitive predictive accuracy and generalization, and evaluation on the TCR-XAI benchmark demonstrates improved explainability compared with existing approaches.},
 author = {Jiarui Li and Zixiang Yin and Zhengming Ding and Samuel J. Landry and Ramgopal R. Mettu},
 comment = {},
 doi = {},
 eprint = {2510.04377v1},
 journal = {arXiv preprint},
 title = {TCR-EML: Explainable Model Layers for TCR-pMHC Prediction},
 url = {http://arxiv.org/abs/2510.04377v1},
 year = {2025}
}

@article{2510.04776v1,
 abstract = {Structural biology has made significant progress in determining membrane proteins, leading to a remarkable increase in the number of available structures in dedicated databases. The inherent complexity of membrane protein structures, coupled with challenges such as missing data, inconsistencies, and computational barriers from disparate sources, underscores the need for improved database integration. To address this gap, we present MetaMP, a framework that unifies membrane-protein databases within a web application and uses machine learning for classification. MetaMP improves data quality by enriching metadata, offering a user-friendly interface, and providing eight interactive views for streamlined exploration. MetaMP was effective across tasks of varying difficulty, demonstrating advantages across different levels without compromising speed or accuracy, according to user evaluations. Moreover, MetaMP supports essential functions such as structure classification and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in membrane protein research: predicting transmembrane segments, reconciling legacy databases, and classifying structures with explainable AI support. In a validation focused on statistics, MetaMP resolved 77% of data discrepancies and accurately predicted the class of newly identified membrane proteins 98% of the time and overtook expert curation. Altogether, MetaMP is a much-needed resource that harmonizes current knowledge and empowers AI-driven exploration of membrane-protein architecture.},
 author = {Ebenezer Awotoro and Chisom Ezekannagha and Florian Schwarz and Johannes Tauscher and Dominik Heider and Katharina Ladewig and Christel Le Bon and Karine Moncoq and Bruno Miroux and Georges Hattab},
 comment = {},
 doi = {},
 eprint = {2510.04776v1},
 journal = {arXiv preprint},
 title = {MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis},
 url = {http://arxiv.org/abs/2510.04776v1},
 year = {2025}
}

@article{2510.05120v1,
 abstract = {The growing complexity of machine learning (ML) models in big data analytics, especially in domains such as environmental monitoring, highlights the critical need for interpretability and explainability to promote trust, ethical considerations, and regulatory adherence (e.g., GDPR). Traditional "black-box" models obstruct transparency, whereas post-hoc explainable AI (XAI) techniques like LIME and SHAP frequently compromise accuracy or fail to deliver inherent insights. This paper presents a novel framework that combines type-2 fuzzy sets, granular computing, and clustering to boost explainability and fairness in big data environments. When applied to the UCI Air Quality dataset, the framework effectively manages uncertainty in noisy sensor data, produces linguistic rules, and assesses fairness using silhouette scores and entropy. Key contributions encompass: (1) A type-2 fuzzy clustering approach that enhances cohesion by about 4% compared to type-1 methods (silhouette 0.365 vs. 0.349) and improves fairness (entropy 0.918); (2) Incorporation of fairness measures to mitigate biases in unsupervised scenarios; (3) A rule-based component for intrinsic XAI, achieving an average coverage of 0.65; (4) Scalable assessments showing linear runtime (roughly 0.005 seconds for sampled big data sizes). Experimental outcomes reveal superior performance relative to baselines such as DBSCAN and Agglomerative Clustering in terms of interpretability, fairness, and efficiency. Notably, the proposed method achieves a 4% improvement in silhouette score over type-1 fuzzy clustering and outperforms baselines in fairness (entropy reduction by up to 1%) and efficiency.},
 author = {Farjana Yesmin and Nusrat Shirmin},
 comment = {8 pages},
 doi = {},
 eprint = {2510.05120v1},
 journal = {arXiv preprint},
 title = {A Fuzzy Logic-Based Framework for Explainable Machine Learning in Big Data Analytics},
 url = {http://arxiv.org/abs/2510.05120v1},
 year = {2025}
}

@article{2510.06165v1,
 abstract = {Feature attributions are post-training analysis methods that assess how various input features of a machine learning model contribute to an output prediction. Their interpretation is straightforward when features act independently, but becomes less direct when the predictive model involves interactions such as multiplicative relationships or joint feature contributions. In this work, we propose a general theory of higher-order feature attribution, which we develop on the foundation of Integrated Gradients (IG). This work extends existing frameworks in the literature on explainable AI. When using IG as the method of feature attribution, we discover natural connections to statistics and topological signal processing. We provide several theoretical results that establish the theory, and we validate our theory on a few examples.},
 author = {Kurt Butler and Guanchao Feng and Petar Djuric},
 comment = {5 pages, 3 figures},
 doi = {},
 eprint = {2510.06165v1},
 journal = {arXiv preprint},
 title = {Higher-Order Feature Attribution: Bridging Statistics, Explainable AI, and Topological Signal Processing},
 url = {http://arxiv.org/abs/2510.06165v1},
 year = {2025}
}

@article{2510.07050v2,
 abstract = {As artificial intelligence becomes increasingly pervasive and powerful, the ability to audit AI-based systems is growing in importance. However, explainability for artificial intelligence systems is not a one-size-fits-all solution; different target audiences have varying requirements and expectations for explanations. While various approaches to explainability have been proposed, most explainable artificial intelligence methods for tabular data focus on explaining the outputs of supervised machine learning models using the input features. However, a user's ability to understand an explanation depends on their understanding of such features. Therefore, it is in the best interest of the system designer to try to pre-select understandable features for producing a global explanation of an ML model. Unfortunately, no measure currently exists to assess the degree to which a user understands a given input feature. This work introduces two psychometrically validated scales that quantitatively seek to assess users' understanding of tabular input features for supervised classification problems. Specifically, these scales, one for numerical and one for categorical data, each with two factors and comprising 8 and 9 items, aim to assign a score to each input feature, effectively producing a rank, and allowing for the quantification of feature prioritisation. A confirmatory factor analysis demonstrates a strong relationship between such items and a good fit of the two-factor structure for each scale. This research presents a novel method for assessing understanding and outlines potential applications in the domain of explainable artificial intelligence.},
 author = {Nicola Rossberg and Bennett Kleinberg and Barry O'Sullivan and Luca Longo and Andrea Visentin},
 comment = {47 pages, 6 figures, 12 tables},
 doi = {},
 eprint = {2510.07050v2},
 journal = {arXiv preprint},
 title = {The Feature Understandability Scale for Human-Centred Explainable AI: Assessing Tabular Feature Importance},
 url = {http://arxiv.org/abs/2510.07050v2},
 year = {2025}
}

@article{2510.08303v1,
 abstract = {As AI becomes a native component of 6G network control, AI models must adapt to continuously changing conditions, including the introduction of new features and measurements driven by multi-vendor deployments, hardware upgrades, and evolving service requirements. To address this growing need for flexible learning in non-stationary environments, this vision paper highlights Adaptive Random Forests (ARFs) as a reliable solution for dynamic feature adaptation in communication network scenarios. We show that iterative training of ARFs can effectively lead to stable predictions, with accuracy improving over time as more features are added. In addition, we highlight the importance of explainability in AI-driven networks, proposing Drift-Aware Feature Importance (DAFI) as an efficient XAI feature importance (FI) method. DAFI uses a distributional drift detector to signal when to apply computationally intensive FI methods instead of lighter alternatives. Our tests on 3 different datasets indicate that our approach reduces runtime by up to 2 times, while producing more consistent feature importance values. Together, ARFs and DAFI provide a promising framework to build flexible AI methods adapted to 6G network use-cases.},
 author = {Yannis Belkhiter and Seshu Tirupathi and Giulio Zizzo and Merim Dzaferagic and John D. Kelleher},
 comment = {Accepted at AI4NextG Workshop, NeurIPS 2025},
 doi = {},
 eprint = {2510.08303v1},
 journal = {arXiv preprint},
 title = {Dynamic Features Adaptation in Networking: Toward Flexible training and Explainable inference},
 url = {http://arxiv.org/abs/2510.08303v1},
 year = {2025}
}

@article{2510.08591v1,
 abstract = {Recent advancements in QML and SNNs have generated considerable excitement, promising exponential speedups and brain-like energy efficiency to revolutionize AI. However, this paper argues that they are unlikely to displace DNNs in the near term. QML struggles with adapting backpropagation due to unitary constraints, measurement-induced state collapse, barren plateaus, and high measurement overheads, exacerbated by the limitations of current noisy intermediate-scale quantum hardware, overfitting risks due to underdeveloped regularization techniques, and a fundamental misalignment with machine learning's generalization. SNNs face restricted representational bandwidth, struggling with long-range dependencies and semantic encoding in language tasks due to their discrete, spike-based processing. Furthermore, the goal of faithfully emulating the brain might impose inherent inefficiencies like cognitive biases, limited working memory, and slow learning speeds. Even their touted energy-efficient advantages are overstated; optimized DNNs with quantization can outperform SNNs in energy costs under realistic conditions. Finally, SNN training incurs high computational overhead from temporal unfolding. In contrast, DNNs leverage efficient backpropagation, robust regularization, and innovations in LRMs that shift scaling to inference-time compute, enabling self-improvement via RL and search algorithms like MCTS while mitigating data scarcity. This superiority is evidenced by recent models such as xAI's Grok-4 Heavy, which advances SOTA performance, and gpt-oss-120b, which surpasses or approaches the performance of leading industry models despite its modest 120-billion-parameter size deployable on a single 80GB GPU. Furthermore, specialized ASICs amplify these efficiency gains. Ultimately, QML and SNNs may serve niche hybrid roles, but DNNs remain the dominant, practical paradigm for AI advancement.},
 author = {Takehiro Ishikawa},
 comment = {},
 doi = {},
 eprint = {2510.08591v1},
 journal = {arXiv preprint},
 title = {The Enduring Dominance of Deep Neural Networks: A Critical Analysis of the Fundamental Limitations of Quantum Machine Learning and Spiking Neural Networks},
 url = {http://arxiv.org/abs/2510.08591v1},
 year = {2025}
}

@article{2510.08737v1,
 abstract = {In this growing age of data and technology, large black-box models are becoming the norm due to their ability to handle vast amounts of data and learn incredibly complex input-output relationships. The deficiency of these methods, however, is their inability to explain the prediction process, making them untrustworthy and their use precarious in high-stakes situations. SHapley Additive exPlanations (SHAP) analysis is an explainable AI method growing in popularity for its ability to explain model predictions in terms of the original features. For each sample and feature in the data set, we associate a SHAP value that quantifies the contribution of that feature to the prediction of that sample. Clustering these SHAP values can provide insight into the data by grouping samples that not only received the same prediction, but received the same prediction for similar reasons. In doing so, we map the various pathways through which distinct samples arrive at the same prediction. To showcase this methodology, we present a simulated experiment in addition to a case study in Alzheimer's disease using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. We also present a novel generalization of the waterfall plot for multi-classification.},
 author = {Justin Lin and Julia Fukuyama},
 comment = {23 pages, 15 figures, 3 tables},
 doi = {},
 eprint = {2510.08737v1},
 journal = {arXiv preprint},
 title = {SHAP-Based Supervised Clustering for Sample Classification and the Generalized Waterfall Plot},
 url = {http://arxiv.org/abs/2510.08737v1},
 year = {2025}
}

@article{2510.11164v1,
 abstract = {The use of Artificial Intelligence (AI) models in real-world and high-risk applications has intensified the discussion about their trustworthiness and ethical usage, from both a technical and a legislative perspective. The field of eXplainable Artificial Intelligence (XAI) addresses this challenge by proposing explanations that bring to light the decision-making processes of complex black-box models. Despite being an essential property, the robustness of explanations is often an overlooked aspect during development: only robust explanation methods can increase the trust in the system as a whole. This paper investigates the role of robustness through the usage of a feature importance aggregation derived from multiple models ($k$-nearest neighbours, random forest and neural networks). Preliminary results showcase the potential in increasing the trustworthiness of the application, while leveraging multiple model's predictive power.},
 author = {Ilaria Vascotto and Alex Rodriguez and Alessandro Bonaita and Luca Bortolussi},
 comment = {Accepted at the European Workshop on Trustworthy Artificial Intelligence (TRUST-AI), co-located within ECAI 2025},
 doi = {},
 eprint = {2510.11164v1},
 journal = {arXiv preprint},
 title = {Beyond single-model XAI: aggregating multi-model explanations for enhanced trustworthiness},
 url = {http://arxiv.org/abs/2510.11164v1},
 year = {2025}
}

@article{2510.12957v1,
 abstract = {Standard benchmark datasets, such as MNIST, often fail to expose latent biases and multimodal feature complexities, limiting the trustworthiness of deep neural networks in high-stakes applications. We propose a novel multimodal Explainable AI (XAI) framework that unifies attention-augmented feature fusion, Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop for bias detection and mitigation. Evaluated on multimodal extensions of MNIST, our approach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1% explanation fidelity (IoU-XAI), outperforming unimodal and non-explainable baselines. Ablation studies demonstrate that integrating interpretability with bias-aware learning enhances robustness and human alignment. Our work bridges the gap between performance, transparency, and fairness, highlighting a practical pathway for trustworthy AI in sensitive domains.},
 author = {Noor Islam S. Mohammad},
 comment = {},
 doi = {},
 eprint = {2510.12957v1},
 journal = {arXiv preprint},
 title = {A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning},
 url = {http://arxiv.org/abs/2510.12957v1},
 year = {2025}
}

@article{2510.13030v1,
 abstract = {Computer models are indispensable tools for understanding the Earth system. While high-resolution operational models have achieved many successes, they exhibit persistent biases, particularly in simulating extreme events and statistical distributions. In contrast, coarse-grained idealized models isolate fundamental processes and can be precisely calibrated to excel in characterizing specific dynamical and statistical features. However, different models remain siloed by disciplinary boundaries. By leveraging the complementary strengths of models of varying complexity, we develop an explainable AI framework for Earth system emulators. It bridges the model hierarchy through a reconfigured latent data assimilation technique, uniquely suited to exploit the sparse output from the idealized models. The resulting bridging model inherits the high resolution and comprehensive variables of operational models while achieving global accuracy enhancements through targeted improvements from idealized models. Crucially, the mechanism of AI provides a clear rationale for these advancements, moving beyond black-box correction to physically insightful understanding in a computationally efficient framework that enables effective physics-assisted digital twins and uncertainty quantification. We demonstrate its power by significantly correcting biases in CMIP6 simulations of El Niño spatiotemporal patterns, leveraging statistically accurate idealized models. This work also highlights the importance of pushing idealized model development and advancing communication between modeling communities.},
 author = {Pouria Behnoudfar and Charlotte Moser and Marc Bocquet and Sibo Cheng and Nan Chen},
 comment = {},
 doi = {},
 eprint = {2510.13030v1},
 journal = {arXiv preprint},
 title = {Bridging Idealized and Operational Models: An Explainable AI Framework for Earth System Emulators},
 url = {http://arxiv.org/abs/2510.13030v1},
 year = {2025}
}

@article{2510.13814v2,
 abstract = {Both humans and machine learning models learn from experience, particularly in safety- and reliability-critical domains. While psychology seeks to understand human cognition, the field of Explainable AI (XAI) develops methods to interpret machine learning models. This study bridges these domains by applying computational tools from XAI to analyze human learning. We modeled human behavior during a complex real-world task -- tuning a particle accelerator -- by constructing graphs of operator subtasks. Applying techniques such as community detection and hierarchical clustering to archival operator data, we reveal how operators decompose the problem into simpler components and how these problem-solving structures evolve with expertise. Our findings illuminate how humans develop efficient strategies in the absence of globally optimal solutions, and demonstrate the utility of XAI-based methods for quantitatively studying human cognition.},
 author = {Roussel Rahman and Aashwin Ananda Mishra and Wan-Lin Hu},
 comment = {},
 doi = {},
 eprint = {2510.13814v2},
 journal = {arXiv preprint},
 title = {Reversing the Lens: Using Explainable AI to Understand Human Expertise},
 url = {http://arxiv.org/abs/2510.13814v2},
 year = {2025}
}

@article{2510.13828v1,
 abstract = {Explainable Artificial Intelligence (XAI) has been presented as the critical component for unlocking the potential of machine learning in mental health screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI techniques, such as SHAP and LIME, excel at producing technically faithful outputs such as feature importance scores, but fail to deliver clinically relevant, actionable insights that can be used by clinicians or understood by patients. This disconnect between technical transparency and human utility is the primary barrier to real-world adoption. This paper argues that this gap is a translation problem and proposes the Generative Operational Framework, a novel system architecture that leverages Large Language Models (LLMs) as a central translation engine. This framework is designed to ingest the raw, technical outputs from diverse XAI tools and synthesize them with clinical guidelines (via RAG) to automatically generate human-readable, evidence-backed clinical narratives. To justify our solution, we provide a systematic analysis of the components it integrates, tracing the evolution from intrinsic models to generative XAI. We demonstrate how this framework directly addresses key operational barriers, including workflow integration, bias mitigation, and stakeholder-specific communication. This paper also provides a strategic roadmap for moving the field beyond the generation of isolated data points toward the delivery of integrated, actionable, and trustworthy AI in clinical practice.},
 author = {Ratna Kandala and Akshata Kishore Moharir and Divya Arvinda Nayak},
 comment = {},
 doi = {},
 eprint = {2510.13828v1},
 journal = {arXiv preprint},
 title = {From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening},
 url = {http://arxiv.org/abs/2510.13828v1},
 year = {2025}
}

@article{2510.14936v1,
 abstract = {The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.},
 author = {Elena Golimblevskaia and Aakriti Jain and Bruno Puri and Ammar Ibrahim and Wojciech Samek and Sebastian Lapuschkin},
 comment = {},
 doi = {},
 eprint = {2510.14936v1},
 journal = {arXiv preprint},
 title = {Circuit Insights: Towards Interpretability Beyond Activations},
 url = {http://arxiv.org/abs/2510.14936v1},
 year = {2025}
}

@article{2510.15986v1,
 abstract = {Sleep disorders have a major impact on patients' health and quality of life, but their diagnosis remains complex due to the diversity of symptoms. Today, technological advances, combined with medical data analysis, are opening new perspectives for a better understanding of these disorders. In particular, explainable artificial intelligence (XAI) aims to make AI model decisions understandable and interpretable for users. In this study, we propose a clustering-based method to group patients according to different sleep disorder profiles. By integrating an explainable approach, we identify the key factors influencing these pathologies. An experiment on anonymized real data illustrates the effectiveness and relevance of our approach.},
 author = {Sifeddine Sellami and Juba Agoun and Lamia Yessad and Louenas Bounia},
 comment = {in French language, Plate-Forme Intelligence Artificielle, Jun 2025, Dijon (FRANCE), France},
 doi = {},
 eprint = {2510.15986v1},
 journal = {arXiv preprint},
 title = {User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis},
 url = {http://arxiv.org/abs/2510.15986v1},
 year = {2025}
}

@article{2510.16233v1,
 abstract = {Climate change demands effective legislative action to mitigate its impacts. This study explores the application of machine learning (ML) to understand the progression of climate policy from announcement to adoption, focusing on policies within the European Green Deal. We present a dataset of 165 policies, incorporating text and metadata. We aim to predict a policy's progression status, and compare text representation methods, including TF-IDF, BERT, and ClimateBERT. Metadata features are included to evaluate the impact on predictive performance. On text features alone, ClimateBERT outperforms other approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods from explainable AI highlights the influence of factors such as policy wording and metadata including political party and country representation. These findings underscore the potential of ML tools in supporting climate policy analysis and decision-making.},
 author = {Patricia West and Michelle WL Wan and Alexander Hepburn and Edwin Simpson and Raul Santos-Rodriguez and Jeffrey N Clark},
 comment = {},
 doi = {},
 eprint = {2510.16233v1},
 journal = {arXiv preprint},
 title = {Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal},
 url = {http://arxiv.org/abs/2510.16233v1},
 year = {2025}
}

@article{2510.16547v1,
 abstract = {Life satisfaction is a crucial facet of human well-being. Hence, research on life satisfaction is incumbent for understanding how individuals experience their lives and influencing interventions targeted at enhancing mental health and well-being. Life satisfaction has traditionally been measured using analog, complicated, and frequently error-prone methods. These methods raise questions concerning validation and propagation. However, this study demonstrates the potential for machine learning algorithms to predict life satisfaction with a high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a government survey of 19000 people aged 16-64 years in Denmark. Using feature learning techniques, 27 significant questions for assessing contentment were extracted, making the study highly reproducible, simple, and easily interpretable. Furthermore, clinical and biomedical large language models (LLMs) were explored for predicting life satisfaction by converting tabular data into natural language sentences through mapping and adding meaningful counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It was found that life satisfaction prediction is more closely related to the biomedical domain than the clinical domain. Ablation studies were also conducted to understand the impact of data resampling and feature selection techniques on model performance. Moreover, the correlation between primary determinants with different age brackets was analyzed, and it was found that health condition is the most important determinant across all ages. This study demonstrates how machine learning, large language models and XAI can jointly contribute to building trust and understanding in using AI to investigate human behavior, with significant ramifications for academics and professionals working to quantify and comprehend subjective well-being.},
 author = {Alif Elham Khan and Mohammad Junayed Hasan and Humayra Anjum and Nabeel Mohammed and Sifat Momen},
 comment = {},
 doi = {10.1016/j.heliyon.2024.e31158},
 eprint = {2510.16547v1},
 journal = {arXiv preprint},
 title = {Predicting life satisfaction using machine learning and explainable AI},
 url = {http://arxiv.org/abs/2510.16547v1},
 year = {2025}
}

@article{2510.17088v1,
 abstract = {Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity freezes, contagion cascades, regime shifts), but existing detectors treat all anomalies uniformly, producing scalar scores without revealing which mechanism is failing, where risks concentrate, or how to intervene. This opacity prevents targeted regulatory responses. Three unsolved challenges persist: (1) static graph structures cannot adapt when market correlations shift during regime changes; (2) uniform detection mechanisms miss type-specific signatures across multiple temporal scales while failing to integrate individual behaviors with network contagion; (3) black-box outputs provide no actionable guidance on anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks that provide built-in interpretability. Our framework captures multi-scale temporal dependencies through BiLSTM with self-attention, fuses temporal and spatial information via cross-modal attention, learns dynamic graphs through neural multi-source interpolation, adaptively balances learned dynamics with structural priors via stress-modulated fusion, routes anomalies to four mechanism-specific experts, and produces dual-level interpretable attributions. Critically, interpretability is embedded architecturally rather than applied post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley Bank case study demonstrates anomaly evolution tracking: Price-Shock expert weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48 (66% above baseline) one week later, revealing automatic temporal mechanism identification without labeled supervision.},
 author = {Zan Li and Rui Fan},
 comment = {},
 doi = {},
 eprint = {2510.17088v1},
 journal = {arXiv preprint},
 title = {Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing},
 url = {http://arxiv.org/abs/2510.17088v1},
 year = {2025}
}

@article{2510.17458v1,
 abstract = {Deep neural networks like PhaseNet show high accuracy in detecting microseismic events, but their black-box nature is a concern in critical applications. We apply explainable AI (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive Explanations (SHAP), to interpret the PhaseNet model's decisions and improve its reliability. Grad-CAM highlights that the network's attention aligns with P- and S-wave arrivals. SHAP values quantify feature contributions, confirming that vertical-component amplitudes drive P-phase picks while horizontal components dominate S-phase picks, consistent with geophysical principles. Leveraging these insights, we introduce a SHAP-gated inference scheme that combines the model's output with an explanation-based metric to reduce errors. On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of 0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet (F1-score 0.97) and demonstrating enhanced robustness to noise. These results show that XAI can not only interpret deep learning models but also directly enhance their performance, providing a template for building trust in automated seismic detectors.},
 author = {Ayrat Abdullin and Denis Anikiev and Umair bin Waheed},
 comment = {Submitted to Artificial Intelligence in Geosciences},
 doi = {},
 eprint = {2510.17458v1},
 journal = {arXiv preprint},
 title = {Explainable AI for microseismic event detection},
 url = {http://arxiv.org/abs/2510.17458v1},
 year = {2025}
}

@article{2510.18038v1,
 abstract = {The red palm mite infestation has become a serious concern, particularly in regions with extensive palm cultivation, leading to reduced productivity and economic losses. Accurate and early identification of mite-infested plants is critical for effective management. The current study focuses on evaluating and comparing the ML model for classifying the affected plants and detecting the infestation. TriggerNet is a novel interpretable AI framework that integrates Grad-CAM, RISE, FullGrad, and TCAV to generate novel visual explanations for deep learning models in plant classification and disease detection. This study applies TriggerNet to address red palm mite (Raoiella indica) infestation, a major threat to palm cultivation and agricultural productivity. A diverse set of RGB images across 11 plant species, Arecanut, Date Palm, Bird of Paradise, Coconut Palm, Ginger, Citrus Tree, Palm Oil, Orchid, Banana Palm, Avocado Tree, and Cast Iron Plant was utilized for training and evaluation. Advanced deep learning models like CNN, EfficientNet, MobileNet, ViT, ResNet50, and InceptionV3, alongside machine learning classifiers such as Random Forest, SVM, and KNN, were employed for plant classification. For disease classification, all plants were categorized into four classes: Healthy, Yellow Spots, Reddish Bronzing, and Silk Webbing. Snorkel was used to efficiently label these disease classes by leveraging heuristic rules and patterns, reducing manual annotation time and improving dataset reliability.},
 author = {Harshini Suresha and Kavitha SH},
 comment = {17 pages, 9 figures},
 doi = {},
 eprint = {2510.18038v1},
 journal = {arXiv preprint},
 title = {TriggerNet: A Novel Explainable AI Framework for Red Palm Mite Detection and Multi-Model Comparison and Heuristic-Guided Annotation},
 url = {http://arxiv.org/abs/2510.18038v1},
 year = {2025}
}

@article{2510.18193v2,
 abstract = {Fair, transparent, and explainable decision-making remains a critical challenge in Olympic and Paralympic combat sports. This paper presents \emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees, coaches, and athletes in real time during Taekwondo competitions and training. The system integrates {pose-based action recognition} using graph convolutional networks (GCNs), {epistemic uncertainty modeling} through credal sets, and {explainability overlays} for visual decision support. A set of {interactive dashboards} enables human--AI collaboration in referee evaluation, athlete performance analysis, and Para-Taekwondo classification. Beyond automated scoring, FST.ai~2.0 incorporates modules for referee training, fairness monitoring, and policy-level analytics within the World Taekwondo ecosystem. Experimental validation on competition data demonstrates an {85\% reduction in decision review time} and {93\% referee trust} in AI-assisted decisions. The framework thus establishes a transparent and extensible pipeline for trustworthy, data-driven officiating and athlete assessment. By bridging real-time perception, explainable inference, and governance-aware design, FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned AI in sports.},
 author = {Keivan Shariatmadar and Ahmad Osman and Ramin Ray and Kisam Kim},
 comment = {23 pages, 12 figures},
 doi = {},
 eprint = {2510.18193v2},
 journal = {arXiv preprint},
 title = {FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo},
 url = {http://arxiv.org/abs/2510.18193v2},
 year = {2025}
}

@article{2510.18417v1,
 abstract = {Open RAN introduces a flexible, cloud-based architecture for the Radio Access Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning (ML)-driven automation across heterogeneous, multi-vendor deployments. While EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI models, explainability alone does not guarantee reliable network operations. In this article, we propose a lightweight verification approach based on interpretable models to validate the behavior of Deep Reinforcement Learning (DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use Decision Tree (DT)-based verifiers to perform near-real-time consistency checks at runtime, which would be otherwise unfeasible with computationally expensive state-of-the-art verifiers. We analyze the landscape of XAI and AI verification, propose a scalable architectural integration, and demonstrate feasibility with a DT-based slice-verifier. We also outline future challenges to ensure trustworthy AI adoption in Open RAN.},
 author = {Rahul Soundrarajan and Claudio Fiandrino and Michele Polese and Salvatore D'Oro and Leonardo Bonati and Tommaso Melodia},
 comment = {},
 doi = {},
 eprint = {2510.18417v1},
 journal = {arXiv preprint},
 title = {On AI Verification in Open RAN},
 url = {http://arxiv.org/abs/2510.18417v1},
 year = {2025}
}

@article{2510.19514v1,
 abstract = {In eXplainable Artificial Intelligence (XAI), instance-based explanations for time series have gained increasing attention due to their potential for actionable and interpretable insights in domains such as healthcare. Addressing the challenges of explainability of state-of-the-art models, we propose a prototype-driven framework for generating sparse counterfactual explanations tailored to 12-lead ECG classification models. Our method employs SHAP-based thresholds to identify critical signal segments and convert them into interval rules, uses Dynamic Time Warping (DTW) and medoid clustering to extract representative prototypes, and aligns these prototypes to query R-peaks for coherence with the sample being explained. The framework generates counterfactuals that modify only 78% of the original signal while maintaining 81.3% validity across all classes and achieving 43% improvement in temporal stability. We evaluate three variants of our approach, Original, Sparse, and Aligned Sparse, with class-specific performance ranging from 98.9% validity for myocardial infarction (MI) to challenges with hypertrophy (HYP) detection (13.2%). This approach supports near realtime generation (< 1 second) of clinically valid counterfactuals and provides a foundation for interactive explanation platforms. Our findings establish design principles for physiologically-aware counterfactual explanations in AI-based diagnosis systems and outline pathways toward user-controlled explanation interfaces for clinical deployment.},
 author = {Maciej Mozolewski and Betül Bayrak and Kerstin Bach and Grzegorz J. Nalepa},
 comment = {},
 doi = {},
 eprint = {2510.19514v1},
 journal = {arXiv preprint},
 title = {From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification},
 url = {http://arxiv.org/abs/2510.19514v1},
 year = {2025}
}

@article{2510.20611v1,
 abstract = {Breast cancer is considered the most critical and frequently diagnosed cancer in women worldwide, leading to an increase in cancer-related mortality. Early and accurate detection is crucial as it can help mitigate possible threats while improving survival rates. In terms of prediction, conventional diagnostic methods are often limited by variability, cost, and, most importantly, risk of misdiagnosis. To address these challenges, machine learning (ML) has emerged as a powerful tool for computer-aided diagnosis, with feature selection playing a vital role in improving model performance and interpretability. This research study proposes an integrated framework that incorporates customized Particle Swarm Optimization (PSO) for feature selection. This framework has been evaluated on a comprehensive set of 29 different models, spanning classical classifiers, ensemble techniques, neural networks, probabilistic algorithms, and instance-based algorithms. To ensure interpretability and clinical relevance, the study uses cross-validation in conjunction with explainable AI methods. Experimental evaluation showed that the proposed approach achieved a superior score of 99.1\% across all performance metrics, including accuracy and precision, while effectively reducing dimensionality and providing transparent, model-agnostic explanations. The results highlight the potential of combining swarm intelligence with explainable ML for robust, trustworthy, and clinically meaningful breast cancer diagnosis.},
 author = {Mirza Raquib and Niloy Das and Farida Siddiqi Prity and Arafath Al Fahim and Saydul Akbar Murad and Mohammad Amzad Hossain and MD Jiabul Hoque and Mohammad Ali Moni},
 comment = {},
 doi = {},
 eprint = {2510.20611v1},
 journal = {arXiv preprint},
 title = {PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection},
 url = {http://arxiv.org/abs/2510.20611v1},
 year = {2025}
}

@article{2510.21389v1,
 abstract = {Artificial intelligence (AI) systems increasingly match or surpass human experts in biomedical signal interpretation. However, their effective integration into clinical practice requires more than high predictive accuracy. Clinicians must discern \textit{when} and \textit{why} to trust algorithmic recommendations. This work presents an application-grounded user study with eight professional sleep medicine practitioners, who score nocturnal arousal events in polysomnographic data under three conditions: (i) manual scoring, (ii) black-box (BB) AI assistance, and (iii) transparent white-box (WB) AI assistance. Assistance is provided either from the \textit{start} of scoring or as a post-hoc quality-control (\textit{QC}) review. We systematically evaluate how the type and timing of assistance influence event-level and clinically most relevant count-based performance, time requirements, and user experience. When evaluated against the clinical standard used to train the AI, both AI and human-AI teams significantly outperform unaided experts, with collaboration also reducing inter-rater variability. Notably, transparent AI assistance applied as a targeted QC step yields median event-level performance improvements of approximately 30\% over black-box assistance, and QC timing further enhances count-based outcomes. While WB and QC approaches increase the time required for scoring, start-time assistance is faster and preferred by most participants. Participants overwhelmingly favor transparency, with seven out of eight expressing willingness to adopt the system with minor or no modifications. In summary, strategically timed transparent AI assistance effectively balances accuracy and clinical efficiency, providing a promising pathway toward trustworthy AI integration and user acceptance in clinical workflows.},
 author = {Stefan Kraft and Andreas Theissler and Vera Wienhausen-Wilke and Gjergji Kasneci and Hendrik Lensch},
 comment = {},
 doi = {},
 eprint = {2510.21389v1},
 journal = {arXiv preprint},
 title = {Assessing the Real-World Utility of Explainable AI for Arousal Diagnostics: An Application-Grounded User Study},
 url = {http://arxiv.org/abs/2510.21389v1},
 year = {2025}
}

@article{2510.21796v1,
 abstract = {The Madden-Julian Oscillation (MJO) is an important driver of global weather and climate extremes, but its prediction in operational dynamical models remains challenging, with skillful forecasts typically limited to 3-4 weeks. Here, we introduce a novel deep learning framework, the Physics-guided Cascaded Corrector for MJO (PCC-MJO), which acts as a universal post-processor to correct MJO forecasts from dynamical models. This two-stage model first employs a physics-informed 3D U-Net to correct spatial-temporal field errors, then refines the MJO's RMM index using an LSTM optimized for forecast skill. When applied to three different operational forecasts from CMA, ECMWF and NCEP, our unified framework consistently extends the skillful forecast range (bivariate correlation > 0.5) by 2-8 days. Crucially, the model effectively mitigates the "Maritime Continent barrier", enabling more realistic eastward propagation and amplitude. Explainable AI analysis quantitatively confirms that the model's decision-making is spatially congruent with observed MJO dynamics (correlation > 0.93), demonstrating that it learns physically meaningful features rather than statistical fittings. Our work provides a promising physically consistent, computationally efficient, and highly generalizable pathway to break through longstanding barriers in subseasonal forecasting.},
 author = {Xiao Zhou and Yuze Sun and Jie Wu and Xiaomeng Huang},
 comment = {},
 doi = {},
 eprint = {2510.21796v1},
 journal = {arXiv preprint},
 title = {A Physics-Guided AI Cascaded Corrector Model Significantly Extends Madden-Julian Oscillation Prediction Skill},
 url = {http://arxiv.org/abs/2510.21796v1},
 year = {2025}
}

@article{2510.22035v4,
 abstract = {Robustness has become one of the most critical problems in machine learning (ML). The science of interpreting ML models to understand their behavior and improve their robustness is referred to as explainable artificial intelligence (XAI). One of the state-of-the-art XAI methods for computer vision problems is to generate saliency maps. A saliency map highlights the pixel space of an image that excites the ML model the most. However, this property could be misleading if spurious and salient features are present in overlapping pixel spaces. In this paper, we propose a caption-based XAI method, which integrates a standalone model to be explained into the contrastive language-image pre-training (CLIP) model using a novel network surgery approach. The resulting caption-based XAI model identifies the dominant concept that contributes the most to the models prediction. This explanation minimizes the risk of the standalone model falling for a covariate shift and contributes significantly towards developing robust ML models. Our code is available at https://github.com/patch0816/caption-driven-xai},
 author = {Patrick Koller and Amil V. Dravid and Guido M. Schuster and Aggelos K. Katsaggelos},
 comment = {Accepted and presented at the IEEE ICIP 2025 Satellite Workshop "Generative AI for World Simulations and Communications & Celebrating 40 Years of Excellence in Education: Honoring Prof. Aggelos Katsaggelos", Anchorage, USA, Sept 14, 2025. Camera-ready preprint; IEEE Xplore version to follow. Author variant: Amil Dravid. Code: https://github.com/patch0816/caption-driven-xai},
 doi = {},
 eprint = {2510.22035v4},
 journal = {arXiv preprint},
 title = {Caption-Driven Explainability: Probing CNNs for Bias via CLIP},
 url = {http://arxiv.org/abs/2510.22035v4},
 year = {2025}
}

@article{2510.22160v1,
 abstract = {Developing benchmark datasets for low-resource languages poses significant challenges, primarily due to the limited availability of native linguistic experts and the substantial time and cost involved in annotation. Given these challenges, Maithili is still underrepresented in natural language processing research. It is an Indo-Aryan language spoken by more than 13 million people in the Purvanchal region of India, valued for its rich linguistic structure and cultural significance. While sentiment analysis has achieved remarkable progress in high-resource languages, resources for low-resource languages, such as Maithili, remain scarce, often restricted to coarse-grained annotations and lacking interpretability mechanisms. To address this limitation, we introduce a novel dataset comprising 3,221 Maithili sentences annotated for sentiment polarity and accompanied by natural language justifications. Moreover, the dataset is carefully curated and validated by linguistic experts to ensure both label reliability and contextual fidelity. Notably, the justifications are written in Maithili, thereby promoting culturally grounded interpretation and enhancing the explainability of sentiment models. Furthermore, extensive experiments using both classical machine learning and state-of-the-art transformer architectures demonstrate the dataset's effectiveness for interpretable sentiment analysis. Ultimately, this work establishes the first benchmark for explainable affective computing in Maithili, thus contributing a valuable resource to the broader advancement of multilingual NLP and explainable AI.},
 author = {Rahul Ranjan and Mahendra Kumar Gurve and Anuj and Nitin and Yamuna Prasad},
 comment = {},
 doi = {},
 eprint = {2510.22160v1},
 journal = {arXiv preprint},
 title = {SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language},
 url = {http://arxiv.org/abs/2510.22160v1},
 year = {2025}
}

@article{2510.22266v2,
 abstract = {Identifying the factors that influence student performance in basic education is a central challenge for formulating effective public policies in Brazil. This study introduces a multi-level machine learning approach to classify the proficiency of 9th-grade and high school students using microdata from the System of Assessment of Basic Education (SAEB). Our model uniquely integrates four data sources: student socioeconomic characteristics, teacher professional profiles, school indicators, and principal management profiles. A comparative analysis of four ensemble algorithms confirmed the superiority of a Random Forest model, which achieved 90.2% accuracy and an Area Under the Curve (AUC) of 96.7%. To move beyond prediction, we applied Explainable AI (XAI) using SHAP, which revealed that the school's average socioeconomic level is the most dominant predictor, demonstrating that systemic factors have a greater impact than individual characteristics in isolation. The primary conclusion is that academic performance is a systemic phenomenon deeply tied to the school's ecosystem. This study provides a data-driven, interpretable tool to inform policies aimed at promoting educational equity by addressing disparities between schools.},
 author = {Rodrigo Tertulino and Ricardo Almeida},
 comment = {},
 doi = {},
 eprint = {2510.22266v2},
 journal = {arXiv preprint},
 title = {A Multi-level Analysis of Factors Associated with Student Performance: A Machine Learning Approach to the SAEB Microdata},
 url = {http://arxiv.org/abs/2510.22266v2},
 year = {2025}
}

@article{2510.22572v1,
 abstract = {The task here is to predict the toxicological activity of chemical compounds based on the Tox21 dataset, a benchmark in computational toxicology.
  After a domain-specific overview of chemical toxicity, we discuss current computational strategies, focusing on machine learning and deep learning. Several architectures are compared in terms of performance, robustness, and interpretability.
  This research introduces a novel image-based pipeline based on DenseNet121, which processes 2D graphical representations of chemical structures. Additionally, we employ Grad-CAM visualizations, an explainable AI technique, to interpret the model's predictions and highlight molecular regions contributing to toxicity classification. The proposed architecture achieves competitive results compared to traditional models, demonstrating the potential of deep convolutional networks in cheminformatics. Our findings emphasize the value of combining image-based representations with explainable AI methods to improve both predictive accuracy and model transparency in toxicology.},
 author = {Eduard Popescu and Adrian Groza and Andreea Cernat},
 comment = {},
 doi = {},
 eprint = {2510.22572v1},
 journal = {arXiv preprint},
 title = {Combining Deep Learning and Explainable AI for Toxicity Prediction of Chemical Compounds},
 url = {http://arxiv.org/abs/2510.22572v1},
 year = {2025}
}

@article{2510.24115v1,
 abstract = {For doctors to truly trust artificial intelligence, it can't be a black box. They need to understand its reasoning, almost as if they were consulting a colleague. We created HistoLens1 to be that transparent, collaborative partner. It allows a pathologist to simply ask a question in plain English about a tissue slide--just as they would ask a trainee. Our system intelligently translates this question into a precise query for its AI engine, which then provides a clear, structured report. But it doesn't stop there. If a doctor ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any finding--a heatmap that points to the exact cells and regions the AI used for its analysis. We've also ensured the AI focuses only on the patient's tissue, just like a trained pathologist would, by teaching it to ignore distracting background noise. The result is a workflow where the pathologist remains the expert in charge, using a trustworthy AI assistant to verify their insights and make faster, more confident diagnoses.},
 author = {Sandeep Vissapragada and Vikrant Sahu and Gagan Raj Gupta and Vandita Singh},
 comment = {},
 doi = {},
 eprint = {2510.24115v1},
 journal = {arXiv preprint},
 title = {HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology},
 url = {http://arxiv.org/abs/2510.24115v1},
 year = {2025}
}

@article{2510.24598v1,
 abstract = {Current quantum machine learning approaches often face challenges balancing predictive accuracy, robustness, and interpretability. To address this, we propose a novel quantum adversarial framework that integrates a hybrid quantum neural network (QNN) with classical deep learning layers, guided by an evaluator model with LIME-based interpretability, and extended through quantum GAN and self-supervised variants. In the proposed model, an adversarial evaluator concurrently guides the QNN by computing feedback loss, thereby optimizing both prediction accuracy and model explainability. Empirical evaluations show that the Vanilla model achieves RMSE = 0.27, MSE = 0.071, MAE = 0.21, and R^2 = 0.59, delivering the most consistent performance across regression metrics compared to adversarial counterparts. These results demonstrate the potential of combining quantum-inspired methods with classical architectures to develop lightweight, high-performance, and interpretable predictive models, advancing the applicability of QML beyond current limitations.},
 author = {Sathwik Narkedimilli and N V Saran Kumar and Aswath Babu H and Manjunath K Vanahalli and Manish M and Vinija Jain and Aman Chadha},
 comment = {},
 doi = {},
 eprint = {2510.24598v1},
 journal = {arXiv preprint},
 title = {A Novel XAI-Enhanced Quantum Adversarial Networks for Velocity Dispersion Modeling in MaNGA Galaxies},
 url = {http://arxiv.org/abs/2510.24598v1},
 year = {2025}
}

@article{2510.25633v1,
 abstract = {The midlatitude climate and weather are shaped by storms, yet the factors governing their predictability remain insufficiently understood. Here, we use a Convolutional Neural Network (CNN) to predict and quantify uncertainty in the intensity growth and trajectory of over 200,000 storms simulated in a 200-year aquaplanet GCM. This idealized framework provides a controlled climate background for isolating factors that govern predictability. Results show that storm intensity is less predictable than trajectory. Strong baroclinicity accelerates storm intensification and reduces its predictability, consistent with theory. Crucially, enhanced jet meanders further degrade forecast skill, revealing a synoptic source of uncertainty. Using sensitivity maps from explainable AI, we find that the error growth rate is nearly doubled by the more meandering structure. These findings highlight the potential of machine learning for advancing understanding of predictability and its governing mechanisms.},
 author = {Wuqiushi Yao and Or Hadas and Yohai Kaspi},
 comment = {Wuqiushi Yao and Or Hadas have contributed equally to this work},
 doi = {},
 eprint = {2510.25633v1},
 journal = {arXiv preprint},
 title = {Predictability of Storms in an Idealized Climate Revealed by Machine Learning},
 url = {http://arxiv.org/abs/2510.25633v1},
 year = {2025}
}

@article{2510.25775v1,
 abstract = {Contemporary chess engines offer precise yet opaque evaluations, typically expressed as centipawn scores. While effective for decision-making, these outputs obscure the underlying contributions of individual pieces or patterns. In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the domain of chess analysis, aiming to attribute a chess engines evaluation to specific pieces on the board. By treating pieces as features and systematically ablating them, we compute additive, per-piece contributions that explain the engines output in a locally faithful and human-interpretable manner. This method draws inspiration from classical chess pedagogy, where players assess positions by mentally removing pieces, and grounds it in modern explainable AI techniques. Our approach opens new possibilities for visualization, human training, and engine comparison. We release accompanying code and data to foster future research in interpretable chess AI.},
 author = {Francesco Spinnato},
 comment = {},
 doi = {},
 eprint = {2510.25775v1},
 journal = {arXiv preprint},
 title = {Towards Piece-by-Piece Explanations for Chess Positions with SHAP},
 url = {http://arxiv.org/abs/2510.25775v1},
 year = {2025}
}

@article{2510.26941v1,
 abstract = {The Internet of Things has expanded rapidly, transforming communication and operations across industries but also increasing the attack surface and security breaches. Artificial Intelligence plays a key role in securing IoT, enabling attack detection, attack behavior analysis, and mitigation suggestion. Despite advancements, evaluations remain purely qualitative, and the lack of a standardized, objective benchmark for quantitatively measuring AI-based attack analysis and mitigation hinders consistent assessment of model effectiveness. In this work, we propose a hybrid framework combining Machine Learning (ML) for multi-class attack detection with Large Language Models (LLMs) for attack behavior analysis and mitigation suggestion. After benchmarking several ML and Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we applied structured role-play prompt engineering with Retrieval-Augmented Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed, context-aware responses. We introduce novel evaluation metrics for quantitative assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o, DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the responses. Results show that Random Forest has the best detection model, and ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.},
 author = {Seif Ikbarieh and Maanak Gupta and Elmahedi Mahalal},
 comment = {},
 doi = {},
 eprint = {2510.26941v1},
 journal = {arXiv preprint},
 title = {LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks},
 url = {http://arxiv.org/abs/2510.26941v1},
 year = {2025}
}

@article{2510.27207v1,
 abstract = {Explainable AI (XAI) is critical for building trust in complex machine learning models, yet mainstream attribution methods often provide an incomplete, static picture of a model's final state. By collapsing a feature's role into a single score, they are confounded by non-linearity and interactions. To address this, we introduce Feature-Function Curvature Analysis (FFCA), a novel framework that analyzes the geometry of a model's learned function. FFCA produces a 4-dimensional signature for each feature, quantifying its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction. Crucially, we extend this framework into Dynamic Archetype Analysis, which tracks the evolution of these signatures throughout the training process. This temporal view moves beyond explaining what a model learned to revealing how it learns. We provide the first direct, empirical evidence of hierarchical learning, showing that models consistently learn simple linear effects before complex interactions. Furthermore, this dynamic analysis provides novel, practical diagnostics for identifying insufficient model capacity and predicting the onset of overfitting. Our comprehensive experiments demonstrate that FFCA, through its static and dynamic components, provides the essential geometric context that transforms model explanation from simple quantification to a nuanced, trustworthy analysis of the entire learning process.},
 author = {Hamed Najafi and Dongsheng Luo and Jason Liu},
 comment = {},
 doi = {},
 eprint = {2510.27207v1},
 journal = {arXiv preprint},
 title = {Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models},
 url = {http://arxiv.org/abs/2510.27207v1},
 year = {2025}
}

@article{2510.27397v1,
 abstract = {Despite their enormous predictive power, machine learning models are often unsuitable for applications in regulated industries such as finance, due to their limited capacity to provide explanations. While model-agnostic frameworks such as Shapley values have proved to be convenient and popular, they rarely align with the kinds of causal explanations that are typically sought after. Counterfactual case-based explanations, where an individual is informed of which circumstances would need to be different to cause a change in outcome, may be more intuitive and actionable. However, finding appropriate counterfactual cases is an open challenge, as is interpreting which features are most critical for the change in outcome. Here, we pose the question of counterfactual search and interpretation in terms of similarity learning, exploiting the representation learned by the random forest predictive model itself. Once a counterfactual is found, the feature importance of the explanation is computed as a function of which random forest partitions are crossed in order to reach it from the original instance. We demonstrate this method on both the MNIST hand-drawn digit dataset and the German credit dataset, finding that it generates explanations that are sparser and more useful than Shapley values.},
 author = {Joshua S. Harvey and Guanchao Feng and Sai Anusha Meesala and Tina Zhao and Dhagash Mehta},
 comment = {Presented at XAI-FIN-2025: International Joint Workshop on Explainable AI in Finance: Achieving Trustworthy Financial Decision-Making; November 15, 2025; Singapore},
 doi = {},
 eprint = {2510.27397v1},
 journal = {arXiv preprint},
 title = {Interpretable Model-Aware Counterfactual Explanations for Random Forest},
 url = {http://arxiv.org/abs/2510.27397v1},
 year = {2025}
}

@article{2510.27413v1,
 abstract = {Interpretability is crucial for building safe, reliable, and controllable language models, yet existing interpretability pipelines remain costly and difficult to scale. Interpreting a new model typically requires costly training of model-specific sparse autoencoders, manual or semi-automated labeling of SAE components, and their subsequent validation. We introduce Atlas-Alignment, a framework for transferring interpretability across language models by aligning unknown latent spaces to a Concept Atlas - a labeled, human-interpretable latent space - using only shared inputs and lightweight representational alignment techniques. Once aligned, this enables two key capabilities in previously opaque models: (1) semantic feature search and retrieval, and (2) steering generation along human-interpretable atlas concepts. Through quantitative and qualitative evaluations, we show that simple representational alignment methods enable robust semantic retrieval and steerable generation without the need for labeled concept data. Atlas-Alignment thus amortizes the cost of explainable AI and mechanistic interpretability: by investing in one high-quality Concept Atlas, we can make many new models transparent and controllable at minimal marginal cost.},
 author = {Bruno Puri and Jim Berend and Sebastian Lapuschkin and Wojciech Samek},
 comment = {},
 doi = {},
 eprint = {2510.27413v1},
 journal = {arXiv preprint},
 title = {Atlas-Alignment: Making Interpretability Transferable Across Language Models},
 url = {http://arxiv.org/abs/2510.27413v1},
 year = {2025}
}

@article{2510.27650v1,
 abstract = {Class imbalance poses a fundamental challenge in machine learning, frequently leading to unreliable classification performance. While prior methods focus on data- or loss-reweighting schemes, we view imbalance as a data condition that amplifies Clever Hans (CH) effects by underspecification of minority classes. In a counterfactual explanations-based approach, we propose to leverage Explainable AI to jointly identify and eliminate CH effects emerging under imbalance. Our method achieves competitive classification performance on three datasets and demonstrates how CH effects emerge under imbalance, a perspective largely overlooked by existing approaches.},
 author = {Jakob Hackstein and Sidney Bender},
 comment = {},
 doi = {},
 eprint = {2510.27650v1},
 journal = {arXiv preprint},
 title = {Imbalanced Classification through the Lens of Spurious Correlations},
 url = {http://arxiv.org/abs/2510.27650v1},
 year = {2025}
}

@article{2511.00246v1,
 abstract = {Melanoma is one of the most aggressive and deadliest skin cancers, leading to mortality if not detected and treated in the early stages. Artificial intelligence techniques have recently been developed to help dermatologists in the early detection of melanoma, and systems based on deep learning (DL) have been able to detect these lesions with high accuracy. However, the entire community must overcome the explainability limit to get the maximum benefit from DL for diagnostics in the healthcare domain. Because of the black box operation's shortcomings in DL models' decisions, there is a lack of reliability and trust in the outcomes. However, Explainable Artificial Intelligence (XAI) can solve this problem by interpreting the predictions of AI systems. This paper proposes a machine learning model using ensemble learning of three state-of-the-art deep transfer Learning networks, along with an approach to ensure the reliability of the predictions by utilizing XAI techniques to explain the basis of the predictions.},
 author = {Wadduwage Shanika Perera and ABM Islam and Van Vung Pham and Min Kyung An},
 comment = {Publisher-formatted version provided under CC BY-NC-ND 4.0 license. Original source produced by SciTePress},
 doi = {10.5220/0012575400003657},
 eprint = {2511.00246v1},
 journal = {arXiv preprint},
 title = {Melanoma Classification Through Deep Ensemble Learning and Explainable AI},
 url = {http://arxiv.org/abs/2511.00246v1},
 year = {2025}
}

@article{2511.03545v1,
 abstract = {This paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning (ML) models. Contrary to the prevalent black-box perception, our study focuses on models with transparent internal mechanisms. We address two principal types of explanation problems: abductive and contrastive, both in their local and global variants. Our analysis encompasses diverse ML models, including Decision Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof, each offering unique explanatory challenges. This research fills a significant gap in explainable AI (XAI) by providing a foundational understanding of the complexities of generating explanations for these models. This work provides insights vital for further research in the domain of XAI, contributing to the broader discourse on the necessity of transparency and accountability in AI systems.},
 author = {Sebastian Ordyniak and Giacomo Paesani and Mateusz Rychlicki and Stefan Szeider},
 comment = {Part I of a greatly enhanced version of https://doi.org/10.24963/kr.2024/53, whose full version is available on arXiv under https://doi.org/10.48550/arXiv.2407.15780},
 doi = {},
 eprint = {2511.03545v1},
 journal = {arXiv preprint},
 title = {Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)},
 url = {http://arxiv.org/abs/2511.03545v1},
 year = {2025}
}

@article{2511.04094v1,
 abstract = {This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011 and 2024. After excluding financial firms, firms with non-December fiscal year ends, capital impairment, and negative pre-tax income, the final dataset consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed to treat corporate tax avoidance as a predictor variable and link it to multiple domains, including earnings management (accrual- and activity-based), profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE, INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance itself is measured using complementary indicators cash effective tax rate (CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA, TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is its balanced panel structure with standardized variables and its consistency with international literature on the distribution and correlation of core indicators. At the same time, it reflects distinctive institutional features of Korean firms, such as concentrated ownership, high foreign shareholding, and elevated liquidity ratios, providing both international comparability and contextual uniqueness. KoTaP enables applications in benchmarking econometric and deep learning models, external validity checks, and explainable AI analyses. It further supports policy evaluation, audit planning, and investment analysis, making it a critical open resource for accounting, finance, and interdisciplinary research.},
 author = {Hyungjong Na and Wonho Song and Seungyong Han and Donghyeon Jo and Sejin Myung and Hyungjoon Kim},
 comment = {18 pages, 3 figures, 8 tables. Submitted to Scientific Data; currently under review. Data and codebook available at Zenodo (DOI: 10.5281/zenodo.17149808)},
 doi = {},
 eprint = {2511.04094v1},
 journal = {arXiv preprint},
 title = {KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea},
 url = {http://arxiv.org/abs/2511.04094v1},
 year = {2025}
}

@article{2511.04980v1,
 abstract = {The financial industry faces a significant challenge modeling and risk portfolios: balancing the predictability of advanced machine learning models, neural network models, and explainability required by regulatory entities (such as Office of the Comptroller of the Currency, Consumer Financial Protection Bureau). This paper intends to fill the gap in the application between these "black box" models and explainability frameworks, such as LIME and SHAP. Authors elaborate on the application of these frameworks on different models and demonstrates the more complex models with better prediction powers could be applied and reach the same level of the explainability, using SHAP and LIME. Beyond the comparison and discussion of performances, this paper proposes a novel five dimensional framework evaluating Inherent Interpretability, Global Explanations, Local Explanations, Consistency, and Complexity to offer a nuanced method for assessing and comparing model explainability beyond simple accuracy metrics. This research demonstrates the feasibility of employing sophisticated, high performing ML models in regulated financial environments by utilizing modern explainability techniques and provides a structured approach to evaluate the crucial trade offs between model performance and interpretability.},
 author = {Rongbin Ye and Jiaqi Chen},
 comment = {},
 doi = {},
 eprint = {2511.04980v1},
 journal = {arXiv preprint},
 title = {Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk},
 url = {http://arxiv.org/abs/2511.04980v1},
 year = {2025}
}

@article{2511.05614v1,
 abstract = {Scientific machine learning research spans diverse domains and data modalities, yet existing benchmark efforts remain siloed and lack standardization. This makes novel and transformative applications of machine learning to critical scientific use-cases more fragmented and less clear in pathways to impact. This paper introduces an ontology for scientific benchmarking developed through a unified, community-driven effort that extends the MLCommons ecosystem to cover physics, chemistry, materials science, biology, climate science, and more. Building on prior initiatives such as XAI-BENCH, FastML Science Benchmarks, PDEBench, and the SciMLBench framework, our effort consolidates a large set of disparate benchmarks and frameworks into a single taxonomy of scientific, application, and system-level benchmarks. New benchmarks can be added through an open submission workflow coordinated by the MLCommons Science Working Group and evaluated against a six-category rating rubric that promotes and identifies high-quality benchmarks, enabling stakeholders to select benchmarks that meet their specific needs. The architecture is extensible, supporting future scientific and AI/ML motifs, and we discuss methods for identifying emerging computing patterns for unique scientific workloads. The MLCommons Science Benchmarks Ontology provides a standardized, scalable foundation for reproducible, cross-domain benchmarking in scientific machine learning. A companion webpage for this work has also been developed as the effort evolves: https://mlcommons-science.github.io/benchmark/},
 author = {Ben Hawks and Gregor von Laszewski and Matthew D. Sinclair and Marco Colombo and Shivaram Venkataraman and Rutwik Jain and Yiwei Jiang and Nhan Tran and Geoffrey Fox},
 comment = {16 Pages, 3 Figures},
 doi = {},
 eprint = {2511.05614v1},
 journal = {arXiv preprint},
 title = {An MLCommons Scientific Benchmarks Ontology},
 url = {http://arxiv.org/abs/2511.05614v1},
 year = {2025}
}

@article{2511.05973v1,
 abstract = {Wolff-Parkinson-White (WPW) syndrome is a cardiac electrophysiology (EP) disorder caused by the presence of an accessory pathway (AP) that bypasses the atrioventricular node, faster ventricular activation rate, and provides a substrate for atrio-ventricular reentrant tachycardia (AVRT). Accurate localization of the AP is critical for planning and guiding catheter ablation procedures. While traditional diagnostic tree (DT) methods and more recent machine learning (ML) approaches have been proposed to predict AP location from surface electrocardiogram (ECG), they are often constrained by limited anatomical localization resolution, poor interpretability, and the use of small clinical datasets. In this study, we present a Deep Learning (DL) model for the localization of single manifest APs across 24 cardiac regions, trained on a large, physiologically realistic database of synthetic ECGs generated using a personalized virtual heart model. We also integrate eXplainable Artificial Intelligence (XAI) methods, Guided Backpropagation, Grad-CAM, and Guided Grad-CAM, into the pipeline. This enables interpretation of DL decision-making and addresses one of the main barriers to clinical adoption: lack of transparency in ML predictions. Our model achieves localization accuracy above 95%, with a sensitivity of 94.32% and specificity of 99.78%. XAI outputs are physiologically validated against known depolarization patterns, and a novel index is introduced to identify the most informative ECG leads for AP localization. Results highlight lead V2 as the most critical, followed by aVF, V1, and aVL. This work demonstrates the potential of combining cardiac digital twins with explainable DL to enable accurate, transparent, and non-invasive AP localization.},
 author = {Alice Ragonesi and Stefania Fresca and Karli Gillette and Stefan Kurath-Koller and Gernot Plank and Elena Zappon},
 comment = {27 pages, 9 figures, 4 tables},
 doi = {},
 eprint = {2511.05973v1},
 journal = {arXiv preprint},
 title = {Explainable Deep Learning-based Classification of Wolff-Parkinson-White Electrocardiographic Signals},
 url = {http://arxiv.org/abs/2511.05973v1},
 year = {2025}
}

@article{2511.06197v1,
 abstract = {The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.},
 author = {Dilli Prasad Sharma and Liang Xue and Xiaowei Sun and Xiaodong Lin and Pulei Xiong},
 comment = {},
 doi = {},
 eprint = {2511.06197v1},
 journal = {arXiv preprint},
 title = {Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting},
 url = {http://arxiv.org/abs/2511.06197v1},
 year = {2025}
}

@article{2511.06492v1,
 abstract = {Sepsis is a life-threatening condition that requires rapid detection and treatment to prevent progression to severe sepsis, septic shock, or multi-organ failure. Despite advances in medical technology, it remains a major challenge for clinicians. While recent machine learning models have shown promise in predicting sepsis onset, their black-box nature limits interpretability and clinical trust. In this study, we present an interpretable AI approach for sepsis analysis that integrates machine learning with clinical knowledge. Our method not only delivers accurate predictions of sepsis onset but also enables clinicians to understand, validate, and align model outputs with established medical expertise.},
 author = {Atharva Thakur and Shruti Dhumal},
 comment = {},
 doi = {},
 eprint = {2511.06492v1},
 journal = {arXiv preprint},
 title = {Explainable AI For Early Detection Of Sepsis},
 url = {http://arxiv.org/abs/2511.06492v1},
 year = {2025}
}

@article{2511.07475v1,
 abstract = {Cultural infrastructures, such as libraries, museums, theaters, and galleries, support learning, civic life, health, and local economies, yet access is uneven across cities. We present a novel, scalable, and open-data framework to measure spatial equity in cultural access. We map cultural infrastructures and compute a metric called Cultural Infrastructure Accessibility Score (CIAS) using exponential distance decay at fine spatial resolution, then aggregate the score per capita and integrate socio-demographic indicators. Interpretable tree-ensemble models with SHapley Additive exPlanation (SHAP) are used to explain associations between accessibility, income, density, and tract-level racial/ethnic composition. Results show a pronounced core-periphery gradient, where non-library cultural infrastructures cluster near urban cores, while libraries track density and provide broader coverage. Non-library accessibility is modestly higher in higher-income tracts, and library accessibility is slightly higher in denser, lower-income areas.},
 author = {Protik Bose Pranto and Minhazul Islam and Ripon Kumar Saha and Abimelec Mercado Rivera and Namig Abbasov},
 comment = {},
 doi = {},
 eprint = {2511.07475v1},
 journal = {arXiv preprint},
 title = {From Hubs to Deserts: Urban Cultural Accessibility Patterns with Explainable AI},
 url = {http://arxiv.org/abs/2511.07475v1},
 year = {2025}
}

@article{2511.07897v1,
 abstract = {Deep learning models have been successful in many areas but understanding their behaviors still remains a black-box. Most prior explainable AI (XAI) approaches have focused on interpreting and explaining how models make predictions. In contrast, we would like to understand how data can be explained with deep learning model training and propose a novel approach to understand the data via one of the most common media - language - so that humans can easily understand. Our approach proposes a pipeline to generate textual descriptions that can explain the data with large language models by incorporating external knowledge bases. However, generated data descriptions may still include irrelevant information, so we introduce to exploit influence estimation to choose the most informative textual descriptions, along with the CLIP score. Furthermore, based on the phenomenon of cross-modal transferability, we propose a novel benchmark task named cross-modal transfer classification to examine the effectiveness of our textual descriptions. In the experiment of zero-shot setting, we show that our textual descriptions are more effective than other baseline descriptions, and furthermore, we successfully boost the performance of the model trained only on images across all nine image classification datasets. These results are further supported by evaluation using GPT-4o. Through our approach, we may gain insights into the inherent interpretability of the decision-making process of the model.},
 author = {Chaeri Kim and Jaeyeon Bae and Taehwan Kim},
 comment = {},
 doi = {},
 eprint = {2511.07897v1},
 journal = {arXiv preprint},
 title = {Data Descriptions from Large Language Models with Influence Estimation},
 url = {http://arxiv.org/abs/2511.07897v1},
 year = {2025}
}

@article{2511.08108v1,
 abstract = {Machine learning is an essential tool for optimizing industrial quality control processes. However, the complexity of machine learning models often limits their practical applicability due to a lack of interpretability. Additionally, many industrial machines lack comprehensive sensor technology, making data acquisition incomplete and challenging. Explainable Artificial Intelligence offers a solution by providing insights into model decision-making and identifying the most relevant features for classification. In this paper, we investigate the impact of feature reduction using XAI techniques on the quality classification of injection-molded parts. We apply SHAP, Grad-CAM, and LIME to analyze feature importance in a Long Short-Term Memory model trained on real production data. By reducing the original 19 input features to 9 and 6, we evaluate the trade-off between model accuracy, inference speed, and interpretability. Our results show that reducing features can improve generalization while maintaining high classification performance, with an small increase in inference speed. This approach enhances the feasibility of AI-driven quality control, particularly for industrial settings with limited sensor capabilities, and paves the way for more efficient and interpretable machine learning applications in manufacturing.},
 author = {Georg Rottenwalter and Marcel Tilly and Victor Owolabi},
 comment = {Accepted and published at the 2025 IEEE 9th Forum on Research and Technologies for Society and Industry (RTSI). 10 pages, 6 figures. DOI: 10.1109/RTSI64020.2025.11212395. This is the author-accepted manuscript (AAM) version},
 doi = {10.1109/RTSI64020.2025.11212395},
 eprint = {2511.08108v1},
 journal = {arXiv preprint},
 title = {Improving Industrial Injection Molding Processes with Explainable AI for Quality Classification},
 url = {http://arxiv.org/abs/2511.08108v1},
 year = {2025}
}

@article{2511.08361v1,
 abstract = {The complexity and opacity of neural networks (NNs) pose significant challenges, particularly in high-stakes fields such as healthcare, finance, and law, where understanding decision-making processes is crucial. To address these issues, the field of explainable artificial intelligence (XAI) has developed various methods aimed at clarifying AI decision-making, thereby facilitating appropriate trust and validating the fairness of outcomes. Among these methods, prototype-based explanations offer a promising approach that uses representative examples to elucidate model behavior. However, a critical gap exists regarding standardized benchmarks to objectively compare prototype-based XAI methods, especially in the context of time series data. This lack of reliable benchmarks results in subjective evaluations, hindering progress in the field. We aim to establish a robust framework, ProtoScore, for assessing prototype-based XAI methods across different data types with a focus on time series data, facilitating fair and comprehensive evaluations. By integrating the Co-12 properties of Nauta et al., this framework allows for effectively comparing prototype methods against each other and against other XAI methods, ultimately assisting practitioners in selecting appropriate explanation methods while minimizing the costs associated with user studies. All code is publicly available at https://github.com/HelenaM23/ProtoScore .},
 author = {Helena Monke and Benjamin Sae-Chew and Benjamin Fresz and Marco F. Huber},
 comment = {},
 doi = {10.1145/3715275.3732151},
 eprint = {2511.08361v1},
 journal = {arXiv preprint},
 title = {From Confusion to Clarity: ProtoScore -- A Framework for Evaluating Prototype-Based XAI},
 url = {http://arxiv.org/abs/2511.08361v1},
 year = {2025}
}

@article{2511.08588v1,
 abstract = {We present the first application of federated learning (FL) to the U.S. National Financial Capability Study, introducing an interpretable framework for predicting consumer financial distress across all 50 states and the District of Columbia without centralizing sensitive data. Our cross-silo FL setup treats each state as a distinct data silo, simulating real-world governance in nationwide financial systems. Unlike prior work, our approach integrates two complementary explainable AI techniques to identify both global (nationwide) and local (state-specific) predictors of financial hardship, such as contact from debt collection agencies. We develop a machine learning model specifically suited for highly categorical, imbalanced survey data. This work delivers a scalable, regulation-compliant blueprint for early warning systems in finance, demonstrating how FL can power socially responsible AI applications in consumer credit risk and financial inclusion.},
 author = {Lorenzo Carta and Fernando Spadea and Oshani Seneviratne},
 comment = {},
 doi = {},
 eprint = {2511.08588v1},
 journal = {arXiv preprint},
 title = {Explainable Federated Learning for U.S. State-Level Financial Distress Modeling},
 url = {http://arxiv.org/abs/2511.08588v1},
 year = {2025}
}

@article{2511.08636v1,
 abstract = {Worldwide, suicide is the second leading cause of death for adolescents with past suicide attempts to be an important predictor for increased future suicides. While some people with suicidal thoughts may try to suppress them, many signal their intentions in social media platforms. To address these issues, we propose a new type of hybrid deep learning scheme, i.e., the combination of a CNN architecture and a BiGRU technique, which can accurately identify the patterns of suicidal ideation from SN datasets. Also, we apply Explainable AI methods using SHapley Additive exPlanations to interpret the prediction results and verifying the model reliability. This integration of CNN local feature extraction, BiGRU bidirectional sequence modeling, attention mechanisms, and SHAP interpretability provides a comprehensive framework for suicide detection. Training and evaluation of the system were performed on a publicly available dataset. Several performance metrics were used for evaluating model performance. Our method was found to have achieved 93.97 accuracy in experimental results. Comparative study to different state-of-the-art Machine Learning and DL models and existing literature demonstrates the superiority of our proposed technique over all the competing methods.},
 author = {Mohaiminul Islam Bhuiyan and Nur Shazwani Kamarudin and Nur Hafieza Ismail},
 comment = {6 pages, 4 figures, 2025 IEEE 9th International Conference on Software Engineering & Computer Systems},
 doi = {},
 eprint = {2511.08636v1},
 journal = {arXiv preprint},
 title = {Detecting Suicidal Ideation in Text with Interpretable Deep Learning: A CNN-BiGRU with Attention Mechanism},
 url = {http://arxiv.org/abs/2511.08636v1},
 year = {2025}
}

@article{2511.09299v1,
 abstract = {Although neural networks are a powerful tool, their widespread use is hindered by the opacity of their decisions and their black-box nature, which result in a lack of trustworthiness. To alleviate this problem, methods in the field of explainable Artificial Intelligence try to unveil how such automated decisions are made. But explainable AI methods are often plagued by missing faithfulness/correctness, meaning that they sometimes provide explanations that do not align with the neural network's decision and logic. Recently, transformations to decision trees have been proposed to overcome such problems. Unfortunately, they typically lack exactness, scalability, or interpretability as the size of the neural network grows. Thus, we generalize these previous results, especially by considering convolutional neural networks, recurrent neural networks, non-ReLU activation functions, and bias terms. Our findings are accompanied by rigorous proofs and we present a novel algorithm RENTT (Runtime Efficient Network to Tree Transformation) designed to compute an exact equivalent decision tree representation of neural networks in a manner that is both runtime and memory efficient. The resulting decision trees are multivariate and thus, possibly too complex to understand. To alleviate this problem, we also provide a method to calculate the ground truth feature importance for neural networks via the equivalent decision trees - for entire models (global), specific input regions (regional), or single decisions (local). All theoretical results are supported by detailed numerical experiments that emphasize two key aspects: the computational efficiency and scalability of our algorithm, and that only RENTT succeeds in uncovering ground truth explanations compared to conventional approximation methods like LIME and SHAP. All code is available at https://github.com/HelenaM23/RENTT .},
 author = {Helena Monke and Benjamin Fresz and Marco Bernreuther and Yilin Chen and Marco F. Huber},
 comment = {},
 doi = {},
 eprint = {2511.09299v1},
 journal = {arXiv preprint},
 title = {Efficiently Transforming Neural Networks into Decision Trees: A Path to Ground Truth Explanations with RENTT},
 url = {http://arxiv.org/abs/2511.09299v1},
 year = {2025}
}

@article{2511.09775v1,
 abstract = {The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications.},
 author = {Dilli Prasad Sharma and Xiaowei Sun and Liang Xue and Xiaodong Lin and Pulei Xiong},
 comment = {},
 doi = {},
 eprint = {2511.09775v1},
 journal = {arXiv preprint},
 title = {Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization},
 url = {http://arxiv.org/abs/2511.09775v1},
 year = {2025}
}

@article{2511.10088v1,
 abstract = {Post-hoc explainability methods are a subset of Machine Learning (ML) that aim to provide a reason for why a model behaves in a certain way. In this paper, we show a new black-box model-agnostic adversarial attack for post-hoc explainable Artificial Intelligence (XAI), particularly in the image domain. The goal of the attack is to modify the original explanations while being undetected by the human eye and maintain the same predicted class. In contrast to previous methods, we do not require any access to the model or its weights, but only to the model's computed predictions and explanations. Additionally, the attack is accomplished in a single step while significantly changing the provided explanations, as demonstrated by empirical evaluation. The low requirements of our method expose a critical vulnerability in current explainability methods, raising concerns about their reliability in safety-critical applications. We systematically generate attacks based on the explanations generated by post-hoc explainability methods (saliency maps, integrated gradients, and DeepLIFT SHAP) for pretrained ResNet-18 and ViT-B16 on ImageNet. The results show that our attacks could lead to dramatically different explanations without changing the predictive probabilities. We validate the effectiveness of our attack, compute the induced change based on the explanation with mean absolute difference, and verify the closeness of the original image and the corrupted one with the Structural Similarity Index Measure (SSIM).},
 author = {Leonardo Pesce and Jiawen Wei and Gianmarco Mengaldo},
 comment = {},
 doi = {},
 eprint = {2511.10088v1},
 journal = {arXiv preprint},
 title = {eXIAA: eXplainable Injections for Adversarial Attack},
 url = {http://arxiv.org/abs/2511.10088v1},
 year = {2025}
}

@article{2511.10861v1,
 abstract = {Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.},
 author = {Daisuke Yasui and Toshitaka Matsuki and Hiroshi Sato},
 comment = {},
 doi = {},
 eprint = {2511.10861v1},
 journal = {arXiv preprint},
 title = {Accuracy-Preserving CNN Pruning Method under Limited Data Availability},
 url = {http://arxiv.org/abs/2511.10861v1},
 year = {2025}
}

@article{2511.11152v1,
 abstract = {Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.},
 author = {Tanmay Ghosh and Shaurabh Anand and Rakesh Gomaji Nannewar and Nithin Nagaraj},
 comment = {},
 doi = {},
 eprint = {2511.11152v1},
 journal = {arXiv preprint},
 title = {Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI},
 url = {http://arxiv.org/abs/2511.11152v1},
 year = {2025}
}

@article{2511.11590v2,
 abstract = {Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.},
 author = {Robert Gigiu},
 comment = {33 pages, 5 figures},
 doi = {},
 eprint = {2511.11590v2},
 journal = {arXiv preprint},
 title = {Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)},
 url = {http://arxiv.org/abs/2511.11590v2},
 year = {2025}
}

@article{2511.11680v1,
 abstract = {Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.},
 author = {Udaya Bhasker Cheerala and Varun Teja Chirukuri and Venkata Akhil Kumar Gummadi and Jintu Moni Bhuyan and Praveen Damacharla},
 comment = {7 pages, 2025 IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS)},
 doi = {},
 eprint = {2511.11680v1},
 journal = {arXiv preprint},
 title = {Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP},
 url = {http://arxiv.org/abs/2511.11680v1},
 year = {2025}
}

@article{2511.11691v1,
 abstract = {Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.},
 author = {Seham Nasr and Zhao Ren and David Johnson},
 comment = {5 pages, 2 figures},
 doi = {},
 eprint = {2511.11691v1},
 journal = {arXiv preprint},
 title = {Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues},
 url = {http://arxiv.org/abs/2511.11691v1},
 year = {2025}
}

@article{2511.11945v1,
 abstract = {In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.},
 author = {Mohammed Temraz and Mark T Keane},
 comment = {31 pages, 8 figures},
 doi = {},
 eprint = {2511.11945v1},
 journal = {arXiv preprint},
 title = {Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes},
 url = {http://arxiv.org/abs/2511.11945v1},
 year = {2025}
}

@article{2511.12085v1,
 abstract = {Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.},
 author = {Sajad U P},
 comment = {},
 doi = {},
 eprint = {2511.12085v1},
 journal = {arXiv preprint},
 title = {Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness},
 url = {http://arxiv.org/abs/2511.12085v1},
 year = {2025}
}

@article{2511.13237v2,
 abstract = {Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.},
 author = {Alan G. Paredes Cetina and Kaouther Benguessoum and Raoni Lourenço and Sylvain Kubler},
 comment = {Accepted in AAAI 2026 Technical Main Track},
 doi = {},
 eprint = {2511.13237v2},
 journal = {arXiv preprint},
 title = {Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification},
 url = {http://arxiv.org/abs/2511.13237v2},
 year = {2025}
}

@article{2511.13712v1,
 abstract = {As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.},
 author = {Kiana Vu and İsmet Selçuk Özer and Phung Lai and Zheng Wu and Thilanka Munasinghe and Jennifer Wei},
 comment = {},
 doi = {},
 eprint = {2511.13712v1},
 journal = {arXiv preprint},
 title = {From Black Box to Insight: Explainable AI for Extreme Event Preparedness},
 url = {http://arxiv.org/abs/2511.13712v1},
 year = {2025}
}

@article{2511.13791v1,
 abstract = {Proteins perform essential biological functions, and accurate classification of their sequences is critical for understanding structure-function relationships, enzyme mechanisms, and molecular interactions. This study presents a deep learning-based framework for functional group classification of protein sequences derived from the Protein Data Bank (PDB). Four architectures were implemented: Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), CNN-BiLSTM hybrid, and CNN with Attention. Each model was trained using k-mer integer encoding to capture both local and long-range dependencies. Among these, the CNN achieved the highest validation accuracy of 91.8%, demonstrating the effectiveness of localized motif detection. Explainable AI techniques, including Grad-CAM and Integrated Gradients, were applied to interpret model predictions and identify biologically meaningful sequence motifs. The discovered motifs, enriched in histidine, aspartate, glutamate, and lysine, represent amino acid residues commonly found in catalytic and metal-binding regions of transferase enzymes. These findings highlight that deep learning models can uncover functionally relevant biochemical signatures, bridging the gap between predictive accuracy and biological interpretability in protein sequence analysis.},
 author = {Pratik Chakraborty and Aryan Bhargava},
 comment = {8 pages, 4 figures},
 doi = {},
 eprint = {2511.13791v1},
 journal = {arXiv preprint},
 title = {XAI-Driven Deep Learning for Protein Sequence Functional Group Classification},
 url = {http://arxiv.org/abs/2511.13791v1},
 year = {2025}
}

@article{2511.16482v1,
 abstract = {Explainable AI (XAI) is increasingly essential as modern models become more complex and high-stakes applications demand transparency, trust, and regulatory compliance. Existing global attribution methods often incur high computational costs, lack stability under correlated inputs, and fail to scale efficiently to large or heterogeneous datasets. We address these gaps with \emph{ExCIR} (Explainability through Correlation Impact Ratio), a correlation-aware attribution score equipped with a lightweight transfer protocol that reproduces full-model rankings using only a fraction of the data. ExCIR quantifies sign-aligned co-movement between features and model outputs after \emph{robust centering} (subtracting a robust location estimate, e.g., median or mid-mean, from features and outputs). We further introduce \textsc{BlockCIR}, a \emph{groupwise} extension of ExCIR that scores \emph{sets} of correlated features as a single unit. By aggregating the same signed-co-movement numerators and magnitudes over predefined or data-driven groups, \textsc{BlockCIR} mitigates double-counting in collinear clusters (e.g., synonyms or duplicated sensors) and yields smoother, more stable rankings when strong dependencies are present. Across diverse text, tabular, signal, and image datasets, ExCIR shows trustworthy agreement with established global baselines and the full model, delivers consistent top-$k$ rankings across settings, and reduces runtime via lightweight evaluation on a subset of rows. Overall, ExCIR provides \emph{computationally efficient}, \emph{consistent}, and \emph{scalable} explainability for real-world deployment.},
 author = {Poushali Sengupta and Yan Zhang and Frank Eliassen and Sabita Maharjan},
 comment = {Accepted, 2026 International Conference on Advances in Artificial Intelligence and Machine Learning (AAIML 2026)},
 doi = {},
 eprint = {2511.16482v1},
 journal = {arXiv preprint},
 title = {Correlation-Aware Feature Attribution Based Explainable AI},
 url = {http://arxiv.org/abs/2511.16482v1},
 year = {2025}
}

@article{2511.16588v1,
 abstract = {Case-based reasoning networks are machine-learning models that make predictions based on similarity between the input and prototypical parts of training samples, called prototypes. Such models are able to explain each decision by pointing to the prototypes that contributed the most to the final outcome. As the explanation is a core part of the prediction, they are often qualified as ``interpretable by design". While promising, we show that such explanations are sometimes misleading, which hampers their usefulness in safety-critical contexts. In particular, several instances may lead to different predictions and yet have the same explanation. Drawing inspiration from the field of formal eXplainable AI (FXAI), we propose Abductive Latent Explanations (ALEs), a formalism to express sufficient conditions on the intermediate (latent) representation of the instance that imply the prediction. Our approach combines the inherent interpretability of case-based reasoning models and the guarantees provided by formal XAI. We propose a solver-free and scalable algorithm for generating ALEs based on three distinct paradigms, compare them, and present the feasibility of our approach on diverse datasets for both standard and fine-grained image classification. The associated code can be found at https://github.com/julsoria/ale},
 author = {Jules Soria and Zakaria Chihani and Julien Girard-Satabin and Alban Grastien and Romain Xu-Darme and Daniela Cancila},
 comment = {Accepted at AAAI-26},
 doi = {},
 eprint = {2511.16588v1},
 journal = {arXiv preprint},
 title = {Formal Abductive Latent Explanations for Prototype-Based Networks},
 url = {http://arxiv.org/abs/2511.16588v1},
 year = {2025}
}

@article{2511.18083v1,
 abstract = {Background and Objective: Deep learning models have high computational needs and lack interpretability but are often the first choice for medical image classification tasks. This study addresses whether complex neural networks are essential for the simple binary classification task of malaria. We introduce the Extracted Morphological Feature Engineered (EMFE) pipeline, a transparent, reproducible, and low compute machine learning approach tailored explicitly for simple cell morphology, designed to achieve deep learning performance levels on a simple CPU only setup with the practical aim of real world deployment.
  Methods: The study used the NIH Malaria Cell Images dataset, with two features extracted from each cell image: the number of non background pixels and the number of holes within the cell. Logistic Regression and Random Forest were compared against ResNet18, DenseNet121, MobileNetV2, and EfficientNet across accuracy, model size, and CPU inference time. An ensemble model was created by combining Logistic Regression and Random Forests to achieve higher accuracy while retaining efficiency.
  Results: The single variable Logistic Regression model achieved a test accuracy of 94.80 percent with a file size of 1.2 kB and negligible inference latency (2.3 ms). The two stage ensemble improved accuracy to 97.15 percent. In contrast, the deep learning methods require 13.6 MB to 44.7 MB of storage and show significantly higher inference times (68 ms).
  Conclusion: This study shows that a compact feature engineering approach can produce clinically meaningful classification performance while offering gains in transparency, reproducibility, speed, and deployment feasibility. The proposed pipeline demonstrates that simple interpretable features paired with lightweight models can serve as a practical diagnostic solution for environments with limited computational resources.},
 author = {Md Abdullah Al Kafi and Raka Moni and Sumit Kumar Banshal},
 comment = {},
 doi = {},
 eprint = {2511.18083v1},
 journal = {arXiv preprint},
 title = {Less Is More: An Explainable AI Framework for Lightweight Malaria Classification},
 url = {http://arxiv.org/abs/2511.18083v1},
 year = {2025}
}

@article{2511.19265v1,
 abstract = {The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.},
 author = {Bianka Kowalska and Halina Kwaśnicka},
 comment = {},
 doi = {},
 eprint = {2511.19265v1},
 journal = {arXiv preprint},
 title = {Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks},
 url = {http://arxiv.org/abs/2511.19265v1},
 year = {2025}
}

@article{2511.20395v2,
 abstract = {Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.},
 author = {M. C. Schoppema and B. H. M. van der Velden and A. Hürriyetoğlu and M. D. Klijnstra and E. J. Faassen and A. Gerssen and H. J. van der Fels-Klerx},
 comment = {18 pages, 6 figures, submitted to npj Science of Food},
 doi = {},
 eprint = {2511.20395v2},
 journal = {arXiv preprint},
 title = {Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI},
 url = {http://arxiv.org/abs/2511.20395v2},
 year = {2025}
}

@article{2511.20500v1,
 abstract = {Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.},
 author = {Sidahmed Benabderrahmane and Talal Rahwan},
 comment = {},
 doi = {},
 eprint = {2511.20500v1},
 journal = {arXiv preprint},
 title = {From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection},
 url = {http://arxiv.org/abs/2511.20500v1},
 year = {2025}
}

@article{2511.21588v1,
 abstract = {This systematic review examines how machine learning (ML) and deep learning (DL) have transformed forecasting, decision-making, and financial modelling, promoting innovation and efficiency in financial systems. Following PRISMA 2020 guidelines, we analyze 22 peer-reviewed and open-access articles (2024 to 2026) indexed in Scopus, applying ML and DL models across credit risk prediction, cryptocurrency, asset pricing, and macroeconomic policy modeling. The most used models include Random Forest, XG-Boost, Support Vector Machine, Long Short-Term Memory (LSTM), Bidirectional LSTM, Convolutional Neural Network (CNN), and hybrid or ensemble approaches combining statistical and AI methods. ML and DL techniques outperform traditional models by capturing nonlinear dependencies and enhancing predictive accuracy, while explainable AI methods (e.g., SHAP and feature importance analysis) improve transparency and interpretability. Emerging trends include cross-domain applications and the integration of responsible AI in finance. Despite notable progress, challenges remain in interpretability, generalizability, and data quality. Overall, this review provides a comprehensive overview of AI-driven computational finance and outlines future research directions.},
 author = {Soufiane El Amine El Alami and Abderazzak Mouiha and Abdelatif Hafid and Ahmed El Hilali Alaoui},
 comment = {},
 doi = {},
 eprint = {2511.21588v1},
 journal = {arXiv preprint},
 title = {Machine Learning and Deep Learning in Computational Finance: A Systematic Review},
 url = {http://arxiv.org/abs/2511.21588v1},
 year = {2025}
}

@article{2511.21959v1,
 abstract = {This paper presents a comprehensive comparative model analysis on a novel gastrointestinal medical imaging dataset, comprised of 4,000 endoscopic images spanning four critical disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging state-of-the-art deep learning techniques, the study confronts common endoscopic challenges such as variable lighting, fluctuating camera angles, and frequent imaging artifacts. The best performing models, VGG16 and MobileNetV2, each achieved a test accuracy of 96.5%, while Xception reached 94.24%, establishing robust benchmarks and baselines for automated disease classification. In addition to strong classification performance, the approach includes explainable AI via Grad-CAM visualization, enabling identification of image regions most influential to model predictions and enhancing clinical interpretability. Experimental results demonstrate the potential for robust, accurate, and interpretable medical image analysis even in complex real-world conditions. This work contributes original benchmarks, comparative insights, and visual explanations, advancing the landscape of gastrointestinal computer-aided diagnosis and underscoring the importance of diverse, clinically relevant datasets and model explainability in medical AI research.},
 author = {Walid Houmaidi and Mohamed Hadadi and Youssef Sabiri and Yousra Chtouki},
 comment = {7 pages, 4 figures, 2 tables. Accepted at DASET 2026},
 doi = {},
 eprint = {2511.21959v1},
 journal = {arXiv preprint},
 title = {DeepGI: Explainable Deep Learning for Gastrointestinal Image Classification},
 url = {http://arxiv.org/abs/2511.21959v1},
 year = {2025}
}

@article{2511.22420v1,
 abstract = {While the increased integration of AI technologies into interactive systems enables them to solve an increasing number of tasks, the black-box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. This challenge not only pertains to standard XAI techniques but also to human examination and conversational XAI approaches that need access to model internals to interpret them correctly and completely. To this end, we propose conceptually representing such interactive systems as sequences of structural building blocks. These include the AI models themselves, as well as control mechanisms grounded in literature. The structural building blocks can then be explained through complementary explanatory building blocks, such as established XAI techniques like LIME and SHAP. The flow and APIs of the structural building blocks form an unambiguous overview of the underlying system, serving as a communication basis for both human and automated agents, thus aligning human and machine interpretability of the embedded AI models. In this paper, we present our flow-based approach and a selection of building blocks as MATCH: a framework for engineering Multi-Agent Transparent and Controllable Human-centered systems. This research contributes to the field of (conversational) XAI by facilitating the integration of interpretability into existing interactive systems.},
 author = {Sebe Vanbrabant and Gustavo Rovelo Ruiz and Davy Vanacken},
 comment = {Submitted Version accepted for publication in an LNCS Volume "Engineering Interactive Computer Systems - EICS 2025 - International Workshops and Doctoral Consortium"},
 doi = {},
 eprint = {2511.22420v1},
 journal = {arXiv preprint},
 title = {MATCH: Engineering Transparent and Controllable Conversational XAI Systems through Composable Building Blocks},
 url = {http://arxiv.org/abs/2511.22420v1},
 year = {2025}
}

@article{2511.23036v1,
 abstract = {Explaining online time series monitoring models is crucial across sensitive domains such as healthcare and finance, where temporal and contextual prediction dynamics underpin critical decisions. While recent XAI methods have improved the explainability of time series models, they mostly analyze each time step independently, overlooking temporal dependencies. This results in further challenges: explaining prediction changes is non-trivial, methods fail to leverage online dynamics, and evaluation remains difficult. To address these challenges, we propose Delta-XAI, which adapts 14 existing XAI methods through a wrapper function and introduces a principled evaluation suite for the online setting, assessing diverse aspects, such as faithfulness, sufficiency, and coherence. Experiments reveal that classical gradient-based methods, such as Integrated Gradients (IG), can outperform recent approaches when adapted for temporal analysis. Building on this, we propose Shifted Window Integrated Gradients (SWING), which incorporates past observations in the integration path to systematically capture temporal dependencies and mitigate out-of-distribution effects. Extensive experiments consistently demonstrate the effectiveness of SWING across diverse settings with respect to diverse metrics. Our code is publicly available at https://anonymous.4open.science/r/Delta-XAI.},
 author = {Changhun Kim and Yechan Mun and Hyeongwon Jang and Eunseo Lee and Sangchul Hahn and Eunho Yang},
 comment = {Under review at ICLR 2026},
 doi = {},
 eprint = {2511.23036v1},
 journal = {arXiv preprint},
 title = {Delta-XAI: A Unified Framework for Explaining Prediction Changes in Online Time Series Monitoring},
 url = {http://arxiv.org/abs/2511.23036v1},
 year = {2025}
}

@article{2512.00399v1,
 abstract = {Macroeconomic nowcasting sits at the intersection of traditional econometrics, data-rich information systems, and AI applications in business, economics, and policy. Machine learning (ML) methods are increasingly used to nowcast quarterly GDP growth, but adoption in high-stakes settings requires that predictive accuracy be matched by interpretability and robust uncertainty quantification. This article reviews recent developments in macroeconomic nowcasting and compares econometric benchmarks with ML approaches in data-rich and shock-prone environments, emphasizing the use of nowcasts as decision inputs rather than as mere error-minimization exercises. The discussion is organized along three axes. First, we contrast penalized regressions, dimension-reduction techniques, tree ensembles, and neural networks with autoregressive models, Dynamic Factor Models, and Random Walks, emphasizing how each family handles small samples, collinearity, mixed frequencies, and regime shifts. Second, we examine explainability tools (intrinsic measures and model-agnostic XAI methods), focusing on temporal stability, sign coherence, and their ability to sustain credible economic narratives and nowcast revisions. Third, we analyze non-parametric uncertainty quantification via block bootstrapping for predictive intervals and confidence bands on feature importance under serial dependence and ragged edge. We translate these elements into a reference workflow for "decision-grade" nowcasting systems, including vintage management, time-aware validation, and automated reliability audits, and we outline a research agenda on regime-dependent model comparison, bootstrap design for latent components, and temporal stability of explanations. Explainable ML and uncertainty quantification emerge as structural components of a responsible forecasting pipeline, not optional refinements.},
 author = {Luca Attolico},
 comment = {32 pages, 2 tables, 1 figure. Submitted to the special issue "Emerging Trends in FinTech and AI: Theory and Applications in Business, Economics, and Law" (EUM Series "Economics and Law")},
 doi = {},
 eprint = {2512.00399v1},
 journal = {arXiv preprint},
 title = {Explainable Machine Learning for Macroeconomic and Financial Nowcasting: A Decision-Grade Framework for Business and Policy},
 url = {http://arxiv.org/abs/2512.00399v1},
 year = {2025}
}

@article{2512.01333v1,
 abstract = {Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.},
 author = {A S M Ahsanul Sarkar Akib and Raduana Khawla and Abdul Hasib},
 comment = {},
 doi = {},
 eprint = {2512.01333v1},
 journal = {arXiv preprint},
 title = {Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI},
 url = {http://arxiv.org/abs/2512.01333v1},
 year = {2025}
}

@article{2512.01412v1,
 abstract = {Explainability is essential for neural networks that model long time series, yet most existing explainable AI methods only produce point-wise importance scores and fail to capture temporal structures such as trends, cycles, and regime changes. This limitation weakens human interpretability and trust in long-horizon models. To address these issues, we identify four key requirements for interpretable time-series modeling: temporal continuity, pattern-centric explanation, causal disentanglement, and faithfulness to the model's inference process. We propose EXCAP, a unified framework that satisfies all four requirements. EXCAP combines an attention-based segmenter that extracts coherent temporal patterns, a causally structured decoder guided by a pre-trained causal graph, and a latent aggregation mechanism that enforces representation stability. Our theoretical analysis shows that EXCAP provides smooth and stable explanations over time and is robust to perturbations in causal masks. Extensive experiments on classification and forecasting benchmarks demonstrate that EXCAP achieves strong predictive accuracy while generating coherent and causally grounded explanations. These results show that EXCAP offers a principled and scalable approach to interpretable modeling of long time series with relevance to high-stakes domains such as healthcare and finance.},
 author = {Ziqian Wang and Yuxiao Cheng and Jinli Suo},
 comment = {Approximately 30 pages, 8 figures, and 5 tables. Preprint version. Includes theoretical analysis, model architecture, interpretability evaluation, and extensive benchmark experiments},
 doi = {},
 eprint = {2512.01412v1},
 journal = {arXiv preprint},
 title = {A Self-explainable Model of Long Time Series by Extracting Informative Structured Causal Patterns},
 url = {http://arxiv.org/abs/2512.01412v1},
 year = {2025}
}

@article{2512.01562v1,
 abstract = {Change-point detection (CPD) in high-dimensional, large-volume time series is challenging for statistical consistency, scalability, and interpretability. We introduce TimePred, a self-supervised framework that reduces multivariate CPD to univariate mean-shift detection by predicting each sample's normalized time index. This enables efficient offline CPD using existing algorithms and supports the integration of XAI attribution methods for feature-level explanations. Our experiments show competitive CPD performance while reducing computational cost by up to two orders of magnitude. In an industrial manufacturing case study, we demonstrate improved detection accuracy and illustrate the practical value of interpretable change-point insights.},
 author = {Simon Leszek},
 comment = {6 pages, 3 figures},
 doi = {},
 eprint = {2512.01562v1},
 journal = {arXiv preprint},
 title = {TimePred: efficient and interpretable offline change point detection for high volume data - with application to industrial process monitoring},
 url = {http://arxiv.org/abs/2512.01562v1},
 year = {2025}
}

@article{2512.02092v1,
 abstract = {Timely assessment of current conditions is essential especially for small, open economies such as Singapore, where external shocks transmit rapidly to domestic activity. We develop a real-time nowcasting framework for quarterly GDP growth using a high-dimensional panel of approximately 70 indicators, encompassing economic and financial indicators over 1990Q1-2023Q2. The analysis covers penalized regressions, dimensionality-reduction methods, ensemble learning algorithms, and neural architectures, benchmarked against a Random Walk, an AR(3), and a Dynamic Factor Model. The pipeline preserves temporal ordering through an expanding-window walk-forward design with Bayesian hyperparameter optimization, and uses moving block-bootstrap procedures both to construct prediction intervals and to obtain confidence bands for feature-importance measures. It adopts model-specific and XAI-based explainability tools. A Model Confidence Set procedure identifies statistically superior learners, which are then combined through simple, weighted, and exponentially weighted schemes; the resulting time-varying weights provide an interpretable representation of model contributions. Predictive ability is assessed via Giacomini-White tests. Empirical results show that penalized regressions, dimensionality-reduction models, and GRU networks consistently outperform all benchmarks, with RMSFE reductions of roughly 40-60%; aggregation delivers further gains. Feature-attribution methods highlight industrial production, external trade, and labor-market indicators as dominant drivers of Singapore's short-run growth dynamics.},
 author = {Luca Attolico},
 comment = {PhD thesis, University of Macerata (2025). PhD programme: Quantitative Methods for Policy Evaluation (Cycle XXXVII). Supervisors: Rosaria Romano, Jamus Jerome Lim},
 doi = {},
 eprint = {2512.02092v1},
 journal = {arXiv preprint},
 title = {Opening the Black Box: Nowcasting Singapore's GDP Growth and its Explainability},
 url = {http://arxiv.org/abs/2512.02092v1},
 year = {2025}
}

@article{2512.03112v1,
 abstract = {Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.},
 author = {Jialai She},
 comment = {},
 doi = {},
 eprint = {2512.03112v1},
 journal = {arXiv preprint},
 title = {Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability},
 url = {http://arxiv.org/abs/2512.03112v1},
 year = {2025}
}

@article{2512.04530v1,
 abstract = {Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.},
 author = {Xudong Wang and Ziheng Sun and Chris Ding and Jicong Fan},
 comment = {Full version with appendix of the paper published in the Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence (IJCAI-25), Main Track},
 doi = {10.24963/ijcai.2025/381},
 eprint = {2512.04530v1},
 journal = {arXiv preprint},
 title = {Explainable Graph Representation Learning via Graph Pattern Analysis},
 url = {http://arxiv.org/abs/2512.04530v1},
 year = {2025}
}

@article{2512.05338v1,
 abstract = {Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.},
 author = {Hiroki Hasegawa and Yukihiko Okada},
 comment = {30 pages},
 doi = {},
 eprint = {2512.05338v1},
 journal = {arXiv preprint},
 title = {Interaction Tensor Shap},
 url = {http://arxiv.org/abs/2512.05338v1},
 year = {2025}
}

@article{2512.06390v1,
 abstract = {The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.},
 author = {Mehrab Hosain and Sabbir Alom Shuvo and Matthew Ogbe and Md Shah Jalal Mazumder and Yead Rahman and Md Azizul Hakim and Anukul Pandey},
 comment = {Accepted at 2025 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob). 7 pages, 5 figures},
 doi = {10.1109/APWiMob67231.2025.11269122},
 eprint = {2512.06390v1},
 journal = {arXiv preprint},
 title = {Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses},
 url = {http://arxiv.org/abs/2512.06390v1},
 year = {2025}
}

@article{2512.06591v1,
 abstract = {Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.},
 author = {Joe Shymanski and Jacob Brue and Sandip Sen},
 comment = {21 pages, 7 figures, 6 tables. EXTRAAMAS 2025 submission. Preprint version},
 doi = {10.1007/978-3-032-01399-6_10},
 eprint = {2512.06591v1},
 journal = {arXiv preprint},
 title = {Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability},
 url = {http://arxiv.org/abs/2512.06591v1},
 year = {2025}
}

@article{2512.07178v1,
 abstract = {Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.},
 author = {Latifa Dwiyanti and Sergio Ryan Wibisono and Hidetaka Nambo},
 comment = {This paper was accepted and presented at the 7th World Symposium on Software Engineering (WSSE) 2025 on 25 October 2025 in Okayama, Japan, and is currently awaiting publication},
 doi = {},
 eprint = {2512.07178v1},
 journal = {arXiv preprint},
 title = {ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation},
 url = {http://arxiv.org/abs/2512.07178v1},
 year = {2025}
}

@article{2512.07864v1,
 abstract = {New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.},
 author = {Muhammad Sukri Bin Ramli},
 comment = {},
 doi = {},
 eprint = {2512.07864v1},
 journal = {arXiv preprint},
 title = {Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data},
 url = {http://arxiv.org/abs/2512.07864v1},
 year = {2025}
}

@article{2512.07981v1,
 abstract = {Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.},
 author = {Federico Di Valerio and Michela Proietti and Alessio Ragno and Roberto Capobianco},
 comment = {},
 doi = {},
 eprint = {2512.07981v1},
 journal = {arXiv preprint},
 title = {CIP-Net: Continual Interpretable Prototype-based Network},
 url = {http://arxiv.org/abs/2512.07981v1},
 year = {2025}
}

@article{2512.08329v1,
 abstract = {Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.},
 author = {Michael R. Martin and Garrick Chan and Kwan-Liu Ma},
 comment = {32 pages, 17 figures, 1 table, 5 algorithms, preprint},
 doi = {},
 eprint = {2512.08329v1},
 journal = {arXiv preprint},
 title = {Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models},
 url = {http://arxiv.org/abs/2512.08329v1},
 year = {2025}
}

@article{2512.08344v1,
 abstract = {Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.},
 author = {Tien Cuong Bui},
 comment = {157 pages, Doctoral dissertation at Seoul National University (submitted in 2024.08 to SNU library, slightly updated in 2025.11 for open digital version)},
 doi = {},
 eprint = {2512.08344v1},
 journal = {arXiv preprint},
 title = {Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions},
 url = {http://arxiv.org/abs/2512.08344v1},
 year = {2025}
}

@article{2512.09517v1,
 abstract = {This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (ε = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.},
 author = {Nabil Anan Orka and Ehtashamul Haque and Maftahul Jannat and Md Abdul Awal and Mohammad Ali Moni},
 comment = {Under review},
 doi = {},
 eprint = {2512.09517v1},
 journal = {arXiv preprint},
 title = {QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder},
 url = {http://arxiv.org/abs/2512.09517v1},
 year = {2025}
}

@article{2512.10169v1,
 abstract = {Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.},
 author = {Alexander Wan and Kevin Klyman and Sayash Kapoor and Nestor Maslej and Shayne Longpre and Betty Xiong and Percy Liang and Rishi Bommasani},
 comment = {Website: https://crfm.stanford.edu/fmti/December-2025/index.html},
 doi = {},
 eprint = {2512.10169v1},
 journal = {arXiv preprint},
 title = {The 2025 Foundation Model Transparency Index},
 url = {http://arxiv.org/abs/2512.10169v1},
 year = {2025}
}

@article{2512.11614v1,
 abstract = {Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.},
 author = {Björn Deiseroth and Max Henning Höth and Kristian Kersting and Letitia Parcalabescu},
 comment = {34 pages, 19 figures},
 doi = {},
 eprint = {2512.11614v1},
 journal = {arXiv preprint},
 title = {Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols},
 url = {http://arxiv.org/abs/2512.11614v1},
 year = {2025}
}

@article{2512.11852v1,
 abstract = {The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.},
 author = {Muhammad Jawad Bashir and Shagufta Henna and Eoghan Furey},
 comment = {7 pages, Accepted in 36th Irish Signals and Systems Conference, ISSC 2025},
 doi = {},
 eprint = {2512.11852v1},
 journal = {arXiv preprint},
 title = {Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things},
 url = {http://arxiv.org/abs/2512.11852v1},
 year = {2025}
}

@article{2512.12506v1,
 abstract = {Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.},
 author = {Agustín García-García and Pablo Hidalgo and Julio E. Sandubete},
 comment = {11 pages, 1 table},
 doi = {},
 eprint = {2512.12506v1},
 journal = {arXiv preprint},
 title = {Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts},
 url = {http://arxiv.org/abs/2512.12506v1},
 year = {2025}
}

@article{2512.13910v1,
 abstract = {Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.},
 author = {Matheus Corrêa Domingos and Valdivino Alexandre de Santiago Júnior and Juliana Aparecida Anochi and Elcio Hideiti Shiguemori and Luísa Mirelle Costa dos Santos and Hércules Carlos dos Santos Pereira and André Estevam Costa Oliveira},
 comment = {},
 doi = {},
 eprint = {2512.13910v1},
 journal = {arXiv preprint},
 title = {Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America},
 url = {http://arxiv.org/abs/2512.13910v1},
 year = {2025}
}
