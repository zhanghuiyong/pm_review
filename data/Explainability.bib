@article{-2024explainable,
  title = {Explainable {{AI}}: {{Developing Interpretable Deep Learning Models}} for {{Medical Diagnosis}}},
  shorttitle = {Explainable {{AI}}},
  author = {{-}, Ruchi Thakur},
  year = {2024},
  month = jul,
  journal = {International Journal For Multidisciplinary Research},
  volume = {6},
  number = {4},
  pages = {25281},
  issn = {2582-2160},
  doi = {10.36948/ijfmr.2024.v06i04.25281},
  urldate = {2025-05-28},
  abstract = {Artificial Intelligence (AI) and Deep Learning (DL) have demonstrated remarkable potential in enhancing medical diagnosis across various specialties. However, the inherent complexity and opacity of these models pose significant challenges in clinical adoption, particularly due to the critical nature of healthcare decisions. This research paper explores the development of interpretable deep learning models for medical diagnosis, focusing on the integration of Explainable AI (XAI) techniques to enhance transparency, accountability, and trust in AI-assisted medical decision-making. We investigate various XAI methodologies, their application in different medical domains, and their impact on diagnostic accuracy and clinical interpretability. Through a comprehensive analysis of case studies, we demonstrate how explainable models can not only maintain high diagnostic performance but also provide valuable insights into their decision-making processes, potentially revolutionizing the synergy between AI and human expertise in healthcare.},
  annotation = {TLDR: Through a comprehensive analysis of case studies, it is demonstrated how explainable models can not only maintain high diagnostic performance but also provide valuable insights into their decision-making processes, potentially revolutionizing the synergy between AI and human expertise in healthcare.},
  timestamp = {2025-05-28T07:55:42Z}
}

@article{10.1145/3400051.3400058,
  ids = {moraffah2020causal},
  title = {Causal Interpretability for Machine Learning - Problems, Methods and Evaluation},
  author = {Moraffah, Raha and Karami, Mansooreh and Guo, Ruocheng and Raglin, Adrienne and Liu, Huan},
  year = {2020},
  month = may,
  journal = {SIGKDD Explor. Newsl.},
  volume = {22},
  number = {1},
  pages = {18--33},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1931-0145},
  doi = {10.1145/3400051.3400058},
  urldate = {2025-03-03},
  abstract = {Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more humanfriendly explanations, recent work on interpretability tries to answer questions related to causality such as "Why does this model makes such decisions?" or "Was it a specific feature that caused the decision made by the model?". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.},
  issue_date = {June 2020},
  langid = {english},
  keywords = {causal inference,counterfactuals,explainability,interpratablity,machine learning},
  annotation = {TLDR: This work presents a comprehensive survey on causal interpretable models from the aspects of the problems and methods and provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.},
  timestamp = {2025-06-16T02:56:44Z}
}

@article{10.1145/3637487,
  ids = {hossain2025explainable},
  title = {Explainable {{AI}} for Medical Data: {{Current}} Methods, Limitations, and Future Directions},
  shorttitle = {Explainable {{AI}} for {{Medical Data}}},
  author = {Hossain, Md Imran and Zamzmi, Ghada and Mouton, Peter R. and Salekin, Md Sirajus and Sun, Yu and Goldgof, Dmitry},
  year = {2025},
  month = feb,
  journal = {Acm Computing Surveys},
  volume = {57},
  number = {6},
  pages = {1--46},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/3637487},
  urldate = {2025-04-21},
  abstract = {With the power of parallel processing, large datasets, and fast computational resources, deep neural networks (DNNs) have outperformed highly trained and experienced human experts in medical applications. However, the large global community of healthcare professionals, many of whom routinely face potentially life-or-death outcomes with complex medicolegal consequences, have yet to embrace this powerful technology. The major problem is that most current AI solutions function as a metaphorical black-box positioned between input data and output decisions without a rigorous explanation for their internal processes. With the goal of enhancing trust and improving acceptance of artificial intelligence-- (AI) based technology in clinical medicine, there is a large and growing effort to address this challenge using eXplainable AI (XAI), a set of techniques, strategies, and algorithms with an explicit focus on explaining the ``hows and whys'' of DNNs. Here, we provide a comprehensive review of the state-of-the-art XAI techniques concerning healthcare applications and discuss current challenges and future directions. We emphasize the strengths and limitations of each category, including image, tabular, and textual explanations, and explore a range of evaluation metrics for assessing the effectiveness of XAI solutions. Finally, we highlight promising opportunities for XAI research to enhance the acceptance of DNNs by the healthcare community.},
  articleno = {148},
  issue_date = {June 2025},
  langid = {english},
  keywords = {deep neural networks,Explainability,interpretable AI,medical data,responsible AI},
  annotation = {TLDR: A comprehensive review of the state-of-the-art XAI techniques concerning healthcare applications, including image, tabular, and textual explanations, and a range of evaluation metrics for assessing the effectiveness of XAI solutions are provided.},
  timestamp = {2025-05-12T00:31:21Z}
}

@article{10.1145/3729531,
  title = {A Survey of Explainable Artificial Intelligence ({{XAI}}) in Financial Time Series Forecasting},
  author = {Arsenault, Pierre-Daniel and Wang, Shengrui and Patenaude, Jean-Marc},
  year = {2025},
  month = may,
  journal = {Acm Computing Surveys},
  volume = {57},
  number = {10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {0360-0300},
  doi = {10.1145/3729531},
  abstract = {Artificial intelligence (AI) models have reached a very significant level of accuracy. While their superior performance offers considerable benefits, their inherent complexity often decreases human trust, which slows their application in high-risk decision-making domains, such as finance. The field of explainable AI (XAI) seeks to bridge this gap, aiming to make AI models more understandable. This survey, focusing on published work from 2018 to 2024, categorizes XAI approaches that predict financial time series. In this article, explainability and interpretability are distinguished, emphasizing the need to treat these concepts separately, as they are not applied the same way in practice. Through clear definitions, a rigorous taxonomy of XAI approaches, a complementary characterization, and examples of XAI's application in the finance industry, this article provides a comprehensive view of XAI's current role in finance. It can also serve as a guide for selecting the most appropriate XAI approach for future applications.},
  articleno = {265},
  issue_date = {October 2025},
  keywords = {Explainable artificial intelligence,explainable model,finance,interpretable model,time series,XAI},
  annotation = {TLDR: This article provides a comprehensive view of XAI's current role in finance, including clear definitions, a rigorous taxonomy of XAI approaches, a complementary characterization, and examples of XAI's application in the finance industry.},
  timestamp = {2025-08-28T03:39:51Z}
}

@inproceedings{10.5555/3666122.3669475,
  title = {Interpretable Prototype-Based Graph Information Bottleneck},
  booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
  author = {Seo, Sangwoo and Kim, Sungwon and Park, Chanyoung},
  year = {2023},
  series = {Nips '23},
  publisher = {Curran Associates Inc.},
  address = {New Orleans, LA, USA and Red Hook, NY, USA},
  doi = {https://doi.org/10.48550/arXiv.2310.19906},
  abstract = {The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB), that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph that is important for the model prediction. This is the first work that incorporates prototype learning into the process of identifying the key subgraphs that have a critical impact on the prediction performance. Extensive experiments, including qualitative analysis, demonstrate that PGIB outperforms state-of-the-art methods in terms of both prediction performance and explainability. The source code of PGIB is available at https://github.com/sang-woo-seo/PGIB.},
  articleno = {3353},
  timestamp = {2025-05-17T11:48:14Z}
}

@article{2004anemia,
  title = {Anemia as a Risk Factor and Therapeutic Target in Heart Failure},
  year = {2004},
  month = sep,
  journal = {Journal of the American College of Cardiology},
  volume = {44},
  number = {5},
  pages = {959--966},
  publisher = {Elsevier},
  issn = {0735-1097},
  doi = {10.1016/j.jacc.2004.05.070},
  urldate = {2025-05-06},
  abstract = {Anemia has recently been recognized as an important comorbid condition and potentially novel therapeutic target in patients with heart failure (HF). A{\dots}},
  langid = {american},
  annotation = {TLDR: Although preliminary data suggest that treatment of anemia may result in significant symptomatic improvement in HF, aggressive treatment of the condition may also be associated with increased risk of hypertension or thrombosis, and multiple ongoing studies will provide definitive data on the balance of risks and benefits.},
  timestamp = {2025-05-06T06:58:43Z}
}

@article{2019uncovering,
  title = {Uncovering Convolutional Neural Network Decisions for Diagnosing Multiple Sclerosis on Conventional {{MRI}} Using Layer-Wise Relevance Propagation},
  year = {2019},
  month = jan,
  journal = {NeuroImage: Clinical},
  volume = {24},
  pages = {102003},
  publisher = {Elsevier},
  issn = {2213-1582},
  doi = {10.1016/j.nicl.2019.102003},
  urldate = {2025-05-17},
  abstract = {Machine learning-based imaging diagnostics has recently reached or even surpassed the level of clinical experts in several clinical domains. However, {\dots}},
  langid = {american},
  annotation = {TLDR: LRP and the proposed framework have the capability to make diagnostic decisions of CNN models transparent, which could serve to justify classification decisions for clinical review, verify diagnosis-relevant features and potentially gather new disease knowledge.},
  timestamp = {2025-05-17T10:59:42Z}
}

@incollection{2020ethical,
  title = {Ethical and Legal Challenges of Artificial Intelligence-Driven Healthcare},
  booktitle = {Artificial {{Intelligence}} in {{Healthcare}}},
  year = {2020},
  month = jan,
  pages = {295--336},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-818438-7.00012-5},
  urldate = {2025-05-05},
  abstract = {This chapter will map the ethical and legal challenges posed by artificial intelligence (AI) in healthcare and suggest directions for resolving them. {\dots}},
  langid = {american},
  timestamp = {2025-05-05T02:22:44Z}
}

@article{2021evaluating,
  title = {Evaluating {{XAI}}: {{A}} Comparison of Rule-Based and Example-Based Explanations},
  shorttitle = {Evaluating {{XAI}}},
  year = {2021},
  month = feb,
  journal = {Artificial Intelligence},
  volume = {291},
  pages = {103404},
  publisher = {Elsevier},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2020.103404},
  urldate = {2025-05-03},
  abstract = {Current developments in Artificial Intelligence (AI) led to a resurgence of Explainable AI (XAI). New methods are being researched to obtain informati{\dots}},
  langid = {american},
  annotation = {TLDR: The results show that rule-based explanations have a small positive effect on system understanding, whereas both rule- and example- based explanations seem to persuade users in following the advice even when incorrect, and show the importance of user evaluations in assessing the current assumptions and intuitions on effective explanations.},
  timestamp = {2025-05-03T07:48:29Z}
}

@misc{2022deep,
  title = {Deep Learning of Causal Structures in High Dimensions under Data Limitations {\textbar} {{Nature Machine Intelligence}}},
  year = {2022},
  month = dec,
  urldate = {2024-12-29},
  howpublished = {https://www.nature.com/articles/s42256-023-00744-z\#code-availability},
  langid = {american},
  timestamp = {2025-02-02T09:38:16Z}
}

@article{2022information,
  title = {Information Fusion as an Integrative Cross-Cutting Enabler to Achieve Robust, Explainable, and Trustworthy Medical Artificial Intelligence},
  year = {2022},
  month = mar,
  journal = {Information Fusion},
  volume = {79},
  pages = {263--278},
  publisher = {Elsevier},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.10.007},
  urldate = {2025-03-25},
  abstract = {Medical artificial intelligence (AI) systems have been remarkably successful, even outperforming human performance at certain tasks. There is no doubt{\dots}},
  langid = {american},
  annotation = {TLDR: The goal of this paper is to motivate how information fusion in a comprehensive and integrative manner can not only help bring these three areas together, but also have a transformative role by bridging the gap between research and practical applications in the context of future trustworthy medical AI.},
  timestamp = {2025-04-13T07:40:13Z}
}

@article{2023assessing,
  title = {Assessing the Communication Gap between {{AI}} Models and Healthcare Professionals: {{Explainability}}, Utility and Trust in {{AI-driven}} Clinical Decision-Making},
  shorttitle = {Assessing the Communication Gap between {{AI}} Models and Healthcare Professionals},
  year = {2023},
  month = mar,
  journal = {Artificial Intelligence},
  volume = {316},
  pages = {103839},
  publisher = {Elsevier},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2022.103839},
  urldate = {2025-03-03},
  abstract = {This paper contributes with a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support. The study rev{\dots}},
  langid = {american},
  annotation = {TLDR: New significant positive effects are found which repositions the role of explanations within a clinical context: these include reduction of automation bias, addressing ambiguous clinical cases and support of less experienced HCPs in the acquisition of new domain knowledge.},
  timestamp = {2025-04-16T08:49:21Z}
}

@article{2023local,
  title = {From Local Counterfactuals to Global Feature Importance: Efficient, Robust, and Model-Agnostic Explanations for Brain Connectivity Networks},
  shorttitle = {From Local Counterfactuals to Global Feature Importance},
  year = {2023},
  month = jun,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {236},
  pages = {107550},
  publisher = {Elsevier},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2023.107550},
  urldate = {2025-04-02},
  abstract = {Background: Explainable artificial intelligence (XAI) is a technology that can enhance trust in mental state classifications by providing explanations{\dots}},
  langid = {american},
  annotation = {TLDR: A new procedure for computing global feature importance that involves aggregating local counterfactual explanations and is specifically tailored to fMRI signals, which shows that the proposed BoCSoR measure is more robust to feature correlation and less computationally expensive than state-of-the-art methods.},
  timestamp = {2025-04-16T09:01:17Z}
}

@misc{2024bridging,
  title = {Bridging the {{Gap Between Black Box AI}} and {{Clinical Practice}}: {{Advancing Explainable AI}} for {{Trust}}, {{Ethics}}, and {{Personalized Healthcare Diagnostics}}},
  shorttitle = {Bridging the {{Gap Between Black Box AI}} and {{Clinical Practice}}},
  year = {2024},
  month = sep,
  journal = {Sciety},
  urldate = {2025-04-13},
  abstract = {Explainable AI (XAI) has emerged as a pivotal tool in healthcare diagnostics, offering much-needed transparency and interpretability in complex AI models. XAI techniques, such as SHAP, Grad-CAM, and LIME, enable clinicians to understand AI-driven decisions, fostering greater trust and collaboration between human and machine in clinical settings. This review explores the key benefits of XAI in enhancing diagnostic accuracy, personalizing patient care, and ensuring compliance with regulatory standards. However, despite its advantages, XAI faces significant challenges, including balancing model accuracy with interpretability, scaling for real-time clinical use, and mitigating biases inherent in medical data. Ethical concerns, particularly surrounding fairness and accountability, are also discussed in relation to AI's growing role in healthcare. The review emphasizes the importance of developing hybrid models that combine high accuracy with improved interpretability and suggests that future research should focus on explainable-by-design systems, reducing computational costs, and addressing ethical issues. As AI continues to integrate into healthcare, XAI will play an essential role in ensuring that AI systems are transparent, accountable, and aligned with the ethical standards required in clinical practice.},
  howpublished = {https://sciety.org/articles/activity/10.20944/preprints202409.1974.v2},
  langid = {english},
  timestamp = {2025-04-13T16:10:39Z}
}

@article{2024impact,
  title = {Impact of Example-Based {{XAI}} for Neural Networks on Trust, Understanding, and Performance},
  year = {2024},
  month = aug,
  journal = {International Journal of Human-Computer Studies},
  volume = {188},
  pages = {103277},
  publisher = {Academic Press},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2024.103277},
  urldate = {2025-05-03},
  abstract = {The purpose of this study is to examine the impact of an example-based explainable artificial intelligence (XAI) interface on trust, understanding, an{\dots}},
  langid = {american},
  timestamp = {2025-05-03T07:46:18Z}
}

@article{2024posthoc,
  ids = {2024posthoca},
  title = {Post-Hoc vs Ante-Hoc Explanations: {{xAI}} Design Guidelines for Data Scientists},
  shorttitle = {Post-Hoc vs Ante-Hoc Explanations},
  year = {2024},
  month = aug,
  journal = {Cognitive Systems Research},
  volume = {86},
  pages = {101243},
  publisher = {Elsevier},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2024.101243},
  urldate = {2025-03-29},
  abstract = {The growing field of explainable Artificial Intelligence (xAI) has given rise to a multitude of techniques and methodologies, yet this expansion has c{\dots}},
  langid = {american},
  timestamp = {2025-04-08T12:08:06Z}
}

@article{2025impact,
  title = {Impact on Clinical Guideline Adherence of {{Orient-COVID}}, a Clinical Decision Support System Based on Dynamic Decision Trees for {{COVID19}} Management: {{A}} Randomized Simulation Trial with Medical Trainees},
  shorttitle = {Impact on Clinical Guideline Adherence of {{Orient-COVID}}, a Clinical Decision Support System Based on Dynamic Decision Trees for {{COVID19}} Management},
  year = {2025},
  month = mar,
  journal = {International Journal of Medical Informatics},
  volume = {195},
  pages = {105772},
  publisher = {Elsevier},
  issn = {1386-5056},
  doi = {10.1016/j.ijmedinf.2024.105772},
  urldate = {2025-05-04},
  abstract = {The adherence of clinicians to clinical practice guidelines is known to be low, including for the management of COVID-19, due to their difficult use a{\dots}},
  langid = {american},
  annotation = {TLDR: An innovative visual interface to allow clinicians easily navigating inside decision trees for the management of COVID-19 patients significantly improved the clinician adherence to guidelines.},
  timestamp = {2025-05-04T00:46:11Z}
}

@article{2025scientific,
  title = {From Scientific Theory to Duality of Predictive Artificial Intelligence Models},
  year = {2025},
  month = apr,
  journal = {Cell Reports Physical Science},
  volume = {6},
  number = {4},
  pages = {102516},
  publisher = {Cell Press},
  issn = {2666-3864},
  doi = {10.1016/j.xcrp.2025.102516},
  urldate = {2025-05-04},
  abstract = {In studies employing explainable artificial intelligence (XAI), model explanation, interpretation, and causality are often not clearly distinguished, {\dots}},
  langid = {american},
  timestamp = {2025-05-04T13:09:50Z}
}

@inproceedings{abtahi2024privacypreserving,
  title = {Privacy-{{Preserving Federated Interpretability}}},
  booktitle = {2024 {{IEEE International Conference}} on {{Big Data}} ({{BigData}})},
  author = {Abtahi, Azra and Aminifar, Amin and Aminifar, Amir},
  year = {2024},
  month = dec,
  pages = {7592--7601},
  publisher = {IEEE},
  address = {Washington, DC, USA},
  doi = {10.1109/BigData62323.2024.10825590},
  urldate = {2025-05-21},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-6248-0},
  annotation = {TLDR: This paper proposes a federated interpretability scheme based on SHAP (SHapley Additive exPlanations) value and DeepLIFT (Deep Learning Important FeaTures) to interpret ML models, without sharing sensitive data and in a privacy-preserving fashion.},
  timestamp = {2025-05-21T02:32:31Z}
}

@article{achour2001umlsbased,
  title = {A {{UMLS-based Knowledge Acquisition Tool}} for {{Rule-based Clinical Decision Support System Development}}},
  author = {Achour, Soumeya L. and Dojat, Michel and Rieux, Claire and Bierling, Philippe and Lepage, Eric},
  year = {2001},
  month = jul,
  journal = {Journal of the American Medical Informatics Association},
  volume = {8},
  number = {4},
  pages = {351--360},
  issn = {1067-5027},
  doi = {10.1136/jamia.2001.0080351},
  urldate = {2025-04-03},
  abstract = {Decision support systems in the medical field have to be easily modified by medical experts themselves. The authors have designed a knowledge acquisition tool to facilitate the creation and maintenance of a knowledge base by the domain expert and its sharing and reuse by other institutions. The Unified Medical Language System (UMLS) contains the domain entities and constitutes the relations repository from which the expert builds, through a specific browser, the explicit domain ontology. The expert is then guided in creating the knowledge base according to the pre-established domain ontology and condition--action rule templates that are well adapted to several clinical decision-making processes. Corresponding medical logic modules are eventually generated. The application of this knowledge acquisition tool to the construction of a decision support system in blood transfusion demonstrates the value of such a pragmatic methodology for the design of rule-based clinical systems that rely on the highly progressive knowledge embedded in hospital information systems.},
  annotation = {TLDR: The application of this knowledge acquisition tool to the construction of a decision support system in blood transfusion demonstrates the value of such a pragmatic methodology for the design of rule-based clinical systems that rely on the highly progressive knowledge embedded in hospital information systems.},
  timestamp = {2025-04-03T02:21:38Z}
}

@article{achtibat2023attribution,
  title = {From Attribution Maps to Human-Understandable Explanations through {{Concept Relevance Propagation}}},
  author = {Achtibat, Reduan and Dreyer, Maximilian and Eisenbraun, Ilona and Bosse, Sebastian and Wiegand, Thomas and Samek, Wojciech and Lapuschkin, Sebastian},
  year = {2023},
  month = sep,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {9},
  pages = {1006--1019},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00711-8},
  urldate = {2025-05-17},
  abstract = {The field of explainable artificial intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in the form of attribution maps, thereby identifying `where' important features occur (but not providing information about `what' they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of method thus provide only partial insights and leave the burden of interpreting the model's reasoning to the user. Here we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the `where' and `what' questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model's representation and reasoning through concept atlases, concept-composition analyses, and quantitative investigations of concept subspaces and their role in fine-grained decision-making.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computer science,Statistics},
  annotation = {TLDR: The authors propose a concept-level explanation method that bridges the local and global perspectives, enabling more comprehensive and human-understandable explanations.},
  timestamp = {2025-05-17T13:57:12Z}
}

@article{acosta2022multimodal,
  title = {Multimodal Biomedical {{AI}}},
  author = {Acosta, Juli{\'a}n N. and Falcone, Guido J. and Rajpurkar, Pranav and Topol, Eric J.},
  year = {2022},
  month = sep,
  journal = {Nature Medicine},
  volume = {28},
  number = {9},
  pages = {1773--1784},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-022-01981-2},
  urldate = {2025-03-03},
  abstract = {The increasing availability of biomedical data from large biobanks, electronic health records, medical imaging, wearable and ambient biosensors, and the lower cost of genome and microbiome sequencing have set the stage for the development of multimodal artificial intelligence solutions that capture the complexity of human health and disease. In this Review, we outline the key applications enabled, along with the technical and analytical challenges. We explore opportunities in personalized medicine, digital clinical trials, remote monitoring and care, pandemic surveillance, digital twin technology and virtual health assistants. Further, we survey the data, modeling and privacy challenges that must be overcome to realize the full potential of multimodal artificial intelligence in health.},
  copyright = {2022 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Health care},
  annotation = {TLDR: This Review outlines the most promising uses and the technical pitfalls to avoid of multimodal artificial intelligence in health, and explores opportunities in personalized medicine, digital clinical trials, remote monitoring and care, pandemic surveillance, digital twin technology and virtual health assistants.},
  timestamp = {2025-03-03T07:30:49Z}
}

@article{adadi2018peeking,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  urldate = {2025-06-10},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  keywords = {Biological system modeling,black-box models,Conferences,Explainable artificial intelligence,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms},
  annotation = {TLDR: This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI, and review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  timestamp = {2025-06-10T03:53:35Z}
}

@article{adeniran2024explainable,
  title = {Explainable {{AI}} ({{XAI}}) in Healthcare: {{Enhancing}} Trust and Transparency in Critical Decision-Making},
  author = {Adeniran, Adewale Abayomi and Onebunne, Amaka Peace and William, Paul},
  year = {2024},
  journal = {World J. Adv. Res. Rev},
  volume = {23},
  pages = {2647--2658},
  timestamp = {2025-04-15T14:20:49Z}
}

@misc{agarwal2020causal,
  title = {Towards {{Causal VQA}}: {{Revealing}} and {{Reducing Spurious Correlations}} by {{Invariant}} and {{Covariant Semantic Editing}}},
  shorttitle = {Towards {{Causal VQA}}},
  author = {Agarwal, Vedika and Shetty, Rakshith and Fritz, Mario},
  year = {2020},
  month = may,
  number = {arXiv:1912.07538},
  eprint = {1912.07538},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.07538},
  urldate = {2025-03-20},
  abstract = {Despite significant success in Visual Question Answering (VQA), VQA models have been shown to be notoriously brittle to linguistic variations in the questions. Due to deficiencies in models and datasets, today's models often rely on correlations rather than predictions that are causal w.r.t. data. In this paper, we propose a novel way to analyze and measure the robustness of the state of the art models w.r.t semantic visual variations as well as propose ways to make models more robust against spurious correlations. Our method performs automated semantic image manipulations and tests for consistency in model predictions to quantify the model robustness as well as generate synthetic data to counter these problems. We perform our analysis on three diverse, state of the art VQA models and diverse question types with a particular focus on challenging counting questions. In addition, we show that models can be made significantly more robust against inconsistent predictions using our edited data. Finally, we show that results also translate to real-world error cases of state of the art models, which results in improved overall performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  timestamp = {2025-03-20T11:44:00Z}
}

@article{ahmed2024efficient,
  title = {Efficient Differential Privacy Enabled Federated Learning Model for Detecting {{COVID-19}} Disease Using Chest {{X-ray}} Images},
  author = {Ahmed, Rawia and Maddikunta, Praveen Kumar Reddy and Gadekallu, Thippa Reddy and Alshammari, Naif Khalaf and Hendaoui, Fatma Ali},
  year = {2024},
  month = jun,
  journal = {Frontiers in Medicine},
  volume = {11},
  pages = {1409314},
  issn = {2296-858X},
  doi = {10.3389/fmed.2024.1409314},
  urldate = {2025-04-12},
  abstract = {The rapid spread of COVID-19 pandemic across the world has not only disturbed the global economy but also raised the demand for accurate disease detection models. Although many studies have proposed effective solutions for the early detection and prediction of COVID-19 with Machine Learning (ML) and Deep learning (DL) based techniques, but these models remain vulnerable to data privacy and security breaches. To overcome the challenges of existing systems, we introduced Adaptive Differential Privacy-based Federated Learning (DPFL) model for predicting COVID-19 disease from chest X-ray images which introduces an innovative adaptive mechanism that dynamically adjusts privacy levels based on real-time data sensitivity analysis, improving the practical applicability of Federated Learning (FL) in diverse healthcare environments. We compared and analyzed the performance of this distributed learning model with a traditional centralized model. Moreover, we enhance the model by integrating a FL approach with an early stopping mechanism to achieve efficient COVID-19 prediction with minimal communication overhead. To ensure privacy without compromising model utility and accuracy, we evaluated the proposed model under various noise scales. Finally, we discussed strategies for increasing the model's accuracy while maintaining robustness as well as privacy.},
  pmcid = {PMC11193384},
  pmid = {38912338},
  annotation = {TLDR: Adaptive Differential Privacy-based Federated Learning (DPFL) model for predicting COVID-19 disease from chest X-ray images is introduced which introduces an innovative adaptive mechanism that dynamically adjusts privacy levels based on real-time data sensitivity analysis, improving the practical applicability of Federated Learning (FL) in diverse healthcare environments.},
  timestamp = {2025-04-12T06:15:34Z}
}

@misc{aix360-github,
  title = {{{AI}} Explainability 360 ({{AIX360}})},
  author = {{IBM Research}},
  year = {2019},
  timestamp = {2025-04-12T13:11:01Z}
}

@misc{aix360-sept-2019,
  title = {One Explanation Does Not Fit All: A Toolkit and Taxonomy of {{AI}} Explainability Techniques},
  author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilovi{\'c}, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
  year = {2019},
  month = sep,
  timestamp = {2025-04-12T13:07:34Z}
}

@misc{alaa2017deep,
  title = {Deep {{Counterfactual Networks}} with {{Propensity-Dropout}}},
  author = {Alaa, Ahmed M. and Weisz, Michael and van der Schaar, Mihaela},
  year = {2017},
  month = jun,
  number = {arXiv:1706.05966},
  eprint = {1706.05966},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.05966},
  urldate = {2025-03-20},
  abstract = {We propose a novel approach for inferring the individualized causal effects of a treatment (intervention) from observational data. Our approach conceptualizes causal inference as a multitask learning problem; we model a subject's potential outcomes using a deep multitask network with a set of shared layers among the factual and counterfactual outcomes, and a set of outcome-specific layers. The impact of selection bias in the observational data is alleviated via a propensity-dropout regularization scheme, in which the network is thinned for every training example via a dropout probability that depends on the associated propensity score. The network is trained in alternating phases, where in each phase we use the training examples of one of the two potential outcomes (treated and control populations) to update the weights of the shared layers and the respective outcome-specific layers. Experiments conducted on data based on a real-world observational study show that our algorithm outperforms the state-of-the-art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-20T08:05:05Z}
}

@article{alaa2024large,
  title = {Large Language Models as Co-Pilots for Causal Inference in Medical Studies},
  author = {Alaa, Ahmed and Phillips, Rachael V and K{\i}c{\i}man, Emre and Balzer, Laura B and {van der Laan}, Mark and Petersen, Maya},
  year = {2024},
  journal = {arXiv preprint arXiv:2407.19118},
  eprint = {2407.19118},
  archiveprefix = {arXiv},
  timestamp = {2025-04-15T16:25:21Z}
}

@article{alanvarghese2024explainable,
  title = {Explainable {{AI}} in {{Healthcare Applications}}},
  author = {{Alan Varghese} and {Jefin Varghese} and {Jubin Biju} and {Roshan Thomas} and {Merlin Thomas}},
  year = {2024},
  month = dec,
  journal = {International Research Journal on Advanced Engineering and Management (IRJAEM)},
  volume = {2},
  number = {12},
  pages = {3671--3679},
  issn = {2584-2854},
  doi = {10.47392/IRJAEM.2024.0545},
  urldate = {2025-04-02},
  abstract = {The entry of artificial intelligence into health care systems brings unprecedented advances in diagnosing, personalized treatment, and predictive analytics. Many of these AI models, especially the deep-learning algorithms, have been referred to as "black boxes" and raise gigantic questions about trust, transparency, and reliability in clinical settings. Therefore, explainable AI answers the challenges by drawing to the fore methodologies that make AI models more interpretable, thereby making them more accepted and usable in the fraternity of health. It engages with XAI in healthcare by scrutinizing various aspects of feature importance analysis, architectures of the interpretable model, and visual explication of decisions driven by AI. The case studies regarding applications of XAI in the fields of radiology, disease prediction, and personalized medicine illustrate how this technology has its own importance even in terms of improving the precision of diagnosis and clinician-patient communication. We are answerable to ethics, such as how to explain respect for the trust of patients, legalities in deploying XAI in healthcare, and further directions to be taken so that XAI does not lag behind the timeline of AI innovation. Our results repeat again that XAI indeed is the much-needed solution to be technically suitable and necessary for ensuring the responsible adoption of AI within healthcare systems to empower clinicians using not only accurate but also transparent, understandable, and aligned AI systems in clinical best practice. In conclusion, this paper concludes by highlighting explainable AI as one of the key enablers toward safe, effective, and widely accepted AI applications in health care.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0},
  langid = {american},
  timestamp = {2025-04-02T03:53:22Z}
}

@misc{alattal2025integrating,
  title = {Integrating {{Explainable AI}} in {{Medical Devices}}: {{Technical}}, {{Clinical}} and {{Regulatory Insights}} and {{Recommendations}}},
  shorttitle = {Integrating {{Explainable AI}} in {{Medical Devices}}},
  author = {Alattal, Dima and Azar, Asal Khoshravan and Myles, Puja and Branson, Richard and Abdulhussein, Hatim and Tucker, Allan},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2505.06620},
  urldate = {2025-06-13},
  abstract = {There is a growing demand for the use of Artificial Intelligence (AI) and Machine Learning (ML) in healthcare, particularly as clinical decision support systems to assist medical professionals. However, the complexity of many of these models, often referred to as black box models, raises concerns about their safe integration into clinical settings as it is difficult to understand how they arrived at their predictions. This paper discusses insights and recommendations derived from an expert working group convened by the UK Medicine and Healthcare products Regulatory Agency (MHRA). The group consisted of healthcare professionals, regulators, and data scientists, with a primary focus on evaluating the outputs from different AI algorithms in clinical decision-making contexts. Additionally, the group evaluated findings from a pilot study investigating clinicians' behaviour and interaction with AI methods during clinical diagnosis. Incorporating AI methods is crucial for ensuring the safety and trustworthiness of medical AI devices in clinical settings. Adequate training for stakeholders is essential to address potential issues, and further insights and recommendations for safely adopting AI systems in healthcare settings are provided.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,H.5.2,Human-Computer Interaction (cs.HC)},
  timestamp = {2025-06-13T11:31:59Z}
}

@misc{albuquerque2024characterizing,
  title = {Characterizing the {{Interpretability}} of {{Attention Maps}} in {{Digital Pathology}}},
  author = {Albuquerque, Tom{\'e} and Y{\"u}ce, Anil and Herrmann, Markus D. and Gomariz, Alvaro},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2407.02484},
  urldate = {2025-05-03},
  abstract = {Interpreting machine learning model decisions is crucial for high-risk applications like healthcare. In digital pathology, large whole slide images (WSIs) are decomposed into smaller tiles and tile-derived features are processed by attention-based multiple instance learning (ABMIL) models to predict WSI-level labels. These networks generate tile-specific attention weights, which can be visualized as attention maps for interpretability. However, a standardized evaluation framework for these maps is lacking, questioning their reliability and ability to detect spurious correlations that can mislead models. We herein propose a framework to assess the ability of attention networks to attend to relevant features in digital pathology by creating artificial model confounders and using dedicated interpretability metrics. Models are trained and evaluated on data with tile modifications correlated with WSI labels, enabling the analysis of model sensitivity to artificial confounders and the accuracy of attention maps in highlighting them. Confounders are introduced either through synthetic tile modifications or through tile ablations based on their specific image-based features, with the latter being used to assess more clinically relevant scenarios. We also analyze the impact of varying confounder quantities at both the tile and WSI levels. Our results show that ABMIL models perform as desired within our framework. While attention maps generally highlight relevant regions, their robustness is affected by the type and number of confounders. Our versatile framework has the potential to be used in the evaluation of various methods and the exploration of image-based features driving model predictions, which could aid in biomarker discovery.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV)},
  annotation = {TLDR: A framework to assess the ability of attention networks to attend to relevant features in digital pathology by creating artificial model confounders and using dedicated interpretability metrics is proposed and results show that ABMIL models perform as desired within this framework.},
  timestamp = {2025-05-03T08:44:27Z}
}

@misc{ali2021explainability,
  title = {Explainability {{Guided Multi-Site COVID-19 CT Classification}}},
  author = {Ali, Ameen and Shaharabany, Tal and Wolf, Lior},
  year = {2021},
  month = mar,
  number = {arXiv:2103.13677},
  eprint = {2103.13677},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.13677},
  urldate = {2025-05-17},
  abstract = {Radiologist examination of chest CT is an effective way for screening COVID-19 cases. In this work, we overcome three challenges in the automation of this process: (i) the limited number of supervised positive cases, (ii) the lack of region-based supervision, and (iii) the variability across acquisition sites. These challenges are met by incorporating a recent augmentation solution called SnapMix, by a new patch embedding technique, and by performing a test-time stability analysis. The three techniques are complementary and are all based on utilizing the heatmaps produced by the Class Activation Mapping (CAM) explainability method. Compared to the current state of the art, we obtain an increase of five percent in the F1 score on a site with a relatively high number of cases, and a gap twice as large for a site with much fewer training images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  timestamp = {2025-05-17T10:49:52Z}
}

@misc{aliee2023conditionally,
  title = {Conditionally {{Invariant Representation Learning}} for {{Disentangling Cellular Heterogeneity}}},
  author = {Aliee, Hananeh and Kapl, Ferdinand and {Hediyeh-Zadeh}, Soroor and Theis, Fabian J.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.00558},
  eprint = {2307.00558},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.00558},
  urldate = {2025-05-17},
  abstract = {This paper presents a novel approach that leverages domain variability to learn representations that are conditionally invariant to unwanted variability or distractors. Our approach identifies both spurious and invariant latent features necessary for achieving accurate reconstruction by placing distinct conditional priors on latent features. The invariant signals are disentangled from noise by enforcing independence which facilitates the construction of an interpretable model with a causal semantic. By exploiting the interplay between data domains and labels, our method simultaneously identifies invariant features and builds invariant predictors. We apply our method to grand biological challenges, such as data integration in single-cell genomics with the aim of capturing biological variations across datasets with many samples, obtained from different conditions or multiple laboratories. Our approach allows for the incorporation of specific biological mechanisms, including gene programs, disease states, or treatment conditions into the data integration process, bridging the gap between the theoretical assumptions and real biological applications. Specifically, the proposed approach helps to disentangle biological signals from data biases that are unrelated to the target task or the causal explanation of interest. Through extensive benchmarking using large-scale human hematopoiesis and human lung cancer data, we validate the superiority of our approach over existing methods and demonstrate that it can empower deeper insights into cellular heterogeneity and the identification of disease cell states.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  annotation = {TLDR: This paper presents a novel approach that leverages domain variability to learn representations that are conditionally invariant to unwanted variability or distractors and helps to disentangle biological signals from data biases that are unrelated to the target task or the causal explanation of interest.},
  timestamp = {2025-05-17T07:05:48Z}
}

@inproceedings{alizadeh2020explainable,
  title = {{{eXplainable AI}}: Take One Step Back, Move Two Steps Forward},
  booktitle = {Mensch Und Computer 2020-Workshopband},
  author = {Alizadeh, Fatemeh and Esau, Margarita and Stevens, Gunnar and Cassens, Lena},
  year = {2020},
  pages = {10--18420},
  publisher = {Gesellschaft f{\"u}r Informatik eV},
  timestamp = {2025-05-17T12:44:39Z}
}

@article{alkhanbouli2025role,
  title = {The Role of Explainable Artificial Intelligence in Disease Prediction: A Systematic Literature Review and Future Research Directions},
  shorttitle = {The Role of Explainable Artificial Intelligence in Disease Prediction},
  author = {Alkhanbouli, Razan and Matar Abdulla Almadhaani, Hour and Alhosani, Farah and Simsekler, Mecit Can Emre},
  year = {2025},
  month = mar,
  journal = {BMC medical informatics and decision making},
  volume = {25},
  number = {1},
  pages = {110},
  issn = {1472-6947},
  doi = {10.1186/s12911-025-02944-6},
  abstract = {Explainable Artificial Intelligence (XAI) enhances transparency and interpretability in AI models, which is crucial for trust and accountability in healthcare. A potential application of XAI is disease prediction using various data modalities. This study conducts a Systematic Literature Review (SLR) following the PRISMA protocol, synthesizing findings from 30 selected studies to examine XAI's evolving role in disease prediction. It explores commonly used XAI methods, such as Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME), and their impact across medical fields in disease prediction. The review highlights key gaps, including limited dataset diversity, model complexity, and reliance on single data types, emphasizing the need for greater interpretability and data integration. Addressing these issues is crucial for advancing AI in healthcare. This study contributes by outlining current challenges and potential solutions, suggesting directions for future research to develop more reliable and robust XAI methods.},
  langid = {english},
  pmcid = {PMC11877768},
  pmid = {40038704},
  keywords = {Artificial Intelligence,Decision support systems,Diagnosis,Disease,Disease prediction,Disease recognition,Explainable artificial intelligence,Healthcare AI,Humans,Machine learning,Patient safety,Risk management,XAI},
  annotation = {TLDR: A Systematic Literature Review following the PRISMA protocol is conducted, synthesizing findings from 30 selected studies to examine XAI's evolving role in disease prediction, and highlights key gaps, including limited dataset diversity, model complexity, and reliance on single data types.},
  timestamp = {2025-05-28T00:27:11Z}
}

@article{allgaier2023how,
  title = {How Does the Model Make Predictions? {{A}} Systematic Literature Review on the Explainability Power of Machine Learning in Healthcare},
  shorttitle = {How Does the Model Make Predictions?},
  author = {Allgaier, Johannes and Mulansky, Lena and Draelos, Rachel Lea and Pryss, R{\"u}diger},
  year = {2023},
  month = sep,
  journal = {Artificial Intelligence in Medicine},
  volume = {143},
  pages = {102616},
  issn = {09333657},
  doi = {10.1016/j.artmed.2023.102616},
  urldate = {2025-09-08},
  langid = {english},
  annotation = {TLDR: An overview of the taxonomy of explainability methods is provided and popular methods are reviewed to investigate which explainable artificial intelligence (XAI) methods are used in 450 specific medical supervised ML use cases and how the precision of describing ML pipelines has evolved over the past 20~years.},
  timestamp = {2025-09-08T10:23:25Z}
}

@article{almet2024inferring,
  ids = {almet2024inferringa},
  title = {Inferring Pattern-Driving Intercellular Flows from Single-Cell and Spatial Transcriptomics},
  author = {Almet, Axel A. and Tsai, Yuan-Chen and Watanabe, Momoko and Nie, Qing},
  year = {2024},
  month = oct,
  journal = {Nature Methods},
  volume = {21},
  number = {10},
  pages = {1806--1817},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02380-w},
  urldate = {2024-10-23},
  abstract = {From single-cell RNA-sequencing (scRNA-seq) and spatial transcriptomics (ST), one can extract high-dimensional gene expression patterns that can be described by intercellular communication networks or decoupled gene modules. These two descriptions of information flow are often assumed to occur independently. However, intercellular communication drives directed flows of information that are mediated by intracellular gene modules, in turn triggering outflows of other signals. Methodologies to describe such intercellular flows are lacking. We present FlowSig, a method that infers communication-driven intercellular flows from scRNA-seq or ST data using graphical causal modeling and conditional independence. We benchmark FlowSig using newly generated experimental cortical organoid data and synthetic data generated from mathematical modeling. We demonstrate FlowSig's utility by applying it to various studies, showing that FlowSig can capture stimulation-induced changes to paracrine signaling in pancreatic islets, demonstrate shifts in intercellular flows due to increasing COVID-19 severity and reconstruct morphogen-driven activator--inhibitor patterns in mouse embryogenesis.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Cellular signalling networks,Computational models,Software,Transcriptomics},
  annotation = {TLDR: FlowSig is presented, a method that infers communication-driven intercellular flows from scRNA-seq or ST data using graphical causal modeling and conditional independence and its utility is demonstrated by applying it to various studies.},
  timestamp = {2025-04-21T02:51:50Z}
}

@inproceedings{alomar2023lung,
  title = {Lung {{Cancer Detection Using Deep Learning}} and {{Explainable Methods}}},
  booktitle = {2023 14th {{International Conference}} on {{Information}} and {{Communication Systems}} ({{ICICS}})},
  author = {Alomar, Ayah and Alazzam, Moayed and Mustafa, Hala and Mustafa, Ahmad},
  year = {2023},
  month = nov,
  pages = {1--4},
  publisher = {IEEE},
  address = {Irbid, Jordan},
  issn = {2573-3346},
  doi = {10.1109/ICICS60529.2023.10330443},
  urldate = {2025-05-06},
  abstract = {Lung cancer is one of the most prevalent deadly diseases and it can extend to the rest of the human body. One way to detect it in CT scan images is by using deep learning models. Explaining these models by XAI techniques and radiologists make the results trusted for medical use. In this paper, the deep learning models inceptionV3and ResNet50 were used to classify CT scans of lungs for the presence of cancer. The models were trained on a Kaggle dataset that is pre-processed and augmented. It was able to accurately detect lung cancer in new patients. Additionally, an XAI model was used to explain the decision-making process of the deep learning model, providing insights into which features of the CT scan were most important for the model's diagnosis. ResNet50 achieved the highest performance with 100\% accuracy on testing images, and InceptionV3 got 99.92\%. LIME and GRAD-CAM explained the model's performance by highlighting the most important features for each model. On the other hand, radiologists provided insight on whether the deep learning and XAI models are correct by determining the cancer region in each image and point out the model's misclassification. The results show that the radiologist's diagnosis is important even though the model's accuracy is high.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0786-3},
  langid = {american},
  keywords = {Artificial Intelligence,Biological system modeling,Communication systems,Computed tomography,CT scan,Decision making,Deep learning,Explain-ability,Lung,lung cancer,Lung cancer,radiologist's diagnosis},
  annotation = {TLDR: The results show that the radiologist's diagnosis is important even though the model's accuracy is high, and the deep learning models inceptionV3and ResNet50 were used to classify CT scans of lungs for the presence of cancer.},
  timestamp = {2025-05-06T04:04:58Z}
}

@article{alpert2025pharmacogenomics,
  title = {Pharmacogenomics: {{Implementation}} of {{Precision Medicine}}},
  shorttitle = {Pharmacogenomics},
  author = {Alpert, Joseph S. and Chen, Qin M. and Mulyar, Oleh Alex and Acharya, Deepak and Hung, Olivia and Rajendran, Iniya and Lim, Jan and Freeman, Ryan and Fedorova, Larissa},
  year = {2025},
  month = mar,
  journal = {The American Journal of Medicine},
  volume = {0},
  number = {0},
  publisher = {Elsevier},
  issn = {0002-9343, 1555-7162},
  doi = {10.1016/j.amjmed.2025.03.011},
  urldate = {2025-05-28},
  langid = {english},
  pmid = {40090392},
  timestamp = {2025-05-28T00:03:55Z}
}

@article{altan2022deepoct,
  title = {{{DeepOCT}}: {{An}} Explainable Deep Learning Architecture to Analyze Macular Edema on {{OCT}} Images},
  shorttitle = {{{DeepOCT}}},
  author = {Altan, Gokhan},
  year = {2022},
  month = oct,
  journal = {Engineering Science and Technology, an International Journal},
  volume = {34},
  pages = {101091},
  publisher = {Elsevier BV},
  issn = {2215-0986},
  doi = {10.1016/j.jestch.2021.101091},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  timestamp = {2025-07-25T14:23:17Z}
}

@inproceedings{amershi2019guidelines,
  title = {Guidelines for Human-{{AI}} Interaction},
  booktitle = {Proceedings of the 2019 Chi Conference on Human Factors in Computing Systems},
  author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N and Inkpen, Kori and others},
  year = {2019},
  pages = {1--13},
  timestamp = {2025-03-28T08:10:39Z}
}

@article{amjad2023attention,
  title = {Attention-Based Explainability Approaches in Healthcare Natural Language Processing.},
  author = {Amjad, Haadia and Ashraf, Mohammad Shehroz and Sherazi, Syed Zoraiz Ali and Khan, Saad and Fraz, Muhammad Moazam and Hameed, Tahir and Bukhari, Syed Ahmad Chan},
  year = {2023},
  journal = {HEALTHINF},
  pages = {689--696},
  timestamp = {2025-04-15T14:26:53Z}
}

@article{an2021medical,
  title = {Medical {{Image Classification Algorithm Based}} on {{Visual Attention Mechanism}}-{{MCNN}}},
  author = {An, Fengping and Li, Xiaowei and Ma, Xingmin},
  editor = {Lorenzini, Antonello},
  year = {2021},
  month = jan,
  journal = {Oxidative Medicine and Cellular Longevity},
  volume = {2021},
  number = {1},
  pages = {6280690},
  issn = {1942-0900, 1942-0994},
  doi = {10.1155/2021/6280690},
  urldate = {2025-05-20},
  abstract = {Due to the complexity of medical images, traditional medical image classification methods have been unable to meet the actual application needs. In recent years, the rapid development of deep learning theory has provided a technical approach for solving medical image classification. However, deep learning has the following problems in the application of medical image classification. First, it is impossible to construct a deep learning model with excellent performance according to the characteristics of medical images. Second, the current deep learning network structure and training strategies are less adaptable to medical images. Therefore, this paper first introduces the visual attention mechanism into the deep learning model so that the information can be extracted more effectively according to the problem of medical images, and the reasoning is realized at a finer granularity. It can increase the interpretability of the model. Additionally, to solve the problem of matching the deep learning network structure and training strategy to medical images, this paper will construct a novel multiscale convolutional neural network model that can automatically extract high-level discriminative appearance features from the original image, and the loss function uses the Mahalanobis distance optimization model to obtain a better training strategy, which can improve the robust performance of the network model. The medical image classification task is completed by the above method. Based on the above ideas, this paper proposes a medical classification algorithm based on a visual attention mechanism-multiscale convolutional neural network. The lung nodules and breast cancer images were classified by the method in this paper. The experimental results show that the accuracy of medical image classification in this paper is not only higher than that of traditional machine learning methods but also improved compared with other deep learning methods, and the method has good stability and robustness.},
  langid = {english},
  annotation = {TLDR: The visual attention mechanism is introduced into the deep learning model so that the information can be extracted more effectively according to the problem of medical images, and the reasoning is realized at a finer granularity to increase the interpretability of the model.},
  timestamp = {2025-05-20T22:46:54Z}
}

@article{anders2022finding,
  title = {Finding and Removing {{Clever Hans}}: {{Using}} Explanation Methods to Debug and Improve Deep Models},
  shorttitle = {Finding and Removing {{Clever Hans}}},
  author = {Anders, Christopher J. and Weber, Leander and Neumann, David and Samek, Wojciech and M{\"u}ller, Klaus-Robert and Lapuschkin, Sebastian},
  year = {2022},
  month = jan,
  journal = {Information Fusion},
  volume = {77},
  pages = {261--295},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.07.015},
  urldate = {2025-05-16},
  langid = {english},
  annotation = {TLDR: This paper provides a comprehensive analysis framework based on a scalable statistical analysis of attributions from explanation methods for large data corpora, and proposes several approaches denoted as Class Artifact Compensation (ClArC), which are able to effectively and significantly reduce a model's CH behavior.},
  timestamp = {2025-05-16T12:26:27Z}
}

@article{anderson2020external,
  title = {External {{Validation}} of {{PATHFx Version}} 3.0 in {{Patients Treated Surgically}} and {{Nonsurgically}} for {{Symptomatic Skeletal Metastases}}},
  author = {Anderson, Ashley B. and Wedin, Rikard and Fabbri, Nicola and Boland, Patrick and Healey, John and Forsberg, Jonathan A.},
  year = {2020},
  month = apr,
  journal = {Clinical Orthopaedics and Related Research{\textregistered}},
  volume = {478},
  number = {4},
  pages = {808},
  issn = {0009-921X},
  doi = {10.1097/CORR.0000000000001081},
  urldate = {2025-04-03},
  abstract = {Background~           PATHFx is a clinical decision-support tool based on machine learning capable of estimating the likelihood of survival after surgery for patients with skeletal metastases. The applicability of any machine-learning tool depends not only on successful external validation in unique patient populations but also on remaining relevant as more effective systemic treatments are introduced. With advancements in the treatment of metastatic disease, it is our responsibility to patients to ensure clinical support tools remain contemporary and accurate.           Question/purposes~           Therefore, we sought to (1) generate updated PATHFx models using recent data from patients treated at one large, urban tertiary referral center and (2) externally validate the models using two contemporary patient populations treated either surgically or nonsurgically with external-beam radiotherapy alone for symptomatic skeletal metastases for symptomatic lesions.           Methods~           After obtaining institutional review board approval, we collected data on 208 patients undergoing surgical treatment for pathologic fractures at Memorial Sloan Kettering Cancer Center between 2015 and 2018. These data were combined with the original PATHFx training set (n = 189) to create the final training set (n = 397). We then created six Bayesian belief networks designed to estimate the likelihood of 1-month, 3-month, 6-month, 12-month, 18-month, and 24-month survival after treatment. Bayesian belief analysis is a statistical method that allows data-driven learning to arise from conditional probabilities by exploring relationships between variables to estimate the likelihood of an outcome using observed data. For external validation, we extracted the records of patients treated between 2016 and 2018 from the International Bone Metastasis Registry and records of patients treated nonoperatively with external-beam radiation therapy for symptomatic skeletal metastases from 2012 to 2016 using the Military Health System Data Repository (radiotherapy-only group). From each record, we collected the date of treatment, laboratory values at the time of treatment initiation, demographic data, details of diagnosis, and the date of death. All records reported sufficient follow-up to establish survival (yes/no) at 24-months after treatment. For external validation, we applied the data from each record to the new PATHFx models. We assessed calibration (calibration plots), accuracy (Brier score), discriminatory ability (area under the receiver operating characteristic curve [AUC]).           Results~           The updated PATHFx version 3.0 models successfully classified survival at each time interval in both external validation sets and demonstrated appropriate discriminatory ability and model calibration. The Bayesian models were reasonably calibrated to the Memorial Sloan Kettering Cancer Center training set. External validation with 197 records from the International Bone Metastasis Registry and 192 records from the Military Health System Data Repository for analysis found Brier scores that were all less than 0.20, with upper bounds of the 95\% confidence intervals all less than 0.25, both for the radiotherapy-only and International Bone Metastasis Registry groups. Additionally, AUC estimates were all greater than 0.70, with lower bounds of the 95\% CI all greater than 0.68, except for the 1-month radiotherapy-only group. To complete external validation, decision curve analysis demonstrated clinical utility. This means it was better to use the PATHFx models when compared to the default assumption that all or no patients would survive at all time periods except for the 1-month models. We believe the favorable Brier scores ({$<$} 0.20) as well as DCA indicate these models are suitable for clinical use.           Conclusions~           We successfully updated PATHFx using contemporary data from patients undergoing either surgical or nonsurgical treatment for symptomatic skeletal metastases. These models have been incorporated for clinical use on PATHFx version 3.0 (https://www.pathfx.org). Clinically, external validation suggests it is better to use PATHFx version 3.0 for all time periods except when deciding whether to give radiotherapy to patients with the life expectancy of less than 1 month. This is partly because most patients survived 1-month after treatment. With the advancement of medical technology in treatment and diagnosis for patients with metastatic bone disease, part of our fiduciary responsibility is to the main current clinical support tools.           Level of Evidence~           Level III, therapeutic study.},
  langid = {american},
  annotation = {TLDR: Clinically, external validation suggests it is better to use PATHFx version 3.0 for all time periods except when deciding whether to give radiotherapy to patients with the life expectancy of less than 1 month.},
  timestamp = {2025-04-03T15:14:16Z}
}

@article{anguita-ruiz2020explainable,
  ids = {anguita-ruiz2020explainablea},
  title = {{{eXplainable Artificial Intelligence}} ({{XAI}}) for the Identification of Biologically Relevant Gene Expression Patterns in Longitudinal Human Studies, Insights from Obesity Research},
  author = {{Anguita-Ruiz}, Augusto and {Segura-Delgado}, Alberto and Alcal{\'a}, Rafael and Aguilera, Concepci{\'o}n M. and {Alcal{\'a}-Fdez}, Jes{\'u}s},
  year = {2020},
  month = apr,
  journal = {PLoS computational biology},
  volume = {16},
  number = {4},
  pages = {e1007792},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007792},
  abstract = {Until date, several machine learning approaches have been proposed for the dynamic modeling of temporal omics data. Although they have yielded impressive results in terms of model accuracy and predictive ability, most of these applications are based on "Black-box" algorithms and more interpretable models have been claimed by the research community. The recent eXplainable Artificial Intelligence (XAI) revolution offers a solution for this issue, were rule-based approaches are highly suitable for explanatory purposes. The further integration of the data mining process along with functional-annotation and pathway analyses is an additional way towards more explanatory and biologically soundness models. In this paper, we present a novel rule-based XAI strategy (including pre-processing, knowledge-extraction and functional validation) for finding biologically relevant sequential patterns from longitudinal human gene expression data (GED). To illustrate the performance of our pipeline, we work on in vivo temporal GED collected within the course of a long-term dietary intervention in 57 subjects with obesity (GSE77962). As validation populations, we employ three independent datasets following the same experimental design. As a result, we validate primarily extracted gene patterns and prove the goodness of our strategy for the mining of biologically relevant gene-gene temporal relations. Our whole pipeline has been gathered under open-source software and could be easily extended to other human temporal GED applications.},
  langid = {english},
  pmcid = {PMC7176286},
  pmid = {32275707},
  keywords = {Algorithms,Artificial Intelligence,Computational Biology,Data Mining,Databases Genetic,Gene Expression,Gene Expression Profiling,Humans,Longitudinal Studies,Machine Learning,Obesity,Software,Transcriptome},
  annotation = {TLDR: A novel rule-based XAI strategy (including pre-processing, knowledge-extraction and functional validation) for finding biologically relevant sequential patterns from longitudinal human gene expression data (GED) and proves the goodness of this strategy for the mining of biologically relevant gene-gene temporal relations.},
  timestamp = {2025-08-13T09:20:23Z}
}

@inproceedings{anjomshoae2020py,
  title = {Py-{{CIU}}: A Python Library for Explaining Machine Learning Predictions Using Contextual Importance and Utility},
  booktitle = {{{IJCAI-PRICAI}} 2020 Workshop on Explainable Artificial Intelligence ({{XAI}}), January 8, 2020},
  author = {Anjomshoae, Sule and Kampik, Timotheus and Fr{\"a}mling, Kary},
  year = {2020},
  timestamp = {2025-03-27T03:38:40Z}
}

@article{antoniadi2021current,
  title = {Current {{Challenges}} and {{Future Opportunities}} for {{XAI}} in {{Machine Learning-Based Clinical Decision Support Systems}}: {{A Systematic Review}}},
  shorttitle = {Current {{Challenges}} and {{Future Opportunities}} for {{XAI}} in {{Machine Learning-Based Clinical Decision Support Systems}}},
  author = {Antoniadi, Anna Markella and Du, Yuhan and Guendouz, Yasmine and Wei, Lan and Mazo, Claudia and Becker, Brett A. and Mooney, Catherine},
  year = {2021},
  month = may,
  journal = {Applied Sciences},
  volume = {11},
  number = {11},
  pages = {5088},
  issn = {2076-3417},
  doi = {10.3390/app11115088},
  urldate = {2025-08-28},
  abstract = {Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: An overall distinct lack of application of XAI is found in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians is found.},
  timestamp = {2025-08-28T03:49:09Z}
}

@article{antony2021comprehensive,
  title = {A {{Comprehensive Unsupervised Framework}} for {{Chronic Kidney Disease Prediction}}},
  author = {Antony, Linta and Azam, Sami and Ignatious, Eva and Quadir, Ryana and Beeravolu, Abhijith Reddy and Jonkman, Mirjam and De Boer, Friso},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {126481--126501},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3109168},
  urldate = {2025-03-25},
  abstract = {The incidence, prevalence, and progression of chronic kidney disease (CKD) conditions have evolved over time, especially in countries that have varied social determinants of health. In most countries, diabetics and hypertension are the main causes of CKDs. The global guidelines classify CKD as a condition that results in decreased kidney function over time, as indicated by glomerular filtration rate (GFR) and markers of kidney damage. People with CKDs are likely to die at an early age. It is crucial for doctors to diagnose various conditions associated with CKD in an early stage because early detection may prevent or even reverse kidney damage. Early detection can provide better treatment and proper care to the patients. In many regional hospital/clinics, there is a shortage of nephrologists or general medical persons who diagnose the symptoms. This has resulted in patients waiting longer to get a diagnosis. Therefore, this research believes developing an intelligent system to classify a patient into classes of `CKD' or `Non-CKD' can help the doctors to deal with multiple patients and provide diagnosis faster. In time, organizations can implement the proposed machine learning framework in regional clinics that have lower medical expert retention, this can provide early diagnosis to patients in regional areas. Although, several researchers have tried to address the situation by developing intelligent systems using supervised machine learning methods, till date limited studies have used unsupervised machine learning algorithms. The primary aim of this research is to implement and compare the performance of various unsupervised algorithms and identify best possible combinations that can provide better accuracy and detection rate. This research has implemented five unsupervised algorithms, K-Means Clustering, DB-Scan, I-Forest, and Autoencoder. And integrating them with various feature selection methods. Integrating feature reduction methods with K-Means Clustering algorithm has achieved an overall accuracy of 99\% in classifying the clinical data of CKD and Non-CKD.},
  keywords = {autoencoder,Chronic kidney disease,Classification algorithms,DB-scan,Diseases,Feature extraction,feature selection,glomerular filtration rate,isolation forest,K means clustering,Kidney,Machine learning,Machine learning algorithms,Support vector machines,unsupervised learning techniques},
  annotation = {TLDR: Five unsupervised algorithms are implemented and integrating feature reduction methods with K-Means Clustering algorithm has achieved an overall accuracy of 99\% in classifying the clinical data of CKD and Non-CKD and integrating them with various feature selection methods.},
  timestamp = {2025-03-25T15:33:23Z}
}

@article{aoki2024deep,
  title = {Deep Convolutional Neural Network for Differentiating between Sarcoidosis and Lymphoma Based on [{{18F}}]{{FDG}} Maximum-Intensity Projection Images},
  author = {Aoki, Hikaru and Miyazaki, Yasunari and Anzai, Tatsuhiko and Yokoyama, Kota and Tsuchiya, Junichi and Shirai, Tsuyoshi and Shibata, Sho and Sakakibara, Rie and Mitsumura, Takahiro and Honda, Takayuki and Furusawa, Haruhiko and Okamoto, Tsukasa and Tateishi, Tomoya and Tamaoka, Meiyo and Yamamoto, Masahide and Takahashi, Kunihiko and Tateishi, Ukihide and Yamaguchi, Tetsuo},
  year = {2024},
  month = jan,
  journal = {European Radiology},
  volume = {34},
  number = {1},
  pages = {374--383},
  issn = {1432-1084},
  doi = {10.1007/s00330-023-09937-x},
  abstract = {OBJECTIVES: To compare the [18F]FDG PET/CT findings of untreated sarcoidosis and malignant lymphoma (ML) and develop convolutional neural network (CNN) models to differentiate between these diseases using maximum intensity projection (MIP) [18F]FDG PET images. METHODS: We retrospectively collected data on consecutive patients newly diagnosed with sarcoidosis and ML who underwent [18F]FDG PET/CT before treatment. Two nuclear radiologists reviewed the images. CNN models were created using MIP PET images and evaluated with k-fold cross-validation. The points of interest were visualized using gradient-weighted class activation mapping (Grad-CAM). RESULTS: A total of 56 patients with sarcoidosis and 62 patients with ML were included. Patients with sarcoidosis had more prominent FDG accumulation in the mediastinal lymph nodes and lung lesions, while those with ML had more prominent accumulation in the cervical lymph nodes (all p {$<$} 0.001). For the mediastinal lymph nodes, sarcoidosis patients had significant FDG accumulation in the level 2, 4, 7, and 10 lymph nodes (all p {$<$} 0.01). Otherwise, the accumulation in ML patients tended to be in the level 1 lymph nodes (p = 0.08). The CNN model using frontal and lateral MIP images achieved an average accuracy of 0.890 (95\% CI: 0.804-0.977), a sensitivity of 0.898 (95\% CI: 0.782-1.000), a specificity of 0.907 (95\% CI: 0.799-1.000), and an area under the curve of 0.963 (95\% CI: 0.899-1.000). Grad-CAM showed that the model focused on the sites of abnormal FDG accumulation. CONCLUSIONS: CNN models based on differences in FDG accumulation sites archive high performance in differentiating between sarcoidosis and ML. CLINICAL RELEVANCE STATEMENT: We developed a CNN model using MIP images of [18F]FDG PET/CT to distinguish between sarcoidosis and malignant lymphoma. It achieved high performance and could be useful in diagnosing diseases with involvement across organs and lymph nodes. KEY POINTS: {$\bullet$} There are differences in FDG distribution when comparing whole-body [18F]FDG PET/CT findings in patients with sarcoidosis and malignant lymphoma before treatment. {$\bullet$} Convolutional neural networks, a type of deep learning technique, trained with maximum-intensity projection PET images from two angles showed high performance. {$\bullet$} A deep learning model that utilizes differences in FDG distribution may be helpful in differentiating between diseases with lesions that are characteristically widespread among organs and lymph nodes.},
  langid = {english},
  pmid = {37535157},
  keywords = {[18F]FDG PET/CT,Convolutional neural network,Deep learning,Fluorodeoxyglucose F18,Humans,Lymphoma,Malignant lymphoma,Neural Networks Computer,Positron Emission Tomography Computed Tomography,Retrospective Studies,Sarcoidosis},
  annotation = {TLDR: There are differences in FDG distribution when comparing whole-body [18F]FDG PET/CT findings in patients with sarcoidosis and malignant lymphoma before treatment, which may be helpful in differentiating between diseases with lesions that are characteristically widespread among organs and lymph nodes.},
  timestamp = {2025-05-06T05:12:35Z}
}

@article{ARAUJO2020101715,
  ids = {araujo2020dr|graduate},
  title = {{{DR}}{\textbar}{{GRADUATE}}: {{Uncertainty-aware}} Deep Learning-Based Diabetic Retinopathy Grading in Eye Fundus Images},
  shorttitle = {{{DR}}\_{{GRADUATE}}},
  author = {Ara{\'u}jo, Teresa and Aresta, Guilherme and Mendon{\c c}a, Lu{\'i}s and Penas, Susana and Maia, Carolina and Carneiro, {\^A}ngela and Mendon{\c c}a, Ana Maria and Campilho, Aur{\'e}lio},
  year = {2020},
  journal = {Medical Image Analysis},
  volume = {63},
  pages = {101715},
  issn = {1361-8415},
  doi = {10.1016/j.media.2020.101715},
  abstract = {Diabetic retinopathy (DR) grading is crucial in determining the adequate treatment and follow up of patient, but the screening process can be tiresome and prone to errors. Deep learning approaches have shown promising performance as computer-aided diagnosis (CAD) systems, but their black-box behaviour hinders clinical application. We propose DR{\textbar}GRADUATE, a novel deep learning-based DR grading CAD system that supports its decision by providing a medically interpretable explanation and an estimation of how uncertain that prediction is, allowing the ophthalmologist to measure how much that decision should be trusted. We designed DR{\textbar}GRADUATE taking into account the ordinal nature of the DR grading problem. A novel Gaussian-sampling approach built upon a Multiple Instance Learning framework allow DR{\textbar}GRADUATE to infer an image grade associated with an explanation map and a prediction uncertainty while being trained only with image-wise labels. DR{\textbar}GRADUATE was trained on the Kaggle DR detection training set and evaluated across multiple datasets. In DR grading, a quadratic-weighted Cohen's kappa ({$\kappa$}) between 0.71 and 0.84 was achieved in five different datasets. We show that high {$\kappa$} values occur for images with low prediction uncertainty, thus indicating that this uncertainty is a valid measure of the predictions' quality. Further, bad quality images are generally associated with higher uncertainties, showing that images not suitable for diagnosis indeed lead to less trustworthy predictions. Additionally, tests on unfamiliar medical image data types suggest that DR{\textbar}GRADUATE allows outlier detection. The attention maps generally highlight regions of interest for diagnosis. These results show the great potential of DR{\textbar}GRADUATE as a second-opinion system in DR severity grading.},
  langid = {english},
  keywords = {Deep learning,Deep Learning,Diabetes Mellitus,Diabetic Retinopathy,Diabetic retinopathy grading,Diagnosis Computer-Assisted,Explainability,Fundus Oculi,Humans,Uncertainty},
  timestamp = {2025-08-26T08:18:55Z}
}

@article{arbelaezossa2022refocusing,
  title = {Re-Focusing Explainability in Medicine},
  author = {Arbelaez Ossa, Laura and Starke, Georg and Lorenzini, Giorgia and Vogt, Julia E. and Shaw, David M. and Elger, Bernice Simone},
  year = {2022},
  journal = {Digital Health},
  volume = {8},
  pages = {20552076221074488},
  issn = {2055-2076},
  doi = {10.1177/20552076221074488},
  abstract = {Using artificial intelligence to improve patient care is a cutting-edge methodology, but its implementation in clinical routine has been limited due to significant concerns about understanding its behavior. One major barrier is the explainability dilemma and how much explanation is required to use artificial intelligence safely in healthcare. A key issue is the lack of consensus on the definition of explainability by experts, regulators, and healthcare professionals, resulting in a wide variety of terminology and expectations. This paper aims to fill the gap by defining minimal explainability standards to serve the views and needs of essential stakeholders in healthcare. In that sense, we propose to define minimal explainability criteria that can support doctors' understanding, meet patients' needs, and fulfill legal requirements. Therefore, explainability need not to be exhaustive but sufficient for doctors and patients to comprehend the artificial intelligence models' clinical implications and be integrated safely into clinical practice. Thus, minimally acceptable standards for explainability are context-dependent and should respond to the specific need and potential risks of each clinical scenario for a responsible and ethical implementation of artificial intelligence.},
  langid = {english},
  pmcid = {PMC8841907},
  pmid = {35173981},
  keywords = {digital health,Explainability,explainable AI,human-center AI,medicine},
  annotation = {TLDR: The proposed minimal explainability criteria can support doctors' understanding, meet patients' needs, and fulfill legal requirements and should respond to the specific need and potential risks of each clinical scenario for a responsible and ethical implementation of artificial intelligence.},
  timestamp = {2025-04-12T07:26:34Z}
}

@misc{arik2020tabnet,
  title = {{{TabNet}}: {{Attentive Interpretable Tabular Learning}}},
  shorttitle = {{{TabNet}}},
  author = {Arik, Sercan O. and Pfister, Tomas},
  year = {2020},
  month = dec,
  number = {arXiv:1908.07442},
  eprint = {1908.07442},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.07442},
  urldate = {2024-12-29},
  abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2024-12-29T23:11:07Z}
}

@book{aristotle1984complete,
  title = {The Complete Works of {{Aristotle}}},
  author = {Aristotle, Jonathan Barnes and others},
  year = {1984},
  volume = {2},
  publisher = {Princeton University Press Princeton, NJ},
  timestamp = {2025-09-05T16:35:33Z}
}

@article{arrieta2020explainable,
  ids = {barredoarrieta2020explainable},
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Arrieta, Alejandro Barredo and {D{\'{\i}}az-Rodr{\'{\i}}guez}, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'{\i}}a, Salvador and {Gil-L{\'o}pez}, Sergio and Molina, Daniel and Benjamins, Richard and others},
  year = {2020},
  journal = {Information fusion},
  volume = {58},
  pages = {82--115},
  publisher = {Elsevier},
  issn = {15662535},
  doi = {10.1016/j.inffus.2019.12.012},
  urldate = {2025-03-15},
  langid = {english},
  annotation = {TLDR: Previous efforts to define explainability in Machine Learning are summarized, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought, and a taxonomy of recent contributions related to the explainability of different Machine Learning models are proposed.},
  timestamp = {2025-03-18T12:41:57Z}
}

@inproceedings{arsenyan2024large,
  title = {Large {{Language Models}} for {{Biomedical Knowledge Graph Construction}}: {{Information}} Extraction from {{EMR}} Notes},
  shorttitle = {Large {{Language Models}} for {{Biomedical Knowledge Graph Construction}}},
  booktitle = {Proceedings of the 23rd {{Workshop}} on {{Biomedical Natural Language Processing}}},
  author = {Arsenyan, Vahan and Bughdaryan, Spartak and Shaya, Fadi and Small, Kent and Shahnazaryan, Davit},
  year = {2024},
  eprint = {2301.12473},
  primaryclass = {cs},
  pages = {295--317},
  doi = {10.18653/v1/2024.bionlp-1.23},
  urldate = {2025-04-04},
  abstract = {The automatic construction of knowledge graphs (KGs) is an important research area in medicine, with far-reaching applications spanning drug discovery and clinical trial design. These applications hinge on the accurate identification of interactions among medical and biological entities. In this study, we propose an end-to-end machine learning solution based on large language models (LLMs) that utilize electronic medical record notes to construct KGs. The entities used in the KG construction process are diseases, factors, treatments, as well as manifestations that coexist with the patient while experiencing the disease. Given the critical need for high-quality performance in medical applications, we embark on a comprehensive assessment of 12 LLMs of various architectures, evaluating their performance and safety attributes. To gauge the quantitative efficacy of our approach by assessing both precision and recall, we manually annotate a dataset provided by the Macula and Retina Institute. We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation. Additionally, we provide guided prompt design to utilize such LLMs. The application of the proposed methodology is demonstrated on age-related macular degeneration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {TLDR: This study proposes an end-to-end machine learning solution based on large language models (LLMs) that utilize electronic medical record notes to construct KGs and demonstrates the application of the proposed methodology on age-related macular degeneration.},
  timestamp = {2025-04-04T02:16:04Z}
}

@misc{aryal2023explainable,
  title = {Explainable and {{Position-Aware Learning}} in {{Digital Pathology}}},
  author = {Aryal, Milan and Yahyasoltani, Nasim},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.08198},
  urldate = {2025-06-12},
  abstract = {Encoding whole slide images (WSI) as graphs is well motivated since it makes it possible for the gigapixel resolution WSI to be represented in its entirety for the purpose of graph learning. To this end, WSIs can be broken into smaller patches that represent the nodes of the graph. Then, graph-based learning methods can be utilized for the grading and classification of cancer. Message passing among neighboring nodes is the foundation of graph-based learning methods. However, they do not take into consideration any positional information for any of the patches, and if two patches are found in topologically isomorphic neighborhoods, their embeddings are nearly similar to one another. In this work, classification of cancer from WSIs is performed with positional embedding and graph attention. In order to represent the positional embedding of the nodes in graph classification, the proposed method makes use of spline convolutional neural networks (CNN). The algorithm is then tested with the WSI dataset for grading prostate cancer and kidney cancer. A comparison of the proposed method with leading approaches in cancer diagnosis and grading verify improved performance. The identification of cancerous regions in WSIs is another critical task in cancer diagnosis. In this work, the explainability of the proposed model is also addressed. A gradient-based explainbility approach is used to generate the saliency mapping for the WSIs. This can be used to look into regions of WSI that are responsible for cancer diagnosis thus rendering the proposed model explainable.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV),Machine Learning (cs.LG)},
  annotation = {TLDR: In this work, classification of cancer from WSIs is performed with positional embedding and graph attention, and a gradient-based explainbility approach is used to generate the saliency mapping for the WSIs.},
  timestamp = {2025-06-12T16:09:00Z}
}

@article{asan2020artificial,
  title = {Artificial Intelligence and Human Trust in Healthcare: Focus on Clinicians},
  author = {Asan, Onur and Bayrak, Alparslan Emrah and Choudhury, Avishek and others},
  year = {2020},
  journal = {Journal of medical Internet research},
  volume = {22},
  number = {6},
  pages = {e15154},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  timestamp = {2025-03-29T14:10:37Z}
}

@misc{ashwani2024cause,
  title = {Cause and {{Effect}}: {{Can Large Language Models Truly Understand Causality}}?},
  shorttitle = {Cause and {{Effect}}},
  author = {Ashwani, Swagata and Hegde, Kshiteesh and Mannuru, Nishith Reddy and Jindal, Mayank and Sengar, Dushyant Singh and Kathala, Krishna Chaitanya Rao and Banga, Dishant and Jain, Vinija and Chadha, Aman},
  year = {2024},
  month = sep,
  number = {arXiv:2402.18139},
  eprint = {2402.18139},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.18139},
  urldate = {2025-03-23},
  abstract = {With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multiple causal reasoning tasks such as causal discovery, causal identification and counterfactual reasoning. The counterfactual sentences add explicit knowledge of the not caused by scenarios. By combining these powerful modules, our model aims to provide a deeper understanding of causal relationships, enabling enhanced interpretability. Evaluation of benchmark datasets shows improved performance across all metrics, such as accuracy, precision, recall, and F1 scores. We also introduce CausalNet, a new dataset accompanied by our code, to facilitate further research in this domain.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {TLDR: This research proposes a novel architecture called Context-Aware Reasoning Enhancement with Counterfactual Analysis (CARE-CA) to enhance causal reasoning and explainability and presents CausalNet, a novel dataset specifically curated to benchmark and enhance the causal reasoning capabilities of LLMs.},
  timestamp = {2025-03-23T10:53:49Z}
}

@article{atan2018deeptreat,
  title = {Deep-{{Treat}}: {{Learning Optimal Personalized Treatments From Observational Data Using Neural Networks}}},
  shorttitle = {Deep-{{Treat}}},
  author = {Atan, Onur and Jordon, James and Van Der Schaar, Mihaela},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11841},
  urldate = {2025-05-21},
  abstract = {We propose a novel approach for constructing effective treatment policies when the observed data is biased and lacks counterfactual information. Learning in settings where the observed data does not contain all possible outcomes for all treatments is difficult since the observed data is typically biased due to existing clinical guidelines. This is an important problem in the medical domain as collecting unbiased data is expensive and so learning from the wealth of existing biased data is a worthwhile task. Our approach separates the problem into two stages: first we reduce the bias by learning a representation map using a novel auto-encoder network---this allows us to control the trade-off between the bias-reduction and the information loss---and then we construct effective treatment policies on the transformed data using a novel feedforward network. Separation of the problem into these two stages creates an algorithm that can be adapted to the problem at hand---the bias-reduction step can be performed as a preprocessing step for other algorithms. We compare our algorithm against state-of-art algorithms on two semi-synthetic datasets and demonstrate that our algorithm achieves a significant improvement in performance.},
  annotation = {TLDR: A novel approach for constructing effective treatment policies when the observed data is biased and lacks counterfactual information is proposed, which creates an algorithm that can be adapted to the problem at hand and achieves a significant improvement in performance.},
  timestamp = {2025-05-21T01:20:31Z}
}

@article{athey2016recursive,
  title = {Recursive Partitioning for Heterogeneous Causal Effects},
  author = {Athey, Susan and Imbens, Guido},
  year = {2016},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {27},
  pages = {7353--7360},
  publisher = {National Academy of Sciences},
  timestamp = {2025-03-20T06:54:52Z}
}

@misc{augmenting,
  title = {Augmenting {{Statistical Data Dissemination}} by {{Short Quantified Sentences}} of {{Natural Language}} - {{Miroslav Hudec}}, {{Erika Bedn{\'a}rov{\'a}}}, {{Andreas Holzinger}}, 2018},
  urldate = {2025-03-28},
  howpublished = {https://journals.sagepub.com/doi/abs/10.2478/jos-2018-0048},
  timestamp = {2025-03-28T08:01:14Z}
}

@article{Austin2011,
  title = {An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies},
  author = {Austin, Peter C.},
  year = {2011},
  journal = {Multivariate Behavioral Research},
  volume = {46},
  number = {3},
  pages = {399--424},
  timestamp = {2025-03-20T04:31:47Z}
}

@article{avsec2021baseresolution,
  title = {Base-Resolution Models of Transcription-Factor Binding Reveal Soft Motif Syntax},
  author = {Avsec, {\v Z}iga and Weilert, Melanie and Shrikumar, Avanti and Krueger, Sabrina and Alexandari, Amr and Dalal, Khyati and Fropf, Robin and McAnany, Charles and Gagneur, Julien and Kundaje, Anshul and Zeitlinger, Julia},
  year = {2021},
  month = mar,
  journal = {Nature Genetics},
  volume = {53},
  number = {3},
  pages = {354--366},
  issn = {1546-1718},
  doi = {10.1038/s41588-021-00782-6},
  abstract = {The arrangement (syntax) of transcription factor (TF) binding motifs is an important part of the cis-regulatory code, yet remains elusive. We introduce a deep learning model, BPNet, that uses DNA sequence to predict base-resolution chromatin immunoprecipitation (ChIP)-nexus binding profiles of pluripotency TFs. We develop interpretation tools to learn predictive motif representations and identify soft syntax rules for cooperative TF binding interactions. Strikingly, Nanog preferentially binds with helical periodicity, and TFs often cooperate in a directional manner, which we validate using clustered regularly interspaced short palindromic repeat (CRISPR)-induced point mutations. Our model represents a powerful general approach to uncover the motifs and syntax of cis-regulatory sequences in genomics data.},
  langid = {english},
  pmcid = {PMC8812996},
  pmid = {33603233},
  keywords = {Animals,Binding Sites,Chromatin Immunoprecipitation,Clustered Regularly Interspaced Short Palindromic Repeats,Computational Biology,Deep Learning,Mice,Mouse Embryonic Stem Cells,Nanog Homeobox Protein,Neural Networks Computer,Nucleotide Motifs,Octamer Transcription Factor-3,Reproducibility of Results,SOXB1 Transcription Factors,Transcription Factors},
  annotation = {TLDR: A deep learning model, BPNet, is introduced that uses DNA sequence to predict base-resolution chromatin immunoprecipitation--nexus binding profiles of pluripotency TFs and develops interpretation tools to learn predictive motif representations and identify soft syntax rules for cooperative TF binding interactions.},
  timestamp = {2025-07-05T11:15:00Z}
}

@article{awwad2024editorial,
  title = {Editorial: {{Precision}} Medicine: Recent Advances, Current Challenges and Future Perspectives},
  shorttitle = {Editorial},
  author = {Awwad, Oriana and Ahram, Mamoun and Coperchini, Francesca and Jalil, Mariam Abdel},
  year = {2024},
  month = jun,
  journal = {Frontiers in Pharmacology},
  volume = {15},
  pages = {1439276},
  issn = {1663-9812},
  doi = {10.3389/fphar.2024.1439276},
  urldate = {2025-05-27},
  timestamp = {2025-05-27T13:25:23Z}
}

@article{azad2021using,
  title = {Using Explainable Deep Learning in Da {{Vinci Xi}} Robot for Tumor Detection},
  author = {Azad, Rohan Ibn and Mukhopadhyay, Subhas and Asadnia, Mohsen},
  year = {2021},
  month = oct,
  journal = {International Journal on Smart Sensing and Intelligent Systems},
  volume = {14},
  number = {1},
  pages = {1--16},
  doi = {10.21307/ijssis-2021-017},
  urldate = {2025-04-12},
  abstract = {Deep learning has proved successful in computer-aided detection in interpreting ultrasound images, COVID infections, identifying tumors from computed...},
  langid = {english},
  annotation = {TLDR: Approaches of deep learning in detecting cancerous cells inside patients via laparoscopic camera on da Vinci Xi surgical robots using GRAD-CAM are proposed.},
  timestamp = {2025-04-12T07:51:50Z}
}

@article{aziz2024explainable,
  title = {Explainable {{AI}} in Healthcare: {{Systematic}} Review of Clinical Decision Support Systems},
  author = {Aziz, Noor A and Manzoor, Awais and Mazhar Qureshi, Muhammad Deedahwar and Qureshi, M Atif and Rashwan, Wael},
  year = {2024},
  journal = {medRxiv : the preprint server for health sciences},
  pages = {2024--08},
  publisher = {Cold Spring Harbor Laboratory Press},
  langid = {american},
  timestamp = {2025-04-02T00:49:22Z}
}

@article{b2025explainable,
  title = {Explainable {{AI}} for {{Pancreatic Cancer Prediction}} and {{Survival Prognosis}}: {{An Interpretable Deep Learning}} and {{Machine Learning Approach}}},
  shorttitle = {Explainable {{AI}} for {{Pancreatic Cancer Prediction}} and {{Survival Prognosis}}},
  author = {B, Srinidhi and M S, Bhargavi},
  year = {2025},
  month = jan,
  journal = {Informatica},
  volume = {48},
  number = {4},
  issn = {1854-3871, 0350-5596},
  doi = {10.31449/inf.v48i4.5151},
  urldate = {2025-05-16},
  timestamp = {2025-05-16T06:49:40Z}
}

@article{bach2015pixelwise,
  title = {On {{Pixel-Wise Explanations}} for {{Non-Linear Classifier Decisions}} by {{Layer-Wise Relevance Propagation}}},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2015},
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  urldate = {2025-05-17},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  langid = {english},
  keywords = {Algorithms,Coding mechanisms,Imaging techniques,Kernel functions,Neural networks,Neurons,Sensory perception,Vision},
  annotation = {TLDR: This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers by introducing a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks.},
  timestamp = {2025-05-17T13:51:13Z}
}

@article{baek2021accurate,
  title = {Accurate Prediction of Protein Structures and Interactions Using a Three-Track Neural Network},
  author = {Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and Wang, Jue and Cong, Qian and Kinch, Lisa N. and Schaeffer, R. Dustin and Mill{\'a}n, Claudia and Park, Hahnbeom and Adams, Carson and Glassman, Caleb R. and DeGiovanni, Andy and Pereira, Jose H. and Rodrigues, Andria V. and van Dijk, Alberdina A. and Ebrecht, Ana C. and Opperman, Diederik J. and Sagmeister, Theo and Buhlheller, Christoph and {Pavkov-Keller}, Tea and Rathinaswamy, Manoj K. and Dalwadi, Udit and Yip, Calvin K. and Burke, John E. and Garcia, K. Christopher and Grishin, Nick V. and Adams, Paul D. and Read, Randy J. and Baker, David},
  year = {2021},
  month = aug,
  journal = {Science},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.abj8754},
  urldate = {2025-05-04},
  abstract = {Protein structure modeling enables the rapid solution of protein structures and provides insights into function.},
  copyright = {Copyright {\copyright} 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
  langid = {english},
  annotation = {TLDR: A three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging X-ray crystallography and cryo-EM structure modeling problems, and provides insights into the functions of proteins of currently unknown structure.},
  timestamp = {2025-05-04T13:51:17Z}
}

@misc{bai2022temporal,
  title = {Temporal {{Domain Generalization}} with {{Drift-Aware Dynamic Neural Networks}}},
  author = {Bai, Guangji and Ling, Chen and Zhao, Liang},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.10664},
  urldate = {2025-04-04},
  abstract = {Temporal domain generalization is a promising yet extremely challenging area where the goal is to learn models under temporally changing data distributions and generalize to unseen data distributions following the trends of the change. The advancement of this area is challenged by: 1) characterizing data distribution drift and its impacts on models, 2) expressiveness in tracking the model dynamics, and 3) theoretical guarantee on the performance. To address them, we propose a Temporal Domain Generalization with Drift-Aware Dynamic Neural Network (DRAIN) framework. Specifically, we formulate the problem into a Bayesian framework that jointly models the relation between data and model dynamics. We then build a recurrent graph generation scenario to characterize the dynamic graph-structured neural networks learned across different time points. It captures the temporal drift of model parameters and data distributions and can predict models in the future without the presence of future data. In addition, we explore theoretical guarantees of the model performance under the challenging temporal DG setting and provide theoretical analysis, including uncertainty and generalization error. Finally, extensive experiments on several real-world benchmarks with temporal drift demonstrate the effectiveness and efficiency of the proposed method.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: This work forms the problem into a Bayesian framework that jointly models the relation between data and model dynamics and builds a recurrent graph generation scenario to characterize the dynamic graph-structured neural networks learned across different time points.},
  timestamp = {2025-04-04T03:12:44Z}
}

@misc{bai2024breast,
  title = {Breast {{Cancer Diagnosis}}: {{A Comprehensive Exploration}} of {{Explainable Artificial Intelligence}} ({{XAI}}) {{Techniques}}},
  shorttitle = {Breast {{Cancer Diagnosis}}},
  author = {Bai, Samita and Nasir, Sidra and Khan, Rizwan Ahmed and Arif, Sheeraz and Meyer, Alexandre and Konik, Hubert},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2406.00532},
  urldate = {2025-04-04},
  abstract = {Breast cancer (BC) stands as one of the most common malignancies affecting women worldwide, necessitating advancements in diagnostic methodologies for better clinical outcomes. This article provides a comprehensive exploration of the application of Explainable Artificial Intelligence (XAI) techniques in the detection and diagnosis of breast cancer. As Artificial Intelligence (AI) technologies continue to permeate the healthcare sector, particularly in oncology, the need for transparent and interpretable models becomes imperative to enhance clinical decision-making and patient care. This review discusses the integration of various XAI approaches, such as SHAP, LIME, Grad-CAM, and others, with machine learning and deep learning models utilized in breast cancer detection and classification. By investigating the modalities of breast cancer datasets, including mammograms, ultrasounds and their processing with AI, the paper highlights how XAI can lead to more accurate diagnoses and personalized treatment plans. It also examines the challenges in implementing these techniques and the importance of developing standardized metrics for evaluating XAI's effectiveness in clinical settings. Through detailed analysis and discussion, this article aims to highlight the potential of XAI in bridging the gap between complex AI models and practical healthcare applications, thereby fostering trust and understanding among medical professionals and improving patient outcomes.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: The integration of various XAI approaches, such as SHAP, LIME, Grad-CAM, and others, with machine learning and deep learning models utilized in breast cancer detection and classification are discussed.},
  timestamp = {2025-04-04T06:38:15Z}
}

@article{bailey2024causal,
  title = {Causal Inference on Human Behaviour},
  author = {Bailey, Drew H. and Jung, Alexander J. and Beltz, Adriene M. and Eronen, Markus I. and Gische, Christian and Hamaker, Ellen L. and Kording, Konrad P. and Lebel, Catherine and Lindquist, Martin A. and Moeller, Julia and Razi, Adeel and Rohrer, Julia M. and Zhang, Baobao and Murayama, Kou},
  year = {2024},
  month = aug,
  journal = {Nature Human Behaviour},
  volume = {8},
  number = {8},
  pages = {1448--1459},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-024-01939-z},
  urldate = {2024-12-03},
  abstract = {Making causal inferences regarding human behaviour is difficult given the complex interplay between countless contributors to behaviour, including factors in the external world and our internal states. We provide a non-technical conceptual overview of challenges and opportunities for causal inference on human behaviour. The challenges include our ambiguous causal language and thinking, statistical under- or over-control, effect heterogeneity, interference, timescales of effects and complex treatments. We explain how methods optimized for addressing one of these challenges frequently exacerbate other problems. We thus argue that clearly specified research questions are key to improving causal inference from data. We suggest a triangulation approach that compares causal estimates from (quasi-)experimental research with causal estimates generated from observational data and theoretical assumptions. This approach allows a systematic investigation of theoretical and methodological factors that might lead estimates to converge or diverge across studies.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Economics,Human behaviour,Social policy,Sociology},
  timestamp = {2025-02-02T09:39:06Z}
}

@article{band2023application,
  ids = {application2023},
  title = {Application of Explainable Artificial Intelligence in Medical Health: {{A}} Systematic Review of Interpretability Methods},
  shorttitle = {Application of Explainable Artificial Intelligence in Medical Health},
  author = {S Band, Shahab and Yarahmadi, Atefeh and Hsu, Chung-Chian and Biyari, Meghdad and Sookhak, Mehdi and Ameri, Rasoul and Dehzangi, Iman and Chronopoulos, Anthony Theodore and Liang, Huey-Wen},
  year = {2023},
  journal = {Informatics in Medicine Unlocked},
  volume = {40},
  pages = {101286},
  publisher = {Elsevier},
  issn = {23529148},
  doi = {10.1016/j.imu.2023.101286},
  urldate = {2025-05-03},
  abstract = {This paper investigates the applications of explainable AI (XAI) in healthcare, which aims to provide transparency, fairness, accuracy, generality, an{\dots}},
  langid = {english},
  timestamp = {2025-05-03T16:20:53Z}
}

@misc{bao2022importance,
  title = {On the {{Importance}} of {{Architecture}} and {{Feature Selection}} in {{Differentially Private Machine Learning}}},
  author = {Bao, Wenxuan and Bauer, Luke A. and Bindschaedler, Vincent},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.06720},
  urldate = {2025-04-04},
  abstract = {We study a pitfall in the typical workflow for differentially private machine learning. The use of differentially private learning algorithms in a "drop-in" fashion -- without accounting for the impact of differential privacy (DP) noise when choosing what feature engineering operations to use, what features to select, or what neural network architecture to use -- yields overly complex and poorly performing models. In other words, by anticipating the impact of DP noise, a simpler and more accurate alternative model could have been trained for the same privacy guarantee. We systematically study this phenomenon through theory and experiments. On the theory front, we provide an explanatory framework and prove that the phenomenon arises naturally from the addition of noise to satisfy differential privacy. On the experimental front, we demonstrate how the phenomenon manifests in practice using various datasets, types of models, tasks, and neural network architectures. We also analyze the factors that contribute to the problem and distill our experimental insights into concrete takeaways that practitioners can follow when training models with differential privacy. Finally, we propose privacy-aware algorithms for feature selection and neural network architecture search. We analyze their differential privacy properties and evaluate them empirically.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Cryptography and Security (cs.CR),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: This work provides an explanatory framework and proves that the phenomenon arises naturally from the addition of noise to satisfy differential privacy, and proposes privacy-aware algorithms for feature selection and neural network architecture search.},
  timestamp = {2025-04-04T06:13:07Z}
}

@article{bareinboim2013general,
  title = {A {{General Algorithm}} for {{Deciding Transportability}} of {{Experimental Results}}},
  author = {Bareinboim, Elias and Pearl, Judea},
  year = {2013},
  month = may,
  journal = {Journal of Causal Inference},
  volume = {1},
  number = {1},
  eprint = {1312.7485},
  primaryclass = {cs},
  pages = {107--134},
  issn = {2193-3685, 2193-3677},
  doi = {10.1515/jci-2012-0004},
  urldate = {2025-03-28},
  abstract = {Generalizing empirical findings to new environments, settings, or populations is essential in most scientific explorations. This article treats a particular problem of generalizability, called "transportability", defined as a license to transfer information learned in experimental studies to a different population, on which only observational studies can be conducted. Given a set of assumptions concerning commonalities and differences between the two populations, Pearl and Bareinboim (2011) derived sufficient conditions that permit such transfer to take place. This article summarizes their findings and supplements them with an effective procedure for deciding when and how transportability is feasible. It establishes a necessary and sufficient condition for deciding when causal effects in the target population are estimable from both the statistical information available and the causal information transferred from the experiments. The article further provides a complete algorithm for computing the transport formula, that is, a way of combining observational and experimental information to synthesize bias-free estimate of the desired causal relation. Finally, the article examines the differences between transportability and other variants of generalizability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning,Statistics - Methodology},
  annotation = {TLDR: This article establishes a necessary and sufficient condition for deciding when causal effects in the target population are estimable from both the statistical information available and the causal information transferred from the experiments, and provides a complete algorithm for computing the transport formula.},
  timestamp = {2025-03-28T09:08:46Z}
}

@book{bartsch1998,
  title = {Dynamic Conceptual Semantics: A Logico-Philosophical Investigation into Concept Formation and Understanding},
  author = {Bartsch, Renate},
  year = {1998},
  publisher = {{Center for the Study of Language and Information Publications}},
  timestamp = {2025-09-06T00:39:28Z}
}

@inproceedings{beauvais2020information,
  title = {When Information Is the Treatment? {{Precision}} Medicine in Healthcare},
  booktitle = {Healthcare Management Forum},
  author = {Beauvais, Michael and Knoppers, Bartha Maria},
  year = {2020},
  volume = {33},
  pages = {120--125},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  timestamp = {2025-05-13T11:45:01Z}
}

@misc{behnam2024graph,
  title = {Graph {{Neural Network Causal Explanation}} via {{Neural Causal Models}}},
  author = {Behnam, Arman and Wang, Binghui},
  year = {2024},
  month = jul,
  number = {arXiv:2407.09378},
  eprint = {2407.09378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.09378},
  urldate = {2025-04-18},
  abstract = {Graph neural network (GNN) explainers identify the important subgraph that ensures the prediction for a given graph. Until now, almost all GNN explainers are based on association, which is prone to spurious correlations. We propose \{{\textbackslash}name\}, a GNN causal explainer via causal inference. Our explainer is based on the observation that a graph often consists of a causal underlying subgraph. \{{\textbackslash}name\} includes three main steps: 1) It builds causal structure and the corresponding structural causal model (SCM) for a graph, which enables the cause-effect calculation among nodes. 2) Directly calculating the cause-effect in real-world graphs is computationally challenging. It is then enlightened by the recent neural causal model (NCM), a special type of SCM that is trainable, and design customized NCMs for GNNs. By training these GNN NCMs, the cause-effect can be easily calculated. 3) It uncovers the subgraph that causally explains the GNN predictions via the optimized GNN-NCMs. Evaluation results on multiple synthetic and real-world graphs validate that \{{\textbackslash}name\} significantly outperforms existing GNN explainers in exact groundtruth explanation identification},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {TLDR: Evaluation results on multiple synthetic and real-world graphs validate that the proposed GNN causal explainer significantly outperforms existing GNN explainers in exact groundtruth explanation identification.},
  timestamp = {2025-04-18T03:19:21Z}
}

@misc{bellamy2018ai,
  title = {{{AI Fairness}} 360: {{An Extensible Toolkit}} for {{Detecting}}, {{Understanding}}, and {{Mitigating Unwanted Algorithmic Bias}}},
  shorttitle = {{{AI Fairness}} 360},
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  year = {2018},
  month = oct,
  number = {arXiv:1810.01943},
  eprint = {1810.01943},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.01943},
  urldate = {2025-03-18},
  abstract = {Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license \{https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.\vphantom\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  timestamp = {2025-03-18T00:09:37Z}
}

@misc{belluomo2024unified,
  title = {Toward a {{Unified Graph-Based Representation}} of {{Medical Data}} for {{Precision Oncology Medicine}}},
  author = {Belluomo, Davide and Calamoneri, Tiziana and Paesani, Giacomo and Salvo, Ivano},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2410.14739},
  urldate = {2025-06-12},
  abstract = {We present a new unified graph-based representation of medical data, combining genetic information and medical records of patients with medical knowledge via a unique knowledge graph. This approach allows us to infer meaningful information and explanations that would be unavailable by looking at each data set separately. The systematic use of different databases, managed throughout the built knowledge graph, gives new insights toward a better understanding of oncology medicine. Indeed, we reduce some useful medical tasks to well-known problems in theoretical computer science for which efficient algorithms exist.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {68T30,Artificial Intelligence (cs.AI),E.1; J.3,FOS: Computer and information sciences},
  annotation = {TLDR: Some useful medical tasks are reduced to well-known problems in theoretical computer science for which efficient algorithms exist and the systematic use of different databases, managed throughout the built knowledge graph, gives new insights toward a better understanding of oncology medicine.},
  timestamp = {2025-06-12T13:51:13Z}
}

@article{belyaeva2021dci,
  title = {{{DCI}}: Learning Causal Differences between Gene Regulatory Networks},
  shorttitle = {{{DCI}}},
  author = {Belyaeva, Anastasiya and Squires, Chandler and Uhler, Caroline},
  year = {2021},
  month = sep,
  journal = {Bioinformatics},
  volume = {37},
  number = {18},
  pages = {3067--3069},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btab167},
  urldate = {2025-05-17},
  abstract = {Designing interventions to control gene regulation necessitates modeling a gene regulatory network by a causal graph. Currently, large-scale gene expression datasets from different conditions, cell types, disease states, and developmental time points are being collected. However, application of classical causal inference algorithms to infer gene regulatory networks based on such data is still challenging, requiring high sample sizes and computational resources. Here, we describe an algorithm that efficiently learns the differences in gene regulatory mechanisms between different conditions. Our difference causal inference (DCI) algorithm infers changes (i.e. edges that appeared, disappeared, or changed weight) between two causal graphs given gene expression data from the two conditions. This algorithm is efficient in its use of samples and computation since it infers the differences between causal graphs directly without estimating each possibly large causal graph separately. We provide a user-friendly Python implementation of DCI and also enable the user to learn the most robust difference causal graph across different tuning parameters via stability selection. Finally, we show how to apply DCI to single-cell RNA-seq data from different conditions and cell states, and we also validate our algorithm by predicting the effects of interventions.Python package freely available at http://uhlerlab.github.io/causaldag/dci.Supplementary data are available at Bioinformatics online.},
  langid = {american},
  timestamp = {2025-05-17T06:41:29Z}
}

@article{benallen2024promise,
  ids = {allen2024promise,allen2024promisea},
  title = {The {{Promise}} of {{Explainable AI}} in {{Digital Health}} for {{Precision Medicine}}: {{A Systematic Review}}},
  shorttitle = {The {{Promise}} of {{Explainable AI}} in {{Digital Health}} for {{Precision Medicine}}},
  author = {{Ben Allen}},
  year = {2024},
  journal = {Journal of Personalized Medicine},
  volume = {14},
  number = {3},
  pages = {277},
  issn = {2075-4426},
  doi = {10.3390/jpm14030277},
  abstract = {This review synthesizes the literature on explaining machine-learning models for digital health data in precision medicine. As healthcare increasingly tailors treatments to individual characteristics, the integration of artificial intelligence with digital health data becomes crucial. Leveraging a topic-modeling approach, this paper distills the key themes of 27 journal articles. We included peer-reviewed journal articles written in English, with no time constraints on the search. A Google Scholar search, conducted up to 19 September 2023, yielded 27 journal articles. Through a topic-modeling approach, the identified topics encompassed optimizing patient healthcare through data-driven medicine, predictive modeling with data and algorithms, predicting diseases with deep learning of biomedical data, and machine learning in medicine. This review delves into specific applications of explainable artificial intelligence, emphasizing its role in fostering transparency, accountability, and trust within the healthcare domain. Our review highlights the necessity for further development and validation of explanation methods to advance precision healthcare delivery.},
  langid = {english},
  pmcid = {10971237},
  pmid = {38541019},
  keywords = {digital health,explainable artificial intelligence,machine learning,precision medicine},
  annotation = {S2ID: 96d19a3a38421808d77e4a32049fedf3caf2a845\\
TLDR: This review delves into specific applications of explainable artificial intelligence, emphasizing its role in fostering transparency, accountability, and trust within the healthcare domain and highlights the necessity for further development and validation of explanation methods to advance precision healthcare delivery.},
  timestamp = {2025-08-24T11:31:18Z}
}

@inproceedings{BenDavid2007,
  title = {Analysis of Representations for Domain Adaptation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {{Ben-David}, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  year = {2007},
  pages = {137--144},
  timestamp = {2025-03-20T07:30:59Z}
}

@inproceedings{bender2020,
  title = {Climbing towards {{NLU}}: {{On}} Meaning, Form, and Understanding in the Age of Data},
  booktitle = {Proceedings of {{ACL}}},
  author = {Bender, Emily M. and Koller, Alexander},
  year = {2020},
  timestamp = {2025-09-06T00:42:24Z}
}

@article{benfatto2025explainable,
  title = {Explainable Artificial Intelligence of {{DNA}} Methylation-Based Brain Tumor Diagnostics},
  author = {Benfatto, Salvatore and Sill, Martin and Jones, David T. W. and Pfister, Stefan M. and Sahm, Felix and {von Deimling}, Andreas and Capper, David and Hovestadt, Volker},
  year = {2025},
  month = feb,
  journal = {Nature Communications},
  volume = {16},
  number = {1},
  pages = {1787},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-025-57078-0},
  urldate = {2025-07-23},
  abstract = {We have recently developed a machine learning classifier that enables fast, accurate, and affordable classification of brain tumors based on genome-wide DNA methylation profiles that is widely employed in the clinic. Neuro-oncology research would benefit greatly from understanding the underlying artificial intelligence decision process, which currently remains unclear. Here, we describe an interpretable framework to explain the classifier's decisions. We show that functional genomic regions of various sizes are predominantly employed to distinguish between different tumor classes, ranging from enhancers and CpG islands to large-scale heterochromatic domains. We detect a high degree of genomic redundancy, with many genes distinguishing individual tumor classes, explaining the robustness of the classifier and revealing potential targets for further therapeutic investigation. We anticipate that our resource will build up trust in machine learning in clinical settings, foster biomarker discovery and development of compact point-of-care assays, and enable further epigenome research of brain tumors. Our interpretable framework is accessible to the research community via an interactive web application (https://hovestadtlab.shinyapps.io/shinyMNP/).},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Cancer genomics,CNS cancer,Diagnostic markers,Machine learning,Oncogenes},
  annotation = {TLDR: It is shown that functional genomic regions of various sizes are predominantly employed to distinguish between different tumor classes, ranging from enhancers and CpG islands to large-scale heterochromatic domains.},
  timestamp = {2025-07-23T08:31:17Z}
}

@misc{berrevoets2024causal,
  title = {Causal {{Deep Learning}}},
  author = {Berrevoets, Jeroen and Kacprzyk, Krzysztof and Qian, Zhaozhi and van der Schaar, Mihaela},
  year = {2024},
  month = feb,
  number = {arXiv:2303.02186},
  eprint = {2303.02186},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.02186},
  urldate = {2025-03-19},
  abstract = {Causality has the potential to truly transform the way we solve a large number of real-world problems. Yet, so far, its potential largely remains to be unlocked as causality often requires crucial assumptions which cannot be tested in practice. To address this challenge, we propose a new way of thinking about causality -- we call this causal deep learning. Our causal deep learning framework spans three dimensions: (1) a structural dimension, which incorporates partial yet testable causal knowledge rather than assuming either complete or no causal knowledge among the variables of interest; (2) a parametric dimension, which encompasses parametric forms that capture the type of relationships among the variables of interest; and (3) a temporal dimension, which captures exposure times or how the variables of interest interact (possibly causally) over time. Causal deep learning enables us to make progress on a variety of real-world problems by leveraging partial causal knowledge (including independencies among variables) and quantitatively characterising causal relationships among variables of interest (possibly over time). Our framework clearly identifies which assumptions are testable and which ones are not, such that the resulting solutions can be judiciously adopted in practice. Using our formulation we can combine or chain together causal representations to solve specific problems without losing track of which assumptions are required to build these solutions, pushing real-world impact in healthcare, economics and business, environmental sciences and education, through causal deep learning.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {TLDR: Causal deep learning enables us to make progress on a variety of real-world problems by leveraging partial causal knowledge and quantitatively characterising causal relationships among variables of interest (possibly over time).},
  timestamp = {2025-03-19T07:27:47Z}
}

@misc{bertin2020analysis,
  title = {Analysis of {{Gene Interaction Graphs}} as {{Prior Knowledge}} for {{Machine Learning Models}}},
  author = {Bertin, Paul and Hashir, Mohammad and Weiss, Martin and Frappier, Vincent and Perkins, Theodore J. and Boucher, Genevi{\`e}ve and Cohen, Joseph Paul},
  year = {2020},
  month = jan,
  number = {arXiv:1905.02295},
  eprint = {1905.02295},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.02295},
  urldate = {2025-05-17},
  abstract = {Gene interaction graphs aim to capture various relationships between genes and can represent decades of biology research. When trying to make predictions from genomic data, those graphs could be used to overcome the curse of dimensionality by making machine learning models sparser and more consistent with biological common knowledge. In this work, we focus on assessing how well those graphs capture dependencies seen in gene expression data to evaluate the adequacy of the prior knowledge provided by those graphs. We propose a condition graphs should satisfy to provide good prior knowledge and test it using `Single Gene Inference' tasks. We also compare with randomly generated graphs, aiming to measure the true benefit of using biologically relevant graphs in this context, and validate our findings with five clinical tasks. We find some graphs capture relevant dependencies for most genes while being very sparse. Our analysis with random graphs finds that dependencies can be captured almost as well at random which suggests that, in terms of gene expression levels, the relevant information about the state of the cell is spread across many genes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Genomics,Quantitative Biology - Quantitative Methods},
  timestamp = {2025-05-17T08:47:54Z}
}

@article{bertsch2019karl,
  title = {Karl {{Landsteiner}}: {{The Discovery}} of the {{ABO Blood Group System}} and Its {{Value}} for {{Teaching Medical Students}}},
  shorttitle = {Karl {{Landsteiner}}},
  author = {Bertsch, Thomas and L{\"u}decke, Jochen and Antl, Werner and Nausch, Lydia},
  year = {2019},
  journal = {Clinical Laboratory},
  volume = {65},
  number = {06/2019},
  issn = {1433-6510},
  doi = {10.7754/Clin.Lab.2018.181218},
  urldate = {2025-05-27},
  langid = {english},
  annotation = {TLDR: The history of the discovery of the blood groups by Landsteiner, the link of different blood groups to certain diseases and the experiences regarding teaching to medical students are reported on.},
  timestamp = {2025-05-27T14:52:25Z}
}

@article{bhandari2023integrative,
  title = {Integrative Gene Expression Analysis for the Diagnosis of {{Parkinson}}'s Disease Using Machine Learning and Explainable {{AI}}},
  author = {Bhandari, Nikita and Walambe, Rahee and Kotecha, Ketan and Kaliya, Mehul},
  year = {2023},
  month = sep,
  journal = {Comput. Biol. Med.},
  volume = {163},
  number = {C},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2023.107140},
  urldate = {2025-06-12},
  annotation = {TLDR: The results suggest that the utilization of XAI can be useful in making early therapeutic decisions for the treatment of PD, and the integration of datasets from different sources made this model robust.},
  timestamp = {2025-06-12T15:16:59Z}
}

@article{bharati2023review,
  title = {A Review on Explainable Artificial Intelligence for Healthcare: {{Why}}, How, and When?},
  author = {Bharati, Subrato and Mondal, M Rubaiyat Hossain and Podder, Prajoy},
  year = {2023},
  journal = {IEEE Transactions on Artificial Intelligence},
  publisher = {IEEE},
  timestamp = {2025-04-15T14:48:29Z}
}

@misc{bhatt2019explainable,
  title = {Explainable {{Machine Learning}} in {{Deployment}}},
  author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos{\'e} M. F. and Eckersley, Peter},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1909.06342},
  urldate = {2025-08-28},
  abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computers and Society (cs.CY),FOS: Computer and information sciences,Human-Computer Interaction (cs.HC),Machine Learning (cs.LG),Machine Learning (stat.ML)},
  timestamp = {2025-08-28T03:46:11Z}
}

@inproceedings{bhatt2020explainable,
  title = {Explainable Machine Learning in Deployment},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos{\'e} M. F. and Eckersley, Peter},
  year = {2020},
  month = jan,
  pages = {648--657},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3351095.3375624},
  urldate = {2025-03-15},
  isbn = {978-1-4503-6936-7},
  langid = {english},
  annotation = {TLDR: This study explores how organizations view and use explainability for stakeholder consumption, and synthesizes the limitations of current explainability techniques that hamper their use for end users.},
  timestamp = {2025-08-28T03:46:19Z}
}

@inproceedings{bhattacharya2023directive,
  title = {Directive {{Explanations}} for {{Monitoring}} the {{Risk}} of {{Diabetes Onset}}: {{Introducing Directive Data-Centric Explanations}} and {{Combinations}} to {{Support What-If Explorations}}},
  shorttitle = {Directive {{Explanations}} for {{Monitoring}} the {{Risk}} of {{Diabetes Onset}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Bhattacharya, Aditya and Ooge, Jeroen and Stiglic, Gregor and Verbert, Katrien},
  year = {2023},
  month = mar,
  eprint = {2302.10671},
  primaryclass = {cs},
  pages = {204--219},
  doi = {10.1145/3581641.3584075},
  urldate = {2025-05-02},
  abstract = {Explainable artificial intelligence is increasingly used in machine learning (ML) based decision-making systems in healthcare. However, little research has compared the utility of different explanation methods in guiding healthcare experts for patient care. Moreover, it is unclear how useful, understandable, actionable and trustworthy these methods are for healthcare experts, as they often require technical ML knowledge. This paper presents an explanation dashboard that predicts the risk of diabetes onset and explains those predictions with data-centric, feature-importance, and example-based explanations. We designed an interactive dashboard to assist healthcare experts, such as nurses and physicians, in monitoring the risk of diabetes onset and recommending measures to minimize risk. We conducted a qualitative study with 11 healthcare experts and a mixed-methods study with 45 healthcare experts and 51 diabetic patients to compare the different explanation methods in our dashboard in terms of understandability, usefulness, actionability, and trust. Results indicate that our participants preferred our representation of data-centric explanations that provide local explanations with a global overview over other methods. Therefore, this paper highlights the importance of visually directive data-centric explanation method for assisting healthcare experts to gain actionable insights from patient health records. Furthermore, we share our design implications for tailoring the visual representation of different explanation methods for healthcare experts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  annotation = {TLDR: An explanation dashboard that predicts the risk of diabetes onset and explains those predictions with data-centric, feature-importance, and example-based explanations, and the design implications for tailoring the visual representation of different explanation methods for healthcare experts are shared.},
  timestamp = {2025-05-02T15:21:41Z}
}

@article{bhhatarai2019opportunities,
  title = {Opportunities and Challenges Using Artificial Intelligence in {{ADME}}/{{Tox}}},
  author = {Bhhatarai, Barun and Walters, W. Patrick and Hop, Cornelis E. C. A. and Lanza, Guido and Ekins, Sean},
  year = {2019},
  month = may,
  journal = {Nature Materials},
  volume = {18},
  number = {5},
  pages = {418--422},
  publisher = {Nature Publishing Group},
  issn = {1476-4660},
  doi = {10.1038/s41563-019-0332-5},
  urldate = {2025-05-04},
  abstract = {At the recent Artificial Intelligence Applications in Biopharma Summit in Boston, USA, a panel of scientists from industry who work at the interface of machine learning and pharma discussed the diverging opinions on the past, present and future role of AI for ADME/Tox in drug discovery and development.},
  copyright = {2019 Springer Nature Limited},
  langid = {english},
  keywords = {Biomaterials,Condensed Matter Physics,general,Materials Science,Nanotechnology,Optical and Electronic Materials},
  annotation = {TLDR: A panel of scientists from industry who work at the interface of machine learning and pharma discussed the diverging opinions on the past, present and future role of AI for ADME/Tox in drug discovery and development.},
  timestamp = {2025-05-04T13:55:34Z}
}

@article{bica2021realworld,
  title = {From {{Real-World Patient Data}} to {{Individualized Treatment Effects Using Machine Learning}}: {{Current}} and {{Future Methods}} to {{Address Underlying Challenges}}},
  shorttitle = {From {{Real-World Patient Data}} to {{Individualized Treatment Effects Using Machine Learning}}},
  author = {Bica, Ioana and Alaa, Ahmed M. and Lambert, Craig and {van der Schaar}, Mihaela},
  year = {2021},
  journal = {Clinical Pharmacology \& Therapeutics},
  volume = {109},
  number = {1},
  pages = {87--100},
  issn = {1532-6535},
  doi = {10.1002/cpt.1907},
  urldate = {2025-05-29},
  abstract = {Clinical decision making needs to be supported by evidence that treatments are beneficial to individual patients. Although randomized control trials (RCTs) are the gold standard for testing and introducing new drugs, due to the focus on specific questions with respect to establishing efficacy and safety vs. standard treatment, they do not provide a full characterization of the heterogeneity in the final intended treatment population. Conversely, real-world observational data, such as electronic health records (EHRs), contain large amounts of clinical information about heterogeneous patients and their response to treatments. In this paper, we introduce the main opportunities and challenges in using observational data for training machine learning methods to estimate individualized treatment effects and make treatment recommendations. We describe the modeling choices of the state-of-the-art machine learning methods for causal inference, developed for estimating treatment effects both in the cross-section and longitudinal settings. Additionally, we highlight future research directions that could lead to achieving the full potential of leveraging EHRs and machine learning for making individualized treatment recommendations. We also discuss how experimental data from RCTs and Pharmacometric and Quantitative Systems Pharmacology approaches can be used to not only improve machine learning methods, but also provide ways for validating them. These future research directions will require us to collaborate across the scientific disciplines to incorporate models based on RCTs and known disease processes, physiology, and pharmacology into these machine learning models based on EHRs to fully optimize the opportunity these data present.},
  copyright = {{\copyright} 2020 The Authors. Clinical Pharmacology \& Therapeutics published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics},
  langid = {english},
  annotation = {TLDR: The modeling choices of the state-of-the-art machine learning methods for causal inference, developed for estimating treatment effects both in the cross-section and longitudinal settings are described.},
  timestamp = {2025-05-29T11:26:49Z}
}

@article{bienefeld2023solving,
  ids = {bienefeld2023},
  title = {Solving the Explainable {{AI}} Conundrum by Bridging Clinicians' Needs and Developers' Goals},
  author = {Bienefeld, Nadine and Boss, Jens Michael and L{\"u}thy, Rahel and Brodbeck, Dominique and Azzati, Jan and Blaser, Mirco and Willms, Jan and Keller, Emanuela},
  year = {2023},
  month = may,
  journal = {NPJ digital medicine},
  volume = {6},
  number = {1},
  pages = {94},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00837-4},
  urldate = {2025-02-26},
  abstract = {Explainable artificial intelligence (XAI) has emerged as a promising solution for addressing the implementation challenges of AI/ML in healthcare. However, little is known about how developers and clinicians interpret XAI and what conflicting goals and requirements they may have. This paper presents the findings of a longitudinal multi-method study involving 112 developers and clinicians co-designing an XAI solution for a clinical decision support system. Our study identifies three key differences between developer and clinician mental models of XAI, including opposing goals (model interpretability vs. clinical plausibility), different sources of truth (data vs. patient), and the role of exploring new vs. exploiting old knowledge. Based on our findings, we propose design solutions that can help address the XAI conundrum in healthcare, including the use of causal inference models, personalized explanations, and ambidexterity between exploration and exploitation mindsets. Our study highlights the importance of considering the perspectives of both developers and clinicians in the design of XAI systems and provides practical recommendations for improving the effectiveness and usability of XAI in healthcare.},
  copyright = {2023 The Author(s)},
  langid = {english},
  pmcid = {PMC10202353},
  pmid = {37217779},
  keywords = {Health care,Social sciences},
  annotation = {TLDR: This paper presents the findings of a longitudinal multi-method study involving 112 developers and clinicians co-designing an XAI solution for a clinical decision support system and proposes design solutions that can help address the XAI conundrum in healthcare.},
  timestamp = {2025-02-26T14:46:05Z}
}

@article{biffi2020explainable,
  title = {Explainable {{Anatomical Shape Analysis Through Deep Hierarchical Generative Models}}},
  author = {Biffi, Carlo and Cerrolaza, Juan J. and Tarroni, Giacomo and Bai, Wenjia and De Marvao, Antonio and Oktay, Ozan and Ledig, Christian and Le Folgoc, Loic and Kamnitsas, Konstantinos and Doumou, Georgia and Duan, Jinming and Prasad, Sanjay K. and Cook, Stuart A. and O'Regan, Declan P. and Rueckert, Daniel},
  year = {2020},
  month = jun,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {39},
  number = {6},
  pages = {2088--2099},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/tmi.2020.2964499},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {TLDR: This work proposes a new interpretable deep learning model for shape analysis that scales effectively to large populations, facilitating high-throughput analysis of normal anatomy and pathology in large-scale studies of volumetric imaging.},
  timestamp = {2025-07-25T03:12:13Z}
}

@inproceedings{binder2016layer,
  title = {Layer-Wise Relevance Propagation for Neural Networks with Local Renormalization Layers},
  booktitle = {Artificial Neural Networks and Machine Learning--{{ICANN}} 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part {{II}} 25},
  author = {Binder, Alexander and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2016},
  pages = {63--71},
  publisher = {Springer},
  timestamp = {2025-05-17T14:02:30Z}
}

@article{binder2021morphological,
  title = {Morphological and Molecular Breast Cancer Profiling through Explainable Machine Learning},
  author = {Binder, Alexander and Bockmayr, Michael and H{\"a}gele, Miriam and Wienert, Stephan and Heim, Daniel and Hellweg, Katharina and Ishii, Masaru and Stenzinger, Albrecht and Hocke, Andreas and Denkert, Carsten and M{\"u}ller, Klaus-Robert and Klauschen, Frederick},
  year = {2021},
  month = apr,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {4},
  pages = {355--366},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00303-4},
  urldate = {2025-05-16},
  abstract = {Recent advances in cancer research and diagnostics largely rely on new developments in microscopic or molecular profiling techniques, offering high levels of detail with respect to either spatial or molecular features, but usually not both. Here, we present an explainable machine-learning approach for the integrated profiling of morphological, molecular and clinical features from breast cancer histology. First, our approach allows for the robust detection of cancer cells and tumour-infiltrating lymphocytes in histological images, providing precise heatmap visualizations explaining the classifier decisions. Second, molecular features, including DNA methylation, gene expression, copy number variations, somatic mutations and proteins are predicted from histology. Molecular predictions reach balanced accuracies up to 78\%, whereas accuracies of over 95\% can be achieved for subgroups of patients. Finally, our explainable AI approach allows assessment of the link between morphological and molecular cancer properties. The resulting computational multiplex-histology analysis can help promote basic cancer research and precision medicine through an integrated diagnostic scoring of histological, clinical and molecular features.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Breast cancer,Cancer,Computational biology and bioinformatics,Image processing},
  annotation = {TLDR: An explainable machine-learning approach for the integrated profiling of morphological, molecular and clinical features from breast cancer histology allows for the robust detection of cancer cells and tumour-infiltrating lymphocytes in histological images, and allows assessment of the link between morphological and molecular cancer properties.},
  timestamp = {2025-05-16T12:01:40Z}
}

@article{bingham2019pyro,
  ids = {bingham2019pyroa},
  title = {Pyro: {{Deep}} Universal Probabilistic Programming},
  shorttitle = {Pyro},
  author = {Bingham, Eli and Chen, Jonathan P and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D},
  year = {2019},
  journal = {Journal of machine learning research},
  volume = {20},
  number = {28},
  pages = {1--6},
  issn = {1533-7928},
  urldate = {2025-04-14},
  abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in AI research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of PyTorch, a modern GPU-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
  timestamp = {2025-04-14T09:43:45Z}
}

@article{binz2023using,
  title = {Using Cognitive Psychology to Understand {{GPT-3}}},
  author = {Binz, Marcel and Schulz, Eric},
  year = {2023},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {120},
  number = {6},
  pages = {e2218523120},
  issn = {1091-6490},
  doi = {10.1073/pnas.2218523120},
  abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
  langid = {english},
  pmcid = {PMC9963545},
  pmid = {36730192},
  keywords = {artificial intelligence,cognitive psychology,Cognitive Psychology,Decision Making,decision-making,Humans,language models,Learning,Problem Solving,reasoning,Reinforcement Psychology},
  annotation = {TLDR: The present article finds that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books.},
  timestamp = {2025-09-01T08:59:37Z}
}

@article{biswas2024comprehensive,
  title = {A Comprehensive Review of Explainable {{AI}} for Disease Diagnosis},
  author = {Biswas, Al Amin},
  year = {2024},
  month = jul,
  journal = {Array},
  volume = {22},
  pages = {100345},
  issn = {25900056},
  doi = {10.1016/j.array.2024.100345},
  urldate = {2025-08-13},
  langid = {english},
  timestamp = {2025-08-13T11:05:12Z}
}

@article{bjerregaard-michelsen2025machine,
  title = {Machine Learning for Prediction of 30-Day Mortality in Patients with Advanced Cancer: Comparing Pan-Cancer and Single-Cancer Models},
  shorttitle = {Machine Learning for Prediction of 30-Day Mortality in Patients with Advanced Cancer},
  author = {{Bjerregaard-Michelsen}, S. and Poulsen, L.{\O}. and Bjerrum, A. and B{\o}gsted, M. and Vesteghem, C.},
  year = {2025},
  month = jun,
  journal = {ESMO Real World Data and Digital Oncology},
  volume = {8},
  pages = {100146},
  issn = {29498201},
  doi = {10.1016/j.esmorw.2025.100146},
  urldate = {2025-08-11},
  langid = {english},
  timestamp = {2025-08-11T08:14:10Z}
}

@article{blei2003lda,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  year = {2003},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  pages = {993--1022},
  publisher = {JMLR.org},
  timestamp = {2025-09-06T03:05:31Z}
}

@article{blondel2008louvain,
  title = {Fast Unfolding of Communities in Large Networks},
  author = {Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  year = {2008},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2008},
  number = {10},
  pages = {P10008},
  publisher = {IOP Publishing},
  timestamp = {2025-09-06T03:07:03Z}
}

@article{boge2025causality,
  title = {Causality and Scientific Explanation of Artificial Intelligence Systems in Biomedicine},
  author = {Boge, Florian and Mosig, Axel},
  year = {2025},
  month = apr,
  journal = {Pfl{\"u}gers Archiv - European Journal of Physiology},
  volume = {477},
  number = {4},
  pages = {543--554},
  issn = {1432-2013},
  doi = {10.1007/s00424-024-03033-9},
  urldate = {2025-04-04},
  abstract = {With rapid advances of deep neural networks over the past decade, artificial intelligence (AI) systems are now commonplace in many applications in biomedicine. These systems often achieve high predictive accuracy in clinical studies, and increasingly in clinical practice. Yet, despite their commonly high predictive accuracy, the trustworthiness of AI systems needs to be questioned when it comes to decision-making that affects the well-being of patients or the fairness towards patients or other stakeholders affected by AI-based decisions. To address this, the field of explainable artificial intelligence, or XAI for short, has emerged, seeking to provide means by which AI-based decisions can be explained to experts, users, or other stakeholders. While it is commonly claimed that explanations of artificial intelligence (AI) establish the trustworthiness of AI-based decisions, it remains unclear what traits of explanations cause them to foster trustworthiness. Building on historical cases of scientific explanation in medicine, we here propagate our perspective that, in order to foster trustworthiness, explanations in biomedical AI should meet the criteria of being scientific explanations. To further undermine our approach, we discuss its relation to the concepts of causality and randomized intervention. In our perspective, we combine aspects from the three disciplines of biomedicine, machine learning, and philosophy. From this interdisciplinary angle, we shed light on how the explanation and trustworthiness of artificial intelligence relate to the concepts of causality and robustness. To connect our perspective with AI research practice, we review recent cases of AI-based studies in pathology and, finally, provide guidelines on how to connect AI in biomedicine with scientific explanation.},
  langid = {english},
  keywords = {Artificial Intelligence,Explainable artificial intelligence,Scientific explanation,Trustworthiness},
  annotation = {TLDR: From a perspective from the three disciplines of biomedicine, machine learning, and philosophy, light is shed on how the explanation and trustworthiness of artificial intelligence relate to the concepts of causality and robustness and how to connect AI in biomedicine with scientific explanation.},
  timestamp = {2025-04-04T02:27:09Z}
}

@article{bohle2019layerwise,
  title = {Layer-{{Wise Relevance Propagation}} for {{Explaining Deep Neural Network Decisions}} in {{MRI-Based Alzheimer}}'s {{Disease Classification}}},
  author = {B{\"o}hle, Moritz and Eitel, Fabian and Weygandt, Martin and Ritter, Kerstin},
  year = {2019},
  month = jul,
  journal = {Frontiers in Aging Neuroscience},
  volume = {11},
  publisher = {Frontiers},
  issn = {1663-4365},
  doi = {10.3389/fnagi.2019.00194},
  urldate = {2025-05-17},
  abstract = {Deep neural networks have led to state-of-the-art results in many medical imaging tasks including Alzheimer's disease (AD) detection based on structural magnetic resonance imaging (MRI) data. However, the network decisions are often perceived as being highly non-transparent, making it difficult to apply these algorithms in clinical routine. In this study, we propose using layer-wise relevance propagation (LRP) to visualize convolutional neural network decisions for AD based on MRI data. Similarly to other visualization methods, LRP produces a heatmap in the input space indicating the importance/relevance of each voxel contributing to the final classification outcome. In contrast to susceptibility maps produced by guided backpropagation (``Which change in voxels would change the outcome most?''), the LRP method is able to directly highlight positive contributions to the network classification in the input space. In particular, we show that (1) the LRP method is very specific for individuals (``Why does this person have AD?'') with high inter-patient variability, (2) there is very little relevance for AD in healthy controls and (3) areas that exhibit a lot of relevance correlate well with what is known from literature. To quantify the latter, we compute size-corrected metrics of the summed relevance per brain area, e.g., relevance density or relevance gain. Although these metrics produce very individual ``fingerprints'' of relevance patterns for AD patients, a lot of importance is put on areas in the temporal lobe including the hippocampus. After discussing several limitations such as sensitivity toward the underlying model and computation parameters, we conclude that LRP might have a high potential to assist clinicians in explaining neural network decisions for diagnosing AD (and potentially other diseases) based on structural MRI data.},
  langid = {american},
  keywords = {Alzheimer's disease,Convolutional Neural Networks,deep learning,Explainability,guided backpropagation,Layer-wise relevance propagation,MRI,visualization},
  annotation = {TLDR: It is concluded that LRP might have a high potential to assist clinicians in explaining neural network decisions for diagnosing AD (and potentially other diseases) based on structural MRI data.},
  timestamp = {2025-05-17T10:52:59Z}
}

@article{borys2023explainable,
  title = {Explainable {{AI}} in Medical Imaging: {{An}} Overview for Clinical Practitioners--{{Beyond}} Saliency-Based {{XAI}} Approaches},
  author = {Borys, Katarzyna and Schmitt, Yasmin Alyssa and Nauta, Meike and Seifert, Christin and Kr{\"a}mer, Nicole and Friedrich, Christoph M and Nensa, Felix},
  year = {2023},
  journal = {European journal of radiology},
  volume = {162},
  pages = {110786},
  publisher = {Elsevier},
  timestamp = {2025-03-29T14:12:17Z}
}

@article{borys2023explainablea,
  title = {Explainable {{AI}} in Medical Imaging: {{An}} Overview for Clinical Practitioners -- {{Saliency-based XAI}} Approaches},
  shorttitle = {Explainable {{AI}} in Medical Imaging},
  author = {Borys, Katarzyna and Schmitt, Yasmin Alyssa and Nauta, Meike and Seifert, Christin and Kr{\"a}mer, Nicole and Friedrich, Christoph M. and Nensa, Felix},
  year = {2023},
  month = may,
  journal = {European Journal of Radiology},
  volume = {162},
  pages = {110787},
  issn = {0720048X},
  doi = {10.1016/j.ejrad.2023.110787},
  urldate = {2025-08-13},
  langid = {english},
  annotation = {TLDR: This paper investigates the application of XAI in medical imaging, addressing a broad audience, especially healthcare professionals, and focuses on saliency-based XAI methods, where the explanation can be provided directly on the input data (image) and which naturally are of special importance in medical Imaging.},
  timestamp = {2025-08-13T11:09:48Z}
}

@article{bourgeais2021deep,
  title = {Deep {{GONet}}: Self-Explainable Deep Neural Network Based on {{Gene Ontology}} for Phenotype Prediction from Gene Expression Data},
  shorttitle = {Deep {{GONet}}},
  author = {Bourgeais, Victoria and Zehraoui, Farida and Ben Hamdoune, Mohamed and Hanczar, Blaise},
  year = {2021},
  month = sep,
  journal = {BMC bioinformatics},
  volume = {22},
  number = {Suppl 10},
  pages = {455},
  issn = {1471-2105},
  doi = {10.1186/s12859-021-04370-7},
  abstract = {BACKGROUND: With the rapid advancement of genomic sequencing techniques, massive production of gene expression data is becoming possible, which prompts the development of precision medicine. Deep learning is a promising approach for phenotype prediction (clinical diagnosis, prognosis, and drug response) based on gene expression profile. Existing deep learning models are usually considered as black-boxes that provide accurate predictions but are not interpretable. However, accuracy and interpretation are both essential for precision medicine. In addition, most models do not integrate the knowledge of the domain. Hence, making deep learning models interpretable for medical applications using prior biological knowledge is the main focus of this paper. RESULTS: In this paper, we propose a new self-explainable deep learning model, called Deep GONet, integrating the Gene Ontology into the hierarchical architecture of the neural network. This model is based on a fully-connected architecture constrained by the Gene Ontology annotations, such that each neuron represents a biological function. The experiments on cancer diagnosis datasets demonstrate that Deep GONet is both easily interpretable and highly performant to discriminate cancer and non-cancer samples. CONCLUSIONS: Our model provides an explanation to its predictions by identifying the most important neurons and associating them with biological functions, making the model understandable for biologists and physicians.},
  langid = {english},
  pmcid = {PMC8456586},
  pmid = {34551707},
  keywords = {Deep learning,Gene expression,Gene Expression,Gene Ontology,Humans,Model interpretation,Neoplasms,Neural Networks Computer,Phenotype,Phenotype prediction},
  annotation = {TLDR: This model is based on a fully-connected architecture constrained by the Gene Ontology annotations, such that each neuron represents a biological function, and provides an explanation to its predictions by identifying the most important neurons and associating them with biological functions, making the model understandable for biologists and physicians.},
  timestamp = {2025-05-16T09:02:25Z}
}

@misc{braman2020deepa,
  title = {Deep Learning-Based Prediction of Response to {{HER2-targeted}} Neoadjuvant Chemotherapy from Pre-Treatment Dynamic Breast {{MRI}}: {{A}} Multi-Institutional Validation Study},
  shorttitle = {Deep Learning-Based Prediction of Response to {{HER2-targeted}} Neoadjuvant Chemotherapy from Pre-Treatment Dynamic Breast {{MRI}}},
  author = {Braman, Nathaniel and Adoui, Mohammed El and Vulchi, Manasa and Turk, Paulette and Etesami, Maryam and Fu, Pingfu and Bera, Kaustav and Drisis, Stylianos and Varadan, Vinay and Plecha, Donna and Benjelloun, Mohammed and Abraham, Jame and Madabhushi, Anant},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2001.08570},
  urldate = {2025-05-21},
  abstract = {Predicting response to neoadjuvant therapy is a vexing challenge in breast cancer. In this study, we evaluate the ability of deep learning to predict response to HER2-targeted neo-adjuvant chemotherapy (NAC) from pre-treatment dynamic contrast-enhanced (DCE) MRI acquired prior to treatment. In a retrospective study encompassing DCE-MRI data from a total of 157 HER2+ breast cancer patients from 5 institutions, we developed and validated a deep learning approach for predicting pathological complete response (pCR) to HER2-targeted NAC prior to treatment. 100 patients who received HER2-targeted neoadjuvant chemotherapy at a single institution were used to train (n=85) and tune (n=15) a convolutional neural network (CNN) to predict pCR. A multi-input CNN leveraging both pre-contrast and late post-contrast DCE-MRI acquisitions was identified to achieve optimal response prediction within the validation set (AUC=0.93). This model was then tested on two independent testing cohorts with pre-treatment DCE-MRI data. It achieved strong performance in a 28 patient testing set from a second institution (AUC=0.85, 95\% CI 0.67-1.0, p=.0008) and a 29 patient multicenter trial including data from 3 additional institutions (AUC=0.77, 95\% CI 0.58-0.97, p=0.006). Deep learning-based response prediction model was found to exceed a multivariable model incorporating predictive clinical variables (AUC \&lt; .65 in testing cohorts) and a model of semi-quantitative DCE-MRI pharmacokinetic measurements (AUC \&lt; .60 in testing cohorts). The results presented in this work across multiple sites suggest that with further validation deep learning could provide an effective and reliable tool to guide targeted therapy in breast cancer, thus reducing overtreatment among HER2+ patients.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Applications (stat.AP),Computer Vision and Pattern Recognition (cs.CV),FOS: Biological sciences,FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV),Machine Learning (cs.LG),Machine Learning (stat.ML),Quantitative Methods (q-bio.QM)},
  timestamp = {2025-05-21T00:39:28Z}
}

@article{bresso2021investigating,
  title = {Investigating {{ADR}} Mechanisms with {{Explainable AI}}: A Feasibility Study with Knowledge Graph Mining},
  shorttitle = {Investigating {{ADR}} Mechanisms with {{Explainable AI}}},
  author = {Bresso, Emmanuel and Monnin, Pierre and Bousquet, C{\'e}dric and Calvier, Fran{\c c}ois-Elie and Ndiaye, Ndeye-Coumba and Petitpain, Nadine and {Sma{\"i}l-Tabbone}, Malika and Coulet, Adrien},
  year = {2021},
  month = may,
  journal = {BMC medical informatics and decision making},
  volume = {21},
  number = {1},
  pages = {171},
  issn = {1472-6947},
  doi = {10.1186/s12911-021-01518-6},
  abstract = {BACKGROUND: Adverse drug reactions (ADRs) are statistically characterized within randomized clinical trials and postmarketing pharmacovigilance, but their molecular mechanism remains unknown in most cases. This is true even for hepatic or skin toxicities, which are classically monitored during drug design. Aside from clinical trials, many elements of knowledge about drug ingredients are available in open-access knowledge graphs, such as their properties, interactions, or involvements in pathways. In addition, drug classifications that label drugs as either causative or not for several ADRs, have been established. METHODS: We propose in this paper to mine knowledge graphs for identifying biomolecular features that may enable automatically reproducing expert classifications that distinguish drugs causative or not for a given type of ADR. In an Explainable AI perspective, we explore simple classification techniques such as Decision Trees and Classification Rules because they provide human-readable models, which explain the classification itself, but may also provide elements of explanation for molecular mechanisms behind ADRs. In summary, (1) we mine a knowledge graph for features; (2) we train classifiers at distinguishing, on the basis of extracted features, drugs associated or not with two commonly monitored ADRs: drug-induced liver injuries (DILI) and severe cutaneous adverse reactions (SCAR); (3) we isolate features that are both efficient in reproducing expert classifications and interpretable by experts (i.e., Gene Ontology terms, drug targets, or pathway names); and (4) we manually evaluate in a mini-study how they may be explanatory. RESULTS: Extracted features reproduce with a good fidelity classifications of drugs causative or not for DILI and SCAR (Accuracy~=~0.74 and 0.81, respectively). Experts fully agreed that 73\% and 38\% of the most discriminative features are possibly explanatory for DILI and SCAR, respectively; and partially agreed (2/3) for 90\% and 77\% of them. CONCLUSION: Knowledge graphs provide sufficiently diverse features to enable simple and explainable models to distinguish between drugs that are causative or not for ADRs. In addition to explaining classifications, most discriminative features appear to be good candidates for investigating ADR mechanisms further.},
  langid = {english},
  pmcid = {PMC8157660},
  pmid = {34039343},
  keywords = {Adverse drug reaction,Adverse Drug Reaction Reporting Systems,Artificial Intelligence,Data mining,Drug-Related Side Effects and Adverse Reactions,Explainable AI,Explanation,Feasibility Studies,Humans,Knowledge graph,Machine learning,Mechanism of action,Molecular mechanism,Pattern Recognition Automated,Pharmacovigilance},
  timestamp = {2025-03-19T02:54:10Z}
}

@article{bricken2023monosemanticity,
  title = {Towards Monosemanticity: {{Decomposing}} Language Models with Dictionary Learning},
  author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and {Hatfield-Dodds}, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
  year = {2023},
  journal = {Transformer Circuits Thread},
  timestamp = {2025-09-02T02:39:18Z}
}

@inproceedings{briola2024federated,
  title = {A {{Federated Explainable AI Model}} for {{Breast Cancer Classification}}},
  booktitle = {European {{Interdisciplinary Cybersecurity Conference}}},
  author = {Briola, Eleni and Nikolaidis, Christos Chrysanthos and Perifanis, Vasileios and Pavlidis, Nikolaos and Efraimidis, Pavlos},
  year = {2024},
  month = jun,
  pages = {194--201},
  publisher = {ACM},
  address = {Xanthi Greece},
  doi = {10.1145/3655693.3660255},
  urldate = {2025-04-04},
  isbn = {979-8-4007-1651-5},
  langid = {english},
  annotation = {TLDR: This study explores the intersection of Explainable AI, Privacy, and Federated Learning in breast cancer diagnosis and highlights the potential of federated learning in maintaining privacy and explainability, advancing breast cancer diagnosis and treatment.},
  timestamp = {2025-04-04T06:34:42Z}
}

@article{brito2022explainable,
  title = {An Explainable Artificial Intelligence Approach for Unsupervised Fault Detection and Diagnosis in Rotating Machinery},
  author = {Brito, Lucas C. and Susto, Gian Antonio and Brito, Jorge N. and Duarte, Marcus A.V.},
  year = {2022},
  month = jan,
  journal = {Mechanical Systems and Signal Processing},
  volume = {163},
  pages = {108105},
  issn = {08883270},
  doi = {10.1016/j.ymssp.2021.108105},
  urldate = {2025-04-21},
  abstract = {The monitoring of rotating machinery is an essential task in today's production processes. Currently, several machine learning and deep learning-based modules have achieved excellent results in fault detection and diagnosis. Nevertheless, to further increase user adoption and diffusion of such technologies, users and human experts must be provided with explanations and insights by the modules. Another issue is related, in most cases, with the unavailability of labeled historical data that makes the use of supervised models unfeasible. Therefore, a new approach for fault detection and diagnosis in rotating machinery is here proposed. The methodology consists of three parts: feature extraction, fault detection and fault diagnosis. In the first part, the vibration features in the time and frequency domains are extracted. Secondly, in the fault detection, the presence of fault is verified in an unsupervised manner based on anomaly detection algorithms. The modularity of the methodology allows different algorithms to be implemented. Finally, in fault diagnosis, Shapley Additive Explanations (SHAP), a technique to interpret black-box models, is used. Through the feature importance ranking obtained by the model explainability, the fault diagnosis is performed. Two tools for diagnosis are proposed, namely: unsupervised classification and root cause analysis. The effectiveness of the proposed approach is shown on three datasets containing different mechanical faults in rotating machinery. The study also presents a comparison between models used in machine learning explainability: SHAP and Local Depth-based Feature Importance for the Isolation Forest (Local-DIFFI). Lastly, an analysis of several state-of-art anomaly detection algorithms in rotating machinery is included.},
  langid = {english},
  keywords = {Anomaly detection,Condition monitoring,Explainable artificial intelligence,Fault detection,Fault diagnosis,Rotating machinery},
  annotation = {TLDR: A new approach for fault detection and diagnosis in rotating machinery is proposed, namely: unsupervised classification and root cause analysis, and a comparison between models used in machine learning explainability: SHAP and Local Depth-based Feature Importance for the Isolation Forest (Local-DIFFI).},
  timestamp = {2025-04-21T07:33:34Z}
}

@misc{bubeck2023sparks,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2303.12712},
  urldate = {2025-09-01},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  timestamp = {2025-09-01T12:29:59Z}
}

@article{budhkar2024xsigra,
  title = {{{xSiGra}}: Explainable Model for Single-Cell Spatial Data Elucidation},
  shorttitle = {{{xSiGra}}},
  author = {Budhkar, Aishwarya and Tang, Ziyang and Liu, Xiang and Zhang, Xuhong and Su, Jing and Song, Qianqian},
  year = {2024},
  month = jul,
  journal = {Briefings in Bioinformatics},
  volume = {25},
  number = {5},
  pages = {bbae388},
  issn = {1477-4054},
  doi = {10.1093/bib/bbae388},
  abstract = {Recent advancements in spatial imaging technologies have revolutionized the acquisition of high-resolution multichannel images, gene expressions, and spatial locations at the single-cell level. Our study introduces xSiGra, an interpretable graph-based AI model, designed to elucidate interpretable features of identified spatial cell types, by harnessing multimodal features from spatial imaging technologies. By constructing a spatial cellular graph with immunohistology images and gene expression as node attributes, xSiGra employs hybrid graph transformer models to delineate spatial cell types. Additionally, xSiGra integrates a novel variant of gradient-weighted class activation mapping component to uncover interpretable features, including pivotal genes and cells for various cell types, thereby facilitating deeper biological insights from spatial data. Through rigorous benchmarking against existing methods, xSiGra demonstrates superior performance across diverse spatial imaging datasets. Application of xSiGra on a lung tumor slice unveils the importance score of cells, illustrating that cellular activity is not solely determined by itself but also impacted by neighboring cells. Moreover, leveraging the identified interpretable genes, xSiGra reveals endothelial cell subset interacting with tumor cells, indicating its heterogeneous underlying mechanisms within complex cellular interactions.},
  langid = {english},
  pmcid = {PMC11312371},
  pmid = {39120644},
  keywords = {Algorithms,Computational Biology,explainable AI,Humans,hybrid graph transformer,interpretable features,Lung Neoplasms,Single-Cell Analysis,spatial cell recognition},
  timestamp = {2025-04-17T11:35:08Z}
}

@article{bulten2022artificial,
  title = {Artificial Intelligence for Diagnosis and {{Gleason}} Grading of Prostate Cancer: The {{PANDA}} Challenge},
  shorttitle = {Artificial Intelligence for Diagnosis and {{Gleason}} Grading of Prostate Cancer},
  author = {Bulten, Wouter and Kartasalo, Kimmo and Chen, Po-Hsuan Cameron and Str{\"o}m, Peter and Pinckaers, Hans and Nagpal, Kunal and Cai, Yuannan and Steiner, David F. and {van Boven}, Hester and Vink, Robert and {Hulsbergen-van de Kaa}, Christina and {van der Laak}, Jeroen and Amin, Mahul B. and Evans, Andrew J. and {van der Kwast}, Theodorus and Allan, Robert and Humphrey, Peter A. and Gr{\"o}nberg, Henrik and Samaratunga, Hemamali and Delahunt, Brett and Tsuzuki, Toyonori and H{\"a}kkinen, Tomi and Egevad, Lars and Demkin, Maggie and Dane, Sohier and Tan, Fraser and Valkonen, Masi and Corrado, Greg S. and Peng, Lily and Mermel, Craig H. and Ruusuvuori, Pekka and Litjens, Geert and Eklund, Martin},
  year = {2022},
  month = jan,
  journal = {Nature Medicine},
  volume = {28},
  number = {1},
  pages = {154--163},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-021-01620-2},
  urldate = {2025-05-04},
  abstract = {Artificial intelligence (AI) has shown promise for diagnosing prostate cancer in biopsies. However, results have been limited to individual studies, lacking validation in multinational settings. Competitions have been shown to be accelerators for medical imaging innovations, but their impact is hindered by lack of reproducibility and independent validation. With this in mind, we organized the PANDA challenge---the largest histopathology competition to date, joined by 1,290 developers---to catalyze development of reproducible AI algorithms for Gleason grading using 10,616 digitized prostate biopsies. We validated that a diverse set of submitted algorithms reached pathologist-level performance on independent cross-continental cohorts, fully blinded to the algorithm developers. On United States and European external validation sets, the algorithms achieved agreements of 0.862 (quadratically weighted {$\kappa$}, 95\% confidence interval (CI), 0.840--0.884) and 0.868 (95\% CI, 0.835--0.900) with expert uropathologists. Successful generalization across different patient populations, laboratories and reference standards, achieved by a variety of algorithmic approaches, warrants evaluating AI-based Gleason grading in prospective clinical trials.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Medical imaging,Prostate cancer},
  annotation = {TLDR: The PANDA challenge is organized---the largest histopathology competition to date, joined by 1,290 developers---to catalyze development of reproducible AI algorithms for Gleason grading using 10,616 digitized prostate biopsies and represents a blueprint for evaluating AI algorithms in digital pathology.},
  timestamp = {2025-05-04T03:24:35Z}
}

@article{bunne2023learning,
  title = {Learning Single-Cell Perturbation Responses Using Neural Optimal Transport},
  author = {Bunne, Charlotte and Stark, Stefan G. and Gut, Gabriele and {del Castillo}, Jacobo Sarabia and Levesque, Mitch and Lehmann, Kjong-Van and Pelkmans, Lucas and Krause, Andreas and R{\"a}tsch, Gunnar},
  year = {2023},
  month = nov,
  journal = {Nature Methods},
  volume = {20},
  number = {11},
  pages = {1759--1768},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-023-01969-x},
  urldate = {2025-05-17},
  abstract = {Understanding and predicting molecular responses in single cells upon chemical, genetic or mechanical perturbations is a core question in biology. Obtaining single-cell measurements typically requires the cells to be destroyed. This makes learning heterogeneous perturbation responses challenging as we only observe unpaired distributions of perturbed or non-perturbed cells. Here we leverage the theory of optimal transport and the recent advent of input convex neural architectures to present CellOT, a framework for learning the response of individual cells to a given perturbation by mapping these unpaired distributions. CellOT outperforms current methods at predicting single-cell drug responses, as profiled by scRNA-seq and a multiplexed protein-imaging technology. Further, we illustrate that CellOT generalizes well on unseen settings by (1) predicting the scRNA-seq responses of holdout patients with lupus exposed to interferon-{$\beta$} and patients with glioblastoma to panobinostat; (2) inferring lipopolysaccharide responses across different species; and (3) modeling the hematopoietic developmental trajectories of different subpopulations.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Machine learning,Molecular biology},
  annotation = {TLDR: CellOT combines the benefits of optimal transport and input convex neural architectures to directly learn and uncover maps between control and perturbed cell states at the single-cell level, a framework for learning the response of individual cells to a given perturbation by mapping these unpaired distributions.},
  timestamp = {2025-05-17T07:24:10Z}
}

@article{burkart2021survey,
  title = {A {{Survey}} on the {{Explainability}} of {{Supervised Machine Learning}}},
  author = {Burkart, Nadia and Huber, Marco F.},
  year = {2021},
  month = jan,
  journal = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {245--317},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12228},
  urldate = {2024-11-11},
  abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
  annotation = {TLDR: This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning, and a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions.},
  timestamp = {2024-11-11T15:38:43Z}
}

@article{byeon2025interpretable,
  title = {Interpretable Multimodal Transformer for Prediction of Molecular Subtypes and Grades in Adult-Type Diffuse Gliomas},
  author = {Byeon, Yunsu and Park, Yae Won and Lee, Soohyun and Park, Doohyun and Shin, HyungSeob and Han, Kyunghwa and Chang, Jong Hee and Kim, Se Hoon and Lee, Seung-Koo and Ahn, Sung Soo and Hwang, Dosik},
  year = {2025},
  month = mar,
  journal = {npj Digital Medicine},
  volume = {8},
  number = {1},
  pages = {1--10},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-025-01530-4},
  urldate = {2025-04-13},
  abstract = {Molecular subtyping and grading of adult-type diffuse gliomas are essential for treatment decisions and patient prognosis. We introduce GlioMT, an interpretable multimodal transformer that integrates imaging and clinical data to predict the molecular subtype and grade of adult-type diffuse gliomas according to the 2021 WHO classification. GlioMT is trained on multiparametric MRI data from an institutional set of 1053 patients with adult-type diffuse gliomas to predict the IDH mutation status, 1p/19q codeletion status, and tumor grade. External validation on the TCGA (200 patients) and UCSF (477 patients) shows that GlioMT outperforms conventional CNNs and visual transformers, achieving AUCs of 0.915 (TCGA) and 0.981 (UCSF) for IDH mutation, 0.854 (TCGA) and 0.806 (UCSF) for 1p/19q codeletion, and 0.862 (TCGA) and 0.960 (UCSF) for grade prediction. GlioMT enhances the reliability of clinical decision-making by offering interpretability through attention maps and contributions of imaging and clinical data.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Biomedical engineering,Cancer,CNS cancer,Computer science,Image processing},
  annotation = {TLDR: GlioMT is an interpretable multimodal transformer that integrates imaging and clinical data to predict the molecular subtype and grade of adult-type diffuse gliomas according to the 2021 WHO classification and enhances the reliability of clinical decision-making by offering interpretability through attention maps and contributions of imaging and clinical data.},
  timestamp = {2025-04-13T07:12:34Z}
}

@article{cabitza2023quod,
  title = {Quod Erat Demonstrandum? - {{Towards}} a Typology of the Concept of Explanation for the Design of Explainable {{AI}}},
  shorttitle = {Quod Erat Demonstrandum?},
  author = {Cabitza, Federico and Campagner, Andrea and Malgieri, Gianclaudio and Natali, Chiara and Schneeberger, David and Stoeger, Karl and Holzinger, Andreas},
  year = {2023},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {213},
  pages = {118888},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.118888},
  urldate = {2025-03-26},
  abstract = {In this paper, we present a fundamental framework for defining different types of explanations of AI systems and the criteria for evaluating their quality. Starting from a structural view of how explanations can be constructed, i.e., in terms of an explanandum (what needs to be explained), multiple explanantia (explanations, clues, or parts of information that explain), and a relationship linking explanandum and explanantia, we propose an explanandum-based typology and point to other possible typologies based on how explanantia are presented and how they relate to explanandia. We also highlight two broad and complementary perspectives for defining possible quality criteria for assessing explainability: epistemological and psychological (cognitive). These definition attempts aim to support the three main functions that we believe should attract the interest and further research of XAI scholars: clear inventories, clear verification criteria, and clear validation methods.},
  keywords = {Artificial intelligence,Explainable AI,Explanations,Machine learning,Taxonomy,XAI},
  timestamp = {2025-03-26T14:31:21Z}
}

@article{cai2024new,
  title = {A New Framework for Exploratory Network Mediator Analysis in Omics Data},
  author = {Cai, Qingpo and Fu, Yinghao and Lyu, Cheng and Wang, Zihe and Rao, Shun and Alvarez, Jessica A. and Bai, Yun and Kang, Jian and Yu, Tianwei},
  year = {2024},
  month = may,
  journal = {Genome Research},
  pages = {genome;gr.278684.123v1},
  issn = {1088-9051, 1549-5469},
  doi = {10.1101/gr.278684.123},
  urldate = {2025-04-03},
  abstract = {Omics methods are widely used in basic biology and translational medicine research. More and more omics data are collected to explain the impact of certain risk factors on clinical outcomes. To explain the mechanism of the risk factors, a core question is how to find the genes/proteins/metabolites that mediate their effects on the clinical outcome. Mediation analysis is a modeling framework to study the relationship between risk factors and pathological outcomes, via mediator variables. However, high-dimensional omics data are far more challenging than traditional data: (1) From tens of thousands of genes, can we overcome the curse of dimensionality to reliably select a set of mediators? (2) How do we ensure that the selected mediators are functionally consistent? (3) Many biological mechanisms contain nonlinear effects. How do we include nonlinear effects in the high-dimensional mediation analysis? (4) How do we consider multiple risk factors at the same time? To meet these challenges, we propose a new exploratory mediation analysis framework, medNet, which focuses on finding mediators through predictive modeling. We propose new definitions for predictive exposure, predictive mediator, and predictive network mediator, using a statistical hypothesis testing framework to identify predictive exposures and mediators. Additionally, two heuristic search algorithms are proposed to identify network mediators, essentially subnetworks in the genome-scale biological network that mediate the effects of single or multiple exposures. We applied medNet on a breast cancer data set and a metabolomics data set combined with food intake questionnaire data. It identified functionally consistent network mediators for the exposures' impact on the outcome, facilitating data interpretation.},
  langid = {english},
  annotation = {TLDR: A new exploratory mediation analysis framework, medNet, which focuses on finding mediators through predictive modeling, and new definitions for predictive exposure, predictive mediator, and predictive network mediator are proposed, using a statistical hypothesis testing framework to identify predictive exposures and mediators.},
  timestamp = {2025-04-03T02:47:13Z}
}

@article{Caliendo2008,
  title = {Some Practical Guidance for the Implementation of Propensity Score Matching},
  author = {Caliendo, Marco and Kopeinig, Sabine},
  year = {2008},
  journal = {Journal of Economic Surveys},
  volume = {22},
  number = {1},
  pages = {31--72},
  timestamp = {2025-03-20T04:46:19Z}
}

@article{campanella2019clinicalgrade,
  title = {Clinical-Grade Computational Pathology Using Weakly Supervised Deep Learning on Whole Slide Images},
  author = {Campanella, Gabriele and Hanna, Matthew G. and Geneslaw, Luke and Miraflor, Allen and Werneck Krauss Silva, Vitor and Busam, Klaus J. and Brogi, Edi and Reuter, Victor E. and Klimstra, David S. and Fuchs, Thomas J.},
  year = {2019},
  month = aug,
  journal = {Nature Medicine},
  volume = {25},
  number = {8},
  pages = {1301--1309},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-019-0508-1},
  urldate = {2025-05-23},
  abstract = {The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44,732 whole slide images from 15,187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0.98 for all cancer types. Its clinical application would allow pathologists to exclude 65--75\% of slides while retaining 100\% sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computer modelling,Computer science,High-throughput screening,Pathology},
  annotation = {TLDR: A multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations, and has the ability to train accurate classification models at unprecedented scale.},
  timestamp = {2025-05-23T01:53:42Z}
}

@article{captier2025integration,
  title = {Integration of Clinical, Pathological, Radiological, and Transcriptomic Data Improves Prediction for First-Line Immunotherapy Outcome in Metastatic Non-Small Cell Lung Cancer},
  author = {Captier, Nicolas and Lerousseau, Marvin and Orlhac, Fanny and {Hovhannisyan-Baghdasarian}, Narin{\'e}e and Luporsi, Marie and Woff, Erwin and Lagha, Sarah and Salamoun Feghali, Paulette and Lonjou, Christine and Beaulaton, Cl{\'e}ment and Zinovyev, Andrei and Salmon, H{\'e}l{\`e}ne and Walter, Thomas and Buvat, Ir{\`e}ne and Girard, Nicolas and Barillot, Emmanuel},
  year = {2025},
  month = jan,
  journal = {Nature Communications},
  volume = {16},
  number = {1},
  pages = {614},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-025-55847-5},
  urldate = {2025-05-19},
  abstract = {Immunotherapy is improving the survival of patients with metastatic non-small cell lung cancer (NSCLC), yet reliable biomarkers are needed to identify responders prospectively and optimize patient care. In this study, we explore the benefits of multimodal approaches to predict immunotherapy outcome using multiple machine learning algorithms and integration strategies. We analyze baseline multimodal data from a cohort of 317 metastatic NSCLC patients treated with first-line immunotherapy, including positron emission tomography images, digitized pathological slides, bulk transcriptomic profiles, and clinical information. Testing multiple integration strategies, most of them yield multimodal models surpassing both the best unimodal models and established univariate biomarkers, such as PD-L1 expression. Additionally, several multimodal combinations demonstrate improved patient risk stratification compared to models built with routine clinical features only. Our study thus provides evidence of the superiority of multimodal over unimodal approaches, advocating for the collection of large multimodal NSCLC datasets to develop and validate robust and powerful immunotherapy biomarkers.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Cancer immunotherapy,Machine learning,Non-small-cell lung cancer,Predictive markers},
  annotation = {TLDR: Evidence of the superiority of multimodal over unimodal approaches is provided, advocating for the collection of large multimodal NSCLC datasets to develop and validate robust and powerful immunotherapy biomarkers.},
  timestamp = {2025-05-19T04:09:34Z}
}

@article{castro2020causality,
  title = {Causality Matters in Medical Imaging},
  author = {Castro, Daniel C. and Walker, Ian and Glocker, Ben},
  year = {2020},
  month = jul,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {3673},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17478-w},
  urldate = {2025-07-25},
  abstract = {Causal reasoning can shed new light on the major challenges in machine learning for medical imaging: scarcity of high-quality annotated data and mismatch between the development dataset and the target environment. A causal perspective on these issues allows decisions about data collection, annotation, preprocessing, and learning strategies to be made and scrutinized more transparently, while providing a detailed categorisation of potential biases and mitigation techniques. Along with worked clinical examples, we highlight the importance of establishing the causal relationship between images and their annotations, and offer step-by-step recommendations for future studies.},
  copyright = {2020 The Author(s)},
  langid = {english},
  pmcid = {PMC7376027},
  pmid = {32699250},
  keywords = {Causality,Computational models,Data processing,Diagnostic Imaging,Humans,Image Interpretation Computer-Assisted,Machine learning,Machine Learning,Medical research,Predictive medicine},
  annotation = {TLDR: The authors show how causal reasoning can shed new light on scarcity of high-quality annotated data and mismatch between the development dataset and the target environment, and show step-by-step recommendations for future studies.},
  timestamp = {2025-07-25T12:08:03Z}
}

@misc{causal,
  title = {Causal {{Attention}} for {{Interpretable}} and {{Generalizable Graph Classification}} {\textbar} {{Proceedings}} of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  journal = {ACM Conferences},
  doi = {10.1145/3534678.3539366},
  urldate = {2025-03-23},
  langid = {english},
  timestamp = {2025-03-23T02:57:12Z}
}

@article{chaddad2023survey,
  title = {Survey of {{Explainable AI Techniques}} in {{Healthcare}}},
  author = {Chaddad, Ahmad and Peng, Jihao and Xu, Jian and Bouridane, Ahmed},
  year = {2023},
  month = jan,
  journal = {Sensors},
  volume = {23},
  number = {2},
  pages = {634},
  issn = {1424-8220},
  doi = {10.3390/s23020634},
  urldate = {2025-05-28},
  abstract = {Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient's symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {deep learning,explainable AI,medical imaging,radiomics},
  annotation = {TLDR: A survey of the most recent XAI techniques used in healthcare and related medical imaging applications, which summarizes and categorizes the XAI types, and highlights the algorithms used to increase interpretability in medical imaging topics.},
  timestamp = {2025-05-28T07:54:06Z}
}

@inproceedings{chakraborty2022visual,
  title = {Visual {{Attention Analysis Of Pathologists Examining Whole Slide Images Of Prostate Cancer}}},
  booktitle = {2022 {{IEEE}} 19th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {Chakraborty, Souradeep and Ma, Ke and Gupta, Rajarsi and Knudsen, Beatrice and Zelinsky, Gregory J. and Saltz, Joel H. and Samaras, Dimitris},
  year = {2022},
  month = mar,
  pages = {1--5},
  issn = {1945-8452},
  doi = {10.1109/ISBI52829.2022.9761489},
  urldate = {2025-05-04},
  abstract = {We study the attention of pathologists as they examine whole-slide images (WSIs) of prostate cancer tissue using a digital microscope. To the best of our knowledge, our study is the first to report in detail how pathologists navigate WSIs of prostate cancer as they accumulate information for their diagnoses. We collected slide navigation data (i.e., viewport location, magnification level, and time) from 13 pathologists in 2 groups (5 genitourinary (GU) specialists and 8 general pathologists) and generated visual attention heatmaps and scanpaths. Each pathologist examined five WSIs from the TCGA PRAD dataset, which were selected by a GU pathology specialist. We examined and analyzed the distributions of visual attention for each group of pathologists after each WSI was examined. To quantify the relationship between a pathologist's attention and evidence for cancer in the WSI, we obtained tumor annotations from a genitourinary specialist. We used these annotations to compute the overlap between the distribution of visual attention and annotated tumor region to identify strong correlations. Motivated by this analysis, we trained a deep learning model to predict visual attention on unseen WSIs. We find that the attention heatmaps predicted by our model correlate quite well with the ground truth attention heatmap and tumor annotations on a test set of 17 WSIs by using various spatial and temporal evaluation metrics.},
  keywords = {Annotations,Deep learning,digital histopathology,Heating systems,Image segmentation,Navigation,Predictive models,Prostate cancer,tumor segmentation,visual attention,Visualization},
  annotation = {TLDR: This study is the first to report in detail how pathologists navigate WSIs of prostate cancer as they accumulate information for their diagnoses, and trains a deep learning model to predict visual attention on unseen WSIs.},
  timestamp = {2025-05-04T04:01:10Z}
}

@article{chakraborty2024discovering,
  title = {Discovering Genetic Biomarkers for Targeted Cancer Therapeutics with {{eXplainable Artificial Intelligence}}},
  author = {Chakraborty, Debaditya and {Gutierrez-Chakraborty}, Elizabeth and {Rodriguez-Aguayo}, Cristian and Ba{\c s}a{\u g}ao{\u g}lu, Hakan and {Lopez-Berestein}, Gabriel and Amero, Paola},
  year = {2024},
  month = may,
  journal = {Cancer Communications (London, England)},
  volume = {44},
  number = {5},
  pages = {584--588},
  issn = {2523-3548},
  doi = {10.1002/cac2.12530},
  langid = {english},
  pmcid = {PMC11110951},
  pmid = {38566430},
  keywords = {Artificial Intelligence,Biomarkers Tumor,Genetic Markers,Humans,Molecular Targeted Therapy,Neoplasms},
  timestamp = {2025-07-25T07:25:00Z}
}

@misc{chan2023assessing,
  title = {Assessing the {{Usability}} of {{GutGPT}}: {{A Simulation Study}} of an {{AI Clinical Decision Support System}} for {{Gastrointestinal Bleeding Risk}}},
  shorttitle = {Assessing the {{Usability}} of {{GutGPT}}},
  author = {Chan, Colleen and You, Kisung and Chung, Sunny and Giuffr{\`e}, Mauro and Saarinen, Theo and Rajashekar, Niroop and Pu, Yuan and Shin, Yeo Eun and Laine, Loren and Wong, Ambrose and Kizilcec, Ren{\'e} and Sekhon, Jasjeet and Shung, Dennis},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10072},
  eprint = {2312.10072},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10072},
  urldate = {2025-07-04},
  abstract = {Applications of large language models (LLMs) like ChatGPT have potential to enhance clinical decision support through conversational interfaces. However, challenges of human-algorithmic interaction and clinician trust are poorly understood. GutGPT, a LLM for gastrointestinal (GI) bleeding risk prediction and management guidance, was deployed in clinical simulation scenarios alongside the electronic health record (EHR) with emergency medicine physicians, internal medicine physicians, and medical students to evaluate its effect on physician acceptance and trust in AI clinical decision support systems (AI-CDSS). GutGPT provides risk predictions from a validated machine learning model and evidence-based answers by querying extracted clinical guidelines. Participants were randomized to GutGPT and an interactive dashboard, or the interactive dashboard and a search engine. Surveys and educational assessments taken before and after measured technology acceptance and content mastery. Preliminary results showed mixed effects on acceptance after using GutGPT compared to the dashboard or search engine but appeared to improve content mastery based on simulation performance. Overall, this study demonstrates LLMs like GutGPT could enhance effective AI-CDSS if implemented optimally and paired with interactive interfaces.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Applications},
  annotation = {TLDR: It is demonstrated LLMs like GutGPT could enhance effective AI-CDSS if implemented optimally and paired with interactive interfaces if implemented optimally and paired with interactive interfaces.},
  timestamp = {2025-07-04T09:18:15Z}
}

@article{chanda2024dermatologistlike,
  title = {Dermatologist-like Explainable {{AI}} Enhances Trust and Confidence in Diagnosing Melanoma},
  author = {Chanda, Tirtha and Hauser, Katja and Hobelsberger, Sarah and Bucher, Tabea-Clara and Garcia, Carina Nogueira and Wies, Christoph and Kittler, Harald and Tschandl, Philipp and {Navarrete-Dechent}, Cristian and Podlipnik, Sebastian and Chousakos, Emmanouil and Crnaric, Iva and Majstorovic, Jovana and Alhajwan, Linda and Foreman, Tanya and Peternel, Sandra and Sarap, Sergei and {\"O}zdemir, {\.I}rem and Barnhill, Raymond L. and {Llamas-Velasco}, Mar and Poch, Gabriela and Korsing, S{\"o}ren and Sondermann, Wiebke and Gellrich, Frank Friedrich and Heppt, Markus V. and Erdmann, Michael and Haferkamp, Sebastian and Drexler, Konstantin and Goebeler, Matthias and Schilling, Bastian and Utikal, Jochen S. and Ghoreschi, Kamran and Fr{\"o}hling, Stefan and {Krieghoff-Henning}, Eva and Brinker, Titus J.},
  year = {2024},
  month = jan,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {524},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-43095-4},
  urldate = {2025-05-01},
  abstract = {Artificial intelligence (AI) systems have been shown to help dermatologists diagnose melanoma more accurately, however they lack transparency, hindering user acceptance. Explainable AI (XAI) methods can help to increase transparency, yet often lack precise, domain-specific explanations. Moreover, the impact of XAI methods on dermatologists' decisions has not yet been evaluated. Building upon previous research, we introduce an XAI system that provides precise and domain-specific explanations alongside its differential diagnoses of melanomas and nevi. Through a three-phase study, we assess its impact on dermatologists' diagnostic accuracy, diagnostic confidence, and trust in the XAI-support. Our results show strong alignment between XAI and dermatologist explanations. We also show that dermatologists' confidence in their diagnoses, and their trust in the support system significantly increase with XAI compared to conventional AI. This study highlights dermatologists' willingness to adopt such XAI systems, promoting future use in the clinic.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Diagnostic markers,Physical examination,Preventive medicine},
  annotation = {TLDR: An explainable AI system which produces text- and region-based explanations alongside its classifications is developed which was assessed using clinicians' diagnostic accuracy, diagnostic confidence, and their trust in the system.},
  timestamp = {2025-05-01T12:48:58Z}
}

@article{chandak2023building,
  title = {Building a Knowledge Graph to Enable Precision Medicine},
  author = {Chandak, Payal and Huang, Kexin and Zitnik, Marinka},
  year = {2023},
  month = feb,
  journal = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {67},
  issn = {2052-4463},
  doi = {10.1038/s41597-023-01960-3},
  urldate = {2025-06-12},
  abstract = {Abstract             Developing personalized diagnostic strategies and targeted treatments requires a deep understanding of disease biology and the ability to dissect the relationship between molecular and genetic factors and their phenotypic consequences. However, such knowledge is fragmented across publications, non-standardized repositories, and evolving ontologies describing various scales of biological organization between genotypes and clinical phenotypes. Here, we present PrimeKG, a multimodal knowledge graph for precision medicine analyses. PrimeKG integrates 20 high-quality resources to describe 17,080 diseases with 4,050,249 relationships representing ten major biological scales, including disease-associated protein perturbations, biological processes and pathways, anatomical and phenotypic scales, and the entire range of approved drugs with their therapeutic action, considerably expanding previous efforts in disease-rooted knowledge graphs. PrimeKG contains an abundance of `indications', `contradictions', and `off-label use' drug-disease edges that lack in other knowledge graphs and can support AI analyses of how drugs affect disease-associated networks. We supplement PrimeKG's graph structure with language descriptions of clinical guidelines to enable multimodal analyses and provide instructions for continual updates of PrimeKG as new data become available.},
  langid = {english},
  annotation = {TLDR: PrimeKG is presented, a multimodal knowledge graph for precision medicine analyses that contains an abundance of `indications', `contradictions', and `off-label use' drug-disease edges that lack in other knowledge graphs and can support AI analyses of how drugs affect disease-associated networks.},
  timestamp = {2025-06-12T13:56:21Z}
}

@article{chang2020assessment,
  title = {Assessment of Knee Pain from {{MR}} Imaging Using a Convolutional {{Siamese}} Network},
  author = {Chang, Gary H. and Felson, David T. and Qiu, Shangran and Guermazi, Ali and Capellini, Terence D. and Kolachalama, Vijaya B.},
  year = {2020},
  month = jun,
  journal = {European Radiology},
  volume = {30},
  number = {6},
  pages = {3538--3548},
  issn = {1432-1084},
  doi = {10.1007/s00330-020-06658-3},
  urldate = {2025-05-17},
  abstract = {It remains difficult to characterize the source of pain in knee joints either using radiographs or magnetic resonance imaging (MRI). We sought to determine if advanced machine learning methods such as deep neural networks could distinguish knees with pain from those without it and identify the structural features that are associated with knee pain.},
  langid = {english},
  keywords = {Diffusion Tensor Imaging,Functional magnetic resonance imaging,Joint Pain,Knee,Machine learning,Magnetic resonance imaging,Magnetic Resonance Imaging,Osteoarthritis,Pain,Pain Medicine},
  annotation = {TLDR: This article is the first to leverage a deep learning framework to associate MR images of the knee with knee pain from MRI scans with a convolutional Siamese network that had the ability to fuse information from multiple two-dimensional MRI slices from the knees with pain and the contralateral knee of the same individual without pain.},
  timestamp = {2025-05-17T10:56:08Z}
}

@misc{charpignon2021drug,
  title = {Drug Repurposing of Metformin for {{Alzheimer}}'s Disease: {{Combining}} Causal Inference in Medical Records Data and Systems Pharmacology for Biomarker Identification},
  shorttitle = {Drug Repurposing of Metformin for {{Alzheimer}}'s Disease},
  author = {Charpignon, Marie-Laure and {Vakulenko-Lagun}, Bella and Zheng, Bang and Magdamo, Colin and Su, Bowen and Evans, Kyle and Rodriguez, Steve and Sokolov, Artem and Boswell, Sarah and Sheu, Yi-Han and Somai, Melek and Middleton, Lefkos and Hyman, Bradley T. and Betensky, Rebecca A. and Finkelstein, Stan N. and Welsch, Roy E. and Tzoulaki, Ioanna and Blacker, Deborah and Das, Sudeshna and Albers, Mark W.},
  year = {2021},
  month = aug,
  pages = {2021.08.10.21261747},
  publisher = {medRxiv},
  doi = {10.1101/2021.08.10.21261747},
  urldate = {2025-05-04},
  abstract = {Metformin, an antidiabetic drug, triggers anti-aging cellular responses. Aging is the principal risk factor for dementia, but previous observational studies of the diabetes drugs metformin vs. sulfonylureas have been mixed. We tested the hypotheses that metformin improves survival and reduces the risk of dementia, relative to the sulfonylureas, by emulating target trials in electronic health records of diabetic patients at an academic-centered healthcare system in the US and a wide-ranging group of primary care practices in the UK. To address metformin's potentially dual influences on dementia risk---that it might reduce the hazard of death and put more people at risk of developing dementia while reducing the hazard of dementia by slowing biological aging, we used a competing risks approach and carefully grounded that within a causal inference emulated trial framework. To identify candidate biomarkers of metformin's actions in the brain that might mediate reduced dementia risk, we conducted an in-vitro systems pharmacology evaluation of metformin and glyburide on differentiated human neural cells through differential gene expression. We named our multi-dimensional approach DRIAD-EHR (Drug Repurposing in Alzheimer's Disease-Electronic Health Records). In intention-to-treat analyses, metformin was associated with a lower hazard of all-cause mortality than sulfonylureas in both cohorts. In competing risks analyses, there was also a lower cause-specific hazard of dementia onset among metformin initiators. In in-vitro studies, metformin reduced human neural cell expression of SPP1 and APOE, two secreted proteins that have been implicated in Alzheimer's disease pathogenesis and whose levels can be quantified in the CSF. Together, our findings suggest that metformin might prevent dementia in patients without type II diabetes. In addition, our results inform the design of clinical trials of metformin in non-diabetics and suggest a pharmacodynamic CSF biomarker, SPP1, for metformin's action in the brain.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  annotation = {TLDR: It is suggested that metformin might prevent dementia in patients without type II diabetes, by emulating target trials in electronic health records of diabetic patients at an academic-centered healthcare system in the US and a wide-ranging group of primary care practices in the UK.},
  timestamp = {2025-05-04T08:45:27Z}
}

@inproceedings{chattopadhyay2018gradcam,
  title = {Grad-{{CAM}}++: {{Improved Visual Explanations}} for {{Deep Convolutional Networks}}},
  shorttitle = {Grad-{{CAM}}++},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Chattopadhyay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
  year = {2018},
  month = mar,
  eprint = {1710.11063},
  primaryclass = {cs},
  pages = {839--847},
  doi = {10.1109/WACV.2018.00097},
  urldate = {2025-04-03},
  abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {TLDR: This paper proposes Grad-CAM++, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the class label under consideration, to provide better visual explanations of CNN model predictions.},
  timestamp = {2025-04-03T12:57:50Z}
}

@misc{chen2020causalml,
  title = {{{CausalML}}: {{Python}} Package for Causal Machine Learning},
  author = {Chen, Huigang and Harinen, Totte and Lee, Jeong-Yoon and Yung, Mike and Zhao, Zhenyu},
  year = {2020},
  eprint = {2002.11631},
  primaryclass = {cs.CY},
  archiveprefix = {arXiv},
  howpublished = {arXiv preprint},
  timestamp = {2025-03-20T10:17:12Z}
}

@article{chen2020concept,
  title = {Concept Whitening for Interpretable Image Recognition},
  author = {Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
  year = {2020},
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {12},
  pages = {772--782},
  publisher = {Nature Publishing Group},
  timestamp = {2025-09-06T03:07:53Z}
}

@article{chen2020identification,
  title = {Identification of the {{Framingham Risk Score}} by an {{Entropy-Based Rule Model}} for {{Cardiovascular Disease}}},
  author = {Chen, You-Shyang and Cheng, Ching-Hsue and Chen, Su-Fen and Jhuang, Jhe-You},
  year = {2020},
  month = dec,
  journal = {Entropy},
  volume = {22},
  number = {12},
  pages = {1406},
  issn = {1099-4300},
  doi = {10.3390/e22121406},
  urldate = {2025-06-12},
  abstract = {Since 2001, cardiovascular disease (CVD) has had the second-highest mortality rate, about 15,700 people per year, in Taiwan. It has thus imposed a substantial burden on medical resources. This study was triggered by the following three factors. First, the CVD problem reflects an urgent issue. A high priority has been placed on long-term therapy and prevention to reduce the wastage of medical resources, particularly in developed countries. Second, from the perspective of preventive medicine, popular data-mining methods have been well learned and studied, with excellent performance in medical fields. Thus, identification of the risk factors of CVD using these popular techniques is a prime concern. Third, the Framingham risk score is a core indicator that can be used to establish an effective prediction model to accurately diagnose CVD. Thus, this study proposes an integrated predictive model to organize five notable classifiers: the rough set (RS), decision tree (DT), random forest (RF), multilayer perceptron (MLP), and support vector machine (SVM), with a novel use of the Framingham risk score for attribute selection (i.e., F-attributes first identified in this study) to determine the key features for identifying CVD. Verification experiments were conducted with three evaluation criteria---accuracy, sensitivity, and specificity---based on 1190 instances of a CVD dataset available from a Taiwan teaching hospital and 2019 examples from a public Framingham dataset. Given the empirical results, the SVM showed the best performance in terms of accuracy (99.67\%), sensitivity (99.93\%), and specificity (99.71\%) in all F-attributes in the CVD dataset compared to the other listed classifiers. The RS showed the highest performance in terms of accuracy (85.11\%), sensitivity (86.06\%), and specificity (85.19\%) in most of the F-attributes in the Framingham dataset. The above study results support novel evidence that no classifier or model is suitable for all practical datasets of medical applications. Thus, identifying an appropriate classifier to address specific medical data is important. Significantly, this study is novel in its calculation and identification of the use of key Framingham risk attributes integrated with the DT technique to produce entropy-based decision rules of knowledge sets, which has not been undertaken in previous research. This study conclusively yielded meaningful entropy-based knowledgeable rules in tree structures and contributed to the differentiation of classifiers from the two datasets with three useful research findings and three helpful management implications for subsequent medical research. In particular, these rules provide reasonable solutions to simplify processes of preventive medicine by standardizing the formats and codes used in medical data to address CVD problems. The specificity of these rules is thus significant compared to those of past research.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: This study conclusively yielded meaningful entropy-based knowledgeable rules in tree structures and contributed to the differentiation of classifiers from the two datasets with three useful research findings and three helpful management implications for subsequent medical research.},
  timestamp = {2025-06-12T14:15:42Z}
}

@inproceedings{chen2021causal,
  title = {An {{Causal XAI Diagnostic Model}} for {{Breast Cancer Based}} on {{Mammography Reports}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Chen, Dehua and Zhao, Hongjin and He, Jianrong and Pan, Qiao and Zhao, Weiliang},
  year = {2021},
  month = dec,
  pages = {3341--3349},
  doi = {10.1109/BIBM52615.2021.9669648},
  urldate = {2025-07-24},
  abstract = {Breast cancer has become one of the most common malignant tumors in women worldwide, and it seriously threatens women's physical and mental health. In recent years, with the development of Artificial Intelligence(AI) and the accumulation of medical data, AI has begun to be deeply integrated with mammography, MRI, ultrasound, etc. to assist physicians in disease diagnosis. However, the existing breast cancer diagnosis model based on Computer Vision(CV) is greatly affected by the image quality; on the other hand, the breast cancer diagnosis model based on Natural Language Processing(NLP) cannot effectively extract the semantic information of the mammography report. The lack of model interpretability also makes the existing diagnostic models have low confidence. In this paper, we proposed Breast Cancer Causal XAI Diagnostic Model(BCCXDM). Specifically, we first structured the mammography report. Then find the causal graph based on the structured table. We combine the existing tabular learning method TabNet with causal graphs(Causal-TabNet) to enable reasoning in the graphs to preserve the correlation between features. More importantly, we use GNN and node transition probability to aggregate node information. We evaluate our model on the real-world mammography report, and compare it with other popular interpretable methods. The experimental results show that our interpretable results are closer to the diagnostic criteria of clinicians.},
  keywords = {Aggregates,breast cancer,causal graph,Computational modeling,interpretability,mammography report,Natural languages,Predictive models,Semantics,Transforms,Ultrasonic imaging},
  annotation = {TLDR: This paper combines the existing tabular learning method TabNet with causal graphs(Causal-TabNet) to enable reasoning in the graphs to preserve the correlation between features and results show that the interpretable results are closer to the diagnostic criteria of clinicians.},
  timestamp = {2025-07-24T12:22:58Z}
}

@misc{chen2021interpretable,
  title = {Interpretable {{Machine Learning}}: {{Moving From Mythos}} to {{Diagnostics}}},
  shorttitle = {Interpretable {{Machine Learning}}},
  author = {Chen, Valerie and Li, Jeffrey and Kim, Joon Sik and Plumb, Gregory and Talwalkar, Ameet},
  year = {2021},
  month = jul,
  number = {arXiv:2103.06254},
  eprint = {2103.06254},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.06254},
  urldate = {2025-08-24},
  abstract = {Despite increasing interest in the field of Interpretable Machine Learning (IML), a significant gap persists between the technical objectives targeted by researchers' methods and the high-level goals of consumers' use cases. In this work, we synthesize foundational work on IML methods and evaluation into an actionable taxonomy. This taxonomy serves as a tool to conceptualize the gap between researchers and consumers, illustrated by the lack of connections between its methods and use cases components. It also provides the foundation from which we describe a three-step workflow to better enable researchers and consumers to work together to discover what types of methods are useful for what use cases. Eventually, by building on the results generated from this workflow, a more complete version of the taxonomy will increasingly allow consumers to find relevant methods for their target use cases and researchers to identify applicable use cases for their proposed methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  timestamp = {2025-08-24T13:16:58Z}
}

@article{chen2021xdeepacpep,
  title = {{{xDeep-AcPEP}}: {{Deep Learning Method}} for {{Anticancer Peptide Activity Prediction Based}} on {{Convolutional Neural Network}} and {{Multitask Learning}}},
  shorttitle = {{{xDeep-AcPEP}}},
  author = {Chen, Jiarui and Cheong, Hong Hin and Siu, Shirley W. I.},
  year = {2021},
  month = aug,
  journal = {Journal of Chemical Information and Modeling},
  volume = {61},
  number = {8},
  pages = {3789--3803},
  issn = {1549-960X},
  doi = {10.1021/acs.jcim.1c00181},
  abstract = {Cancer is one of the leading causes of death worldwide. Conventional cancer treatment relies on radiotherapy and chemotherapy, but both methods bring severe side effects to patients, as these therapies not only attack cancer cells but also damage normal cells. Anticancer peptides (ACPs) are a promising alternative as therapeutic agents that are efficient and selective against tumor cells. Here, we propose a deep learning method based on convolutional neural networks to predict biological activity (EC50, LC50, IC50, and LD50) against six tumor cells, including breast, colon, cervix, lung, skin, and prostate. We show that models derived with multitask learning achieve better performance than conventional single-task models. In repeated 5-fold cross validation using the CancerPPD data set, the best models with the applicability domain defined obtain an average mean squared error of 0.1758, Pearson's correlation coefficient of 0.8086, and Kendall's correlation coefficient of 0.6156. As a step toward model interpretability, we infer the contribution of each residue in the sequence to the predicted activity by means of feature importance weights derived from the convolutional layers of the model. The present method, referred to as xDeep-AcPEP, will help to identify effective ACPs in rational peptide design for therapeutic purposes. The data, script files for reproducing the experiments, and the final prediction models can be downloaded from http://github.com/chen709847237/xDeep-AcPEP. The web server to directly access this prediction method is at https://app.cbbio.online/acpep/home.},
  langid = {english},
  pmid = {34327990},
  keywords = {Deep Learning,Humans,Male,Neural Networks Computer,Peptides},
  annotation = {TLDR: A deep learning method based on convolutional neural networks to predict biological activity against six tumor cells, including breast, colon, cervix, lung, skin, and prostate, and it is shown that models derived with multitask learning achieve better performance than conventional single-task models.},
  timestamp = {2025-07-25T12:49:44Z}
}

@misc{chen2022algorithms,
  title = {Algorithms to Estimate {{Shapley}} Value Feature Attributions},
  author = {Chen, Hugh and Covert, Ian C. and Lundberg, Scott M. and Lee, Su-In},
  year = {2022},
  month = jul,
  number = {arXiv:2207.07605},
  eprint = {2207.07605},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.07605},
  urldate = {2025-04-09},
  abstract = {Feature attributions based on the Shapley value are popular for explaining machine learning models; however, their estimation is complex from both a theoretical and computational standpoint. We disentangle this complexity into two factors: (1){\textasciitilde}the approach to removing feature information, and (2){\textasciitilde}the tractable estimation strategy. These two factors provide a natural lens through which we can better understand and compare 24 distinct algorithms. Based on the various feature removal approaches, we describe the multiple types of Shapley value feature attributions and methods to calculate each one. Then, based on the tractable estimation strategies, we characterize two distinct families of approaches: model-agnostic and model-specific approximations. For the model-agnostic approximations, we benchmark a wide class of estimation approaches and tie them to alternative yet equivalent characterizations of the Shapley value. For the model-specific approximations, we clarify the assumptions crucial to each method's tractability for linear, tree, and deep models. Finally, we identify gaps in the literature and promising future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  timestamp = {2025-04-09T03:40:05Z}
}

@misc{chen2022best,
  title = {Best {{Practices}} for {{Interpretable Machine Learning}} in {{Computational Biology}}},
  author = {Chen, Valerie and Yang, Muyu and Cui, Wenbo and Kim, Joon Sik and Talwalkar, Ameet and Ma, Jian},
  year = {2022},
  month = nov,
  publisher = {Bioinformatics},
  doi = {10.1101/2022.10.28.513978},
  urldate = {2025-08-24},
  abstract = {Abstract           Advances in machine learning (ML) have enabled the development of next-generation prediction models for complex computational biology problems. These developments have spurred the use of interpretable machine learning (IML) to unveil fundamental biological insights through data-driven knowledge discovery. However, in general, standards and guidelines for IML usage in computational biology have not been well-characterized, representing a major gap toward fully realizing the potential of IML. Here, we introduce a workflow on the best practices for using IML methods to perform knowledge discovery which covers verification strategies that bridge data, prediction model, and explanation. We outline a workflow incorporating these verification strategies to increase an IML method's accountability, reliability, and generalizability. We contextualize our proposed workflow in a series of widely applicable computational biology problems. Together, we provide an extensive workflow with important principles for the appropriate use of IML in computational biology, paving the way for a better mechanistic understanding of ML models and advancing the ability to discover novel biological phenomena.},
  archiveprefix = {Bioinformatics},
  langid = {english},
  annotation = {TLDR: A workflow on the best practices for using IML methods to perform knowledge discovery which covers verification strategies that bridge data, prediction model, and explanation and outline a workflow incorporating these verification strategies to increase an IML method's accountability, reliability, and generalizability is introduced.},
  timestamp = {2025-08-24T12:31:24Z}
}

@article{chen2022explainable,
  title = {Explainable Medical Imaging {{AI}} Needs Human-Centered Design: Guidelines and Evidence from a Systematic Review},
  shorttitle = {Explainable Medical Imaging {{AI}} Needs Human-Centered Design},
  author = {Chen, Haomin and Gomez, Catalina and Huang, Chien-Ming and Unberath, Mathias},
  year = {2022},
  month = oct,
  journal = {npj Digital Medicine},
  volume = {5},
  number = {1},
  pages = {1--15},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00699-2},
  urldate = {2025-03-03},
  abstract = {Transparency in Machine Learning (ML), often also referred to as interpretability or explainability, attempts to reveal the working mechanisms of complex models. From a human-centered design perspective, transparency is not a property of the ML model but an affordance, i.e., a relationship between algorithm and users. Thus, prototyping and user evaluations are critical to attaining solutions that afford transparency. Following human-centered design principles in highly specialized and high stakes domains, such as medical image analysis, is challenging due to the limited access to end users and the knowledge imbalance between those users and ML designers. To investigate the state of transparent ML in medical image analysis, we conducted a systematic review of the literature from 2012 to 2021 in PubMed, EMBASE, and Compendex databases. We identified 2508 records and 68 articles met the inclusion criteria. Current techniques in transparent ML are dominated by computational feasibility and barely consider end users, e.g. clinical stakeholders. Despite the different roles and knowledge of ML developers and end users, no study reported formative user research to inform the design and development of transparent ML models. Only a few studies validated transparency claims through empirical user evaluations. These shortcomings put contemporary research on transparent ML at risk of being incomprehensible to users, and thus, clinically irrelevant. To alleviate these shortcomings in forthcoming research, we introduce the INTRPRT guideline, a design directive for transparent ML systems in medical image analysis. The INTRPRT guideline suggests human-centered design principles, recommending formative user research as the first step to understand user needs and domain requirements. Following these guidelines increases the likelihood that the algorithms afford transparency and enable stakeholders to capitalize on the benefits of transparent ML.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Biomedical engineering,Image processing},
  annotation = {TLDR: The INTRPRT guideline is introduced, a design directive for transparent ML systems in medical image analysis, suggesting human-centered design principles, and increases the likelihood that the algorithms afford transparency and enable stakeholders to capitalize on the benefits of transparent ML.},
  timestamp = {2025-03-03T07:43:10Z}
}

@article{chen2022explaining,
  title = {Explaining a Series of Models by Propagating {{Shapley}} Values},
  author = {Chen, Hugh and Lundberg, Scott M. and Lee, Su-In},
  year = {2022},
  month = aug,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {4512},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-31384-3},
  urldate = {2024-12-03},
  abstract = {Local feature attribution methods are increasingly used to explain complex machine learning models. However, current methods are limited because they are extremely expensive to compute or are not capable of explaining a distributed series of models where each model is owned by a separate institution. The latter is particularly important because it often arises in finance where explanations are mandated. Here, we present Generalized DeepSHAP (G-DeepSHAP), a tractable method to propagate local feature attributions through complex series of models based on a connection to the Shapley value. We evaluate G-DeepSHAP across biological, health, and financial datasets to show that it provides equally salient explanations an order of magnitude faster than existing model-agnostic attribution techniques and demonstrate its use in an important distributed series of models setting.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science},
  annotation = {TLDR: G-DeepSHAP is presented, a tractable method to compute local feature attributions for a series of machine learning models inspired by connections to the Shapley value and its use in an important distributed series of models setting is demonstrated.},
  timestamp = {2025-03-03T07:49:34Z}
}

@article{chen2022interpretable,
  title = {Interpretable Machine Learning: Moving from Mythos to Diagnostics},
  shorttitle = {Interpretable Machine Learning},
  author = {Chen, Valerie and Li, Jeffrey and Kim, Joon Sik and Plumb, Gregory and Talwalkar, Ameet},
  year = {2022},
  month = aug,
  journal = {Communications of the ACM},
  volume = {65},
  number = {8},
  pages = {43--50},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3546036},
  urldate = {2025-08-24},
  langid = {english},
  annotation = {TLDR: This work synthesizes foundational work on IML methods and evaluation into an actionable taxonomy that serves as a tool to conceptualize the gap between researchers and consumers and describes a three-step workflow to better enable researchers and consumers to work together to discover what types of methods are useful for what use cases.},
  timestamp = {2025-08-24T13:23:57Z}
}

@article{chen2022pancancer,
  title = {Pan-Cancer Integrative Histology-Genomic Analysis via Multimodal Deep Learning},
  author = {Chen, Richard J. and Lu, Ming Y. and Williamson, Drew F. K. and Chen, Tiffany Y. and Lipkova, Jana and Noor, Zahra and Shaban, Muhammad and Shady, Maha and Williams, Mane and Joo, Bumjin and Mahmood, Faisal},
  year = {2022},
  month = aug,
  journal = {Cancer Cell},
  volume = {40},
  number = {8},
  pages = {865-878.e6},
  issn = {1878-3686},
  doi = {10.1016/j.ccell.2022.07.004},
  abstract = {The rapidly emerging field of computational pathology has demonstrated promise in developing objective prognostic models from histology images. However, most prognostic models are either based on histology or genomics alone and do not address how these data sources can be integrated to develop joint image-omic prognostic models. Additionally, identifying explainable morphological and molecular descriptors from these models that govern such prognosis is of interest. We use multimodal deep learning to jointly examine pathology whole-slide images and molecular profile data from 14 cancer types. Our weakly supervised, multimodal deep-learning algorithm is able to fuse these heterogeneous modalities to predict outcomes and discover prognostic features that correlate with poor and favorable outcomes. We present all analyses for morphological and molecular correlates of patient prognosis across the 14 cancer types at both a disease and a patient level in an interactive open-access database to allow for further exploration, biomarker discovery, and feature assessment.},
  langid = {english},
  pmcid = {PMC10397370},
  pmid = {35944502},
  keywords = {Algorithms,artificial intelligence,biomarker discovery,cancer prognosis,computational pathology,data fusion,deep learning,Deep Learning,digital pathology,Genomics,Humans,multimodal integration,multimodal prognostic models,Neoplasms,pan-cancer,Prognosis},
  annotation = {TLDR: This work uses multimodal deep learning to jointly examine pathology whole-slide images and molecular profile data from 14 cancer types to predict outcomes and discover prognostic features that correlate with poor and favorable outcomes.},
  timestamp = {2025-05-16T03:09:55Z}
}

@inproceedings{chen2023causality,
  title = {Causality and Independence Enhancement for Biased Node Classification},
  booktitle = {Proceedings of the 32nd {{ACM}} International Conference on Information and Knowledge Management},
  author = {Chen, Guoxin and Wang, Yongqing and Guo, Fangda and Guo, Qinglang and Shao, Jiangli and Shen, Huawei and Cheng, Xueqi},
  year = {2023},
  pages = {203--212},
  timestamp = {2025-03-23T02:02:09Z}
}

@article{chen2024predictive,
  title = {Predictive {{Modeling}} with {{Temporal Graphical Representation}} on {{Electronic Health Records}}},
  author = {Chen, Jiayuan and Yin, Changchang and Wang, Yuanlong and Zhang, Ping},
  year = {2024},
  month = aug,
  journal = {IJCAI: proceedings of the conference},
  volume = {2024},
  pages = {5763--5771},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2024/637},
  abstract = {Deep learning-based predictive models, leveraging Electronic Health Records (EHR), are receiving increasing attention in healthcare. An effective representation of a patient's EHR should hierarchically encompass both the temporal relationships between historical visits and medical events, and the inherent structural information within these elements. Existing patient representation methods can be roughly categorized into sequential representation and graphical representation. The sequential representation methods focus only on the temporal relationships among longitudinal visits. On the other hand, the graphical representation approaches, while adept at extracting the graph-structured relationships between various medical events, fall short in effectively integrate temporal information. To capture both types of information, we model a patient's EHR as a novel temporal heterogeneous graph. This graph includes historical visits nodes and medical events nodes. It propagates structured information from medical event nodes to visit nodes and utilizes time-aware visit nodes to capture changes in the patient's health status. Furthermore, we introduce a novel temporal graph transformer (TRANS) that integrates temporal edge features, global positional encoding, and local structural encoding into heterogeneous graph convolution, capturing both temporal and structural information. We validate the effectiveness of TRANS through extensive experiments on three real-world datasets. The results show that our proposed approach achieves state-of-the-art performance.},
  langid = {english},
  pmcid = {PMC11446542},
  pmid = {39359569},
  timestamp = {2025-06-13T07:47:28Z}
}

@misc{chen2024predictivea,
  title = {Predictive {{Modeling}} with {{Temporal Graphical Representation}} on {{Electronic Health Records}}},
  author = {Chen, Jiayuan and Yin, Changchang and Wang, Yuanlong and Zhang, Ping},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2405.03943},
  urldate = {2025-06-13},
  abstract = {Deep learning-based predictive models, leveraging Electronic Health Records (EHR), are receiving increasing attention in healthcare. An effective representation of a patient's EHR should hierarchically encompass both the temporal relationships between historical visits and medical events, and the inherent structural information within these elements. Existing patient representation methods can be roughly categorized into sequential representation and graphical representation. The sequential representation methods focus only on the temporal relationships among longitudinal visits. On the other hand, the graphical representation approaches, while adept at extracting the graph-structured relationships between various medical events, fall short in effectively integrate temporal information. To capture both types of information, we model a patient's EHR as a novel temporal heterogeneous graph. This graph includes historical visits nodes and medical events nodes. It propagates structured information from medical event nodes to visit nodes and utilizes time-aware visit nodes to capture changes in the patient's health status. Furthermore, we introduce a novel temporal graph transformer (TRANS) that integrates temporal edge features, global positional encoding, and local structural encoding into heterogeneous graph convolution, capturing both temporal and structural information. We validate the effectiveness of TRANS through extensive experiments on three real-world datasets. The results show that our proposed approach achieves state-of-the-art performance.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: This work introduces a novel temporal graph transformer (TRANS) that integrates temporal edge features, global positional encoding, and local structural encoding into heterogeneous graph convolution, capturing both temporal and structural information.},
  timestamp = {2025-06-13T08:34:05Z}
}

@article{chen2025deepseek,
  title = {{{DeepSeek Deployed}} in 90 {{Chinese Tertiary Hospitals}}: {{How Artificial Intelligence Is Transforming Clinical Practice}}},
  shorttitle = {{{DeepSeek Deployed}} in 90 {{Chinese Tertiary Hospitals}}},
  author = {Chen, Jishizhan and Miao, Chunying},
  year = {2025},
  month = apr,
  journal = {Journal of Medical Systems},
  volume = {49},
  number = {1},
  pages = {53},
  issn = {1573-689X},
  doi = {10.1007/s10916-025-02181-4},
  urldate = {2025-05-23},
  abstract = {The integration of artificial intelligence (AI) into clinical practice has reached a new milestone in China, with the deployment of DeepSeek across nearly 90 tertiary hospitals. This large-scale adoption represents a significant shift in how AI is utilized beyond diagnostic assistance, extending into hospital administration, research facilitation, and patient management. Notably, DeepSeek's AI-powered systems have demonstrated transformative effects, such as a 40-fold increase in efficiency for patient follow-ups. Our comment explores the implications of DeepSeek's expansion within China's healthcare landscape, situating it within broader national policies promoting AI-driven hospital digitalization. We discuss how hospitals are leveraging DeepSeek, Notably, DeepSeek's role in imaging analysis, clinical decision support, and administrative automation. We also address the ongoing challenges of AI integration. As China accelerates its transition toward ``smart hospitals,'' the widespread adoption of AI like DeepSeek offers a compelling case study on the future of digital health in large-scale healthcare systems.},
  langid = {english},
  keywords = {Big Data,Cloud Computing,e-Health,Health Care,Health Informatics,Medical and Health Technologies},
  annotation = {TLDR: This comment explores the implications of DeepSeek's expansion within China's healthcare landscape, situating it within broader national policies promoting AI-driven hospital digitalization, and addresses the ongoing challenges of AI integration.},
  timestamp = {2025-05-23T01:29:29Z}
}

@article{chen2025unlocking,
  title = {Unlocking Precision Medicine: Clinical Applications of Integrating Health Records, Genetics, and Immunology through Artificial Intelligence},
  shorttitle = {Unlocking Precision Medicine},
  author = {Chen, Yi-Ming and Hsiao, Tzu-Hung and Lin, Ching-Heng and Fann, Yang C.},
  year = {2025},
  month = feb,
  journal = {Journal of Biomedical Science},
  volume = {32},
  number = {1},
  pages = {16},
  issn = {1423-0127},
  doi = {10.1186/s12929-024-01110-w},
  urldate = {2025-05-24},
  abstract = {Artificial intelligence (AI) has emerged as a transformative force in precision medicine, revolutionizing the integration and analysis of health records, genetics, and immunology data. This comprehensive review explores the clinical applications of AI-driven analytics in unlocking personalized insights for patients with autoimmune rheumatic diseases. Through the synergistic approach of integrating AI across diverse data sets, clinicians gain a holistic view of patient health and potential risks. Machine learning models excel at identifying high-risk patients, predicting disease activity, and optimizing therapeutic strategies based on clinical, genomic, and immunological profiles. Deep learning techniques have significantly advanced variant calling, pathogenicity prediction, splicing analysis, and MHC-peptide binding predictions in genetics. AI-enabled immunology data analysis, including dimensionality reduction, cell population identification, and sample classification, provides unprecedented insights into complex immune responses. The review highlights real-world examples of AI-driven precision medicine platforms and clinical decision support tools in rheumatology. Evaluation of outcomes demonstrates the clinical benefits and impact of these approaches in revolutionizing patient care. However, challenges such as data quality, privacy, and clinician trust must be navigated for successful implementation. The future of precision medicine lies in the continued research, development, and clinical integration of AI-driven strategies to unlock personalized patient care and drive innovation in rheumatology.},
  langid = {american},
  keywords = {Artificial intelligence,Autoimmune,Electronic health records,Genetics,Immunology,Precision medicine},
  annotation = {TLDR: This comprehensive review explores the clinical applications of AI-driven analytics in unlocking personalized insights for patients with autoimmune rheumatic diseases and highlights real-world examples of AI-driven precision medicine platforms and clinical decision support tools in rheumatology.},
  timestamp = {2025-05-24T11:32:11Z}
}

@article{cheng2024artificial,
  title = {Artificial Intelligence and Open Science in Discovery of Disease-Modifying Medicines for {{Alzheimer}}'s Disease},
  author = {Cheng, Feixiong and Wang, Fei and Tang, Jian and Zhou, Yadi and Fu, Zhimin and Zhang, Pengyue and Haines, Jonathan L. and Leverenz, James B. and Gan, Li and Hu, Jianying and {Rosen-Zvi}, Michal and Pieper, Andrew A. and Cummings, Jeffrey},
  year = {2024},
  month = feb,
  journal = {Cell Reports Medicine},
  volume = {5},
  number = {2},
  pages = {101379},
  issn = {26663791},
  doi = {10.1016/j.xcrm.2023.101379},
  urldate = {2025-04-16},
  langid = {english},
  annotation = {TLDR: The potential utility of applying AI approaches to big data for discovery of disease-modifying medicines for AD/ADRD is reviewed and how AI tools can be applied to the AD/ADRD drug development pipeline is illustrated through collaborative efforts among neurologists, gerontologists, geneticists, pharmacologists, medicinal chemists, and computational scientists.},
  timestamp = {2025-04-16T08:19:05Z}
}

@article{cheng2025causally,
  title = {Causally-Informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care},
  author = {Cheng, Yuxiao and Song, Xinxin and Wang, Ziqian and Zhong, Qin and He, Kunlun and Suo, Jinli},
  year = {2025},
  journal = {arXiv preprint arXiv:2502.02109},
  eprint = {2502.02109},
  archiveprefix = {arXiv},
  timestamp = {2025-04-12T07:31:18Z}
}

@article{chensiyuan2020causality,
  title = {Causality Inference between Time Series Data and Its Applications},
  author = {{Chen, Siyuan}},
  year = {2020},
  publisher = {Columbia University},
  doi = {10.7916/D8-8W9B-ZX36},
  urldate = {2025-04-03},
  abstract = {Ever since Granger first proposed the idea of quantitatively testing the causal relationship between data streams, the endeavor of accurately inferring the causality in data and using that information to predict the future has not stopped. Artificial Intelligence (AI), by utilizing the massive amounts of data, helps to solve complex problems, whether they include the diagnosis and detection of disease through medical imaging, email spam detection, or self-driving vehicles. Perhaps, this thesis will be trivial in ten years from now. AI has pushed humankind to reach the next technological level in technology. Nowadays, among most machine leaning inquiries, statistical relationships are determined using correlation measures. By feeding data into machine learning algorithms, computers update the algorithm's parameters iteratively by extracting and mapping features to learning targets until the correlation increases to a significant level to cease the training process. However, with the increasing developments of powerful AI, there is really a shortage of exploring causality in data. It is almost self-evident that ''correlation is not causality." Sometimes, the strong correlation established between variables through machine learning can be absurd and meaningless. Providing insight into causality information through data, which most of the machine learning methods fall short to do, is of paramount importance. The subsequent chapters detail the four endeavors of studying causality in financial markets, earthquakes, animal/human brain signals, the predictivity of data sets. In Chapter 2, we further developed the concept of causality networks into a higher-order causality network. We applied these to financial data and tested their validity and ability to capture the system's causal relationship. In next Chapter 3, We examined another type of time series-earthquakes. Violent seismic activities decimate people's lives and destroy entire cities and areas. This begs us to understand how earthquakes work and help us make reliably and evacuation-actionable predictions. The causal relationships of seismic activities in different areas are studied and established. Biological data, specifically brain signals, are time-series data and their causal pattern are explored and studied. Different human and mice brain signals are analyzed and clustered in Chapter 4 using their unique causal pattern to understand different brain cell activity. Finally, we realized that the causal pattern in the time series can be used to compress data. A causal compression ratio is invented and used as the data stream's predictivity index. We describe this in Chapter 5.},
  keywords = {Causation,Computer science,Earthquakes,FOS: Mechanical engineering,Markets,Mechanical engineering,Time-series analysis},
  annotation = {TLDR: The subsequent chapters detail the four endeavors of studying causality in financial markets, earthquakes, animal/human brain signals, the predictivity of data sets, and the causal pattern in the time series can be used to compress data.},
  timestamp = {2025-04-03T03:45:41Z}
}

@article{cheplygina2019notsosupervised,
  title = {Not-so-Supervised: {{A}} Survey of Semi-Supervised, Multi-Instance, and Transfer Learning in Medical Image Analysis},
  shorttitle = {Not-so-Supervised},
  author = {Cheplygina, Veronika and {de Bruijne}, Marleen and Pluim, Josien P. W.},
  year = {2019},
  month = may,
  journal = {Medical Image Analysis},
  volume = {54},
  pages = {280--296},
  issn = {1361-8423},
  doi = {10.1016/j.media.2019.03.009},
  abstract = {Machine learning (ML) algorithms have made a tremendous impact in the field of medical imaging. While medical imaging datasets have been growing in size, a challenge for supervised ML algorithms that is frequently mentioned is the lack of annotated data. As a result, various methods that can learn with less/other types of supervision, have been proposed. We give an overview of semi-supervised, multiple instance, and transfer learning in medical imaging, both in diagnosis or segmentation tasks. We also discuss connections between these learning scenarios, and opportunities for future research. A dataset with the details of the surveyed papers is available via https://figshare.com/articles/Database\_of\_surveyed\_literature\_in\_Not-so-supervised\_a\_survey\_of\_semi-supervised\_multi-instance\_and\_transfer\_learning\_in\_medical\_image\_analysis\_/7479416.},
  langid = {english},
  pmid = {30959445},
  keywords = {Computer aided diagnosis,Diagnostic Imaging,Humans,Image Processing Computer-Assisted,Machine learning,Medical imaging,Multi-task learning,Multiple instance learning,Semi-supervised learning,Supervised Machine Learning,Transfer learning,Weakly-supervised learning},
  annotation = {TLDR: An overview of semi-supervised, multiple instance, and transfer learning in medical imaging, both in diagnosis or segmentation tasks, and connections between these learning scenarios, and opportunities for future research are discussed.},
  timestamp = {2025-07-24T14:54:59Z}
}

@article{chereda2021explaining,
  title = {Explaining Decisions of Graph Convolutional Neural Networks: Patient-Specific Molecular Subnetworks Responsible for Metastasis Prediction in Breast Cancer},
  shorttitle = {Explaining Decisions of Graph Convolutional Neural Networks},
  author = {Chereda, Hryhorii and Bleckmann, Annalen and Menck, Kerstin and {Perera-Bel}, J{\'u}lia and Stegmaier, Philip and Auer, Florian and Kramer, Frank and Leha, Andreas and Bei{\ss}barth, Tim},
  year = {2021},
  month = mar,
  journal = {Genome Medicine},
  volume = {13},
  number = {1},
  pages = {42},
  issn = {1756-994X},
  doi = {10.1186/s13073-021-00845-7},
  urldate = {2025-05-16},
  abstract = {Contemporary deep learning approaches show cutting-edge performance in a variety of complex prediction tasks. Nonetheless, the application of deep learning in healthcare remains limited since deep learning methods are often considered as non-interpretable black-box models. However, the machine learning community made recent elaborations on interpretability methods explaining data point-specific decisions of deep learning techniques. We believe that such explanations can assist the need in personalized precision medicine decisions via explaining patient-specific predictions.},
  keywords = {Classification of cancer,Deep learning,Explainable AI,Gene expression data,Molecular networks,Personalized medicine,Precision medicine,Prior knowledge},
  annotation = {TLDR: Graph Layer-wise Relevance Propagation (GLRP) is presented as a new method to explain the decisions made by Graph-CNNs and it is shown that GLRP provides patient-specific molecular subnetworks that largely agree with clinical knowledge and identify common as well as novel, and potentially druggable, drivers of tumor progression.},
  timestamp = {2025-05-16T08:25:54Z}
}

@article{chernozhukov2018double,
  title = {Double/Debiased Machine Learning for Treatment and Structural Parameters},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  year = {2018},
  month = feb,
  journal = {The Econometrics Journal},
  volume = {21},
  number = {1},
  pages = {C1-C68},
  issn = {1368-4221, 1368-423X},
  doi = {10.1111/ectj.12097},
  urldate = {2025-04-06},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  langid = {english},
  annotation = {TLDR: This work revisits the classic semiparametric problem of inference on a low dimensional parameter {\texttheta}\_0 in the presence of high-dimensional nuisance parameters {$\eta\_$}0 and proves that DML delivers point estimators that concentrate in a N{\textasciicircum}(-1/2)-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements.},
  timestamp = {2025-04-06T10:34:23Z}
}

@article{chien2022usefulness,
  ids = {chien2022usefulnessa},
  title = {The {{Usefulness}} of {{Gradient-Weighted CAM}} in {{Assisting Medical Diagnoses}}},
  author = {Chien, Jong-Chih and Lee, Jiann-Der and Hu, Ching-Shu and Wu, Chieh-Tsai},
  year = {2022},
  month = aug,
  journal = {Applied Sciences},
  volume = {12},
  number = {15},
  pages = {7748},
  issn = {2076-3417},
  doi = {10.3390/app12157748},
  urldate = {2025-06-12},
  abstract = {In modern medicine, medical imaging technologies such as computed tomography (CT), X-ray, ultrasound, magnetic resonance imaging (MRI), nuclear medicine, etc., have been proven to provide useful diagnostic information by displaying areas of a lesion or tumor not visible to the human eye, and may also help provide additional recessive information by using modern data analysis methods. These methods, including Artificial Intelligence (AI) technologies, are based on deep learning architectures, and have shown remarkable results in recent studies. However, the lack of explanatory ability of connection-based, instead of algorithm-based, deep learning technologies is one of the main reasons for the delay in the acceptance of these technologies in the mainstream medical field. One of the recent methods that may offer the explanatory ability for the CNN classes of deep learning neural networks is the gradient-weighted class activation mapping (Grad-CAM) method, which produces heat-maps that may offer explanations of the classification results. There are already many studies in the literature that compare the objective metrics of Grad-CAM-generated heat-maps against other methods. However, the subjective evaluation of AI-based classification/prediction results using medical images by qualified personnel could potentially contribute more to the acceptance of AI than objective metrics. The purpose of this paper is to investigate whether and how the Grad-CAM heat-maps can help physicians and radiologists in making diagnoses by presenting the results from AI-based classifications as well as their associated Grad-CAM-generated heat-maps to a qualified radiologist. The results of this study show that the radiologist considers Grad-CAM-generated heat-maps to be generally helpful toward diagnosis.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: The results of this study show that the radiologist considers Grad-CAM-generated heat-maps to be generally helpful toward diagnosis, and could potentially contribute more to the acceptance of AI than objective metrics.},
  timestamp = {2025-07-21T07:41:57Z}
}

@article{chinta2025aidriven,
  title = {{{AI-driven}} Healthcare: {{Fairness}} in {{AI}} Healthcare: {{A}} Survey},
  shorttitle = {{{AI-driven}} Healthcare},
  author = {Chinta, Sribala Vidyadhari and Wang, Zichong and Palikhe, Avash and Zhang, Xingyu and Kashif, Ayesha and Smith, Monique Antoinette and Liu, Jun and Zhang, Wenbin},
  year = {2025},
  month = may,
  journal = {PLOS digital health},
  volume = {4},
  number = {5},
  pages = {e0000864},
  issn = {2767-3170},
  doi = {10.1371/journal.pdig.0000864},
  abstract = {Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing the efficiency and effectiveness of services across various specialties, including cardiology, ophthalmology, dermatology, emergency medicine, etc. AI applications have significantly improved diagnostic accuracy, treatment personalization, and patient outcome predictions by leveraging technologies such as machine learning, neural networks, and natural language processing. However, these advancements also introduce substantial ethical and fairness challenges, particularly related to biases in data and algorithms. These biases can lead to disparities in healthcare delivery, affecting diagnostic accuracy and treatment outcomes across different demographic groups. This review paper examines the integration of AI in healthcare, highlighting critical challenges related to bias and exploring strategies for mitigation. We emphasize the necessity of diverse datasets, fairness-aware algorithms, and regulatory frameworks to ensure equitable healthcare delivery. The paper concludes with recommendations for future research, advocating for interdisciplinary approaches, transparency in AI decision-making, and the development of innovative and inclusive AI applications.},
  langid = {english},
  pmcid = {PMC12091740},
  pmid = {40392801},
  annotation = {TLDR: This review paper examines the integration of AI in healthcare, highlighting critical challenges related to bias and exploring strategies for mitigation, and emphasizing the necessity of diverse datasets, fairness-aware algorithms, and regulatory frameworks to ensure equitable healthcare delivery.},
  timestamp = {2025-05-28T00:53:28Z}
}

@article{chipman2010bart,
  title = {{{BART}}: {{Bayesian}} Additive Regression Trees},
  author = {Chipman, Hugh A and George, Edward I and McCulloch, Robert E},
  year = {2010},
  timestamp = {2025-03-20T07:05:13Z}
}

@article{chou2022counterfactuals,
  title = {Counterfactuals and Causability in Explainable Artificial Intelligence: {{Theory}}, Algorithms, and Applications},
  shorttitle = {Counterfactuals and Causability in Explainable Artificial Intelligence},
  author = {Chou, Yu-Liang and Moreira, Catarina and Bruza, Peter and Ouyang, Chun and Jorge, Joaquim},
  year = {2022},
  month = may,
  journal = {Information Fusion},
  volume = {81},
  pages = {59--83},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.11.003},
  urldate = {2025-04-17},
  abstract = {Deep learning models have achieved high performance across different domains, such as medical decision-making, autonomous vehicles, decision support systems, among many others. However, despite this success, the inner mechanisms of these models are opaque because their internal representations are too complex for a human to understand. This opacity makes it hard to understand the how or the why of the predictions of deep learning models. There has been a growing interest in model-agnostic methods that make deep learning models more transparent and explainable to humans. Some researchers recently argued that for a machine to achieve human-level explainability, this machine needs to provide human causally understandable explanations, also known as causability. A specific class of algorithms that have the potential to provide causability are counterfactuals. This paper presents an in-depth systematic review of the diverse existing literature on counterfactuals and causability for explainable artificial intelligence (AI). We performed a Latent Dirichlet topic modelling analysis (LDA) under a Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework to find the most relevant literature articles. This analysis yielded a novel taxonomy that considers the grounding theories of the surveyed algorithms, together with their underlying properties and applications to real-world data. Our research suggests that current model-agnostic counterfactual algorithms for explainable AI are not grounded on a causal theoretical formalism and, consequently, cannot promote causability to a human decision-maker. Furthermore, our findings suggest that the explanations derived from popular algorithms in the literature provide spurious correlations rather than cause/effects relationships, leading to sub-optimal, erroneous, or even biased explanations. Thus, this paper also advances the literature with new directions and challenges on promoting causability in model-agnostic approaches for explainable AI.},
  keywords = {Causability,Causality,Counterfactuals,Deep learning,Explainable AI},
  annotation = {TLDR: It is suggested that current model-agnostic counterfactual algorithms for explainable AI are not grounded on a causal theoretical formalism and, consequently, cannot promote causability to a human decision-maker.},
  timestamp = {2025-04-17T11:59:02Z}
}

@article{chung2025evaluating,
  title = {Evaluating {{Visual Explanations}} of {{Attention Maps}} for {{Transformer-based Medical Imaging}}},
  author = {Chung, Minjae and Won, Jong Bum and Kim, Ganghyun and Kim, Yujin and Ozbulak, Utku},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2503.09535},
  urldate = {2025-06-12},
  abstract = {Although Vision Transformers (ViTs) have recently demonstrated superior performance in medical imaging problems, they face explainability issues similar to previous architectures such as convolutional neural networks. Recent research efforts suggest that attention maps, which are part of decision-making process of ViTs can potentially address the explainability issue by identifying regions influencing predictions, especially in models pretrained with self-supervised learning. In this work, we compare the visual explanations of attention maps to other commonly used methods for medical imaging problems. To do so, we employ four distinct medical imaging datasets that involve the identification of (1) colonic polyps, (2) breast tumors, (3) esophageal inflammation, and (4) bone fractures and hardware implants. Through large-scale experiments on the aforementioned datasets using various supervised and self-supervised pretrained ViTs, we find that although attention maps show promise under certain conditions and generally surpass GradCAM in explainability, they are outperformed by transformer-specific interpretability methods. Our findings indicate that the efficacy of attention maps as a method of interpretability is context-dependent and may be limited as they do not consistently provide the comprehensive insights required for robust medical decision-making.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  timestamp = {2025-06-12T15:48:37Z}
}

@article{clark2013cancer,
  title = {The Cancer Imaging Archive ({{TCIA}}): {{Maintaining}} and Operating a Public Information Repository},
  author = {Clark, Kenneth and Vendt, Brad and Smith, Kirk and Freymann, Jonathan and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Samuel and Maffitt, David and Pringle, Matthew and Tarbox, Lubomir and Prior, Fred},
  year = {2013},
  journal = {Journal of Digital Imaging},
  volume = {26},
  number = {6},
  pages = {1045--1057},
  doi = {10.1007/s10278-013-9650-3},
  annotation = {TLDR: A Web-based centralized console that provides remote monitoring, testing, and management over multiple geo-distributed PACS, reducing the need for local technicians and providing a 24/7 monitoring solution.},
  timestamp = {2025-04-12T13:39:48Z}
}

@article{collins2015new,
  title = {A New Initiative on Precision Medicine},
  author = {Collins, Francis S and Varmus, Harold},
  year = {2015},
  journal = {New England journal of medicine},
  volume = {372},
  number = {9},
  pages = {793--795},
  publisher = {Mass Medical Soc},
  timestamp = {2025-05-13T11:37:54Z}
}

@article{confalonieri2021using,
  title = {Using Ontologies to Enhance Human Understandability of Global Post-Hoc Explanations of Black-Box Models},
  author = {Confalonieri, Roberto and Weyde, Tillman and Besold, Tarek R and {del Prado Mart{\'{\i}}n}, Ferm{\'{\i}}n Moscoso},
  year = {2021},
  journal = {Artificial Intelligence},
  volume = {296},
  pages = {103471},
  publisher = {Elsevier},
  timestamp = {2025-03-19T05:08:50Z}
}

@misc{corbin2023deployr,
  title = {{{DEPLOYR}}: {{A}} Technical Framework for Deploying Custom Real-Time Machine Learning Models into the Electronic Medical Record},
  shorttitle = {{{DEPLOYR}}},
  author = {Corbin, Conor K. and Maclay, Rob and Acharya, Aakash and Mony, Sreedevi and Punnathanam, Soumya and Thapa, Rahul and Kotecha, Nikesh and Shah, Nigam H. and Chen, Jonathan H.},
  year = {2023},
  month = mar,
  number = {arXiv:2303.06269},
  eprint = {2303.06269},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.06269},
  urldate = {2025-04-12},
  abstract = {Machine learning (ML) applications in healthcare are extensively researched, but successful translations to the bedside are scant. Healthcare institutions are establishing frameworks to govern and promote the implementation of accurate, actionable and reliable models that integrate with clinical workflow. Such governance frameworks require an accompanying technical framework to deploy models in a resource efficient manner. Here we present DEPLOYR, a technical framework for enabling real-time deployment and monitoring of researcher created clinical ML models into a widely used electronic medical record (EMR) system. We discuss core functionality and design decisions, including mechanisms to trigger inference based on actions within EMR software, modules that collect real-time data to make inferences, mechanisms that close-the-loop by displaying inferences back to end-users within their workflow, monitoring modules that track performance of deployed models over time, silent deployment capabilities, and mechanisms to prospectively evaluate a deployed model's impact. We demonstrate the use of DEPLOYR by silently deploying and prospectively evaluating twelve ML models triggered by clinician button-clicks in Stanford Health Care's production instance of Epic. Our study highlights the need and feasibility for such silent deployment, because prospectively measured performance varies from retrospective estimates. By describing DEPLOYR, we aim to inform ML deployment best practices and help bridge the model implementation gap.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  timestamp = {2025-04-12T07:20:51Z}
}

@misc{correction,
  title = {Correction: {{Explainable}} Lung Cancer Classification with Ensemble Transfer Learning of {{VGG16}}, {{Resnet50}} and {{InceptionV3}} Using Grad-Cam {\textbar} {{BMC Medical Imaging}} {\textbar} {{Full Text}}},
  urldate = {2025-05-06},
  howpublished = {https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-024-01381-7?utm\_source=chatgpt.com},
  timestamp = {2025-05-06T03:59:40Z}
}

@article{costi2025xplainlungshap,
  title = {{{XplainLungSHAP}}: {{Enhancing Lung Cancer Surgery Decision Making}} with {{Feature Selection}} and {{Explainable AI Insights}}},
  shorttitle = {{{XplainLungSHAP}}},
  author = {Costi, Flavia and Covaci, Emanuel and Onchis, Darian},
  year = {2025},
  month = mar,
  journal = {Surgeries},
  volume = {6},
  number = {1},
  pages = {8},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-4095},
  doi = {10.3390/surgeries6010008},
  urldate = {2025-04-12},
  abstract = {Background: Lung cancer surgery often involves complex decision-making, where accurate and interpretable predictive models are crucial for assessing postoperative risks and optimizing outcomes. This study presents XplainLungSHAP, a novel framework combining SHAP (SHapley Additive exPlanations) and attention mechanisms to enhance both predictive accuracy and transparency. The aim is to support clinicians in preoperative evaluations by identifying and prioritizing key clinical features. Methods: The framework was developed using data from 470 patients undergoing lung cancer surgery. Key clinical features were identified through SHAP, ensuring alignment with medical expertise. These features were dynamically weighted using an attention mechanism in a neural network, enhancing their impact on survival predictions. The model's performance was evaluated through accuracy, confusion matrices, and ROC analysis, demonstrating its reliability and interpretability. Results: The XplainLungSHAP model achieved an accuracy of 91.49\%, outperforming traditional machine learning models. SHAP analysis identified critical predictors, including pulmonary function, comorbidities, and age, while the attention mechanism prioritized these features dynamically. The combined approach ensured high accuracy and offered actionable insights into survival predictions. Conclusions: XplainLungSHAP addresses the limitations of black-box models by integrating explainability with state-of-the-art predictive techniques. This framework provides a transparent and clinically relevant tool for guiding surgical decisions, supporting personalized care, and advancing AI applications in thoracic oncology.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {attention mechanism,explainable AI,machine learning in healthcare,postoperative survival,precision medicine,predictive modeling,SHAP,thoracic surgery},
  annotation = {TLDR: XplainLungSHAP addresses the limitations of black-box models by integrating explainability with state-of-the-art predictive techniques and provides a transparent and clinically relevant tool for guiding surgical decisions, supporting personalized care, and advancing AI applications in thoracic oncology.},
  timestamp = {2025-04-12T07:59:31Z}
}

@article{cui2023interpretable,
  title = {Interpretable Artificial Intelligence in Radiology and Radiation Oncology},
  author = {Cui, Sunan and Traverso, Alberto and Niraula, Dipesh and Zou, Jiaren and Luo, Yi and Owen, Dawn and El Naqa, Issam and Wei, Lise},
  year = {2023},
  journal = {The British Journal of Radiology},
  volume = {96},
  number = {1150},
  pages = {20230142},
  publisher = {Oxford University Press},
  timestamp = {2025-04-16T01:11:17Z}
}

@article{da2021learning,
  ids = {brutodacosta2021learning},
  title = {Learning Temporal Causal Sequence Relationships from Real-Time Time-Series},
  author = {{da Costa}, Antonio Anastasio Bruto and Dasgupta, Pallab},
  year = {2021},
  journal = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {205--243},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12395},
  urldate = {2025-04-03},
  abstract = {We aim to mine temporal causal sequences that explain observed events (consequents) in time-series traces. Causal explanations of key events in a time-series have applications in design debugging, anomaly detection, planning, root-cause analysis and many more. We make use of decision trees and interval arithmetic to mine sequences that explain defining events in the time-series. We propose modified decision tree construction metrics to handle the non-determinism introduced by the temporal dimension. The mined sequences are expressed in a readable temporal logic language that is easy to interpret. The application of the proposed methodology is illustrated through various examples.},
  langid = {american},
  annotation = {TLDR: This work makes use of decision trees and interval arithmetic to mine sequences that explain defining events in the time-series and proposes modified decision tree construction metrics to handle the non-determinism introduced by the temporal dimension.},
  timestamp = {2025-04-03T12:20:50Z}
}

@article{dalmolin2025feature,
  title = {Feature {{Selection}} in {{Cancer Classification}}: {{Utilizing Explainable Artificial Intelligence}} to {{Uncover Influential Genes}} in {{Machine Learning Models}}},
  shorttitle = {Feature {{Selection}} in {{Cancer Classification}}},
  author = {Dalmolin, Matheus and Azevedo, Karolayne S. and de Souza, Lu{\'i}sa C. and {de Farias}, Caroline B. and Lichtenfels, Martina and Fernandes, Marcelo A. C.},
  year = {2025},
  month = jan,
  journal = {AI},
  volume = {6},
  number = {1},
  pages = {2},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2673-2688},
  doi = {10.3390/ai6010002},
  urldate = {2025-05-02},
  abstract = {This study investigates the use of machine learning (ML) models combined with explainable artificial intelligence (XAI) techniques to identify the most influential genes in the classification of five recurrent cancer types in women: breast cancer (BRCA), lung adenocarcinoma (LUAD), thyroid cancer (THCA), ovarian cancer (OV), and colon adenocarcinoma (COAD). Gene expression data from RNA-seq, extracted from The Cancer Genome Atlas (TCGA), were used to train ML models, including decision trees (DTs), random forest (RF), and XGBoost (XGB), which achieved accuracies of 98.69\%, 99.82\%, and 99.37\%, respectively. However, the challenges in this analysis included the high dimensionality of the dataset and the lack of transparency in the ML models. To mitigate these challenges, the SHAP (Shapley Additive Explanations) method was applied to generate a list of features, aiming to understand which characteristics influenced the models' decision-making processes and, consequently, the prediction results for the five tumor types. The SHAP analysis identified 119, 80, and 10 genes for the RF, XGB, and DT models, respectively, totaling 209 genes, resulting in 172 unique genes. The new list, representing 0.8\% of the original input features, is coherent and fully explainable, increasing confidence in the applied models. Additionally, the results suggest that the SHAP method can be effectively used as a feature selector in gene expression data. This approach not only enhances model transparency but also maintains high classification performance, highlighting its potential in identifying biologically relevant features that may serve as biomarkers for cancer diagnostics and treatment planning.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {cancer,explainable AI,feature selection,gene expression,machine learning,RNA-seq,SHAP},
  annotation = {TLDR: The results suggest that the SHAP method can be effectively used as a feature selector in gene expression data, highlighting its potential in identifying biologically relevant features that may serve as biomarkers for cancer diagnostics and treatment planning.},
  timestamp = {2025-05-02T16:06:27Z}
}

@inproceedings{daniele2023deep,
  title = {Deep {{Symbolic Learning}}: {{Discovering Symbols}} and {{Rules}} from {{Perceptions}}},
  shorttitle = {Deep {{Symbolic Learning}}},
  booktitle = {Proceedings of the {{Thirty-Second International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Daniele, Alessandro and Campari, Tommaso and Malhotra, Sagar and Serafini, Luciano},
  year = {2023},
  month = aug,
  pages = {3597--3605},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Macau, SAR China},
  doi = {10.24963/ijcai.2023/400},
  urldate = {2025-06-13},
  abstract = {Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems rely on continuous relaxation of logical knowledge, and no discrete decisions are made within the model pipeline. Furthermore, these methods assume that the symbolic rules are given. In this paper, we propose Deep Symboilic Learning (DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a (set of) perception functions which map continuous data to discrete symbols, and a symbolic function over the set of symbols. DSL simultaneously learns the perception and symbolic functions while being trained only on their composition (NeSy-function). The key novelty of DSL is that it can create internal (interpretable) symbolic representations and map them to perception inputs within a differentiable NN learning pipeline. The created symbols are automatically selected to generate symbolic functions that best explain the data. We provide experimental analysis to substantiate the efficacy of DSL  in simultaneously learning perception and symbolic functions.},
  isbn = {978-1-956792-03-4},
  langid = {english},
  annotation = {TLDR: Deep Symboilic Learning is proposed, a NeSy system that learns NeSy-functions, i.e., the composition of a (set of) perception functions which map continuous data to discrete symbols, and a symbolic function over the set of symbols.},
  timestamp = {2025-06-13T04:13:17Z}
}

@misc{das2020opportunities,
  title = {Opportunities and {{Challenges}} in {{Explainable Artificial Intelligence}} ({{XAI}}): {{A Survey}}},
  shorttitle = {Opportunities and {{Challenges}} in {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Das, Arun and Rad, Paul},
  year = {2020},
  month = jun,
  number = {arXiv:2006.11371},
  eprint = {2006.11371},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11371},
  urldate = {2025-05-17},
  abstract = {Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  timestamp = {2025-05-17T12:11:31Z}
}

@inproceedings{das20233dgan,
  title = {{{3D-GAN}} to Generate Representative and Realistic Three-Dimensional Breast Cancer Models for Virtual Clinical Trial Applications},
  booktitle = {Medical {{Imaging}} 2023: {{Physics}} of {{Medical Imaging}}},
  author = {Das, Ritisha and Koukoutegos, Konstantinos and Wang, Yo-Kuan and Keupers, Machteld and Bosmans, Hilde and Houbrechts, Katrien},
  editor = {Fahrig, Rebecca and Sabol, John M. and Yu, Lifeng},
  year = {2023},
  month = apr,
  pages = {95},
  publisher = {SPIE},
  address = {San Diego, United States},
  doi = {10.1117/12.2653561},
  urldate = {2025-04-06},
  isbn = {978-1-5106-6031-1 978-1-5106-6032-8},
  annotation = {TLDR: The 3D-GAN successfully generated breast cancer models of spiculated masses even when trained with a limited dataset, and this same method could be applied to generate other mass models of certain subgroups to allow lesion specific simulations, increasing the efficiency of the process.},
  timestamp = {2025-04-06T12:24:21Z}
}

@misc{deep,
  title = {Deep Convolutional Neural Network for Differentiating between Sarcoidosis and Lymphoma Based on [{{18F}}]{{FDG}} Maximum-Intensity Projection Images {\textbar} {{European Radiology}}},
  urldate = {2025-05-03},
  howpublished = {https://link.springer.com/article/10.1007/s00330-023-09937-x},
  timestamp = {2025-05-03T06:52:11Z}
}

@misc{deepseek-ai2025deepseekr1,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  year = {2025},
  month = jan,
  number = {arXiv:2501.12948},
  eprint = {2501.12948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.12948},
  urldate = {2025-03-19},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {TLDR: This work introduces first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL and achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.},
  timestamp = {2025-03-19T07:33:16Z}
}

@article{degrave2021ai,
  title = {{{AI}} for Radiographic {{COVID-19}} Detection Selects Shortcuts over Signal},
  author = {DeGrave, Alex J. and Janizek, Joseph D. and Lee, Su-In},
  year = {2021},
  month = jul,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {7},
  pages = {610--619},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00338-7},
  urldate = {2025-04-21},
  abstract = {Artificial intelligence (AI) researchers and radiologists have recently reported AI systems that accurately detect COVID-19 in chest radiographs. However, the robustness of these systems remains unclear. Using state-of-the-art techniques in explainable AI, we demonstrate that recent deep learning systems to detect COVID-19 from chest radiographs rely on confounding factors rather than medical pathology, creating an alarming situation in which the systems appear accurate, but fail when tested in new hospitals. We observe that the approach to obtain training data for these AI systems introduces a nearly ideal scenario for AI to learn these spurious `shortcuts'. Because this approach to data collection has also been used to obtain training data for the detection of COVID-19 in computed tomography scans and for medical imaging tasks related to other diseases, our study reveals a far-reaching problem in medical-imaging AI. In addition, we show that evaluation of a model on external data is insufficient to ensure AI systems rely on medically relevant pathology, because the undesired `shortcuts' learned by AI systems may not impair performance in new hospitals. These findings demonstrate that explainable AI should be seen as a prerequisite to clinical deployment of machine-learning healthcare models.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Predictive medicine,Radiography,SARS-CoV-2},
  annotation = {TLDR: It is demonstrated that explainable AI should be seen as a prerequisite to clinical deployment of machine-learning healthcare models, because the undesired `shortcuts' learned by AI systems may not impair performance in new hospitals.},
  timestamp = {2025-04-21T07:26:05Z}
}

@article{degrave2023auditing,
  title = {Auditing the Inference Processes of Medical-Image Classifiers by Leveraging Generative {{AI}} and the Expertise of Physicians},
  author = {DeGrave, Alex J. and Cai, Zhuo Ran and Janizek, Joseph D. and Daneshjou, Roxana and Lee, Su-In},
  year = {2023},
  month = dec,
  journal = {Nature Biomedical Engineering},
  volume = {9},
  number = {3},
  pages = {294--306},
  issn = {2157-846X},
  doi = {10.1038/s41551-023-01160-9},
  urldate = {2025-05-28},
  langid = {english},
  annotation = {TLDR: A general framework for model auditing that combines insights from medical experts with a highly expressive form of explainable artificial intelligence is reported that can be applied to any specialized medical domain to make the powerful inference processes of machine-learning models medically understandable.},
  timestamp = {2025-05-28T07:18:04Z}
}

@article{del2024generating,
  title = {On Generating Trustworthy Counterfactual Explanations},
  author = {Del Ser, Javier and {Barredo-Arrieta}, Alejandro and {D{\'i}az-Rodr{\'i}guez}, Natalia and Herrera, Francisco and Saranti, Anna and Holzinger, Andreas},
  year = {2024},
  month = jan,
  journal = {Information Sciences},
  volume = {655},
  pages = {119898},
  publisher = {Elsevier},
  issn = {00200255},
  doi = {10.1016/j.ins.2023.119898},
  urldate = {2025-04-21},
  langid = {english},
  timestamp = {2025-04-21T07:38:14Z}
}

@article{delaramodeles,
  title = {Mod{\`e}les Contrefactuels Pour Un Apprentissage Machine Explicable et Juste: Une Approche Par Transport de Masse},
  author = {{de Lara}, Lucas},
  langid = {english},
  timestamp = {2025-03-20T04:11:09Z}
}

@article{delavega2021artificial,
  title = {Artificial Intelligence Enables Comprehensive Genome Interpretation and Nomination of Candidate Diagnoses for Rare Genetic Diseases},
  author = {De La Vega, Francisco M. and Chowdhury, Shimul and Moore, Barry and Frise, Erwin and McCarthy, Jeanette and Hernandez, Edgar Javier and Wong, Terence and James, Kiely and Guidugli, Lucia and Agrawal, Pankaj B. and Genetti, Casie A. and Brownstein, Catherine A. and Beggs, Alan H. and L{\"o}scher, Britt-Sabina and Franke, Andre and Boone, Braden and Levy, Shawn E. and {\~O}unap, Katrin and Pajusalu, Sander and Huentelman, Matt and Ramsey, Keri and Naymik, Marcus and Narayanan, Vinodh and Veeraraghavan, Narayanan and Billings, Paul and Reese, Martin G. and Yandell, Mark and Kingsmore, Stephen F.},
  year = {2021},
  month = oct,
  journal = {Genome Medicine},
  volume = {13},
  number = {1},
  pages = {153},
  issn = {1756-994X},
  doi = {10.1186/s13073-021-00965-0},
  urldate = {2025-07-25},
  abstract = {Clinical interpretation of genetic variants in the context of the patient's phenotype is becoming the largest component of cost and time expenditure for genome-based diagnosis of rare genetic diseases. Artificial intelligence (AI) holds promise to greatly simplify and speed genome interpretation by integrating predictive methods with the growing knowledge of genetic disease. Here we assess the diagnostic performance of Fabric GEM, a new, AI-based, clinical decision support tool for expediting genome interpretation.},
  annotation = {TLDR: GEM enabled diagnostic interpretation inclusive of all variant types through automated nomination of a very short list of candidate genes and disorders for final review and reporting and showed similar performance in absence of parental genotypes.},
  timestamp = {2025-07-25T12:02:11Z}
}

@article{denny2021precision,
  title = {Precision Medicine in 2030---Seven Ways to Transform Healthcare},
  author = {Denny, Joshua C. and Collins, Francis S.},
  year = {2021},
  month = mar,
  journal = {Cell},
  volume = {184},
  number = {6},
  pages = {1415--1419},
  issn = {00928674},
  doi = {10.1016/j.cell.2021.01.015},
  urldate = {2025-05-27},
  langid = {english},
  timestamp = {2025-05-27T15:14:08Z}
}

@inproceedings{dhar2024comprehensive,
  title = {A {{Comprehensive Review}} of {{Explainable AI Applications}} in {{Healthcare}}},
  booktitle = {2024 15th {{International Conference}} on {{Computing Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  author = {Dhar, Anuj and Gupta, Shalini and Kumar, E. Sudheer},
  year = {2024},
  month = jun,
  pages = {1--8},
  publisher = {IEEE},
  address = {Kamand, India},
  doi = {10.1109/ICCCNT61001.2024.10725578},
  urldate = {2025-05-28},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-7024-9},
  annotation = {TLDR: This comprehensive analysis emphasizes the critical role of Explainable Artificial Intelligence in improving trust, transparency, and accountability in healthcare AI applications, establishing the path for further breakthroughs and wider implementation.},
  timestamp = {2025-05-28T06:57:43Z}
}

@article{dhurandhar2018explanations,
  title = {Explanations Based on the Missing: {{Towards}} Contrastive Explanations with Pertinent Negatives},
  author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31},
  timestamp = {2025-03-18T02:40:50Z}
}

@article{diao2021humaninterpretable,
  title = {Human-Interpretable Image Features Derived from Densely Mapped Cancer Pathology Slides Predict Diverse Molecular Phenotypes},
  author = {Diao, James A. and Wang, Jason K. and Chui, Wan Fung and Mountain, Victoria and Gullapally, Sai Chowdary and Srinivasan, Ramprakash and Mitchell, Richard N. and Glass, Benjamin and Hoffman, Sara and Rao, Sudha K. and Maheshwari, Chirag and Lahiri, Abhik and Prakash, Aaditya and McLoughlin, Ryan and Kerner, Jennifer K. and Resnick, Murray B. and Montalto, Michael C. and Khosla, Aditya and Wapinski, Ilan N. and Beck, Andrew H. and Elliott, Hunter L. and {Taylor-Weiner}, Amaro},
  year = {2021},
  month = mar,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {1613},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-21896-9},
  urldate = {2025-05-23},
  abstract = {Computational methods have made substantial progress in improving the accuracy and throughput of pathology workflows for diagnostic, prognostic, and genomic prediction. Still, lack of interpretability remains a significant barrier to clinical integration. We present an approach for predicting clinically-relevant molecular phenotypes from whole-slide histopathology images using human-interpretable image features (HIFs). Our method leverages {$>$}1.6 million annotations from board-certified pathologists across {$>$}5700 samples to train deep learning models for cell and tissue classification that can exhaustively map whole-slide images at two and four micron-resolution. Cell- and tissue-type model outputs are combined into 607 HIFs that quantify specific and biologically-relevant characteristics across five cancer types. We demonstrate that these HIFs correlate with well-known markers of the tumor microenvironment and can predict diverse molecular signatures (AUROC 0.601--0.864), including expression of four immune checkpoint proteins and homologous recombination deficiency, with performance comparable to `black-box' methods. Our HIF-based approach provides a comprehensive, quantitative, and interpretable window into the composition and spatial architecture of the tumor microenvironment.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Cancer imaging,Machine learning,Predictive markers},
  annotation = {TLDR: This work presents an approach for predicting clinically-relevant molecular phenotypes from whole-slide histopathology images using human-interpretable image features (HIFs), and demonstrates that these HIFs correlate with well-known markers of the tumor microenvironment and can predict diverse molecular signatures.},
  timestamp = {2025-05-23T02:22:00Z}
}

@article{diaz2012drug,
  title = {Drug {{Dosage Individualization Based}} on a {{Random-Effects Linear Model}}},
  author = {Diaz, Francisco J. and Cogollo, Myladis R. and Spina, Edoardo and Santoro, Vincenza and Rendon, Diego M. and De Leon, Jose},
  year = {2012},
  month = may,
  journal = {Journal of Biopharmaceutical Statistics},
  volume = {22},
  number = {3},
  pages = {463--484},
  issn = {1054-3406, 1520-5711},
  doi = {10.1080/10543406.2010.547264},
  urldate = {2025-04-06},
  langid = {english},
  annotation = {TLDR: It is shown through both decision-theoretic arguments and simulations that a published clinical algorithm may produce better individualized dosages than some traditional methods of therapeutic drug monitoring.},
  timestamp = {2025-04-06T09:22:19Z}
}

@misc{dibaeinia2023cimla,
  title = {{{CIMLA}}: {{Interpretable AI}} for Inference of Differential Causal Networks},
  shorttitle = {{{CIMLA}}},
  author = {Dibaeinia, Payam and Sinha, Saurabh},
  year = {2023},
  month = apr,
  number = {arXiv:2304.12523},
  eprint = {2304.12523},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.12523},
  urldate = {2025-04-17},
  abstract = {The discovery of causal relationships from high-dimensional data is a major open problem in bioinformatics. Machine learning and feature attribution models have shown great promise in this context but lack causal interpretation. Here, we show that a popular feature attribution model estimates a causal quantity reflecting the influence of one variable on another, under certain assumptions. We leverage this insight to implement a new tool, CIMLA, for discovering condition-dependent changes in causal relationships. We then use CIMLA to identify differences in gene regulatory networks between biological conditions, a problem that has received great attention in recent years. Using extensive benchmarking on simulated data sets, we show that CIMLA is more robust to confounding variables and is more accurate than leading methods. Finally, we employ CIMLA to analyze a previously published single-cell RNA-seq data set collected from subjects with and without Alzheimer's disease (AD), discovering several potential regulators of AD.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Methodology},
  annotation = {TLDR: This work shows that a popular feature attribution model estimates a causal quantity reflecting the influence of one variable on another, under certain assumptions, and implements a new tool, CIMLA, for discovering condition-dependent changes in causal relationships.},
  timestamp = {2025-04-17T08:18:38Z}
}

@article{dibaeinia2025interpretable,
  title = {Interpretable {{AI}} for Inference of Causal Molecular Relationships from Omics Data},
  author = {Dibaeinia, Payam and Ojha, Abhishek and Sinha, Saurabh},
  year = {2025},
  month = feb,
  journal = {Science Advances},
  volume = {11},
  number = {7},
  pages = {eadk0837},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.adk0837},
  urldate = {2025-05-04},
  abstract = {A connection is shown between a popular machine learning attribution model and causal molecular relationships in gene networks.},
  copyright = {Copyright {\copyright} 2025 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution NonCommercial License 4.0 (CC BY-NC).},
  langid = {english},
  annotation = {TLDR: This work shows that a popular feature attribution model, under certain assumptions, estimates an average of a causal quantity reflecting the direct influence of one variable on another, and proposes a precise definition of a gene regulatory relationship and implements a new tool, CIMLA (Counterfactual Inference by Machine Learning and Attribution Models).},
  timestamp = {2025-05-04T07:31:24Z}
}

@article{dienstmann2017consensus,
  title = {Consensus Molecular Subtypes and the Evolution of Precision Medicine in Colorectal Cancer},
  author = {Dienstmann, Rodrigo and Vermeulen, Louis and Guinney, Justin and Kopetz, Scott and Tejpar, Sabine and Tabernero, Josep},
  year = {2017},
  month = mar,
  journal = {Nature Reviews. Cancer},
  volume = {17},
  number = {4},
  pages = {268},
  issn = {1474-1768},
  doi = {10.1038/nrc.2017.24},
  langid = {english},
  pmid = {28332502},
  timestamp = {2025-05-27T16:01:00Z}
}

@article{dietterich1997solving,
  title = {Solving the Multiple Instance Problem with Axis-Parallel Rectangles},
  author = {Dietterich, Thomas G. and Lathrop, Richard H. and {Lozano-P{\'e}rez}, Tom{\'a}s},
  year = {1997},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {89},
  number = {1-2},
  pages = {31--71},
  publisher = {Elsevier BV},
  issn = {0004-3702},
  doi = {10.1016/s0004-3702(96)00034-3},
  urldate = {2025-07-24},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  annotation = {TLDR: Three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem are described and compared, giving 89\% correct predictions on a musk odor prediction task.},
  timestamp = {2025-07-24T14:59:21Z}
}

@article{dimartino2023explainable,
  title = {Explainable {{AI}} for Clinical and Remote Health Applications: A Survey on Tabular and Time Series Data},
  shorttitle = {Explainable {{AI}} for Clinical and Remote Health Applications},
  author = {Di Martino, Flavio and Delmastro, Franca},
  year = {2023},
  month = jun,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {6},
  pages = {5261--5315},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10304-3},
  urldate = {2025-04-09},
  abstract = {Nowadays Artificial Intelligence (AI) has become a fundamental component of healthcare applications, both clinical and remote, but the best performing AI systems are often too complex to be self-explaining. Explainable AI (XAI) techniques are defined to unveil the reasoning behind the system's predictions and decisions, and they become even more critical when dealing with sensitive and personal health data. It is worth noting that XAI has not gathered the same attention across different research areas and data types, especially in healthcare. In particular, many clinical and remote health applications are based on tabular and time series data, respectively, and XAI is not commonly analysed on these data types, while computer vision and Natural Language Processing (NLP) are the reference applications. To provide an overview of XAI methods that are most suitable for tabular and time series data in the healthcare domain, this paper provides a review of the literature in the last 5 years, illustrating the type of generated explanations and the efforts provided to evaluate their relevance and quality. Specifically, we identify clinical validation, consistency assessment, objective and standardised quality evaluation, and human-centered quality assessment as key features to ensure effective explanations for the end users. Finally, we highlight the main research challenges in the field as well as the limitations of existing XAI methods.},
  langid = {english},
  keywords = {Artificial Intelligence,Clinical DSS,EHR,Explainable AI,Health,Remote patient monitoring,Time series},
  annotation = {TLDR: Clinical validation, consistency assessment, objective and standardised quality evaluation, and human-centered quality assessment are identified as key features to ensure effective explanations for the end users in the healthcare domain.},
  timestamp = {2025-04-09T03:57:08Z}
}

@misc{ding2019systematic,
  title = {Systematic Comparative Analysis of Single Cell {{RNA-sequencing}} Methods},
  author = {Ding, Jiarui and Adiconis, Xian and Simmons, Sean K. and Kowalczyk, Monika S. and Hession, Cynthia C. and Marjanovic, Nemanja D. and Hughes, Travis K. and Wadsworth, Marc H. and Burks, Tyler and Nguyen, Lan T. and Kwon, John Y. H. and Barak, Boaz and Ge, William and Kedaigle, Amanda J. and Carroll, Shaina and Li, Shuqiang and Hacohen, Nir and {Rozenblatt-Rosen}, Orit and Shalek, Alex K. and Villani, Alexandra-Chlo{\'e} and Regev, Aviv and Levin, Joshua Z.},
  year = {2019},
  month = may,
  primaryclass = {New Results},
  pages = {632216},
  publisher = {bioRxiv},
  doi = {10.1101/632216},
  urldate = {2025-04-18},
  abstract = {A multitude of single-cell RNA sequencing methods have been developed in recent years, with dramatic advances in scale and power, and enabling major discoveries and large scale cell mapping efforts. However, these methods have not been systematically and comprehensively benchmarked. Here, we directly compare seven methods for single cell and/or single nucleus profiling from three types of samples -- cell lines, peripheral blood mononuclear cells and brain tissue -- generating 36 libraries in six separate experiments in a single center. To analyze these datasets, we developed and applied scumi, a flexible computational pipeline that can be used for any scRNA-seq method. We evaluated the methods for both basic performance and for their ability to recover known biological information in the samples. Our study will help guide experiments with the methods in this study as well as serve as a benchmark for future studies and for computational algorithm development.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: This study directly compares seven methods for single cell and/or single nucleus profiling from three types of samples, generating 36 libraries in six separate experiments in a single center and developed and applied scumi, a flexible computational pipeline that can be used for any scRNA-seq method.},
  timestamp = {2025-04-18T03:16:08Z}
}

@misc{directed,
  title = {Directed Acyclic Graphs and Causal Thinking in Clinical Risk Prediction Modeling {\textbar} {{BMC Medical Research Methodology}} {\textbar} {{Full Text}}},
  urldate = {2025-05-03},
  howpublished = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01058-z},
  timestamp = {2025-05-03T12:19:08Z}
}

@article{diwan2025explainable,
  title = {Explainable Machine Learning Models for Mortality Prediction in Patients with Sepsis in Tertiary Care Hospital {{ICU}} in Low- to Middle-Income Countries},
  author = {Diwan, Saumya and Gandhi, Vinay and Baidya Kayal, Esha and Khanna, Puneet and Mehndiratta, Amit},
  year = {2025},
  month = jun,
  journal = {Intensive Care Medicine Experimental},
  volume = {13},
  number = {1},
  pages = {56},
  issn = {2197-425X},
  doi = {10.1186/s40635-025-00765-5},
  urldate = {2025-08-11},
  abstract = {Abstract                            Introduction               Mortality in sepsis patients remains a challenging condition due to its complex nature. It is an even more prevalent health problem in low- and middle-income countries demanding costly treatment and management. This study proposes an explainable artificial intelligence-based approach towards mortality prediction for patients with sepsis admitted to intensive care unit (ICU).                                         Methods                                A total of 500 patients (                 N                 \,=\,500, male: female\,=\,262:238, age\,=\,45.96\,{\textpm}\,20.92~years) with sepsis were analyzed retrospectively. We utilize SHapley Additive exPlanations (SHAP) method to gain insights into the preliminary model's learnings regarding the wide array of demographic, clinical, radiological, and laboratory features. The clinical insights were used for feature selection to fetch the top                 t                 \,=\,80\% feature spread as well as to derive empirical findings from feature dependence plots which could find application in periphery hospital settings. Four machine learning algorithms, Random Forest, XGBoost, Extra Trees and Gradient Boosting classifiers were trained for the binary classification task (discharge from ICU and death in ICU) with the selected influential feature set.                                                        Results               The Extra Trees Classifier showed the best overall performance with AUROC score: 0.87 (95\% CI 0.80--0.93), Accuracy: 0.79 (95\% CI 0.71--0.86), F1 score: 0.78 (95\% CI 0.69--0.86), Precision: 0.88 (95\% CI 0.78--0.98) and Recall: 0.70 (95\% CI 0.57--0.82). All four models perform significantly well on dataset with AUROC scores ranging from 0.81 (CI 0.73--0.89) to 0.87 (CI 0.80--0.93) and F1 scores ranging 0.74 (CI 0.64--0.83) to 0.78 (CI 0.69--0.86) on the hold-out test set and were stable over fivefold cross-validation prior to testing.                                         Conclusions               The proposed approach could provide preemptive estimations into prognostication and outcome prediction of patients with sepsis in low-resource settings. This will aid in clinical decision-making, resource allocation and research for new treatment modalities.},
  langid = {english},
  annotation = {TLDR: An explainable artificial intelligence-based approach towards mortality prediction for patients with sepsis admitted to intensive care unit (ICU) and the proposed approach could provide preemptive estimations into prognostication and outcome prediction of patients with sepsis in low-resource settings.},
  timestamp = {2025-08-11T07:54:27Z}
}

@article{djoumessi2024inherently,
  title = {An Inherently Interpretable {{AI}} Model Improves Screening Speed and Accuracy for Early Diabetic Retinopathy},
  author = {Djoumessi, Kerol and Huang, Ziwei and Kuehlewein, Laura and Rickmann, Annekatrin and Simon, Natalia and Koch, Lisa M and Berens, Philipp},
  year = {2024},
  journal = {medRxiv : the preprint server for health sciences},
  pages = {2024--06},
  publisher = {Cold Spring Harbor Laboratory Press},
  timestamp = {2025-04-16T01:15:46Z}
}

@article{donmez2025explainable,
  title = {Explainable {{AI}} in Action: A Comparative Analysis of Hypertension Risk Factors Using {{SHAP}} and {{LIME}}},
  shorttitle = {Explainable {{AI}} in Action},
  author = {Donmez, Turker Berk and Kutlu, Mustafa and Mansour, Mohammed and Yildiz, Mustafa Zahid},
  year = {2025},
  month = feb,
  journal = {Neural Computing and Applications},
  volume = {37},
  number = {5},
  pages = {4053--4074},
  issn = {1433-3058},
  doi = {10.1007/s00521-024-10724-y},
  urldate = {2025-05-03},
  abstract = {Hypertension, a common and complex cardiovascular disease associated with a high risk of mortality and morbidity, has been targeted for detection using Artificial Intelligence Methods in recent times. However, due to their black-box nature, the reasons behind hypertension could not be identified by doctors. Therefore, there is an urgent need to elucidate the connections between hypertension and other biomarkers. In this study, local interpretable model-agnostic explanations (LIME) and SHapley Additive exPlanations (SHAP) were employed to demystify the hypertension risk prediction made by an extreme gradient boost (XGBoost) model. A comprehensive case record of 623 patients, encompassing reported hypertension and other diseases, medication usage, and laboratory results for 13 critical biomarkers, was analyzed in our study. The exemplary performance of our XGBoost model, demonstrating an accuracy of 99.4\%, precision of 100\%, recall of 97.30\%, and an F1-score of 98.6\%, further confirms the potential of Machine Learning (ML) in healthcare. Furthermore, the Biogeography-Based Optimization (BBO) algorithm was utilized to identify an effective subset of features. Almost half of the initial features were selected by the BBO algorithm. Achieving an accuracy, precision, recall, and F1-score of 97.7\%, 96.9\%, 93.9\%, and 95.4\%, respectively, using only 14 features selected by BBO highlights its efficacy. Additionally, our study demonstrated the impact of features on ML models through LIME and SHAP analyses. The preliminary system developed in this study underscores advanced, interpretable predictive modeling for early hypertension detection, laying the groundwork for enhanced risk assessment in diagnosis and preventive medicine in the future.},
  langid = {english},
  keywords = {Artificial Intelligence,Explainable AI,Hypertension,LIME,SHAP,XGBoost},
  timestamp = {2025-05-03T06:10:53Z}
}

@article{doran2017does,
  title = {What Does Explainable {{AI}} Really Mean? {{A}} New Conceptualization of Perspectives},
  author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.00794},
  eprint = {1710.00794},
  archiveprefix = {arXiv},
  timestamp = {2025-03-18T06:14:13Z}
}

@inproceedings{dosilovic2018explainable,
  title = {Explainable Artificial Intelligence: {{A}} Survey},
  shorttitle = {Explainable Artificial Intelligence},
  booktitle = {2018 41st {{International Convention}} on {{Information}} and {{Communication Technology}}, {{Electronics}} and {{Microelectronics}} ({{MIPRO}})},
  author = {Do{\v s}ilovi{\'c}, Filip Karlo and Br{\v c}i{\'c}, Mario and Hlupi{\'c}, Nikica},
  year = {2018},
  month = may,
  pages = {0210--0215},
  publisher = {IEEE},
  address = {Opatija},
  doi = {10.23919/MIPRO.2018.8400040},
  urldate = {2025-06-10},
  abstract = {In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.},
  isbn = {978-953-233-095-3},
  keywords = {comprehensibility,Decision trees,explainability,explainable artificial intelligence,interpretability,Machine learning,Optimization,Predictive models,Supervised learning,Support vector machines},
  annotation = {TLDR: Recent developments in XAI in supervised learning are summarized, a discussion on its connection with artificial general intelligence is started, and proposals for further research directions are given.},
  timestamp = {2025-06-10T00:30:11Z}
}

@inproceedings{douali2012genomic,
  title = {Genomic and Personalized Medicine Decision Support System},
  booktitle = {2012 {{IEEE International Conference}} on {{Complex Systems}} ({{ICCS}})},
  author = {Douali, Nassim and Jaulent, Marie-Christine},
  year = {2012},
  month = nov,
  pages = {1--4},
  publisher = {IEEE},
  address = {Agadir, Morocco},
  doi = {10.1109/ICoCS.2012.6458611},
  urldate = {2025-06-12},
  isbn = {978-1-4673-4766-2 978-1-4673-4764-8 978-1-4673-4765-5},
  annotation = {TLDR: A CDSS designed to assist physicians for personalized care, and methodology for integration in the clinical workflow to achieve its potential and improve the quality, safety and efficiency of healthcare.},
  timestamp = {2025-06-12T13:09:26Z}
}

@article{doudesis2023machine,
  title = {Machine Learning for Diagnosis of Myocardial Infarction Using Cardiac Troponin Concentrations},
  author = {Doudesis, Dimitrios and Lee, Kuan Ken and Boeddinghaus, Jasper and Bularga, Anda and Ferry, Amy V. and Tuck, Chris and Lowry, Matthew T. H. and {Lopez-Ayala}, Pedro and Nestelberger, Thomas and Koechlin, Luca and Bernabeu, Miguel O. and Neubeck, Lis and Anand, Atul and Schulz, Karen and Apple, Fred S. and Parsonage, William and Greenslade, Jaimi H. and Cullen, Louise and Pickering, John W. and Than, Martin P. and Gray, Alasdair and Mueller, Christian and Mills, Nicholas L.},
  year = {2023},
  month = may,
  journal = {Nature Medicine},
  volume = {29},
  number = {5},
  pages = {1201--1210},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-023-02325-4},
  urldate = {2025-05-02},
  abstract = {Although guidelines recommend fixed cardiac troponin thresholds for the diagnosis of myocardial infarction, troponin concentrations are influenced by age, sex, comorbidities and time from symptom onset. To improve diagnosis, we developed machine learning models that integrate cardiac troponin concentrations at presentation or on serial testing with clinical features and compute the Collaboration for the Diagnosis and Evaluation of Acute Coronary Syndrome (CoDE-ACS) score (0--100) that corresponds to an individual's probability of myocardial infarction. The models were trained on data from 10,038\,patients (48\% women), and their performance was externally validated using data from 10,286\,patients (35\% women) from seven cohorts. CoDE-ACS had excellent discrimination for myocardial infarction (area under curve, 0.953; 95\% confidence interval, 0.947--0.958), performed well across subgroups and identified more patients at presentation as low probability of having myocardial infarction than fixed cardiac troponin thresholds (61 versus 27\%) with a similar negative predictive value and fewer as high probability of having myocardial infarction (10 versus 16\%) with a greater positive predictive value. Patients identified as having a low probability of myocardial infarction had a lower rate of cardiac death than those with intermediate or high probability 30\,days (0.1 versus 0.5 and 1.8\%) and 1\,year (0.3 versus 2.8 and 4.2\%; P\,{$<$}\,0.001 for both) from patient presentation. CoDE-ACS used as a clinical decision support system has the potential to reduce hospital admissions and have major benefits for patients and health care providers.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Diagnostic markers,Myocardial infarction},
  annotation = {TLDR: Machine learning models that integrate cardiac troponin concentrations at presentation or on serial testing with clinical features and compute the Collaboration for the Diagnosis and Evaluation of Acute Coronary Syndrome (CoDE-ACS) score that corresponds to an individual's probability of myocardial infarction outperforms clinical guidelines that use fixed cardiac troponin thresholds for diagnosis.},
  timestamp = {2025-05-02T08:07:52Z}
}

@article{druker1996effects,
  title = {Effects of a Selective Inhibitor of the {{Abl}} Tyrosine Kinase on the Growth of {{Bcr}}--{{Abl}} Positive Cells},
  author = {Druker, Brian J and Tamura, Shu and Buchdunger, Elisabeth and Ohno, Sayuri and Segal, Gerald M and Fanning, Shane and Zimmermann, J{\"u}rg and Lydon, Nicholas B},
  year = {1996},
  journal = {Nature medicine},
  volume = {2},
  number = {5},
  pages = {561--566},
  publisher = {Nature Publishing Group US New York},
  timestamp = {2025-05-13T11:42:35Z}
}

@inproceedings{du2021adarnn,
  title = {{{AdaRNN}}: {{Adaptive Learning}} and {{Forecasting}} of {{Time Series}}},
  shorttitle = {{{AdaRNN}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Du, Yuntao and Wang, Jindong and Feng, Wenjie and Pan, Sinno and Qin, Tao and Xu, Renjun and Wang, Chongjun},
  year = {2021},
  month = oct,
  pages = {402--411},
  publisher = {ACM},
  address = {Virtual Event Queensland Australia},
  doi = {10.1145/3459637.3482315},
  urldate = {2025-04-04},
  isbn = {978-1-4503-8446-9},
  langid = {english},
  annotation = {TLDR: This paper proposes Adaptive RNNs (AdaRNN) to tackle the TCS problem by building an adaptive model that generalizes well on the unseen test data and proposes Temporal Distribution Characterization to better characterize the distribution information in the TS.},
  timestamp = {2025-04-04T03:13:55Z}
}

@article{duan2023deeplogic,
  title = {{{DeepLogic}}: {{Joint Learning}} of {{Neural Perception}} and {{Logical Reasoning}}},
  shorttitle = {{{DeepLogic}}},
  author = {Duan, Xuguang and Wang, Xin and Zhao, Peilin and Shen, Guangyao and Zhu, Wenwu},
  year = {2023},
  month = apr,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {04},
  pages = {4321--4334},
  publisher = {IEEE Computer Society},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2022.3191093},
  urldate = {2025-04-03},
  abstract = {Neural-symbolic learning, aiming to combine the perceiving power of neural perception and the reasoning power of symbolic logic together, has drawn increasing research attention. However, existing works simply cascade the two components together and optimize them isolatedly, failing to utilize the mutual enhancing information between them. To address this problem, we propose DeepLogic, a framework with joint learning of neural perception and logical reasoning, such that these two components are jointly optimized through mutual supervision signals. In particular, the proposed DeepLogic framework contains a deep-logic module that is capable of representing complex first-order-logic formulas in a tree structure with basic logic operators. We then theoretically quantify the mutual supervision signals and propose the deep\&logic optimization algorithm for joint optimization. We further prove the convergence of DeepLogic and conduct extensive experiments on model performance, convergence, and generalization, as well as its extension to the continuous domain. The experimental results show that through jointly learning both perceptual ability and logic formulas in a weakly supervised manner, our proposed DeepLogic framework can significantly outperform DNN-based baselines by a great margin and beat other strong baselines without out-of-box tools.},
  langid = {english},
  annotation = {TLDR: The experimental results show that the proposed DeepLogic framework can significantly outperform DNN-based baselines by a great margin and beat other strong baselines without out-of-box tools.},
  timestamp = {2025-04-16T08:18:21Z}
}

@article{dubey2024ai,
  title = {{{AI}} Readiness in Healthcare through Storytelling {{XAI}}},
  author = {Dubey, Akshat and Yang, Zewen and Hattab, Georges},
  year = {2024},
  journal = {arXiv preprint arXiv:2410.18725},
  eprint = {2410.18725},
  archiveprefix = {arXiv},
  timestamp = {2025-04-16T02:11:52Z}
}

@inproceedings{duell2021comparison,
  title = {A {{Comparison}} of {{Explanations Given}} by {{Explainable Artificial Intelligence Methods}} on {{Analysing Electronic Health Records}}},
  booktitle = {2021 {{IEEE EMBS International Conference}} on {{Biomedical}} and {{Health Informatics}} ({{BHI}})},
  author = {Duell, Jamie and Fan, Xiuyi and Burnett, Bruce and Aarts, Gert and Zhou, Shang-Ming},
  year = {2021},
  month = jul,
  pages = {1--4},
  publisher = {IEEE},
  address = {Athens, Greece},
  doi = {10.1109/BHI50953.2021.9508618},
  urldate = {2025-06-13},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-0358-0},
  annotation = {TLDR: A comparison of explanations given by XAI methods as a tertiary extension in analysing complex Electronic Health Records (EHRs) and their aberrations in shared feature importance merit further exploration from domain-experts to evaluate human trust towards XAI.},
  timestamp = {2025-06-13T07:54:15Z}
}

@incollection{durso-finley2023improving,
  title = {Improving {{Image-Based Precision Medicine}} with {{Uncertainty-Aware Causal Models}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} -- {{MICCAI}} 2023},
  author = {{Durso-Finley}, Joshua and Falet, Jean-Pierre and Mehta, Raghav and Arnold, Douglas L. and Pawlowski, Nick and Arbel, Tal},
  editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and {Syeda-Mahmood}, Tanveer and Taylor, Russell},
  year = {2023},
  volume = {14224},
  pages = {472--481},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-43904-9_46},
  urldate = {2025-06-13},
  isbn = {978-3-031-43903-2 978-3-031-43904-9},
  langid = {english},
  timestamp = {2025-06-13T03:48:42Z}
}

@article{dwivedi2023explainable,
  ids = {dwivedi2023},
  title = {Explainable {{AI}} ({{XAI}}): {{Core Ideas}}, {{Techniques}}, and {{Solutions}}},
  shorttitle = {Explainable {{AI}} ({{XAI}})},
  author = {Dwivedi, Rudresh and Dave, Devam and Naik, Het and Singhal, Smiti and Omer, Rana and Patel, Pankesh and Qian, Bin and Wen, Zhenyu and Shah, Tejal and Morgan, Graham and Ranjan, Rajiv},
  year = {2023},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {9},
  pages = {1--33},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3561048},
  urldate = {2025-04-21},
  abstract = {As our dependence on intelligent machines continues to grow, so does the demand for more transparent and interpretable models. In addition, the ability to explain the model generally is now the gold standard for building trust and deployment of artificial intelligence systems in critical domains. Explainable artificial intelligence~(XAI) aims to provide a suite of machine learning techniques that enable human users to understand, appropriately trust, and produce more explainable models. Selecting an appropriate approach for building an XAI-enabled application requires a clear understanding of the core ideas within XAI and the associated programming frameworks. We survey state-of-the-art programming techniques for XAI and present the different phases of XAI in a typical machine learning development process. We classify the various XAI approaches and, using this taxonomy, discuss the key differences among the existing XAI techniques. Furthermore, concrete examples are used to describe these techniques that are mapped to programming frameworks and software toolkits. It is the intention that this survey will help stakeholders in selecting the appropriate approaches, programming frameworks, and software toolkits by comparing them through the lens of the presented taxonomy.},
  langid = {english},
  annotation = {TLDR: This survey surveys state-of-the-art programming techniques for XAI and presents the different phases of XAI in a typical machine learning development process, and classify the various XAI approaches and discusses the key differences among the existing XAI techniques.},
  timestamp = {2025-04-21T07:31:23Z}
}

@article{ebers2024ai,
  title = {{{AI Robotics}} in {{Healthcare Between}} the {{EU Medical Device Regulation}} and the {{Artificial Intelligence Act}}: {{Gaps}} and {{Inconsistencies}} in the {{Protection}} of {{Patients}} and {{Care Recipients}}},
  shorttitle = {{{AI Robotics}} in {{Healthcare Between}} the {{EU Medical Device Regulation}} and the {{Artificial Intelligence Act}}},
  author = {Ebers, Martin},
  year = {2024},
  month = oct,
  journal = {Oslo Law Review},
  volume = {11},
  number = {1},
  pages = {1--12},
  issn = {2387-3299},
  doi = {10.18261/olr.11.1.2},
  urldate = {2025-06-13},
  langid = {english},
  timestamp = {2025-06-13T11:57:40Z}
}

@article{eke2025role,
  title = {The Role of Explainability and Transparency in Fostering Trust in {{AI}} Healthcare Systems: A Systematic Literature Review, Open Issues and Potential Solutions},
  shorttitle = {The Role of Explainability and Transparency in Fostering Trust in {{AI}} Healthcare Systems},
  author = {Eke, Christopher Ifeanyi and Shuib, Liyana},
  year = {2025},
  month = feb,
  journal = {Neural Computing and Applications},
  volume = {37},
  number = {4},
  pages = {1999--2034},
  issn = {1433-3058},
  doi = {10.1007/s00521-024-10868-x},
  urldate = {2025-04-01},
  abstract = {The healthcare sector has advanced significantly as a result of the ability of artificial intelligence (AI) to solve cognitive problems that once required human intelligence. As artificial intelligence finds more applications in healthcare, trustworthiness must be guaranteed. Even while AI has the potential to improve healthcare, there are still challenging issues because it is yet to be widely adopted, especially when it comes to transparency. Concerns about comprehending the internal workings of AI models, possible biases, model robustness, and generalizability are raised by their opacity which makes them function like black boxes. A solution for worries over the transparency of AI algorithms is explainable AI. Explainable AI seeks to enhance AI explainability and analytical capabilities, particularly in vital industries like healthcare.~Even though earlier research has examined several explainable AI-related topics, such as a lexicon, industry-specific overviews, and applications in the healthcare industry, a thorough analysis concentrating on the function of explainable AI in building trust in AI healthcare systems is required. In an effort to close this gap, a systematic literature review that adheres to PRISMA principles that analyze relevant~papers that were published between 2015 and 2023 was done in this paper. To determine the critical role that explainable AI plays in fostering trust, this study examines widely utilized methodologies, machine learning and deep learning techniques, datasets, performance measures and validation procedures used in AI healthcare research. In addition, research issues and potential research directions are also discussed in this research. Thus, this systematic review provides a thorough summary of the present status of research on explainability and transparency in AI healthcare systems, thus illuminating crucial factors that affect user trust. The results are intended to assist researchers, policymakers and healthcare professionals in developing a more transparent, responsible and reliable AI system in the healthcare sector.},
  langid = {english},
  keywords = {Artificial intelligence,Artificial Intelligence,Explainability,Healthcare systems,Machine learning,Transparency,Trust},
  timestamp = {2025-04-01T14:37:26Z}
}

@article{el-sappagh2021multilayer,
  title = {A Multilayer Multimodal Detection and Prediction Model Based on Explainable Artificial Intelligence for {{Alzheimer}}'s Disease},
  author = {{El-Sappagh}, Shaker and Alonso, Jose M. and Islam, S. M. Riazul and Sultan, Ahmad M. and Kwak, Kyung Sup},
  year = {2021},
  month = jan,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {2660},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-82098-3},
  urldate = {2025-04-21},
  abstract = {Alzheimer's disease (AD) is the most common type of dementia. Its diagnosis and progression detection have been intensively studied. Nevertheless, research studies often have little effect on clinical practice mainly due to the following reasons: (1) Most studies depend mainly on a single modality, especially neuroimaging; (2) diagnosis and progression detection are usually studied separately as two independent problems; and (3) current studies concentrate mainly on optimizing the performance of complex machine learning models, while disregarding their explainability. As a result, physicians struggle to interpret these models, and feel it is hard to trust them. In this paper, we carefully develop an accurate and interpretable AD diagnosis and progression detection model. This model provides physicians with accurate decisions along with a set of explanations for every decision. Specifically, the model integrates 11 modalities of 1048 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) real-world dataset: 294 cognitively normal, 254 stable mild cognitive impairment (MCI), 232 progressive MCI, and 268 AD. It is actually a two-layer model with random forest (RF) as classifier algorithm. In the first layer, the model carries out a multi-class classification for the early diagnosis of AD patients. In the second layer, the model applies binary classification to detect possible MCI-to-AD progression within three years from a baseline diagnosis. The performance of the model is optimized with key markers selected from a large set of biological and clinical measures. Regarding explainability, we provide, for each layer, global and instance-based explanations of the RF classifier by using the SHapley Additive exPlanations (SHAP) feature attribution framework. In addition, we implement 22 explainers based on decision trees and fuzzy rule-based systems to provide complementary justifications for every RF decision in each layer. Furthermore, these explanations are represented in natural language form to help physicians understand the predictions. The designed model achieves a cross-validation accuracy of 93.95\% and an F1-score of 93.94\% in the first layer, while it achieves a cross-validation accuracy of 87.08\% and an F1-Score of 87.09\% in the second layer. The resulting system is not only accurate, but also trustworthy, accountable, and medically applicable, thanks to the provided explanations which are broadly consistent with each other and with the AD medical literature. The proposed system can help to enhance the clinical understanding of AD diagnosis and progression processes by providing detailed insights into the effect of different modalities on the disease risk.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Classification and taxonomy,Computational neuroscience,Data mining,Data processing,Machine learning},
  annotation = {TLDR: An accurate and interpretable AD diagnosis and progression detection model that provides physicians with accurate decisions along with a set of explanations for every decision and provides detailed insights into the effect of different modalities on the disease risk is developed.},
  timestamp = {2025-04-21T07:29:52Z}
}

@article{elliott2018genome,
  title = {Genome-Wide Association Studies of Brain Imaging Phenotypes in {{UK Biobank}}},
  author = {Elliott, Lynn T and Sudlow, Cathie L and {S{\cyrchar\cyrr}{\cyrchar\cyrery}{\cyrchar\cyrv}{\cyrchar\cyrery}} and {Munoz-Maniega}, Sonia and Smith, Colin and {{\cyrchar\CYRS}{\cyrchar\cyrs}{\cyrchar\cyro}{\cyrchar\cyrr}{\cyrchar\cyrery}} and Hagberg, Emma and McCarthy, Nuala S and {{\cyrchar\cyrn}{\cyrchar\cyra}{\cyrchar\cyrs}{\cyrchar\cyrch}{\cyrchar\cyre}{\cyrchar\cyrt}} and Bastin, Mark E and {{\cyrchar\CYRV}{\cyrchar\cyro}{\cyrchar\cyrk}{\cyrchar\cyrr}{\cyrchar\cyru}{\cyrchar\cyrg}}},
  year = {2018},
  journal = {Nature neuroscience},
  volume = {21},
  number = {11},
  pages = {1569},
  publisher = {Nature Publishing Group},
  timestamp = {2025-04-12T13:43:08Z}
}

@article{elmarakeby2021biologically,
  title = {Biologically Informed Deep Neural Network for Prostate Cancer Discovery},
  author = {Elmarakeby, Haitham A. and Hwang, Justin and Arafeh, Rand and Crowdis, Jett and Gang, Sydney and Liu, David and AlDubayan, Saud H. and Salari, Keyan and Kregel, Steven and Richter, Camden and Arnoff, Taylor E. and Park, Jihye and Hahn, William C. and Van Allen, Eliezer M.},
  year = {2021},
  month = oct,
  journal = {Nature},
  volume = {598},
  number = {7880},
  pages = {348--352},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03922-4},
  abstract = {The determination of molecular features that mediate clinically aggressive phenotypes in prostate cancer remains a major biological and clinical challenge1,2. Recent advances in interpretability of machine learning models~as applied to biomedical problems may enable discovery and prediction in clinical cancer genomics3-5. Here we developed P-NET-a biologically informed deep learning model-to stratify patients with~prostate cancer by treatment-resistance state and evaluate molecular drivers of treatment resistance for therapeutic targeting through complete model interpretability. We demonstrate that P-NET can predict cancer state using molecular data with a performance that is superior to other modelling approaches. Moreover, the biological interpretability within P-NET revealed established and novel molecularly altered candidates, such as MDM4 and FGFR1, which were implicated in predicting advanced disease and validated in vitro. Broadly, biologically informed fully interpretable neural networks enable preclinical discovery and clinical prediction in prostate cancer and may have general applicability across cancer types.},
  langid = {english},
  pmcid = {PMC8514339},
  pmid = {34552244},
  keywords = {Cell Cycle Proteins,Deep Learning,Drug Resistance Neoplasm,Humans,Male,Prostatic Neoplasms,Proto-Oncogene Proteins,Receptor Fibroblast Growth Factor Type 1,Receptors Androgen,Reproducibility of Results,Tumor Suppressor Protein p53},
  timestamp = {2025-07-05T13:11:42Z}
}

@article{elshawi2021interpretability,
  ids = {elshawiinterpretability},
  title = {Interpretability in Healthcare: {{A}} Comparative Study of Local Machine Learning Interpretability Techniques},
  shorttitle = {Interpretability in Healthcare},
  author = {ElShawi, Radwa and Sherif, Youssef and {Al-Mallah}, Mouaz and Sakr, Sherif},
  year = {2021},
  journal = {Computational Intelligence},
  volume = {37},
  number = {4},
  pages = {1633--1650},
  publisher = {Wiley Online Library},
  doi = {10.1111/coin.12410},
  urldate = {2025-03-25},
  abstract = {Although complex machine learning models (eg, random forest, neural networks) are commonly outperforming the traditional and simple interpretable models (eg, linear regression, decision tree), in the...},
  langid = {english},
  timestamp = {2025-03-25T11:49:34Z}
}

@misc{ethayarajh2019how,
  title = {How {{Contextual}} Are {{Contextualized Word Representations}}? {{Comparing}} the {{Geometry}} of {{BERT}}, {{ELMo}}, and {{GPT-2 Embeddings}}},
  shorttitle = {How {{Contextual}} Are {{Contextualized Word Representations}}?},
  author = {Ethayarajh, Kawin},
  year = {2019},
  month = sep,
  number = {arXiv:1909.00512},
  eprint = {1909.00512},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.00512},
  urldate = {2025-09-01},
  abstract = {Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5\% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  timestamp = {2025-09-01T12:16:24Z}
}

@misc{eu2024regulation,
  title = {Regulation - {{EU}} - 2024/1689 - {{EN}} - {{EUR-Lex}}},
  author = {EU},
  year = {2024},
  urldate = {2025-05-28},
  howpublished = {https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng},
  langid = {english},
  annotation = {Doc ID: 32024R1689\\
Doc Sector: 3\\
Doc Title: Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA relevance)\\
Doc Type: R\\
Usr\_lan: en},
  timestamp = {2025-05-28T01:54:42Z}
}

@article{eves2022augmented,
  title = {Augmented {{Reality}} in {{Vascular}} and {{Endovascular Surgery}}: {{Scoping Review}}},
  shorttitle = {Augmented {{Reality}} in {{Vascular}} and {{Endovascular Surgery}}},
  author = {Eves, Joshua and Sudarsanam, Abhilash and Shalhoub, Joseph and Amiras, Dimitri},
  year = {2022},
  month = sep,
  journal = {JMIR serious games},
  volume = {10},
  number = {3},
  pages = {e34501},
  issn = {2291-9279},
  doi = {10.2196/34501},
  abstract = {BACKGROUND: Technological advances have transformed vascular intervention in recent decades. In particular, improvements in imaging and data processing have allowed for the development of increasingly complex endovascular and hybrid interventions. Augmented reality (AR) is a subject of growing interest in surgery, with the potential to improve clinicians' understanding of 3D anatomy and aid in the processing of real-time information. This study hopes to elucidate the potential impact of AR technology in the rapidly evolving fields of vascular and endovascular surgery. OBJECTIVE: The aim of this review is to summarize the fundamental concepts of AR technologies and conduct a scoping review of the impact of AR and mixed reality in vascular and endovascular surgery. METHODS: A systematic search of MEDLINE, Scopus, and Embase was performed in accordance with the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. All studies written in English from inception until January 8, 2021, were included in the search. Combinations of the following keywords were used in the systematic search string: ("augmented reality" OR "hololens" OR "image overlay" OR "daqri" OR "magic leap" OR "immersive reality" OR "extended reality" OR "mixed reality" OR "head mounted display") AND ("vascular surgery" OR "endovascular"). Studies were selected through a blinded process between 2 investigators (JE and AS) and assessed using data quality tools. RESULTS: AR technologies have had a number of applications in vascular and endovascular surgery. Most studies (22/32, 69\%) used 3D imaging of computed tomography angiogram-derived images of vascular anatomy to augment clinicians' anatomical understanding during procedures. A wide range of AR technologies were used, with heads up fusion imaging and AR head-mounted displays being the most commonly applied clinically. AR applications included guiding open, robotic, and endovascular surgery while minimizing dissection, improving procedural times, and reducing radiation and contrast exposure. CONCLUSIONS: AR has shown promising developments in the field of vascular and endovascular surgery, with potential benefits to surgeons and patients alike. These include reductions in patient risk and operating times as well as in contrast and radiation exposure for radiological interventions. Further technological advances are required to overcome current limitations, including processing capacity and vascular deformation by instrumentation.},
  langid = {english},
  pmcid = {PMC9547335},
  pmid = {36149736},
  keywords = {augmented reality,endovascular,head-mounted display,mobile phone,surgery,vascular},
  annotation = {TLDR: AR has shown promising developments in the field of vascular and endovascular surgery, with potential benefits to surgeons and patients alike.},
  timestamp = {2025-04-12T07:57:03Z}
}

@misc{explainable,
  title = {Explainable {{AI}} in {{Medical Diagnostics}} {\textbar} {{Elicit}}},
  urldate = {2025-04-01},
  howpublished = {https://elicit.com/review/e41bac33-f097-42f7-b793-e43f4790b81f},
  timestamp = {2025-04-01T02:42:51Z}
}

@misc{explaining,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2025-02-02},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/8631448},
  timestamp = {2025-02-02T09:07:59Z}
}

@misc{ezzeddine2024privacy,
  title = {Privacy {{Implications}} of {{Explainable AI}} in {{Data-Driven Systems}}},
  author = {Ezzeddine, Fatima},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2406.15789},
  urldate = {2025-06-13},
  abstract = {Machine learning (ML) models, demonstrably powerful, suffer from a lack of interpretability. The absence of transparency, often referred to as the black box nature of ML models, undermines trust and urges the need for efforts to enhance their explainability. Explainable AI (XAI) techniques address this challenge by providing frameworks and methods to explain the internal decision-making processes of these complex models. Techniques like Counterfactual Explanations (CF) and Feature Importance play a crucial role in achieving this goal. Furthermore, high-quality and diverse data remains the foundational element for robust and trustworthy ML applications. In many applications, the data used to train ML and XAI explainers contain sensitive information. In this context, numerous privacy-preserving techniques can be employed to safeguard sensitive information in the data, such as differential privacy. Subsequently, a conflict between XAI and privacy solutions emerges due to their opposing goals. Since XAI techniques provide reasoning for the model behavior, they reveal information relative to ML models, such as their decision boundaries, the values of features, or the gradients of deep learning models when explanations are exposed to a third entity. Attackers can initiate privacy breaching attacks using these explanations, to perform model extraction, inference, and membership attacks. This dilemma underscores the challenge of finding the right equilibrium between understanding ML decision-making and safeguarding privacy.},
  copyright = {Creative Commons Zero v1.0 Universal},
  keywords = {Artificial Intelligence (cs.AI),Cryptography and Security (cs.CR),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: A conflict between XAI and privacy solutions emerges due to their opposing goals, which underscores the challenge of finding the right equilibrium between understanding ML decision-making and safeguarding privacy.},
  timestamp = {2025-06-13T07:07:52Z}
}

@article{fagan1980computer,
  title = {Computer-Based Medical Decision Making: From {{MYCIN}} to {{VM}}},
  author = {Fagan, Lawrence M and Shortliffe, Edward H and Buchanan, Bruce G},
  year = {1980},
  journal = {Automedica},
  volume = {3},
  number = {2},
  pages = {97--108},
  timestamp = {2025-03-15T01:57:56Z}
}

@article{fan2024developing,
  title = {Developing Survival Prediction Models in Colorectal Cancer Using Epigenome-Wide {{DNA}} Methylation Data from Whole Blood},
  author = {Fan, Ziwen and Edelmann, Dominic and Yuan, Tanwei and K{\"o}hler, Bruno Christian and Hoffmeister, Michael and Brenner, Hermann},
  year = {2024},
  month = sep,
  journal = {npj Precision Oncology},
  volume = {8},
  number = {1},
  pages = {191},
  issn = {2397-768X},
  doi = {10.1038/s41698-024-00689-5},
  abstract = {While genome-wide association studies are valuable in identifying CRC survival predictors, the benefit of adding blood DNA methylation (blood-DNAm) to clinical features, including the TNM system, remains unclear. In a multi-site population-based patient cohort study of 2116 CRC patients with baseline blood-DNAm, we analyzed survival predictions using eXtreme Gradient Boosting with a 5-fold nested leave-sites-out cross-validation across four groups: traditional and comprehensive clinical features, blood-DNAm, and their combination. Model performance was assessed using time-dependent ROC curves and calibrations. During a median follow-up of 10.3 years, 1166 patients died. Although blood-DNAm-based predictive signatures achieved moderate performances, predictive signatures based on clinical features outperformed blood-DNAm signatures. The inclusion of blood-DNAm did not improve survival prediction over clinical features. M1 stage, age at blood collection, and N2 stage were the top contributors. Despite some prognostic value, incorporating blood DNA methylation did not enhance survival prediction of CRC patients beyond clinical features.},
  annotation = {TLDR: Although blood-DNAm-based predictive signatures achieved moderate performances, predictive signatures based on clinical features outperformed blood-DNAm signatures and the inclusion of blood-DNAm did not improve survival prediction over clinical features.},
  timestamp = {2025-08-11T03:40:59Z}
}

@inproceedings{fang2020conceptbased,
  title = {Concept-Based {{Explanation}} for {{Fine-grained Images}} and {{Its Application}} in {{Infectious Keratitis Classification}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Multimedia}}},
  author = {Fang, Zhengqing and Kuang, Kun and Lin, Yuxiao and Wu, Fei and Yao, Yu-Feng},
  year = {2020},
  month = oct,
  pages = {700--708},
  publisher = {ACM},
  address = {Seattle WA USA},
  doi = {10.1145/3394171.3413557},
  urldate = {2025-05-18},
  isbn = {978-1-4503-7988-5},
  langid = {english},
  annotation = {TLDR: This paper proposes a visual concept mining (VCM) method to explain the fine-grained infectious keratitis images and discovers explainable visual concepts that are highly coherent with the physicians?},
  timestamp = {2025-05-18T12:41:54Z}
}

@article{farhud2013brief,
  title = {A Brief History of Human Blood Groups},
  author = {Farhud, Dariush D. and Zarif Yeganeh, Marjan},
  year = {2013},
  journal = {Iranian Journal of Public Health},
  volume = {42},
  number = {1},
  pages = {1--6},
  issn = {2251-6085},
  abstract = {The evolution of human blood groups, without doubt, has a history as old as man himself. There are at least three hypotheses about the emergence and mutation of human blood groups. Global distribution pattern of blood groups depends on various environmental factors, such as disease, climate, altitude, humidity etc. In this survey, the collection of main blood groups ABO and Rh, along with some minor groups, are presented. Several investigations of blood groups from Iran, particularly a large sampling on 291857 individuals from Iran, including the main blood groups ABO and Rh, as well as minor blood groups such as Duffy, Lutheran, Kell, KP, Kidd, and Xg, have been reviewed.},
  langid = {english},
  pmcid = {PMC3595629},
  pmid = {23514954},
  keywords = {ABO,Iran,Major blood groups,Minor blood groups,Rh},
  timestamp = {2025-05-27T14:37:29Z}
}

@article{faulkner2020being,
  title = {Being {{Precise About Precision Medicine}}: {{What Should Value Frameworks Incorporate}} to {{Address Precision Medicine}}? {{A Report}} of the {{Personalized Precision Medicine Special Interest Group}}},
  shorttitle = {Being {{Precise About Precision Medicine}}},
  author = {Faulkner, Eric and Holtorf, Anke-Peggy and Walton, Surrey and Liu, Christine Y. and Lin, Hwee and Biltaj, Eman and Brixner, Diana and Barr, Charles and Oberg, Jennifer and Shandhu, Gurmit and Siebert, Uwe and Snyder, Susan R. and Tiwana, Simran and Watkins, John and IJzerman, Maarten J. and Payne, Katherine},
  year = {2020},
  month = may,
  journal = {Value in Health},
  volume = {23},
  number = {5},
  pages = {529--539},
  issn = {10983015},
  doi = {10.1016/j.jval.2019.11.010},
  urldate = {2025-05-27},
  langid = {english},
  annotation = {TLDR: The International Society for Pharmacoeconomics and Outcome Research expanded its current work around precision medicine to describe the evolving paradigm of precision medicine with examples of current and evolving applications, and define the core factors that should be considered in a value assessment framework for precision medicine.},
  timestamp = {2025-05-27T14:06:21Z}
}

@misc{FDA2025AI,
  title = {Artificial Intelligence-Enabled Device Software Functions: {{Lifecycle}} Management and Marketing Submission Recommendations},
  author = {{U.S. Food and Drug Administration}},
  year = {2025},
  month = jan,
  publisher = {{U.S. Food and Drug Administration, Center for Devices and Radiological Health (CDRH)}},
  timestamp = {2025-06-13T11:46:44Z}
}

@article{feng2023gene,
  title = {Gene Regulatory Network Inference Based on Causal Discovery Integrating with Graph Neural Network},
  author = {Feng, Ke and Jiang, Hongyang and Yin, Chaoyi and Sun, Huiyan},
  year = {2023},
  month = dec,
  journal = {Quantitative Biology},
  volume = {11},
  number = {4},
  pages = {434--450},
  issn = {2095-4689, 2095-4697},
  doi = {10.1002/qub2.26},
  urldate = {2025-04-18},
  abstract = {Abstract             Gene regulatory network (GRN) inference from gene expression data is a significant approach to understanding aspects of the biological system. Compared with generalized correlation-based methods, causality-inspired ones seem more rational to infer regulatory relationships. We propose GRINCD, a novel GRN inference framework empowered by graph representation learning and causal asymmetric learning, considering both linear and non-linear regulatory relationships. First, high-quality representation of each gene is generated using graph neural network. Then, we apply the additive noise model to predict the causal regulation of each regulator-target pair. Additionally, we design two channels and finally assemble them for robust prediction. Through comprehensive comparisons of our framework with state-of-the-art methods based on different principles on numerous datasets of diverse types and scales, the experimental results show that our framework achieves superior or comparable performance under various evaluation metrics. Our work provides a new clue for constructing GRNs, and our proposed framework GRINCD also shows potential in identifying key factors affecting cancer development.},
  langid = {english},
  annotation = {TLDR: This work proposes GRINCD, a novel GRN inference framework empowered by graph representation learning and causal asymmetric learning, considering both linear and non-linear regulatory relationships, and shows potential in identifying key factors affecting cancer development.},
  timestamp = {2025-04-18T03:24:16Z}
}

@article{ference2015mendelian,
  title = {Mendelian Randomization Studies: Using Naturally Randomized Genetic Data to Fill Evidence Gaps},
  shorttitle = {Mendelian Randomization Studies},
  author = {Ference, Brian A.},
  year = {2015},
  month = dec,
  journal = {Current Opinion in Lipidology},
  volume = {26},
  number = {6},
  pages = {566--571},
  issn = {0957-9672},
  doi = {10.1097/MOL.0000000000000247},
  urldate = {2025-04-06},
  langid = {english},
  annotation = {TLDR: The naturally randomized genetic evidence suggests that HDL-C has a causal and cumulative effect on the risk of CHD, and that the clinical benefit of exposure to lower LDL-C is determined by the absolute magnitude of exposure on the basis of the mechanism by which LDL- C is lowered.},
  timestamp = {2025-04-06T10:25:29Z}
}

@article{feuerriegel2024causal,
  title = {Causal Machine Learning for Predicting Treatment Outcomes},
  author = {Feuerriegel, Stefan and Frauen, Dennis and Melnychuk, Valentyn and Schweisthal, Jonas and Hess, Konstantin and Curth, Alicia and Bauer, Stefan and Kilbertus, Niki and Kohane, Isaac S. and van der Schaar, Mihaela},
  year = {2024},
  month = apr,
  journal = {Nature Medicine},
  volume = {30},
  number = {4},
  eprint = {2410.08770},
  primaryclass = {cs},
  pages = {958--968},
  issn = {1078-8956, 1546-170X},
  doi = {10.1038/s41591-024-02902-1},
  urldate = {2025-03-18},
  abstract = {Causal machine learning (ML) offers flexible, data-driven methods for predicting treatment outcomes including efficacy and toxicity, thereby supporting the assessment and safety of drugs. A key benefit of causal ML is that it allows for estimating individualized treatment effects, so that clinical decision-making can be personalized to individual patient profiles. Causal ML can be used in combination with both clinical trial data and real-world data, such as clinical registries and electronic health records, but caution is needed to avoid biased or incorrect predictions. In this Perspective, we discuss the benefits of causal ML (relative to traditional statistical or ML approaches) and outline the key components and steps. Finally, we provide recommendations for the reliable use of causal ML and effective translation into the clinic.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  annotation = {TLDR: The benefits of causal ML (relative to traditional statistical or ML approaches) are discussed and the key components and steps are outlined and recommendations for the reliable use of causal ML and effective translation into the clinic are provided.},
  timestamp = {2025-03-18T09:10:15Z}
}

@article{fillmore_frame_semantics,
  title = {Frame Semantics (Linguistics)},
  author = {{contributors}, Wikipedia},
  year = {2025},
  journal = {Wikipedia, The Free Encyclopedia},
  timestamp = {2025-09-06T00:36:49Z}
}

@article{finzel2025current,
  title = {Current Methods in Explainable Artificial Intelligence and Future Prospects for Integrative Physiology},
  author = {Finzel, Bettina},
  year = {2025},
  journal = {Pflugers Archiv},
  volume = {477},
  number = {4},
  pages = {513--529},
  issn = {0031-6768},
  doi = {10.1007/s00424-025-03067-7},
  urldate = {2025-04-12},
  abstract = {Explainable artificial intelligence (XAI) is gaining importance in physiological research, where artificial intelligence is now used as an analytical and predictive tool for many medical research questions. The primary goal of XAI is to make AI models understandable for human decision-makers. This can be achieved in particular through providing inherently interpretable AI methods or by making opaque models and their outputs transparent using post hoc explanations. This review introduces XAI core topics and provides a selective overview of current XAI methods in physiology. It further illustrates solved and discusses open challenges in XAI research using existing practical examples from the medical field. The article gives an outlook on two possible future prospects: (1) using XAI methods to provide trustworthy AI for integrative physiological research and (2) integrating physiological expertise about human explanation into XAI method development for useful and beneficial human-AI partnerships.},
  pmcid = {PMC11958383},
  pmid = {39994035},
  annotation = {TLDR: An outlook on two possible future prospects are given: using XAI methods to provide trustworthy AI for integrative physiological research and integrating physiological expertise about human explanation into XAI method development for useful and beneficial human-AI partnerships.},
  timestamp = {2025-04-12T04:05:42Z}
}

@misc{fisher2019all,
  title = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}: {{Learning}} a {{Variable}}'s {{Importance}} by {{Studying}} an {{Entire Class}} of {{Prediction Models Simultaneously}}},
  shorttitle = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}},
  author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
  year = {2019},
  month = dec,
  number = {arXiv:1801.01489},
  eprint = {1801.01489},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01489},
  urldate = {2025-03-18},
  abstract = {Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model \$f({\textbackslash}mathbf\{x\})={\textbackslash}mathbf\{x\}{\textasciicircum}\{T\}{\textbackslash}beta\$ with a fixed coefficient vector \${\textbackslash}beta\$) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  timestamp = {2025-03-18T01:39:46Z}
}

@article{fortelny2020knowledgeprimed,
  title = {Knowledge-Primed Neural Networks Enable Biologically Interpretable Deep Learning on Single-Cell Sequencing Data},
  author = {Fortelny, Nikolaus and Bock, Christoph},
  year = {2020},
  month = aug,
  journal = {Genome Biology},
  volume = {21},
  number = {1},
  pages = {190},
  issn = {1474-760X},
  doi = {10.1186/s13059-020-02100-5},
  abstract = {BACKGROUND: Deep learning has emerged as a versatile approach for predicting complex biological phenomena. However, its utility for biological discovery has so far been limited, given that generic deep neural networks provide little insight into the biological mechanisms that underlie a successful prediction. Here we demonstrate deep learning on biological networks, where every node has a molecular equivalent, such as a protein or gene, and every edge has a mechanistic interpretation, such as a regulatory interaction along a signaling pathway. RESULTS: With knowledge-primed neural networks (KPNNs), we exploit the ability of deep learning algorithms to assign meaningful weights in multi-layered networks, resulting in a widely applicable approach for interpretable deep learning. We present a learning method that enhances the interpretability of trained KPNNs by stabilizing node weights in the presence of redundancy, enhancing the quantitative interpretability of node weights, and controlling for uneven connectivity in biological networks. We validate KPNNs on simulated data with known ground truth and demonstrate their practical use and utility in five biological applications with single-cell RNA-seq~data for cancer and immune cells. CONCLUSIONS: We introduce KPNNs as a method that combines the predictive power of deep learning with the interpretability of biological networks. While demonstrated here on single-cell sequencing data, this method is broadly relevant to other research areas where prior domain knowledge can be represented as networks.},
  langid = {american},
  pmcid = {PMC7397672},
  pmid = {32746932},
  keywords = {Artificial neural networks,Bioinformatic modeling,Cell signaling networks,Deep learning,Deep Learning,Functional genomics,Gene regulation,Humans,Interpretable machine learning,Receptors Antigen T-Cell,Sequence Analysis RNA,Signal Transduction,Single-Cell Analysis,Single-cell sequencing},
  annotation = {TLDR: This work introduces KPNNs as a method that combines the predictive power of deep learning with the interpretability of biological networks, and presents a learning method that enhances the interpretability of trained KPNNs by stabilizing node weights in the presence of redundancy, enhancing the quantitative interpretability of node weights, and controlling for uneven connectivity in biological networks.},
  timestamp = {2025-07-25T14:12:43Z}
}

@article{frasca2024explainable,
  title = {Explainable and Interpretable Artificial Intelligence in Medicine: A Systematic Bibliometric Review},
  shorttitle = {Explainable and Interpretable Artificial Intelligence in Medicine},
  author = {Frasca, Maria and La Torre, Davide and Pravettoni, Gabriella and Cutica, Ilaria},
  year = {2024},
  month = feb,
  journal = {Discover Artificial Intelligence},
  volume = {4},
  number = {1},
  pages = {15},
  issn = {2731-0809},
  doi = {10.1007/s44163-024-00114-7},
  urldate = {2025-08-13},
  abstract = {Abstract             This review aims to explore the growing impact of machine learning and deep learning algorithms in the medical field, with a specific focus on the critical issues of explainability and interpretability associated with black-box algorithms. While machine learning algorithms are increasingly employed for medical analysis and diagnosis, their complexity underscores the importance of understanding how these algorithms explain and interpret data to take informed decisions. This review comprehensively analyzes challenges and solutions presented in the literature, offering an overview of the most recent techniques utilized in this field. It also provides precise definitions of interpretability and explainability, aiming to clarify the distinctions between these concepts and their implications for the decision-making process. Our analysis, based on 448 articles and addressing seven research questions, reveals an exponential growth in this field over the last decade. The psychological dimensions of public perception underscore the necessity for effective communication regarding the capabilities and limitations of artificial intelligence. Researchers are actively developing techniques to enhance interpretability, employing visualization methods and reducing model complexity. However, the persistent challenge lies in finding the delicate balance between achieving high performance and maintaining interpretability. Acknowledging the growing significance of artificial intelligence in aiding medical diagnosis and therapy, and the creation of interpretable artificial intelligence models is considered essential. In this dynamic context, an unwavering commitment to transparency, ethical considerations, and interdisciplinary collaboration is imperative to ensure the responsible use of artificial intelligence. This collective commitment is vital for establishing enduring trust between clinicians and patients, addressing emerging challenges, and facilitating the informed adoption of these advanced technologies in medicine.},
  langid = {english},
  annotation = {TLDR: This review comprehensively analyzes challenges and solutions presented in the literature, offering an overview of the most recent techniques utilized in this field and provides precise definitions of interpretability and explainability, aiming to clarify the distinctions between these concepts and their implications for the decision-making process.},
  timestamp = {2025-08-13T11:08:37Z}
}

@article{frege1892,
  title = {{\"U}ber Sinn Und Bedeutung},
  author = {Frege, Gottlob},
  year = {1892},
  journal = {Zeitschrift f{\"u}r Philosophie und philosophische Kritik},
  volume = {100},
  pages = {25--50},
  timestamp = {2025-09-06T00:42:24Z}
}

@article{friedman2001greedy,
  title = {Greedy Function Approximation: A Gradient Boosting Machine},
  author = {Friedman, Jerome H},
  year = {2001},
  journal = {Annals of statistics},
  pages = {1189--1232},
  publisher = {JSTOR},
  timestamp = {2025-03-18T01:51:54Z}
}

@article{froling2024artificial,
  title = {Artificial {{Intelligence}} in {{Medical Affairs}}: {{A New Paradigm}} with {{Novel Opportunities}}},
  shorttitle = {Artificial {{Intelligence}} in {{Medical Affairs}}},
  author = {Fr{\"o}ling, Emma and Rajaeean, Neda and Hinrichsmeyer, Klara Sonnie and {Domr{\"o}s-Zoungrana}, Dina and Urban, Johannes Nico and Lenz, Christian},
  year = {2024},
  month = sep,
  journal = {Pharmaceutical Medicine},
  volume = {38},
  number = {5},
  pages = {331--342},
  issn = {1179-1993},
  doi = {10.1007/s40290-024-00536-9},
  abstract = {The advent of artificial intelligence (AI) revolutionizes the ways of working in many areas of business and life science. In Medical Affairs (MA) departments of the pharmaceutical industry AI holds great potential for positively influencing the medical mission of identifying and addressing unmet medical needs and care gaps, and fostering solutions that improve the egalitarian and unbiased access of patients to treatments worldwide. Given the essential position of MA in corporate interactions with various healthcare stakeholders, AI offers broad possibilities to support strategic decision-making and to pioneer novel approaches in medical stakeholder interactions. By analyzing data derived from the healthcare environment and by streamlining operations in medical content generation, AI advances data-based prioritization and strategy execution. In this review, we discuss promising AI-based solutions in MA that support the effective use of heterogenous information from observations of the healthcare environment, the enhancement of medical education, and the analysis of real-world data. For a successful implementation of such solutions, specific considerations partly unique to healthcare must be taken care of, for example, transparency, data privacy, healthcare regulations, and in predictive applications, explainability.},
  langid = {english},
  pmcid = {PMC11473552},
  pmid = {39259426},
  keywords = {Artificial Intelligence,Delivery of Health Care,Drug Industry,Humans},
  annotation = {TLDR: Promising AI-based solutions in Medical Affairs that support the effective use of heterogenous information from observations of the healthcare environment, the enhancement of medical education, and the analysis of real-world data are discussed.},
  timestamp = {2025-04-16T03:01:40Z}
}

@misc{frontiers,
  title = {Frontiers {\textbar} {{Layer-Wise Relevance Propagation}} for {{Explaining Deep Neural Network Decisions}} in {{MRI-Based Alzheimer}}'s {{Disease Classification}}},
  urldate = {2025-05-17},
  howpublished = {https://www.frontiersin.org/journals/aging-neuroscience/articles/10.3389/fnagi.2019.00194/full},
  timestamp = {2025-05-17T10:52:23Z}
}

@article{fuji2019explainable,
  title = {Explainable {{AI}} through Combination of Deep Tensor and Knowledge Graph},
  author = {Fuji, Masaru and Morita, Hajime and Goto, Keisuke and Maruhashi, Koji and Anai, Hirokazu and Igata, Nobuyuki},
  year = {2019},
  journal = {Fujitsu Sci. Tech. J},
  volume = {55},
  number = {2},
  pages = {58--64},
  timestamp = {2025-03-19T06:56:41Z}
}

@inproceedings{gade2019explainable,
  title = {Explainable {{AI}} in {{Industry}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Gade, Krishna and Geyik, Sahin Cem and Kenthapadi, Krishnaram and Mithal, Varun and Taly, Ankur},
  year = {2019},
  month = jul,
  pages = {3203--3204},
  publisher = {ACM},
  address = {Anchorage AK USA},
  doi = {10.1145/3292500.3332281},
  urldate = {2025-03-15},
  isbn = {978-1-4503-6201-6},
  langid = {english},
  timestamp = {2025-03-15T10:57:12Z}
}

@article{gambetti2025survey,
  title = {A {{Survey}} on {{Human-Centered Evaluation}} of {{Explainable AI Methods}} in {{Clinical Decision Support Systemss}}},
  author = {Gambetti, Alessandro and Han, Qiwei and Shen, Hong and Soares, Claudia},
  year = {2025},
  journal = {arXiv preprint arXiv:2502.09849},
  eprint = {2502.09849},
  archiveprefix = {arXiv},
  timestamp = {2025-03-04T03:02:58Z}
}

@article{gao2023clinical,
  title = {Clinical Knowledge Embedded Method Based on Multi-Task Learning for Thyroid Nodule Classification with Ultrasound Images},
  author = {Gao, Zixiong and Chen, Yufan and Sun, Pengtao and Liu, Hongmei and Lu, Yao},
  year = {2023},
  month = feb,
  journal = {Physics in Medicine \& Biology},
  volume = {68},
  number = {4},
  pages = {045018},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/1361-6560/acb481},
  urldate = {2025-04-06},
  abstract = {Abstract                            Objective               . Thyroid nodules are common glandular abnormality that need to be diagnosed as benign or malignant to determine further treatments. Clinically, ultrasonography is the main diagnostic method, but it is highly subjective with severe variability. Recently, many deep-learning-based methods have been proposed to alleviate subjectivity and achieve good results yet, these methods often neglect important guidance from clinical knowledge. Our objective is to utilize such guidance for accurate and reliable thyroid nodule classification.               Approach               . In this study, a multi-task learning model embedded with clinical knowledge of ACR Thyroid Imaging, Reporting and Data System guideline is proposed. The clinical features defined in the guideline have strong correlations with malignancy and they were modeled as tasks alongside the pathological type. Multi-task learning was utilized to exploit the correlations to improve diagnostic performance. To alleviate the impact of noisy labels on clinical features, a loss-weighting strategy was proposed. Five-fold cross-validation was applied to an internal training set of size 4989, and an external test set of size 243 was used for evaluation.               Main results               . The proposed multi-task learning model achieved an average AUC of 0.901 and an ensemble AUC of 0.917 on the test set, which significantly outperformed the single-task baseline models.               Significance               . The results indicated that multi-task learning of clinical features can effectively classify thyroid nodules and reveal the possibility of using clinical indicators as auxiliary tasks to improve performance when diagnosing other diseases.},
  annotation = {TLDR: The results indicated that multi-task learning of clinical features can effectively classify thyroid nodules and reveal the possibility of using clinical indicators as auxiliary tasks to improve performance when diagnosing other diseases.},
  timestamp = {2025-04-06T09:36:42Z}
}

@article{gao2023leveraging,
  title = {Leveraging {{Medical Knowledge Graphs Into Large Language Models}} for {{Diagnosis Prediction}}: {{Design}} and {{Application Study}}},
  shorttitle = {Leveraging {{Medical Knowledge Graphs Into Large Language Models}} for {{Diagnosis Prediction}}},
  author = {Gao, Yanjun and Li, Ruizhe and Croxford, Emma and Caskey, John and Patterson, Brian W and Churpek, Matthew and Miller, Timothy and Dligach, Dmitriy and Afshar, Majid},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2308.14321},
  urldate = {2025-05-03},
  abstract = {Electronic Health Records (EHRs) and routine documentation practices play a vital role in patients' daily care, providing a holistic record of health, diagnoses, and treatment. However, complex and verbose EHR narratives overload healthcare providers, risking diagnostic inaccuracies. While Large Language Models (LLMs) have showcased their potential in diverse language tasks, their application in the healthcare arena needs to ensure the minimization of diagnostic errors and the prevention of patient harm. In this paper, we outline an innovative approach for augmenting the proficiency of LLMs in the realm of automated diagnosis generation, achieved through the incorporation of a medical knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the clinical diagnostic reasoning process. We derive the KG from the National Library of Medicine's Unified Medical Language System (UMLS), a robust repository of biomedical knowledge. Our method negates the need for pre-training and instead leverages the KG as an auxiliary instrument aiding in the interpretation and summarization of complex medical concepts. Using real-world hospital datasets, our experimental results demonstrate that the proposed approach of combining LLMs with KG has the potential to improve the accuracy of automated diagnosis generation. More importantly, our approach offers an explainable diagnostic pathway, edging us closer to the realization of AI-augmented diagnostic decision support systems.},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  annotation = {TLDR: This paper outlines an innovative approach for augmenting the proficiency of LLMs in the realm of automated diagnosis generation, achieved through the incorporation of a medical knowledge graph (KG) and a novel graph model: D R .K NOWS , inspired by the clinical diagnostic reasoning process.},
  timestamp = {2025-05-03T13:39:53Z}
}

@misc{gao2024causal,
  title = {Causal Disentanglement for Single-Cell Representations and Controllable Counterfactual Generation},
  author = {Gao, Yicheng and Dong, Kejing and Shan, Caihua and Li, Dongsheng and Liu, Qi},
  year = {2024},
  month = dec,
  publisher = {Bioinformatics},
  doi = {10.1101/2024.12.11.628077},
  urldate = {2025-04-18},
  abstract = {Abstract           Conducting disentanglement learning on single-cell omics data offers a promising alternative to traditional black-box representation learning by separating the semantic concepts embedded in a biological process. We present CausCell, which incorporates the causal relationships among disentangled concepts within a diffusion model to perform disentanglement learning, with the aim of increasing the explainability, generalizability and controllability of single-cell data, including spatial and temporal omics data, relative to those of the existing black-box representation learning models. Two quantitative evaluation scenarios, i.e., disentanglement and reconstruction, are presented to conduct the first comprehensive single-cell disentanglement learning benchmark, which demonstrates that CausCell outperforms the state-of-the-art methods in both scenarios. Additionally, CausCell can implement controllable generation by intervening with the concepts of single-cell data when given a causal structure. It also has the potential to uncover biological insights by generating counterfactuals from small and noisy single-cell datasets.},
  archiveprefix = {Bioinformatics},
  copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  annotation = {TLDR: CausCell, which incorporates the causal relationships among disentangled concepts within a diffusion model to perform disentanglement learning, is presented with the aim of increasing the explainability, generalizability and controllability of single-cell data, including spatial and temporal omics data, relative to those of the existing black-box representation learning models.},
  timestamp = {2025-04-18T03:32:41Z}
}

@article{gao2025leveraging,
  title = {Leveraging {{Medical Knowledge Graphs Into Large Language Models}} for {{Diagnosis Prediction}}: {{Design}} and {{Application Study}}},
  shorttitle = {Leveraging {{Medical Knowledge Graphs Into Large Language Models}} for {{Diagnosis Prediction}}},
  author = {Gao, Yanjun and Li, Ruizhe and Croxford, Emma and Caskey, John and Patterson, Brian W. and Churpek, Matthew and Miller, Timothy and Dligach, Dmitriy and Afshar, Majid},
  year = {2025},
  month = feb,
  journal = {JMIR AI},
  volume = {4},
  eprint = {2308.14321},
  primaryclass = {cs},
  pages = {e58670},
  issn = {2817-1705},
  doi = {10.2196/58670},
  urldate = {2025-05-03},
  abstract = {Electronic Health Records (EHRs) and routine documentation practices play a vital role in patients' daily care, providing a holistic record of health, diagnoses, and treatment. However, complex and verbose EHR narratives overload healthcare providers, risking diagnostic inaccuracies. While Large Language Models (LLMs) have showcased their potential in diverse language tasks, their application in the healthcare arena needs to ensure the minimization of diagnostic errors and the prevention of patient harm. In this paper, we outline an innovative approach for augmenting the proficiency of LLMs in the realm of automated diagnosis generation, achieved through the incorporation of a medical knowledge graph (KG) and a novel graph model: Dr.Knows, inspired by the clinical diagnostic reasoning process. We derive the KG from the National Library of Medicine's Unified Medical Language System (UMLS), a robust repository of biomedical knowledge. Our method negates the need for pre-training and instead leverages the KG as an auxiliary instrument aiding in the interpretation and summarization of complex medical concepts. Using real-world hospital datasets, our experimental results demonstrate that the proposed approach of combining LLMs with KG has the potential to improve the accuracy of automated diagnosis generation. More importantly, our approach offers an explainable diagnostic pathway, edging us closer to the realization of AI-augmented diagnostic decision support systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {TLDR: Although further work is required to address KG biases and extend generalizability, DR.KNOWS represents progress toward trustworthy artificial intelligence--driven clinical decision support, with a human evaluation framework focused on diagnostic safety and alignment with clinical standards.},
  timestamp = {2025-05-03T13:40:27Z}
}

@article{gao2025revolutionary,
  title = {A Revolutionary Neural Network Architecture with Interpretability and Flexibility Based on {{Kolmogorov}}--{{Arnold}} for Solar Radiation and Temperature Forecasting},
  author = {Gao, Yuan and Hu, Zehuan and Chen, Wei-An and Liu, Mingzhe and Ruan, Yingjun},
  year = {2025},
  month = jan,
  journal = {Applied Energy},
  volume = {378},
  pages = {124844},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2024.124844},
  urldate = {2025-04-20},
  abstract = {Deep learning models are increasingly being used to predict renewable energy-related variables, such as solar radiation and outdoor temperature. However, the black-box nature of these models results in a lack of interpretability in their predictions, and the design of deep network architectures significantly impacts the final prediction outcomes. The introduction of Kolmogorov--Arnold Network (KAN) provides an excellent solution to both of these issues. We hope that the KAN mechanism can provide fully interpretable neural network models, enhancing the potential for practical deployment. At the same time, KAN is capable of achieving good prediction results across various network architectures and neuron counts. We conducted case studies using real-world data from the Tokyo Meteorological Observatory to predict solar radiation and outdoor temperature, comparing the results with those of commonly used recurrent neural network baseline models. The results indicate that KAN can maintain model performance regardless of the chosen number of neurons. For instance, in the solar radiation prediction task, the KAN with a single hidden neuron reduces the MSE error by 75.33\% compared to the baseline model. More importantly, KAN allows for the quantification of each step in the network's computations, thereby enhancing overall interpretability.},
  keywords = {Interpretable neural network,Kolmogorov-Arnold network,Solar radiation,Time series forecasting},
  timestamp = {2025-04-20T14:13:35Z}
}

@article{garcia-barragan2025nssc,
  title = {{{NSSC}}: A Neuro-Symbolic {{AI}} System for Enhancing Accuracy of Named Entity Recognition and Linking from Oncologic Clinical Notes},
  shorttitle = {{{NSSC}}},
  author = {{Garc{\'i}a-Barrag{\'a}n}, {\'A}lvaro and Sakor, Ahmad and Vidal, Maria-Esther and Menasalvas, Ernestina and Gonzalez, Juan Cristobal Sanchez and Provencio, Mariano and Robles, V{\'i}ctor},
  year = {2025},
  month = mar,
  journal = {Medical \& Biological Engineering \& Computing},
  volume = {63},
  number = {3},
  pages = {749--772},
  issn = {1741-0444},
  doi = {10.1007/s11517-024-03227-4},
  urldate = {2025-04-04},
  abstract = {Accurate recognition and linking of oncologic entities in clinical notes is essential for extracting insights across cancer research, patient care, clinical decision-making, and treatment optimization. We present the Neuro-Symbolic System for Cancer (NSSC), a hybrid AI framework that integrates neurosymbolic methods with named entity recognition (NER) and entity linking (EL) to transform unstructured clinical notes into structured terms using medical vocabularies, with the Unified Medical Language System (UMLS) as a case study. NSSC was evaluated on a dataset of clinical notes from breast cancer patients, demonstrating significant improvements in the accuracy of both entity recognition and linking compared to state-of-the-art models. Specifically, NSSC achieved a 33\% improvement over BioFalcon and a 58\% improvement over scispaCy. By combining large language models (LLMs) with symbolic reasoning, NSSC improves the recognition and interoperability of oncologic entities, enabling seamless integration with existing biomedical knowledge. This approach marks a significant advancement in extracting meaningful information from clinical narratives, offering promising applications in cancer research and personalized patient care.},
  langid = {english},
  keywords = {Breast cancer,EHR,EL,LLM,NER,Neuro-symbolic},
  annotation = {TLDR: The Neuro-Symbolic System for Cancer (NSSC), a hybrid AI framework that integrates neurosymbolic methods with named entity recognition (NER) and entity linking (EL) to transform unstructured clinical notes into structured terms using medical vocabularies, with the Unified Medical Language System (UMLS) as a case study.},
  timestamp = {2025-04-04T00:58:23Z}
}

@book{gardenfors2000,
  title = {Conceptual Spaces: {{The}} Geometry of Thought},
  author = {G{\"a}rdenfors, Peter},
  year = {2000},
  publisher = {MIT Press},
  timestamp = {2025-09-06T00:42:24Z}
}

@book{gardenfors2004conceptual,
  title = {Conceptual Spaces: {{The}} Geometry of Thought},
  author = {Gardenfors, Peter},
  year = {2004},
  publisher = {MIT press},
  timestamp = {2025-09-01T11:47:41Z}
}

@book{gardenfors2004conceptual,
  title = {Conceptual Spaces: {{The}} Geometry of Thought},
  author = {Gardenfors, Peter},
  year = {2004},
  publisher = {MIT press},
  timestamp = {2025-09-05T16:44:22Z}
}

@inproceedings{garg2023navigating,
  title = {Navigating {{Healthcare Insights}}: {{A Bird}}'s {{Eye View}} of {{Explainability}} with {{Knowledge Graphs}}},
  shorttitle = {Navigating {{Healthcare Insights}}},
  booktitle = {2023 {{IEEE Sixth International Conference}} on {{Artificial Intelligence}} and {{Knowledge Engineering}} ({{AIKE}})},
  author = {Garg, Satvik and Parikh, Shivam and Garg, Somya},
  year = {2023},
  month = sep,
  pages = {54--61},
  publisher = {IEEE},
  address = {Laguna Hills, CA, USA},
  doi = {10.1109/AIKE59827.2023.00016},
  urldate = {2025-06-12},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-3128-8},
  annotation = {TLDR: This overview summarizes recent literature on the impact of KGs in healthcare and their role in developing explainable AI models and covers KG workflow, including construction, relationship extraction, reasoning, and their applications in areas like Drug-Drug Interactions, Drug Target Interactions, Drug Development, Adverse Drug Reactions, and bioinformatics.},
  timestamp = {2025-06-12T13:26:52Z}
}

@article{garg2025explainable,
  title = {Explainable {{AI}} \& Model Interpretability in Healthcare: {{Challenges}} \& Future Directions},
  author = {Garg, Puneet},
  year = {2025},
  journal = {EKSPLORIUM-BULETIN PUSAT TEKNOLOGI BAHAN GALIAN NUKLIR},
  volume = {46},
  number = {1},
  pages = {104--133},
  timestamp = {2025-07-05T05:04:16Z}
}

@article{garvert2023hippocampal,
  title = {Hippocampal Spatio-Predictive Cognitive Maps Adaptively Guide Reward Generalization},
  author = {Garvert, Mona M and Saanum, Tankred and Schulz, Eric and Schuck, Nicolas W and Doeller, Christian F},
  year = {2023},
  journal = {Nature Neuroscience},
  volume = {26},
  number = {4},
  pages = {615--626},
  publisher = {Nature Publishing Group US New York},
  timestamp = {2025-03-23T03:25:49Z}
}

@misc{geiger2024causal,
  title = {Causal {{Abstraction}}: {{A Theoretical Foundation}} for {{Mechanistic Interpretability}}},
  shorttitle = {Causal {{Abstraction}}},
  author = {Geiger, Atticus and Ibeling, Duligur and Zur, Amir and Chaudhary, Maheep and Chauhan, Sonakshi and Huang, Jing and Arora, Aryaman and Wu, Zhengxuan and Goodman, Noah and Potts, Christopher and Icard, Thomas},
  year = {2024},
  month = aug,
  number = {arXiv:2301.04709},
  eprint = {2301.04709},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.04709},
  urldate = {2025-04-17},
  abstract = {Causal abstraction provides a theoretical foundation for mechanistic interpretability, the field concerned with providing intelligible algorithms that are faithful simplifications of the known, but opaque low-level details of black box AI models. Our contributions are (1) generalizing the theory of causal abstraction from mechanism replacement (i.e., hard and soft interventions) to arbitrary mechanism transformation (i.e., functionals from old mechanisms to new mechanisms), (2) providing a flexible, yet precise formalization for the core concepts of modular features, polysemantic neurons, and graded faithfulness, and (3) unifying a variety of mechanistic interpretability methodologies in the common language of causal abstraction, namely activation and path patching, causal mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept erasure, sparse autoencoders, differential binary masking, distributed alignment search, and activation steering.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {,Computer Science - Artificial Intelligence},
  annotation = {remark: phd1},
  timestamp = {2025-04-17T08:29:37Z}
}

@article{gerdes2024role,
  title = {The Role of Explainability in {{AI-supported}} Medical Decision-Making},
  author = {Gerdes, Anne},
  year = {2024},
  month = apr,
  journal = {Discover Artificial Intelligence},
  volume = {4},
  number = {1},
  pages = {29},
  issn = {2731-0809},
  doi = {10.1007/s44163-024-00119-2},
  urldate = {2025-04-01},
  abstract = {This article positions explainability as an enabler of ethically justified medical decision-making by emphasizing the combination of pragmatically useful explanations and comprehensive validation of AI decision-support systems in real-life clinical settings. In this setting, post hoc medical explainability is defined as practical yet non-exhaustive explanations that facilitate shared decision-making between a physician and a patient in a specific clinical context. However, giving precedence to an explanation-centric approach over a validation-centric one in the domain of AI decision-support systems, it is still pivotal to recognize the inherent tension between the eagerness to deploy AI in healthcare and the necessity for thorough, time-consuming external and prospective validation of AI. Consequently, in clinical decision-making, integrating a retrospectively analyzed and prospectively validated AI system, along with post hoc explanations, can facilitate the explanatory needs of physicians and patients in the context of medical decision-making supported by AI.},
  langid = {english},
  keywords = {Artificial Intelligence,Medical Ethics},
  annotation = {TLDR: In clinical decision-making, integrating a retrospectively analyzed and prospectively validated AI system, along with post hoc explanations, can facilitate the explanatory needs of physicians and patients in the context of medical decision-making supported by AI.},
  timestamp = {2025-04-01T14:43:57Z}
}

@article{ghasemi2024explainable,
  title = {Explainable Artificial Intelligence in Breast Cancer Detection and Risk Prediction: {{A}} Systematic Scoping Review},
  shorttitle = {Explainable Artificial Intelligence in Breast Cancer Detection and Risk Prediction},
  author = {Ghasemi, Amirehsan and Hashtarkhani, Soheil and Schwartz, David L. and Shaban-Nejad, Arash},
  year = {2024},
  month = oct,
  journal = {Cancer Innovation},
  volume = {3},
  number = {5},
  pages = {e136},
  publisher = {Wiley},
  issn = {2770-9191, 2770-9183},
  doi = {10.1002/cai2.136},
  urldate = {2025-07-24},
  abstract = {AbstractWith the advances in artificial intelligence (AI), data-driven algorithms are becoming increasingly popular in the medical domain. However, due to the nonlinear and complex behavior of many of these algorithms, decision-making by such algorithms is not trustworthy for clinicians and is considered a black-box process. Hence, the scientific community has introduced explainable artificial intelligence (XAI) to remedy the problem. This systematic scoping review investigates the application of XAI in breast cancer detection and risk prediction. We conducted a comprehensive search on Scopus, IEEE Explore, PubMed, and Google Scholar (first 50 citations) using a systematic search strategy. The search spanned from January 2017 to July 2023, focusing on peer-reviewed studies implementing XAI methods in breast cancer datasets. Thirty studies met our inclusion criteria and were included in the analysis. The results revealed that SHapley Additive exPlanations (SHAP) is the top model-agnostic XAI technique in breast cancer research in terms of usage, explaining the model prediction results, diagnosis and classification of biomarkers, and prognosis and survival analysis. Additionally, the SHAP model primarily explained tree-based ensemble machine learning models. The most common reason is that SHAP is model agnostic, which makes it both popular and useful for explaining any model prediction. Additionally, it is relatively easy to implement effectively and completely suits performant models, such as tree-based models. Explainable AI improves the transparency, interpretability, fairness, and trustworthiness of AI-enabled health systems and medical devices and, ultimately, the quality of care and outcomes.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: It is revealed that SHapley Additive exPlanations (SHAP) is the top model-agnostic XAI technique in breast cancer research in terms of usage, explaining the model prediction results, diagnosis and classification of biomarkers, and prognosis and survival analysis.},
  timestamp = {2025-07-24T12:39:22Z}
}

@article{gimeno2022explainable,
  ids = {gimeno2022explainablea},
  title = {Explainable Artificial Intelligence for Precision Medicine in Acute Myeloid Leukemia},
  author = {Gimeno, Marian and {San Jos{\'e}-En{\'e}riz}, Edurne and Villar, Sara and Agirre, Xabier and Prosper, Felipe and Rubio, Angel and Carazo, Fernando},
  year = {2022},
  journal = {Frontiers in Immunology},
  volume = {13},
  pages = {977358},
  issn = {1664-3224},
  doi = {10.3389/fimmu.2022.977358},
  abstract = {Artificial intelligence (AI) can unveil novel personalized treatments based on drug screening and whole-exome sequencing experiments (WES). However, the concept of "black box" in AI limits the potential of this approach to be translated into the clinical practice. In contrast, explainable AI (XAI) focuses on making AI results understandable to humans. Here, we present a novel XAI method -called multi-dimensional module optimization (MOM)- that associates drug screening with genetic events, while guaranteeing that predictions are interpretable and robust. We applied MOM to an acute myeloid leukemia (AML) cohort of 319 ex-vivo tumor samples with 122 screened drugs and WES. MOM returned a therapeutic strategy based on the FLT3, CBF{$\beta$}-MYH11, and NRAS status, which predicted AML patient response to Quizartinib, Trametinib, Selumetinib, and Crizotinib. We successfully validated the results in three different large-scale screening experiments. We believe that XAI will help healthcare providers and drug regulators better understand AI medical decisions.},
  langid = {english},
  pmcid = {PMC9556772},
  pmid = {36248800},
  keywords = {Artificial Intelligence,assignation problem,biomarkers,Crizotinib,drug repositioning,drug sensitivity,ex-vivo experiment,explainable artificial intelligence,Humans,large-scale screening,Leukemia Myeloid Acute,Precision Medicine,treatment selection},
  annotation = {TLDR: A novel XAI method is presented -called multi-dimensional module optimization (MOM)- that associates drug screening with genetic events, while guaranteeing that predictions are interpretable and robust, and will help healthcare providers and drug regulators better understand AI medical decisions.},
  timestamp = {2025-08-08T07:57:01Z}
}

@article{glocker2019machine,
  title = {Machine Learning with Multi-Site Imaging Data: {{An}} Empirical Study on the Impact of Scanner Effects},
  author = {Glocker, Ben and Robinson, Robert and Castro, Daniel C and Dou, Qi and Konukoglu, Ender},
  year = {2019},
  journal = {arXiv preprint arXiv:1910.04597},
  eprint = {1910.04597},
  archiveprefix = {arXiv},
  timestamp = {2025-05-04T15:29:17Z}
}

@article{goh2021multimodal,
  title = {Multimodal {{Neurons}} in {{Artificial Neural Networks}}},
  author = {Goh, Gabriel and Cammarata, Nick and Voss, Chelsea and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  year = {2021},
  month = mar,
  journal = {Distill},
  volume = {6},
  number = {3},
  pages = {10.23915/distill.00030},
  issn = {2476-0757},
  doi = {10.23915/distill.00030},
  urldate = {2025-09-02},
  timestamp = {2025-09-02T02:40:28Z}
}

@techreport{goldsack2024,
  type = {Technical Report},
  title = {Billions of Dollars Have Been Invested in Healthcare Ai. but Are We Spending in the Right Places?},
  author = {Goldsack, Jennifer and Overgaard, Shauna},
  year = {2024},
  institution = {World Economic Forum},
  timestamp = {2025-03-19T07:10:23Z}
}

@misc{goldstein2014peeking,
  ids = {goldstein2014peekinga},
  title = {Peeking {{Inside}} the {{Black Box}}: {{Visualizing Statistical Learning}} with {{Plots}} of {{Individual Conditional Expectation}}},
  shorttitle = {Peeking {{Inside}} the {{Black Box}}},
  author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  year = {2014},
  month = mar,
  number = {arXiv:1309.6392},
  eprint = {1309.6392},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1309.6392},
  urldate = {2025-03-18},
  abstract = {This article presents Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the partial dependence plot by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data generating model. Through simulated examples and real data sets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  timestamp = {2025-04-06T14:32:23Z}
}

@article{gomez2025causal,
  title = {Causal Inference for Time Series Datasets with Partially Overlapping Variables},
  author = {Gomez, Louis Adedapo and Claassen, Jan and Kleinberg, Samantha},
  year = {2025},
  month = apr,
  journal = {Journal of Biomedical Informatics},
  volume = {166},
  pages = {104828},
  issn = {1532-0480},
  doi = {10.1016/j.jbi.2025.104828},
  abstract = {OBJECTIVE: Healthcare data provides a unique opportunity to learn causal relationships but the largest datasets, such as from hospitals or intensive care units, are often observational and do not standardize variables collected for all patients. Rather, the variables depend on a patient's health status, treatment plan, and differences between providers. This poses major challenges for causal inference, which either must restrict analysis to patients with complete data (reducing power) or learn patient-specific models (making it difficult to generalize). While missing variables can lead to confounding, variables absent for one individual are often measured in another. METHODS: We propose a novel method, called Causal Model Combination for Time Series (CMC-TS), to learn causal relationships from time series with partially overlapping variable sets. CMC-TS overcomes errors by specifically leveraging partial overlap between datasets (e.g., patients) to iteratively reconstruct missing variables and correct errors by reweighting inferences using shared information across datasets. We evaluated CMC-TS and compared it to the state of the art on both simulated data and real-world data from stroke patients admitted to a neurological intensive care unit. RESULTS: On simulated data, CMC-TS had the fewest false discoveries and highest F1-score compared to baselines. On real data from stroke patients in a neurological intensive care unit, we found fewer implausible and more highly ranked plausible causes of a clinically important adverse event. CONCLUSION: Our approach may lead to better use of observational healthcare data for causal inference, by enabling causal inference from patient data with partially overlapping variable sets.},
  langid = {english},
  pmid = {40274036},
  keywords = {Causal inference,Health informatics,Overlapping datasets,Time series data},
  annotation = {TLDR: A novel method, called Causal Model Combination for Time Series (CMC-TS), to learn causal relationships from time series with partially overlapping variable sets, which may lead to better use of observational healthcare data for causal inference, by enabling causal inference from patient data with partially overlapping variable sets.},
  timestamp = {2025-05-04T09:24:56Z}
}

@article{gou2021knowledge,
  title = {Knowledge Distillation: {{A}} Survey},
  author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  year = {2021},
  journal = {International Journal of Computer Vision},
  volume = {129},
  number = {6},
  pages = {1789--1819},
  publisher = {Springer},
  timestamp = {2025-04-16T02:08:56Z}
}

@misc{goyal2020explaining,
  title = {Explaining {{Classifiers}} with {{Causal Concept Effect}} ({{CaCE}})},
  author = {Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been},
  year = {2020},
  month = feb,
  number = {arXiv:1907.07165},
  eprint = {1907.07165},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.07165},
  urldate = {2025-05-18},
  abstract = {How can we understand classification decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we define the Causal Concept Effect (CaCE) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net's predictions. We show that the CaCE measure can avoid errors stemming from confounding. Estimating CaCE is difficult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, specifically a Variational AutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis, we show that the VAE-CaCE is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-05-18T12:16:52Z}
}

@article{grimes2002bias,
  title = {Bias and Causal Associations in Observational Research},
  author = {Grimes, David A and Schulz, Kenneth F},
  year = {2002},
  journal = {The lancet},
  volume = {359},
  number = {9302},
  pages = {248--252},
  publisher = {Elsevier},
  timestamp = {2025-05-13T12:05:27Z}
}

@article{groheux2025fdgpet,
  title = {{{FDG-PET}}/{{CT}} and {{Multimodal Machine Learning Model Prediction}} of {{Pathological Complete Response}} to {{Neoadjuvant Chemotherapy}} in {{Triple-Negative Breast Cancer}}},
  author = {Groheux, David and Ferrer, Lo{\"i}c and Vargas, Jennifer and Martineau, Antoine and Borgel, Adrien and Teixeira, Luis and Menu, Philippe and Bertheau, Philippe and Gallinato, Olivier and Colin, Thierry and {Lehmann-Che}, Jacqueline},
  year = {2025},
  month = apr,
  journal = {Cancers},
  volume = {17},
  number = {7},
  pages = {1249},
  issn = {2072-6694},
  doi = {10.3390/cancers17071249},
  urldate = {2025-05-03},
  abstract = {Triple-negative breast cancer is a heterogeneous disease associated with poor outcomes. Often treated with neoadjuvant chemotherapy, achieving pCR at the end of treatment is a goal and predicts patient survival. But predicting pCR from the outset could be of clinical interest and avoid the administration of ineffective treatments. Here we present a proof-of-concept study of a multimodal machine learning algorithm incorporating PET (including radiomics), histopathological, genomic, and clinical features. The algorithm developed can predict pCR in triple negative breast cancers with an AUC of 0,82 and show a tendency to correlate with long-term outcomes. This early prediction of response to chemotherapy could lead to propose more personalized treatment., Purpose: Triple-negative breast cancer (TNBC) is a biologically and clinically heterogeneous disease, associated with poorer outcomes when compared with other subtypes of breast cancer. Neoadjuvant chemotherapy (NAC) is often given before surgery, and achieving a pathological complete response (pCR) has been associated with patient outcomes. There is thus strong clinical interest in the ability to accurately predict pCR status using baseline data. Materials and Methods: A cohort of 57 TNBC patients who underwent FDG-PET/CT before NAC was analyzed to develop a machine learning (ML) algorithm predictive of pCR. A total of 241 predictors were collected for each patient: 11 clinical features, 11 histopathological features, 13 genomic features, and 206 PET features, including 195 radiomic features. The optimization criterion was the area under the ROC curve (AUC). Event-free survival (EFS) was estimated using the Kaplan--Meier method. Results: The best ML algorithm reached an AUC of 0.82. The features with the highest weight in the algorithm were a mix of PET (including radiomics), histopathological, genomic, and clinical features, highlighting the importance of truly multimodal analysis. Patients with predicted pCR tended to have a longer EFS than patients with predicted non-pCR, even though this difference was not significant, probably due to the small sample size and few events observed (p = 0.09). Conclusions: This study suggests that ML applied to baseline multimodal data can help predict pCR status after NAC for TNBC patients and may identify correlations with long-term outcomes. Patients predicted as non-pCR may benefit from concomitant treatment with immunotherapy or dose intensification.},
  pmcid = {PMC11987901},
  pmid = {40227836},
  annotation = {TLDR: The algorithm developed can predict pCR in triple negative breast cancers with an AUC of 0,82 and show a tendency to correlate with long-term outcomes and could lead to propose more personalized treatment.},
  timestamp = {2025-05-03T09:20:19Z}
}

@incollection{grover2024challenges,
  title = {Challenges and Limitations of Explainable {{AI}} in Healthcare},
  booktitle = {Analyzing Explainable {{AI}} in Healthcare and the Pharmaceutical Industry},
  author = {Grover, Veena and Dogra, Mahima},
  year = {2024},
  pages = {72--85},
  publisher = {IGI Global},
  timestamp = {2025-04-16T02:30:49Z}
}

@misc{gu2024causkelnet,
  title = {{{CauSkelNet}}: {{Causal Representation Learning}} for {{Human Behaviour Analysis}}},
  shorttitle = {{{CauSkelNet}}},
  author = {Gu, Xingrui and Jiang, Chuyi and Wang, Erte and Wu, Zekun and Cui, Qiang and Tian, Leimin and Wu, Lianlong and Song, Siyang and Yu, Chuang},
  year = {2024},
  month = sep,
  number = {arXiv:2409.15564},
  eprint = {2409.15564},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.15564},
  urldate = {2024-12-03},
  abstract = {Constrained by the lack of model interpretability and a deep understanding of human movement in traditional movement recognition machine learning methods, this study introduces a novel representation learning method based on causal inference to better understand human joint dynamics and complex behaviors. We propose a two-stage framework that combines the Peter-Clark (PC) algorithm and Kullback-Leibler (KL) divergence to identify and quantify causal relationships between joints. Our method effectively captures interactions and produces interpretable, robust representations. Experiments on the EmoPain dataset show that our causal GCN outperforms traditional GCNs in accuracy, F1 score, and recall, especially in detecting protective behaviors. The model is also highly invariant to data scale changes, enhancing its reliability in practical applications. Our approach advances human motion analysis and paves the way for more adaptive intelligent healthcare solutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {TLDR: A two-stage framework that combines the Peter-Clark (PC) algorithm and Kullback-Leibler (KL) divergence to identify and quantify causal relationships between joints is proposed, which effectively captures interactions and produces interpretable, robust representations.},
  timestamp = {2024-12-28T08:11:25Z}
}

@article{guidance2021ethics,
  title = {Ethics and Governance of Artificial Intelligence for Health},
  author = {Guidance, {\relax WHO}},
  year = {2021},
  month = jun,
  journal = {World Health Organization},
  timestamp = {2025-03-14T01:07:58Z}
}

@misc{guidelines,
  title = {Guidelines for {{Human-AI Interaction}} {\textbar} {{Proceedings}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  journal = {ACM Conferences},
  doi = {10.1145/3290605.3300233},
  urldate = {2025-03-28},
  langid = {english},
  timestamp = {2025-03-28T08:10:17Z}
}

@article{gunning2019darpas,
  title = {{{DARPA}}'s {{Explainable Artificial Intelligence Program}}},
  author = {Gunning, David and Aha, David W.},
  year = {2019},
  month = jun,
  journal = {AI Magazine},
  volume = {40},
  number = {2},
  pages = {44--58},
  issn = {0738-4602, 2371-9621},
  doi = {10.1609/aimag.v40i2.2850},
  urldate = {2025-05-28},
  abstract = {Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA's explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems' explanations improve user understanding, user trust, and user task performance.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  annotation = {TLDR: In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems' explanations improve user understanding, user trust, and user task performance.},
  timestamp = {2025-05-28T11:14:45Z}
}

@article{gunning2019xai,
  ids = {gunning2019},
  title = {{{XAI}}---{{Explainable}} Artificial Intelligence},
  author = {Gunning, David and Stefik, Mark and Choi, Jaesik and Miller, Timothy and Stumpf, Simone and Yang, Guang-Zhong},
  year = {2019},
  month = dec,
  journal = {Science Robotics},
  volume = {4},
  number = {37},
  pages = {eaay7120},
  publisher = {American Association for the Advancement of Science},
  issn = {2470-9476},
  doi = {10.1126/scirobotics.aay7120},
  urldate = {2025-02-02},
  abstract = {Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications.},
  copyright = {Copyright {\copyright} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
  langid = {english},
  annotation = {TLDR: This research presents a meta-modelling architecture that automates the very labor-intensive and therefore time-heavy and therefore expensive and expensive process of manually cataloging and cataloging artificial intelligence applications.},
  timestamp = {2025-02-02T09:06:04Z}
}

@article{guo2024plasma,
  title = {Plasma Proteomic Profiles Predict Future Dementia in Healthy Adults},
  author = {Guo, Yu and You, Jia and Zhang, Yi and Liu, Wei-Shi and Huang, Yu-Yuan and Zhang, Ya-Ru and Zhang, Wei and Dong, Qiang and Feng, Jian-Feng and Cheng, Wei and Yu, Jin-Tai},
  year = {2024},
  month = feb,
  journal = {Nature Aging},
  volume = {4},
  number = {2},
  pages = {247--260},
  issn = {2662-8465},
  doi = {10.1038/s43587-023-00565-0},
  abstract = {The advent of proteomics offers an unprecedented opportunity to predict dementia onset. We examined this in data from 52,645 adults without dementia in the UK Biobank, with 1,417 incident cases and a follow-up time of 14.1\,years. Of 1,463 plasma proteins, GFAP, NEFL, GDF15 and LTBP2 consistently associated most with incident all-cause dementia (ACD), Alzheimer's disease (AD) and vascular dementia (VaD), and ranked high in protein importance ordering. Combining GFAP (or GDF15) with demographics produced desirable predictions for ACD (area under the curve (AUC)\,=\,0.891) and AD (AUC\,=\,0.872) (or VaD (AUC\,=\,0.912)). This was also true when predicting over 10-year ACD, AD and VaD. Individuals with higher GFAP levels were 2.32 times more likely to develop dementia. Notably, GFAP and LTBP2 were highly specific for dementia prediction. GFAP and NEFL began to change at least 10\,years before dementia diagnosis. Our findings strongly highlight GFAP as an optimal biomarker for dementia prediction, even more than 10\,years before the diagnosis, with implications for screening people at high risk for dementia and for early intervention.},
  langid = {english},
  pmid = {38347190},
  keywords = {Alzheimer Disease,Dementia Vascular,Humans,Latent TGF-beta Binding Proteins,Proteomics},
  annotation = {TLDR: GFAP is highlighted as an optimal biomarker for dementia prediction, even more than 10\,years before the diagnosis, with implications for screening people at high risk for dementia and for early intervention.},
  timestamp = {2025-05-03T02:47:20Z}
}

@inproceedings{gupta2020region,
  title = {Region of {{Interest Identification}} for {{Cervical Cancer Images}}},
  booktitle = {2020 {{IEEE}} 17th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {Gupta, Manish and Das, Chetna and Roy, Arnab and Gupta, Prashant and Pillai, G. Radhakrishna and Patole, Kamlakar},
  year = {2020},
  month = apr,
  pages = {1293--1296},
  issn = {1945-8452},
  doi = {10.1109/ISBI45749.2020.9098587},
  urldate = {2025-05-17},
  abstract = {Every two minutes one woman dies of cervical cancer globally, due to lack of sufficient screening. Given a whole slide image (WSI) obtained by scanning a microscope glass slide for a Liquid Based Cytology (LBC) based Pap test, our goal is to assist the pathologist to determine presence of precancerous or cancerous cervical anomalies. Inter-annotator variation, large image sizes, data imbalance, stain variations, and lack of good annotation tools make this problem challenging. Existing related work has focused on sub-problems like cell segmentation and cervical cell classification but does not provide a practically feasible holistic solution. We propose a practical system architecture which is based on displaying regions of interest on WSIs containing potential anomaly for review by pathologists to increase productivity. We build multiple deep learning classifiers as part of the proposed architecture. Our experiments with a dataset of 19000 regions of interest provides an accuracy of 89\% for a balanced dataset both for binary as well as 6-class classification settings. Our deployed system provides a top-5 accuracy of 94\%.},
  keywords = {Anomaly detection,Cervical cancer,Computer architecture,Image segmentation,Machine learning,Microprocessors,Noise measurement},
  annotation = {TLDR: This work proposes a practical system architecture which is based on displaying regions of interest on WSIs containing potential anomaly for review by pathologists to increase productivity and builds multiple deep learning classifiers as part of the proposed architecture.},
  timestamp = {2025-05-17T11:05:53Z}
}

@misc{gupta2025fedalign,
  title = {{{FedAlign}}: {{Federated Domain Generalization}} with {{Cross-Client Feature Alignment}}},
  shorttitle = {{{FedAlign}}},
  author = {Gupta, Sunny and Sutar, Vinay and Singh, Varunav and Sethi, Amit},
  year = {2025},
  month = jan,
  number = {arXiv:2501.15486},
  eprint = {2501.15486},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.15486},
  urldate = {2025-05-21},
  abstract = {Federated Learning (FL) offers a decentralized paradigm for collaborative model training without direct data sharing, yet it poses unique challenges for Domain Generalization (DG), including strict privacy constraints, non-i.i.d. local data, and limited domain diversity. We introduce FedAlign, a lightweight, privacy-preserving framework designed to enhance DG in federated settings by simultaneously increasing feature diversity and promoting domain invariance. First, a cross-client feature extension module broadens local domain representations through domain-invariant feature perturbation and selective cross-client feature transfer, allowing each client to safely access a richer domain space. Second, a dual-stage alignment module refines global feature learning by aligning both feature embeddings and predictions across clients, thereby distilling robust, domain-invariant features. By integrating these modules, our method achieves superior generalization to unseen domains while maintaining data privacy and operating with minimal computational and communication overhead.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  annotation = {TLDR: FedAlign is introduced, a lightweight, privacy-preserving framework designed to enhance DG in federated settings by simultaneously increasing feature diversity and promoting domain invariance and achieves superior generalization to unseen domains while maintaining data privacy and operating with minimal computational and communication overhead.},
  timestamp = {2025-05-21T02:45:04Z}
}

@article{hafeez2025explainable,
  title = {Explainable {{AI}} in Diagnostic Radiology for Neurological Disorders: A Systematic Review, and What Doctors Think about It},
  author = {Hafeez, Yasir and Memon, Khuhed and {Al-Quraishi}, Maged S and Yahya, Norashikin and Elferik, Sami and Ali, Syed Saad Azhar},
  year = {2025},
  journal = {Diagnostics},
  volume = {15},
  number = {2},
  pages = {168},
  publisher = {MDPI},
  timestamp = {2025-04-15T13:36:14Z}
}

@article{hamilton2024neurosymbolic,
  title = {Is Neuro-Symbolic {{AI}} Meeting Its Promises in Natural Language Processing? {{A}} Structured Review},
  shorttitle = {Is Neuro-Symbolic {{AI}} Meeting Its Promises in Natural Language Processing?},
  author = {Hamilton, Kyle and Nayak, Aparna and Bo{\v z}i{\'c}, Bojan and Longo, Luca},
  editor = {Ebrahimi, Monireh and Hitzler, Pascal and Kamruzzaman Sarker, Md and Stepanova, Daria},
  year = {2024},
  month = oct,
  journal = {Semantic Web},
  volume = {15},
  number = {4},
  pages = {1265--1306},
  issn = {22104968, 15700844},
  doi = {10.3233/SW-223228},
  urldate = {2025-06-13},
  abstract = {Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, with the aim of answering the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met. We find many discrepancies in how reasoning is defined, specifically in relation to human level reasoning, which impact decisions about model architectures and drive conclusions which are not always consistent across studies. Hence we advocate for a more methodical approach to the application of theories of human reasoning as well as the development of appropriate benchmarks, which we hope can lead to a better understanding of progress in the field. We make our data and code available on github for further analysis.11 https://github.com/kyleiwaniec/neuro-symbolic-ai-systematic-review},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  annotation = {TLDR: A structured review of studies implementing NeSy for NLP finds that systems where logic is compiled into the neural network lead to the most NeSy goals being satisfied, while other factors such as knowledge representation, or type of neural architecture do not exhibit a clear correlation with goals being met.},
  timestamp = {2025-06-13T04:10:59Z}
}

@article{han2022online,
  title = {Online {{Continual Learning}} via the {{Meta-learning}} Update with {{Multi-scale Knowledge Distillation}} and {{Data Augmentation}}},
  author = {Han, Ya-nan and Liu, Jian-wei},
  year = {2022},
  month = aug,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {113},
  pages = {104966},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2022.104966},
  urldate = {2025-04-12},
  abstract = {Continual learning aims to rapidly and continually learn the current task from a sequence of tasks, using the knowledge obtained in the past, while performing well on prior tasks. A key challenge in this setting is the stability--plasticity dilemma existing in current and previous tasks, i.e., a high-stability network is weak to learn new knowledge in an effort to maintain previous knowledge. Correspondingly, a high-plasticity network can easily forget old tasks while dealing with well on the new task. Compared to other kinds of methods, the methods based on experience replay have shown great advantages to overcome catastrophic forgetting. One common limitation of this method is the data imbalance between the previous and current tasks, which would further aggravate forgetting. Moreover, how to effectively address the stability--plasticity dilemma in this setting is also an urgent problem to be solved. In this paper, we overcome these challenges by proposing a novel framework called Meta-learning update via Multi-scale Knowledge Distillation and Data Augmentation (MMKDDA). Specifically, we apply multi-scale knowledge distillation to grasp the evolution of long-range and short-range spatial relationships at different feature levels to alleviate the problem of data imbalance. Besides, our method mixes the samples from the episodic memory and current task in the online continual training procedure, thus alleviating the side influence due to the change of probability distribution. Moreover, we optimize our model via the meta-learning update by resorting to the number of tasks seen previously, which is helpful to keep a better balance between stability and plasticity. Finally, our extensive experiments on four benchmark datasets show the effectiveness of the proposed MMKDDA framework against other popular baselines, and ablation studies are also conducted to further analyze the role of each component in our framework.},
  keywords = {Continual learning,Data augmentation,Knowledge distillation,Meta-learning,The stability-plasticity dilemma},
  annotation = {TLDR: This paper proposes a novel framework called Meta-learning update via Multi-scale Knowledge Distillation and Data Augmentation (MMKDDA), which applies multiscale knowledge distillation to grasp the evolution of long-range and short-range spatial relationships at different feature levels to alleviate the problem of data imbalance.},
  timestamp = {2025-04-12T07:48:34Z}
}

@article{hansel2023data,
  title = {From {{Data}} to {{Wisdom}}: {{Biomedical Knowledge Graphs}} for {{Real-World Data Insights}}},
  shorttitle = {From {{Data}} to {{Wisdom}}},
  author = {H{\"a}nsel, Katrin and Dudgeon, Sarah N. and Cheung, Kei-Hoi and Durant, Thomas J. S. and Schulz, Wade L.},
  year = {2023},
  month = may,
  journal = {Journal of Medical Systems},
  volume = {47},
  number = {1},
  pages = {65},
  issn = {1573-689X},
  doi = {10.1007/s10916-023-01951-2},
  urldate = {2025-06-12},
  abstract = {Abstract             Graph data models are an emerging approach to structure clinical and biomedical information. These models offer intriguing opportunities for novel approaches in healthcare, such as disease phenotyping, risk prediction, and personalized precision care. The combination of data and information in a graph model to create knowledge graphs has rapidly expanded in biomedical research, but the integration of real-world data from the electronic health record has been limited. To broadly apply knowledge graphs to EHR and other real-world data, a deeper understanding of how to represent these data in a standardized graph model is needed. We provide an overview of the state-of-the-art research for clinical and biomedical data integration and summarize the potential to accelerate healthcare and precision medicine research through insight generation from integrated knowledge graphs.},
  langid = {english},
  annotation = {TLDR: The potential to accelerate healthcare and precision medicine research through insight generation from integrated knowledge graphs is summarized.},
  timestamp = {2025-06-12T13:53:32Z}
}

@article{hao2019interpretable,
  title = {Interpretable Deep Neural Network for Cancer Survival Analysis by Integrating Genomic and Clinical Data},
  author = {Hao, Jie and Kim, Youngsoon and Mallavarapu, Tejaswini and Oh, Jung Hun and Kang, Mingon},
  year = {2019},
  month = dec,
  journal = {BMC Medical Genomics},
  volume = {12},
  number = {10},
  pages = {189},
  issn = {1755-8794},
  doi = {10.1186/s12920-019-0624-2},
  urldate = {2025-07-25},
  abstract = {Understanding the complex biological mechanisms of cancer patient survival using genomic and clinical data is vital, not only to develop new treatments for patients, but also to improve survival prediction. However, highly nonlinear and high-dimension, low-sample size (HDLSS) data cause computational challenges to applying conventional survival analysis.},
  keywords = {Cox-PASNet,Deep neural network,Glioblastoma multiforme,Ovarian cancer,Survival analysis},
  annotation = {TLDR: A novel biologically interpretable pathway-based sparse deep neural network, named Cox-PASNet, which integrates high-dimensional gene expression data and clinical data on a simple neural network architecture for survival analysis, and shows out-performance, compared to the benchmarking methods.},
  timestamp = {2025-07-25T14:03:58Z}
}

@article{harder2020interpretable,
  title = {Interpretable and {{Differentially Private Predictions}}},
  author = {Harder, Frederik and Bauer, Matthias and Park, Mijung},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {4083--4090},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.5827},
  urldate = {2025-04-04},
  abstract = {Interpretable predictions, which clarify why a machine learning model makes a particular decision, can compromise privacy by revealing the characteristics of individual data points. This raises the central question addressed in this paper: Can models be interpretable without compromising privacy? For complex ``big'' data fit by correspondingly rich models, balancing privacy and explainability is particularly challenging, such that this question has remained largely unexplored. In this paper, we propose a family of simple models with the aim of approximating complex models using several locally linear maps per class to provide high classification accuracy, as well as differentially private explanations on the classification. We illustrate the usefulness of our approach on several image benchmark datasets as well as a medical dataset.},
  copyright = {https://www.aaai.org},
  annotation = {TLDR: A family of simple models are proposed with the aim of approximating complex models using several locally linear maps per class to provide high classification accuracy, as well as differentially private explanations on the classification.},
  timestamp = {2025-04-04T06:29:18Z}
}

@misc{harradon2018causal,
  title = {Causal {{Learning}} and {{Explanation}} of {{Deep Neural Networks}} via {{Autoencoded Activations}}},
  author = {Harradon, Michael and Druce, Jeff and Ruttenberg, Brian},
  year = {2018},
  month = feb,
  number = {arXiv:1802.00541},
  eprint = {1802.00541},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.00541},
  urldate = {2025-03-20},
  abstract = {Deep neural networks are complex and opaque. As they enter application in a variety of important and safety critical domains, users seek methods to explain their output predictions. We develop an approach to explaining deep neural networks by constructing causal models on salient concepts contained in a CNN. We develop methods to extract salient concepts throughout a target network by using autoencoders trained to extract human-understandable representations of network activations. We then build a bayesian causal model using these extracted concepts as variables in order to explain image classification. Finally, we use this causal model to identify and visualize features with significant causal influence on final classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-20T23:13:51Z}
}

@article{hassanali2023artificial,
  title = {Artificial Intelligence in Multi-Omics Data Integration: {{Advancing}} Precision Medicine, Biomarker Discovery and Genomic-Driven Disease Interventions},
  shorttitle = {Artificial Intelligence in Multi-Omics Data Integration},
  author = {{Hassan Ali}},
  year = {2023},
  month = feb,
  journal = {International Journal of Science and Research Archive},
  volume = {8},
  number = {1},
  pages = {1012--1030},
  issn = {25828185},
  doi = {10.30574/ijsra.2023.8.1.0189},
  urldate = {2025-04-17},
  abstract = {The integration of multi-omics data---encompassing genomics, transcriptomics, proteomics, and metabolomics---has revolutionized biomedical research, offering unprecedented insights into disease mechanisms and therapeutic interventions. However, the complexity and volume of multi-omics datasets present significant analytical challenges that traditional computational methods struggle to address. Artificial Intelligence (AI), particularly deep learning and neural networks, has emerged as a powerful tool to overcome these limitations by enabling advanced data integration, biomarker discovery, and personalized treatment strategies. This paper explores the role of AI-driven multi-omics data integration in enhancing disease prediction, early diagnosis, and precision medicine. By leveraging AI models such as deep neural networks (DNNs), convolutional neural networks (CNNs), and transformers, researchers can analyze complex biological interactions, identify patterns indicative of disease onset, and stratify patient populations for tailored treatment approaches. Additionally, AI-powered feature selection methods facilitate the identification of disease-specific biomarkers across multiple omics layers, paving the way for more effective targeted therapies. Moreover, AI plays a crucial role in pharmacogenomics by predicting individualized drug responses, optimizing dosage regimens, and minimizing adverse drug reactions. Machine learning algorithms, including reinforcement learning and generative models, enable real-time modeling of drug-gene interactions, leading to safer and more efficacious therapeutic interventions. Despite the transformative potential of AI in multi-omics data analysis, challenges such as data standardization, model interpretability, and ethical considerations must be addressed to ensure reliability and clinical applicability. This paper provides a comprehensive review of AI-driven multi-omics research, highlighting current advancements, challenges, and future directions in precision medicine.},
  annotation = {TLDR: The role of AI-driven multi-omics data integration in enhancing disease prediction, early diagnosis, and precision medicine is explored, highlighting current advancements, challenges, and future directions in precision medicine.},
  timestamp = {2025-04-17T15:14:13Z}
}

@article{hassija2024interpreting,
  title = {Interpreting {{Black-Box Models}}: {{A Review}} on {{Explainable Artificial Intelligence}}},
  shorttitle = {Interpreting {{Black-Box Models}}},
  author = {Hassija, Vikas and Chamola, Vinay and Mahapatra, Atmesh and Singal, Abhinandan and Goel, Divyansh and Huang, Kaizhu and Scardapane, Simone and Spinelli, Indro and Mahmud, Mufti and Hussain, Amir},
  year = {2024},
  month = jan,
  journal = {Cognitive Computation},
  volume = {16},
  number = {1},
  pages = {45--74},
  issn = {1866-9964},
  doi = {10.1007/s12559-023-10179-8},
  urldate = {2025-05-04},
  abstract = {Recent years have seen a tremendous growth in Artificial Intelligence (AI)-based methodological development in a broad range of~domains. In this~rapidly evolving field, large number of methods are being reported using machine learning (ML) and Deep Learning (DL) models. Majority of these models are~inherently complex and lacks explanations~of the decision making process~causing these models to be termed as 'Black-Box'. One of the major bottlenecks to adopt such models in mission-critical application domains, such as~banking, e-commerce, healthcare, and public services and~safety, is the difficulty in~interpreting them. Due to the rapid proleferation of these AI models, explaining their learning and decision making process are~getting harder which require~transparency~and easy predictability.~Aiming to collate the current state-of-the-art in interpreting the black-box models,~this study provides a comprehensive analysis of the~explainable AI (XAI)~models. To reduce~false negative and false positive outcomes of these back-box models,~finding flaws in them~is still difficult and inefficient. In this paper, the development of XAI is reviewed meticulously~through careful selection and analysis of the current state-of-the-art of XAI research. It~also provides a comprehensive and in-depth evaluation of the XAI frameworks and their efficacy to serve as a starting point of XAI for applied and theoretical researchers. Towards the end, it~highlights emerging~and~critical issues pertaining to XAI research to~showcase major, model-specific trends~for better explanation, enhanced~transparency, and improved~prediction~accuracy.},
  langid = {english},
  keywords = {Artificial Intelligence,Black-box models,Interpretability,Machine learning,Responsible AI,Transparency,XAI},
  annotation = {TLDR: The development of XAI is reviewed meticulously through careful selection and analysis of the current state-of-the-art ofXAI research to showcase major, model-specific trends for better explanation, enhanced~transparency, and improved~prediction~accuracy.},
  timestamp = {2025-05-04T00:19:09Z}
}

@misc{he2021incremental,
  title = {Incremental {{Learning In Online Scenario}}},
  author = {He, Jiangpeng and Mao, Runyu and Shao, Zeman and Zhu, Fengqing},
  year = {2021},
  month = apr,
  number = {arXiv:2003.13191},
  eprint = {2003.13191},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.13191},
  urldate = {2025-04-12},
  abstract = {Modern deep learning approaches have achieved great success in many vision applications by training a model using all available task-specific data. However, there are two major obstacles making it challenging to implement for real life applications: (1) Learning new classes makes the trained model quickly forget old classes knowledge, which is referred to as catastrophic forgetting. (2) As new observations of old classes come sequentially over time, the distribution may change in unforeseen way, making the performance degrade dramatically on future data, which is referred to as concept drift. Current state-of-the-art incremental learning methods require a long time to train the model whenever new classes are added and none of them takes into consideration the new observations of old classes. In this paper, we propose an incremental learning framework that can work in the challenging online learning scenario and handle both new classes data and new observations of old classes. We address problem (1) in online mode by introducing a modified cross-distillation loss together with a two-step learning technique. Our method outperforms the results obtained from current state-of-the-art offline incremental learning methods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the same experiment protocol but in online scenario. We also provide a simple yet effective method to mitigate problem (2) by updating exemplar set using the feature of each new observation of old classes and demonstrate a real life application of online food image classification based on our complete framework using the Food-101 dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  timestamp = {2025-04-12T07:46:56Z}
}

@misc{health2023design,
  title = {Design {{Considerations}} for {{Pivotal Clinical Investigations}} for {{Medical Devices}}},
  author = {Health, Center for Devices {and} Radiological},
  year = {Thu, 08/10/2023 - 16:07},
  publisher = {FDA},
  urldate = {2025-05-15},
  abstract = {For those involved in designing clinical studies intended to support pre-market submissions for medical devices and FDA staff who review those submissions.},
  howpublished = {https://www.fda.gov/regulatory-information/search-fda-guidance-documents/design-considerations-pivotal-clinical-investigations-medical-devices},
  langid = {english},
  timestamp = {2025-05-15T02:37:08Z}
}

@article{hendrix2023deep,
  title = {Deep Learning for the Detection of Benign and Malignant Pulmonary Nodules in Non-Screening Chest {{CT}} Scans},
  author = {Hendrix, Ward and Hendrix, Nils and Scholten, Ernst T. and Mourits, Mari{\"e}lle and {Trap-de Jong}, Joline and Schalekamp, Steven and Korst, Mike and {van Leuken}, Maarten and {van Ginneken}, Bram and Prokop, Mathias and Rutten, Matthieu and Jacobs, Colin},
  year = {2023},
  month = oct,
  journal = {Communications Medicine},
  volume = {3},
  number = {1},
  pages = {1--12},
  publisher = {Nature Publishing Group},
  issn = {2730-664X},
  doi = {10.1038/s43856-023-00388-5},
  urldate = {2025-05-01},
  abstract = {Outside a screening program, early-stage lung cancer is generally diagnosed after the detection of incidental nodules in clinically ordered chest CT scans. Despite the advances in artificial intelligence (AI) systems for lung cancer detection, clinical validation of these systems is lacking in a non-screening setting.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Lung cancer},
  annotation = {TLDR: The deep learning-based AI system reliably detects benign and malignant pulmonary nodules in clinically indicated CT scans and can potentially assist radiologists in this setting.},
  timestamp = {2025-05-01T03:41:43Z}
}

@misc{heumos2024pertpy,
  title = {Pertpy: An End-to-End Framework for Perturbation Analysis},
  shorttitle = {Pertpy},
  author = {Heumos, Lukas and Ji, Yuge and May, Lilly and Green, Tessa and Zhang, Xinyue and Wu, Xichen and Ostner, Johannes and Peidli, Stefan and Schumacher, Antonia and Hrovatin, Karin and M{\"u}ller, Michaela and Chong, Faye and Sturm, Gregor and Tejada, Alejandro and Dann, Emma and Dong, Mingze and Bahrami, Mojtaba and Gold, Ilan and Rybakov, Sergei and Namsaraeva, Altana and Moinfar, Amir and Zheng, Zihe and Roellin, Eljas and Mekki, Isra and Sander, Chris and Lotfollahi, Mohammad and Schiller, Herbert B. and Theis, Fabian J.},
  year = {2024},
  month = aug,
  primaryclass = {New Results},
  pages = {2024.08.04.606516},
  publisher = {bioRxiv},
  doi = {10.1101/2024.08.04.606516},
  urldate = {2025-05-17},
  abstract = {Advances in single-cell technology have enabled the measurement of cell-resolved molecular states across a variety of cell lines and tissues under a plethora of genetic, chemical, environmental, or disease perturbations. Current methods focus on differential comparison or are specific to a particular task in a multi-condition setting with purely statistical perspectives. The quickly growing number, size, and complexity of such studies requires a scalable analysis framework that takes existing biological context into account. Here, we present pertpy, a Python-based modular framework for the analysis of large-scale perturbation single-cell experiments. Pertpy provides access to harmonized perturbation datasets and metadata databases along with numerous fast and user-friendly implementations of both established and novel methods such as automatic metadata annotation or perturbation distances to efficiently analyze perturbation data. As part of the scverse ecosystem, pertpy interoperates with existing libraries for the analysis of single-cell data and is designed to be easily extended.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: Pertpy, a Python-based modular framework for the analysis of large-scale perturbation single-cell experiments, provides access to harmonized perturbation datasets and metadata databases along with numerous fast and user-friendly implementations of both established and novel methods to efficiently analyze perturbation data.},
  timestamp = {2025-05-17T07:28:27Z}
}

@inproceedings{himabindu2023neurosymbolic,
  title = {Neuro-{{Symbolic AI}}: {{Integrating Symbolic Reasoning}} with {{Deep Learning}}},
  shorttitle = {Neuro-{{Symbolic AI}}},
  booktitle = {2023 10th {{IEEE Uttar Pradesh Section International Conference}} on {{Electrical}}, {{Electronics}} and {{Computer Engineering}} ({{UPCON}})},
  author = {Himabindu, Modi and V, Revathi and Gupta, Manish and Rana, Ajay and Chandra, Pradeep Kumar and Abdulaali, Hayder Saadoon},
  year = {2023},
  month = dec,
  pages = {1587--1592},
  publisher = {IEEE},
  address = {Gautam Buddha Nagar, India},
  doi = {10.1109/UPCON59197.2023.10434380},
  urldate = {2025-06-13},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-8247-1},
  annotation = {TLDR: A comprehensive framework for neuro-symbolic integration is presented, outlining a harmonized architecture that leverages the strengths of both domains, and showcases a significant reduction in data dependency for model training, increased interpretability of the decision-making process, and robustness to noise and ambiguity.},
  timestamp = {2025-06-13T04:12:04Z}
}

@misc{holzinger2017glassbox,
  title = {A Glass-Box Interactive Machine Learning Approach for Solving {{NP-hard}} Problems with the Human-in-the-Loop},
  author = {Holzinger, Andreas and Plass, Markus and Holzinger, Katharina and Crisan, Gloria Cerasela and Pintea, Camelia-M. and Palade, Vasile},
  year = {2017},
  month = aug,
  number = {arXiv:1708.01104},
  eprint = {1708.01104},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.01104},
  urldate = {2025-06-10},
  abstract = {The goal of Machine Learning to automatically learn from data, extract knowledge and to make decisions without any human intervention. Such automatic (aML) approaches show impressive success. Recent results even demonstrate intriguingly that deep learning applied for automatic classification of skin lesions is on par with the performance of dermatologists, yet outperforms the average. As human perception is inherently limited, such approaches can discover patterns, e.g. that two objects are similar, in arbitrarily high-dimensional spaces what no human is able to do. Humans can deal only with limited amounts of data, whilst big data is beneficial for aML; however, in health informatics, we are often confronted with a small number of data sets, where aML suffer of insufficient training samples and many problems are computationally hard. Here, interactive machine learning (iML) may be of help, where a human-in-the-loop contributes to reduce the complexity of NP-hard problems. A further motivation for iML is that standard black-box approaches lack transparency, hence do not foster trust and acceptance of ML among end-users. Rising legal and privacy aspects, e.g. with the new European General Data Protection Regulations, make black-box approaches difficult to use, because they often are not able to explain why a decision has been made. In this paper, we present some experiments to demonstrate the effectiveness of the human-in-the-loop approach, particularly in opening the black-box to a glass-box and thus enabling a human directly to interact with an learning algorithm. We selected the Ant Colony Optimization framework, and applied it on the Traveling Salesman Problem, which is a good example, due to its relevance for health informatics, e.g. for the study of protein folding. From studies of how humans extract so much from so little data, fundamental ML-research also may benefit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning},
  timestamp = {2025-06-10T00:39:09Z}
}

@misc{holzinger2017what,
  title = {What Do We Need to Build Explainable {{AI}} Systems for the Medical Domain?},
  author = {Holzinger, Andreas and Biemann, Chris and Pattichis, Constantinos S. and Kell, Douglas B.},
  year = {2017},
  month = dec,
  number = {arXiv:1712.09923},
  eprint = {1712.09923},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.09923},
  urldate = {2025-03-17},
  abstract = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning},
  timestamp = {2025-03-17T08:55:08Z}
}

@article{holzinger2019causability,
  ids = {holzinger2019},
  title = {Causability and Explainability of Artificial Intelligence in Medicine},
  author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and M{\"u}ller, Heimo},
  year = {2019},
  month = jul,
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1312},
  issn = {1942-4787, 1942-4795},
  doi = {10.1002/widm.1312},
  urldate = {2025-04-21},
  abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of               explainable medicine               we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system                                         This article is categorized under:                                                   Fundamental Concepts of Data and Knowledge {$>$} Human Centricity and User Interaction},
  langid = {english},
  annotation = {TLDR: This article provides some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology, and argues that there is a need to go beyond explainable AI.},
  timestamp = {2025-04-21T06:13:43Z}
}

@article{holzinger2020explainable,
  title = {Explainable {{AI}} and {{Multi-Modal Causability}} in {{Medicine}}},
  author = {Holzinger, Andreas},
  year = {2020},
  month = dec,
  journal = {i-com},
  volume = {19},
  number = {3},
  pages = {171--179},
  publisher = {Oldenbourg Wissenschaftsverlag},
  issn = {2196-6826},
  doi = {10.1515/icom-2020-0024},
  urldate = {2025-03-28},
  abstract = {Progress in statistical machine learning made AI in medicine successful, in certain classification tasks even beyond human level performance. Nevertheless, correlation is not causation and successful models are often complex ``black-boxes'', which make it hard to understand why a result has been achieved. The explainable AI (xAI) community develops methods, e.\,g. to highlight which input parameters are relevant for a result; however, in the medical domain there is a need for causability: In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations produced by xAI. The key for future human-AI interfaces is to map explainability with causability and to allow a domain expert to ask questions to understand why an AI came up with a result, and also to ask ``what-if'' questions (counterfactuals) to gain insight into the underlying independent explanatory factors of a result. A multi-modal causability is important in the medical domain because often different modalities contribute to a result.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {explainable AI,Human-AI interfaces,Human-Centered AI},
  annotation = {TLDR: The key for future human-AI interfaces is to map explainability with causability and to allow a domain expert to ask questions to understand why an AI came up with a result, and also to ask ``what-if'' questions (counterfactuals) to gain insight into the underlying independent explanatory factors of a result.},
  timestamp = {2025-03-28T07:32:45Z}
}

@article{holzinger2020measuring,
  title = {Measuring the Quality of Explanations: The System Causability Scale ({{SCS}}) Comparing Human and Machine Explanations},
  author = {Holzinger, Andreas and Carrington, Andr{\'e} and M{\"u}ller, Heimo},
  year = {2020},
  journal = {KI-K{\"u}nstliche Intelligenz},
  volume = {34},
  number = {2},
  pages = {193--198},
  publisher = {Springer},
  timestamp = {2025-04-12T07:08:46Z}
}

@article{holzinger2020measuringa,
  title = {Measuring the {{Quality}} of {{Explanations}}: {{The System Causability Scale}} ({{SCS}})},
  shorttitle = {Measuring the {{Quality}} of {{Explanations}}},
  author = {Holzinger, Andreas and Carrington, Andr{\'e} and M{\"u}ller, Heimo},
  year = {2020},
  month = jun,
  journal = {KI - K{\"u}nstliche Intelligenz},
  volume = {34},
  number = {2},
  pages = {193--198},
  issn = {1610-1987},
  doi = {10.1007/s13218-020-00636-z},
  urldate = {2025-03-28},
  abstract = {Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human--AI interfaces for explainable AI. In order to build effective and efficient interactive human--AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale.},
  langid = {english},
  keywords = {Artificial Intelligence,Explainable AI,Human-AI interfaces,System causability scale (SCS)},
  annotation = {TLDR: The System Causability Scale is introduced, based on the notion ofCausability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale to measure the quality of explanations in explainable AI systems.},
  timestamp = {2025-03-28T03:54:57Z}
}

@article{holzinger2021multimodal,
  title = {Towards Multi-Modal Causability with {{Graph Neural Networks}} Enabling Information Fusion for Explainable {{AI}}},
  author = {Holzinger, Andreas and Malle, Bernd and Saranti, Anna and Pfeifer, Bastian},
  year = {2021},
  month = jul,
  journal = {Information Fusion},
  volume = {71},
  pages = {28--37},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.01.008},
  urldate = {2025-06-13},
  abstract = {AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is ``How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?''. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability -- not to confuse with causality -- is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human--AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.},
  langid = {english},
  keywords = {Counterfactuals,Explainable AI,Graph Neural Networks,Information fusion,Knowledge graphs,Multi-modal causability,xAI},
  annotation = {TLDR: Fields of multi-modal embeddings and interactive explainability arefields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human--AI interfaces.},
  timestamp = {2025-06-13T07:06:23Z}
}

@inproceedings{holzinger2021next,
  title = {The {{Next Frontier}}: {{AI We Can Really Trust}}},
  shorttitle = {The {{Next Frontier}}},
  booktitle = {Machine {{Learning}} and {{Principles}} and {{Practice}} of {{Knowledge Discovery}} in {{Databases}}},
  author = {Holzinger, Andreas},
  editor = {Kamp, Michael and Koprinska, Irena and Bibal, Adrien and Bouadi, Tassadit and Fr{\'e}nay, Beno{\^i}t and Gal{\'a}rraga, Luis and Oramas, Jos{\'e} and Adilova, Linara and Krishnamurthy, Yamuna and Kang, Bo and Largeron, Christine and Lijffijt, Jefrey and Viard, Tiphaine and Welke, Pascal and Ruocco, Massimiliano and Aune, Erlend and Gallicchio, Claudio and Schiele, Gregor and Pernkopf, Franz and Blott, Michaela and Fr{\"o}ning, Holger and Schindler, G{\"u}nther and Guidotti, Riccardo and Monreale, Anna and Rinzivillo, Salvatore and Biecek, Przemyslaw and Ntoutsi, Eirini and Pechenizkiy, Mykola and Rosenhahn, Bodo and Buckley, Christopher and Cialfi, Daniela and Lanillos, Pablo and Ramstead, Maxwell and Verbelen, Tim and Ferreira, Pedro M. and Andresini, Giuseppina and Malerba, Donato and Medeiros, Ib{\'e}ria and {Fournier-Viger}, Philippe and Nawaz, M. Saqib and Ventura, Sebastian and Sun, Meng and Zhou, Min and Bitetta, Valerio and Bordino, Ilaria and Ferretti, Andrea and Gullo, Francesco and Ponti, Giovanni and Severini, Lorenzo and Ribeiro, Rita and Gama, Jo{\~a}o and Gavald{\`a}, Ricard and Cooper, Lee and Ghazaleh, Naghmeh and Richiardi, Jonas and Roqueiro, Damian and Saldana Miranda, Diego and Sechidis, Konstantinos and Gra{\c c}a, Guilherme},
  year = {2021},
  pages = {427--440},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-93736-2_33},
  abstract = {Enormous advances in the domain of statistical machine learning, the availability of large amounts of training data, and increasing computing power have made Artificial Intelligence (AI) very successful. For certain tasks, algorithms can even achieve performance beyond the human level. Unfortunately, the most powerful methods suffer from the fact that it is difficult to explain why a certain result was achieved on the one hand, and that they lack robustness on the other. Our most powerful machine learning models are very sensitive to even small changes. Perturbations in the input data can have a dramatic impact on the output and lead to entirely different results. This is of great importance in virtually all critical domains where we suffer from low data quality, i.e. we do not have the expected i.i.d. data. Therefore, the use of AI in domains that impact human life (agriculture, climate, health, ...) has led to an increased demand for trustworthy AI. Explainability is now even mandatory due to regulatory requirements in sensitive domains such as medicine, which requires traceability, transparency and interpretability capabilities. One possible step to make AI more robust is to combine statistical learning with knowledge representations. For certain tasks, it can be advantageous to use a human in the loop. A human expert can - sometimes, of course not always - bring experience, domain knowledge and conceptual understanding to the AI pipeline. Such approaches are not only a solution from a legal point of view, but in many application areas the ``why'' is often more important than a pure classification result. Consequently, both explainability and robustness can promote reliability and trust and ensure that humans remain in control, thus complementing human intelligence with artificial intelligence.},
  isbn = {978-3-030-93736-2},
  langid = {english},
  timestamp = {2025-03-25T11:34:00Z}
}

@article{holzinger2022information,
  title = {Information Fusion as an Integrative Cross-Cutting Enabler to Achieve Robust, Explainable, and Trustworthy Medical Artificial Intelligence},
  author = {Holzinger, Andreas and Dehmer, Matthias and {Emmert-Streib}, Frank and Cucchiara, Rita and Augenstein, Isabelle and Del Ser, Javier and Samek, Wojciech and Jurisica, Igor and {D{\'{\i}}az-Rodr{\'{\i}}guez}, Natalia},
  year = {2022},
  month = mar,
  journal = {Information Fusion},
  volume = {79},
  pages = {263--278},
  publisher = {Elsevier},
  timestamp = {2025-04-13T07:40:03Z}
}

@article{hood2011predictive,
  title = {Predictive, Personalized, Preventive, Participatory ({{P4}}) Cancer Medicine},
  author = {Hood, Leroy and Friend, Stephen H},
  year = {2011},
  journal = {Nature reviews Clinical oncology},
  volume = {8},
  number = {3},
  pages = {184--187},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2025-05-27T11:26:01Z}
}

@article{hood2012systems,
  title = {Systems {{Approaches}} to {{Biology}} and {{Disease Enable Translational Systems Medicine}}},
  author = {Hood, Leroy and Tian, Qiang},
  year = {2012},
  month = aug,
  journal = {Genomics, Proteomics \& Bioinformatics},
  volume = {10},
  number = {4},
  pages = {181--185},
  issn = {1672-0229},
  doi = {10.1016/j.gpb.2012.08.004},
  urldate = {2025-05-24},
  abstract = {The development and application of systems strategies to biology and disease are transforming medical research and clinical practice in an unprecedented rate. In the foreseeable future, clinicians, medical researchers, and ultimately the consumers and patients will be increasingly equipped with a deluge of personal health information, e.g., whole genome sequences, molecular profiling of diseased tissues, and periodic multi-analyte blood testing of biomarker panels for disease and wellness. The convergence of these practices will enable accurate prediction of disease susceptibility and early diagnosis for actionable preventive schema and personalized treatment regimes tailored to each individual. It will also entail proactive participation from all major stakeholders in the health care system. We are at the dawn of predictive, preventive, personalized, and participatory (P4) medicine, the fully implementation of which requires marrying basic and clinical researches through advanced systems thinking and the employment of high-throughput technologies in genomics, proteomics, nanofluidics, single-cell analysis, and computation strategies in a highly-orchestrated discipline we termed translational systems medicine.},
  annotation = {TLDR: The authors are at the dawn of predictive, preventive, personalized, and participatory (P4) medicine, the fully implementation of which requires marrying basic and clinical researches through advanced systems thinking and the employment of high-throughput technologies in genomics, proteomics, nanofluidics, single-cell analysis, and computation strategies in a highly-orchestrated discipline they termed translational systems medicine.},
  timestamp = {2025-05-24T09:29:12Z}
}

@article{horry2021deep,
  title = {Deep Mining Generation of Lung Cancer Malignancy Models from Chest {{X-ray}} Images},
  author = {Horry, Michael and Chakraborty, Subrata and Pradhan, Biswajeet and Paul, Manoranjan and Gomes, Douglas and {Ul-Haq}, Anwaar and Alamri, Abdullah},
  year = {2021},
  journal = {Sensors},
  volume = {21},
  number = {19},
  pages = {6655},
  publisher = {mdpi},
  timestamp = {2025-05-19T01:56:08Z}
}

@misc{hou2024selfexplainable,
  ids = {hou2024selfexplainablea},
  title = {Self-{{eXplainable AI}} for {{Medical Image Analysis}}: {{A Survey}} and {{New Outlooks}}},
  shorttitle = {Self-{{eXplainable AI}} for {{Medical Image Analysis}}},
  author = {Hou, Junlin and Liu, Sicen and Bie, Yequan and Wang, Hongmei and Tan, Andong and Luo, Luyang and Chen, Hao},
  year = {2024},
  month = nov,
  number = {arXiv:2410.02331},
  eprint = {2410.02331},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.02331},
  urldate = {2025-04-12},
  abstract = {The increasing demand for transparent and reliable models, particularly in high-stakes decision-making areas such as medical image analysis, has led to the emergence of eXplainable Artificial Intelligence (XAI). Post-hoc XAI techniques, which aim to explain black-box models after training, have raised concerns about their fidelity to model predictions. In contrast, Self-eXplainable AI (S-XAI) offers a compelling alternative by incorporating explainability directly into the training process of deep learning models. This approach allows models to generate inherent explanations that are closely aligned with their internal decision-making processes, enhancing transparency and supporting the trustworthiness, robustness, and accountability of AI systems in real-world medical applications. To facilitate the development of S-XAI methods for medical image analysis, this survey presents a comprehensive review across various image modalities and clinical applications. It covers more than 200 papers from three key perspectives: 1) input explainability through the integration of explainable feature engineering and knowledge graph, 2) model explainability via attention-based learning, concept-based learning, and prototype-based learning, and 3) output explainability by providing textual and counterfactual explanations. This paper also outlines desired characteristics of explainability and evaluation methods for assessing explanation quality, while discussing major challenges and future research directions in developing S-XAI for medical image analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {TLDR: Wanted characteristics of explainability and evaluation methods for assessing explanation quality are outlined, while discussing major challenges and future research directions in developing S-XAI for medical image analysis.},
  timestamp = {2025-08-15T10:03:38Z}
}

@misc{hu2025stable,
  title = {Stable {{Vision Concept Transformers}} for {{Medical Diagnosis}}},
  author = {Hu, Lijie and Lai, Songning and Hua, Yuan and Yang, Shu and Zhang, Jingfeng and Wang, Di},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2506.05286},
  urldate = {2025-09-05},
  abstract = {Transparency is a paramount concern in the medical field, prompting researchers to delve into the realm of explainable AI (XAI). Among these XAI methods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent space to human-understandable high-level concepts by generating a conceptual layer for extracting conceptual features, which has drawn much attention recently. However, existing methods rely solely on concept features to determine the model's predictions, which overlook the intrinsic feature embeddings within medical images. To address this utility gap between the original models and concept-based models, we propose Vision Concept Transformer (VCT). Furthermore, despite their benefits, CBMs have been found to negatively impact model performance and fail to provide stable explanations when faced with input perturbations, which limits their application in the medical field. To address this faithfulness issue, this paper further proposes the Stable Vision Concept Transformer (SVCT) based on VCT, which leverages the vision transformer (ViT) as its backbone and incorporates a conceptual layer. SVCT employs conceptual features to enhance decision-making capabilities by fusing them with image features and ensures model faithfulness through the integration of Denoised Diffusion Smoothing. Comprehensive experiments on four medical datasets demonstrate that our VCT and SVCT maintain accuracy while remaining interpretable compared to baselines. Furthermore, even when subjected to perturbations, our SVCT model consistently provides faithful explanations, thus meeting the needs of the medical field.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: The Stable Vision Concept Transformer (SVCT) based on VCT, which leverages the vision transformer (ViT) as its backbone and incorporates a conceptual layer and ensures model faithfulness through the integration of Denoised Diffusion Smoothing is proposed.},
  timestamp = {2025-09-05T13:00:31Z}
}

@article{huang2024causekg,
  title = {{{CauseKG}}: {{A Framework Enhancing Causal Inference With Implicit Knowledge Deduced From Knowledge Graphs}}},
  shorttitle = {{{CauseKG}}},
  author = {Huang, Hao and Vidal, Maria-Esther},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {61810--61827},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3395134},
  urldate = {2025-04-17},
  abstract = {Causal inference is a critical technique for inferring causal relationships from data and distinguishing causation from correlation. Causal inference frameworks rely on structured data, typically represented in flat tables or relational models. These frameworks estimate causal effects based only on explicit facts, overlooking implicit information in the data, which can lead to inaccurate causal estimates. Knowledge graphs (KGs) inherently capture implicit information through logical rules applied to explicit facts, providing a unique opportunity to leverage implicit knowledge. However, existing frameworks are not applicable to KGs due to their semi-structured nature. CauseKG is a causal inference framework designed to address the intricacies of KGs and seamlessly integrate implicit information using KG-specific entailment techniques, providing a more accurate causal inference process. We empirically evaluate the effectiveness of CauseKG against benchmarks constructed from synthetic and real-world datasets. The results suggest that CauseKG can produce a lower mean absolute error in causal inference compared to state-of-the-art methods. The empirical results demonstrate CauseKG's ability to address causal questions in a variety of domains. This research highlights the importance of extending causal inference techniques to KGs, emphasising the improved accuracy that can be achieved by integrating implicit and explicit information.},
  keywords = {Causal inference,knowledge graphs,Knowledge graphs,knowledge reasoning,OWL,Resource description framework,Reviews,semantics,Semantics,Urban areas,W3C},
  annotation = {TLDR: This research empirically evaluates the effectiveness of CauseKG against benchmarks constructed from synthetic and real-world datasets and suggests that CauseKG can produce a lower mean absolute error in causal inference compared to state-of-the-art methods.},
  timestamp = {2025-04-17T15:20:44Z}
}

@misc{huang2025surgicalvlmagent,
  title = {{{SurgicalVLM-Agent}}: {{Towards}} an {{Interactive AI Co-Pilot}} for {{Pituitary Surgery}}},
  shorttitle = {{{SurgicalVLM-Agent}}},
  author = {Huang, Jiayuan and He, Runlong and Khan, Danyal Z. and Mazomenos, Evangelos and Stoyanov, Danail and Marcus, Hani J. and Clarkson, Matthew J. and Islam, Mobarakol},
  year = {2025},
  month = mar,
  number = {arXiv:2503.09474},
  eprint = {2503.09474},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.09474},
  urldate = {2025-05-04},
  abstract = {Image-guided surgery demands adaptive, real-time decision support, yet static AI models struggle with structured task planning and providing interactive guidance. Large vision-language models (VLMs) offer a promising solution by enabling dynamic task planning and predictive decision support. We introduce SurgicalVLM-Agent, an AI co-pilot for image-guided pituitary surgery, capable of conversation, planning, and task execution. The agent dynamically processes surgeon queries and plans the tasks such as MRI tumor segmentation, endoscope anatomy segmentation, overlaying preoperative imaging with intraoperative views, instrument tracking, and surgical visual question answering (VQA). To enable structured task planning, we develop the PitAgent dataset, a surgical context-aware dataset covering segmentation, overlaying, instrument localization, tool tracking, tool-tissue interactions, phase identification, and surgical activity recognition. Additionally, we propose FFT-GaLore, a fast Fourier transform (FFT)-based gradient projection technique for efficient low-rank adaptation, optimizing fine-tuning for LLaMA 3.2 in surgical environments. We validate SurgicalVLM-Agent by assessing task planning and prompt generation on our PitAgent dataset and evaluating zero-shot VQA using a public pituitary dataset. Results demonstrate state-of-the-art performance in task planning and query interpretation, with highly semantically meaningful VQA responses, advancing AI-driven surgical assistance.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {TLDR: This work introduces SurgicalVLM-Agent, an AI co-pilot for image-guided pituitary surgery, capable of conversation, planning, and task execution, and develops the PitAgent dataset, a surgical context-aware dataset covering segmentation, overlaying, instrument localization, tool tracking, tool-tissue interactions, phase identification, and surgical activity recognition.},
  timestamp = {2025-05-04T05:52:14Z}
}

@article{hudson2015precision,
  title = {The Precision Medicine Initiative Cohort Program---{{Building}} a {{Research Foundation}} for 21st {{Century Medicine}}},
  author = {Hudson, Kathy and Lifton, Rick and {Patrick-Lake}, B and Burchard, E Gonzalez and Coles, T and Collins, R and Conrad, A},
  year = {2015},
  journal = {Precision Medicine Initiative (PMI) Working Group Report to the Advisory Committee to the Director, ed},
  timestamp = {2025-05-27T15:54:24Z}
}

@article{hulsen2023explainable,
  title = {Explainable Artificial Intelligence ({{XAI}}): Concepts and Challenges in Healthcare},
  author = {Hulsen, Tim},
  year = {2023},
  journal = {AI},
  volume = {4},
  number = {3},
  pages = {652--666},
  publisher = {MDPI},
  timestamp = {2025-04-15T13:48:45Z}
}

@misc{hussain2020ai,
  title = {{{AI Driven Knowledge Extraction}} from {{Clinical Practice Guidelines}}: {{Turning Research}} into {{Practice}}},
  shorttitle = {{{AI Driven Knowledge Extraction}} from {{Clinical Practice Guidelines}}},
  author = {Hussain, Musarrat and Hussain, Jamil and Ali, Taqdir and Satti, Fahad Ahmed and Lee, Sungyoung},
  year = {2020},
  month = dec,
  number = {arXiv:2012.05489},
  eprint = {2012.05489},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2012.05489},
  urldate = {2025-04-09},
  abstract = {Background and Objectives: Clinical Practice Guidelines (CPGs) represent the foremost methodology for sharing state-of-the-art research findings in the healthcare domain with medical practitioners to limit practice variations, reduce clinical cost, improve the quality of care, and provide evidence based treatment. However, extracting relevant knowledge from the plethora of CPGs is not feasible for already burdened healthcare professionals, leading to large gaps between clinical findings and real practices. It is therefore imperative that state-of-the-art Computing research, especially machine learning is used to provide artificial intelligence based solution for extracting the knowledge from CPGs and reducing the gap between healthcare research/guidelines and practice. Methods: This research presents a novel methodology for knowledge extraction from CPGs to reduce the gap and turn the latest research findings into clinical practice. First, our system classifies the CPG sentences into four classes such as condition-action, condition-consequences, action, and not-applicable based on the information presented in a sentence. We use deep learning with state-of-the-art word embedding, improved word vectors technique in classification process. Second, it identifies qualifier terms in the classified sentences, which assist in recognizing the condition and action phrases in a sentence. Finally, the condition and action phrase are processed and transformed into plain rule If Condition(s) Then Action format. Results: We evaluate the methodology on three different domains guidelines including Hypertension, Rhinosinusitis, and Asthma. The deep learning model classifies the CPG sentences with an accuracy of 95\%. While rule extraction was validated by user-centric approach, which achieved a Jaccard coefficient of 0.6, 0.7, and 0.4 with three human experts extracted rules, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  timestamp = {2025-04-09T11:47:29Z}
}

@misc{hussein2024explainable,
  title = {Explainable {{AI Methods}} for {{Multi-Omics Analysis}}: {{A Survey}}},
  shorttitle = {Explainable {{AI Methods}} for {{Multi-Omics Analysis}}},
  author = {Hussein, Ahmad and Prasad, Mukesh and Braytee, Ali},
  year = {2024},
  month = oct,
  number = {arXiv:2410.11910},
  eprint = {2410.11910},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.11910},
  urldate = {2025-04-17},
  abstract = {Advancements in high-throughput technologies have led to a shift from traditional hypothesis-driven methodologies to data-driven approaches. Multi-omics refers to the integrative analysis of data derived from multiple 'omes', such as genomics, proteomics, transcriptomics, metabolomics, and microbiomics. This approach enables a comprehensive understanding of biological systems by capturing different layers of biological information. Deep learning methods are increasingly utilized to integrate multi-omics data, offering insights into molecular interactions and enhancing research into complex diseases. However, these models, with their numerous interconnected layers and nonlinear relationships, often function as black boxes, lacking transparency in decision-making processes. To overcome this challenge, explainable artificial intelligence (xAI) methods are crucial for creating transparent models that allow clinicians to interpret and work with complex data more effectively. This review explores how xAI can improve the interpretability of deep learning models in multi-omics research, highlighting its potential to provide clinicians with clear insights, thereby facilitating the effective application of such models in clinical settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Genomics},
  annotation = {TLDR: This review explores how xAI can improve the interpretability of deep learning models in multi-omics research, highlighting its potential to provide clinicians with clear insights, thereby facilitating the effective application of such models in clinical settings.},
  timestamp = {2025-04-17T15:17:18Z}
}

@misc{hussein2025vision,
  title = {Vision {{Transformers}} with {{Autoencoders}} and {{Explainable AI}} for {{Cancer Patient Risk Stratification Using Whole Slide Imaging}}},
  author = {Hussein, Ahmad and Prasad, Mukesh and Anaissi, Ali and Braytee, Ali},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2504.04749},
  urldate = {2025-06-13},
  abstract = {Cancer remains one of the leading causes of mortality worldwide, necessitating accurate diagnosis and prognosis. Whole Slide Imaging (WSI) has become an integral part of clinical workflows with advancements in digital pathology. While various studies have utilized WSIs, their extracted features may not fully capture the most relevant pathological information, and their lack of interpretability limits clinical adoption.  In this paper, we propose PATH-X, a framework that integrates Vision Transformers (ViT) and Autoencoders with SHAP (Shapley Additive Explanations) to enhance model explainability for patient stratification and risk prediction using WSIs from The Cancer Genome Atlas (TCGA). A representative image slice is selected from each WSI, and numerical feature embeddings are extracted using Google's pre-trained ViT. These features are then compressed via an autoencoder and used for unsupervised clustering and classification tasks. Kaplan-Meier survival analysis is applied to evaluate stratification into two and three risk groups. SHAP is used to identify key contributing features, which are mapped onto histopathological slices to provide spatial context.  PATH-X demonstrates strong performance in breast and glioma cancers, where a sufficient number of WSIs enabled robust stratification. However, performance in lung cancer was limited due to data availability, emphasizing the need for larger datasets to enhance model reliability and clinical applicability.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV),Machine Learning (cs.LG)},
  timestamp = {2025-06-13T09:26:33Z}
}

@article{iasonos2008how,
  title = {How {{To Build}} and {{Interpret}} a {{Nomogram}} for {{Cancer Prognosis}}},
  author = {Iasonos, Alexia and Schrag, Deborah and Raj, Ganesh V. and Panageas, Katherine S.},
  year = {2008},
  month = mar,
  journal = {Journal of Clinical Oncology},
  volume = {26},
  number = {8},
  pages = {1364--1370},
  issn = {0732-183X, 1527-7755},
  doi = {10.1200/JCO.2007.12.9791},
  urldate = {2025-02-03},
  abstract = {Nomograms are widely used for cancer prognosis, primarily because of their ability to reduce statistical predictive models into a single numerical estimate of the probability of an event, such as death or recurrence, that is tailored to the profile of an individual patient. User-friendly graphical interfaces for generating these estimates facilitate the use of nomograms during clinical encounters to inform clinical decision making. However, the statistical underpinnings of these models require careful scrutiny, and the degree of uncertainty surrounding the point estimates requires attention. This guide provides a nonstatistical audience with a methodological approach for building, interpreting, and using nomograms to estimate cancer prognosis or other health outcomes.},
  langid = {english},
  annotation = {TLDR: This guide provides a nonstatistical audience with a methodological approach for building, interpreting, and using nomograms to estimate cancer prognosis or other health outcomes.},
  timestamp = {2025-02-03T11:52:06Z}
}

@article{ibrahim2023explainable,
  title = {Explainable {{Convolutional Neural Networks}}: {{A Taxonomy}}, {{Review}}, and {{Future Directions}}},
  shorttitle = {Explainable {{Convolutional Neural Networks}}},
  author = {Ibrahim, Rami and Shafiq, M. Omair},
  year = {2023},
  month = oct,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {10},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3563691},
  urldate = {2025-05-17},
  abstract = {Convolutional neural networks (CNNs)               have shown promising results and have outperformed classical machine learning techniques in tasks such as image classification and object recognition. Their human-brain like structure enabled them to learn sophisticated features while passing images through their layers. However, their lack of explainability led to the demand for interpretations to justify their predictions. Research on               Explainable AI               or               XAI               has gained momentum to provide knowledge and insights into neural networks. This study summarizes the literature to gain more understanding of explainability in CNNs (i.e., Explainable Convolutional Neural Networks). We classify models that made efforts to improve the CNNs interpretation. We present and discuss taxonomies for XAI models that modify CNN architecture, simplify CNN representations, analyze feature relevance, and visualize interpretations. We review various metrics used to evaluate XAI interpretations. In addition, we discuss the applications and tasks of XAI models. This focused and extensive survey develops a perspective on this area by addressing suggestions for overcoming XAI interpretation challenges, like models' generalization, unifying evaluation criteria, building robust models, and providing interpretations with semantic descriptions. Our taxonomy can be a reference to motivate future research in interpreting neural networks.},
  langid = {english},
  annotation = {TLDR: This study summarizes the literature to gain more understanding of explainability in CNNs (i.e., Explainable Convolutional Neural Networks), and presents and discusses taxonomies for XAI models that modify CNN architecture, simplify CNN representations, analyze feature relevance, and visualize interpretations.},
  timestamp = {2025-05-17T12:14:19Z}
}

@book{imbens2015causal,
  title = {Causal Inference in Statistics, Social, and Biomedical Sciences},
  author = {Imbens, Guido W and Rubin, Donald B},
  year = {2015},
  publisher = {Cambridge university press},
  timestamp = {2025-03-20T04:22:51Z}
}

@misc{imbens2020potential,
  title = {Potential {{Outcome}} and {{Directed Acyclic Graph Approaches}} to {{Causality}}: {{Relevance}} for {{Empirical Practice}} in {{Economics}}},
  shorttitle = {Potential {{Outcome}} and {{Directed Acyclic Graph Approaches}} to {{Causality}}},
  author = {Imbens, Guido W.},
  year = {2020},
  month = mar,
  number = {arXiv:1907.07271},
  eprint = {1907.07271},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.07271},
  urldate = {2025-03-22},
  abstract = {In this essay I discuss potential outcome and graphical approaches to causality, and their relevance for empirical work in economics. I review some of the work on directed acyclic graphs, including the recent "The Book of Why," by Pearl and MacKenzie. I also discuss the potential outcome framework developed by Rubin and coauthors, building on work by Neyman. I then discuss the relative merits of these approaches for empirical work in economics, focusing on the questions each answer well, and why much of the the work in economics is closer in spirit to the potential outcome framework.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  timestamp = {2025-03-22T07:09:11Z}
}

@misc{inc2021,
  title = {About {{DARPA}} Explainable Artificial Intelligence ({{XAI}}) Program},
  author = {Inc, Kitware},
  year = {2021},
  month = sep,
  urldate = {2025-05-28},
  howpublished = {https://xaitk.github.io/about/},
  langid = {english},
  timestamp = {2025-05-28T11:01:18Z}
}

@misc{inferring,
  title = {Inferring Gene Regulatory Networks from Time-Series {{scRNA-seq}} Data via {{GRANGER}} Causal Recurrent Autoencoders {\textbar} {{Briefings}} in {{Bioinformatics}} {\textbar} {{Oxford Academic}}},
  urldate = {2025-04-18},
  howpublished = {https://academic.oup.com/bib/article/26/2/bbaf089/8068119},
  timestamp = {2025-04-18T03:23:31Z}
}

@misc{interpretability,
  title = {Interpretability of {{Clinical Decision Support Systems Based}} on {{Artificial Intelligence}} from {{Technological}} and {{Medical Perspective}}: {{A Systematic Review}} - {{Xu}} - 2023 - {{Journal}} of {{Healthcare Engineering}} - {{Wiley Online Library}}},
  urldate = {2025-04-02},
  howpublished = {https://onlinelibrary.wiley.com/doi/10.1155/2023/9919269},
  timestamp = {2025-04-02T07:55:15Z}
}

@misc{interpretable,
  title = {Interpretable {{Machine Learning}} for {{Weather}} and {{Climate Prediction}}: {{A Survey}}},
  urldate = {2025-04-28},
  howpublished = {https://arxiv.org/html/2403.18864v1},
  timestamp = {2025-04-28T07:06:24Z}
}

@misc{interpretablea,
  title = {Interpretable {{Machine Learning}} -- {{A Brief History}}, {{State-of-the-Art}} and {{Challenges}} {\textbar} {{SpringerLink}}},
  urldate = {2025-05-09},
  howpublished = {https://link.springer.com/chapter/10.1007/978-3-030-65965-3\_28},
  timestamp = {2025-05-09T09:27:20Z}
}

@inproceedings{irvin2019chexpert,
  title = {Chexpert: {{A}} Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and others},
  year = {2019},
  volume = {33},
  pages = {590--597},
  timestamp = {2025-04-12T13:22:59Z}
}

@article{islam2022explainable,
  title = {Explainable {{Transformer-Based Deep Learning Model}} for the {{Detection}} of {{Malaria Parasites}} from {{Blood Cell Images}}},
  author = {Islam, Md Robiul and Nahiduzzaman, Md and Goni, Md Omaer Faruq and Sayeed, Abu and Anower, Md Shamim and Ahsan, Mominul and Haider, Julfikar},
  year = {2022},
  month = jun,
  journal = {Sensors (Basel, Switzerland)},
  volume = {22},
  number = {12},
  pages = {4358},
  issn = {1424-8220},
  doi = {10.3390/s22124358},
  abstract = {Malaria is a life-threatening disease caused by female anopheles mosquito bites. Various plasmodium parasites spread in the victim's blood cells and keep their life in a critical situation. If not treated at the early stage, malaria can cause even death. Microscopy is a familiar process for diagnosing malaria, collecting the victim's blood samples, and counting the parasite and red blood cells. However, the microscopy process is time-consuming and can produce an erroneous result in some cases. With the recent success of machine learning and deep learning in medical diagnosis, it is quite possible to minimize diagnosis costs and improve overall detection accuracy compared with the traditional microscopy method. This paper proposes a multiheaded attention-based transformer model to diagnose the malaria parasite from blood cell images. To demonstrate the effectiveness of the proposed model, the gradient-weighted class activation map (Grad-CAM) technique was implemented to identify which parts of an image the proposed model paid much more attention to compared with the remaining parts by generating a heatmap image. The proposed model achieved a testing accuracy, precision, recall, f1-score, and AUC score of 96.41\%, 96.99\%, 95.88\%, 96.44\%, and 99.11\%, respectively, for the original malaria parasite dataset and 99.25\%, 99.08\%, 99.42\%, 99.25\%, and 99.99\%, respectively, for the modified dataset. Various hyperparameters were also finetuned to obtain optimum results, which were also compared with state-of-the-art (SOTA) methods for malaria parasite detection, and the proposed method outperformed the existing methods.},
  langid = {english},
  pmcid = {PMC9230392},
  pmid = {35746136},
  keywords = {Animals,deep learning,Deep Learning,Erythrocytes,Female,grad-cam visualization,image analysis,Malaria,malaria parasite,Parasites,Plasmodium,transformer-based model},
  annotation = {TLDR: A multiheaded attention-based transformer model to diagnose the malaria parasite from blood cell images is proposed and the proposed method outperformed the existing methods for malaria parasite detection.},
  timestamp = {2025-07-25T14:31:14Z}
}

@article{jaimini2022causalkg,
  title = {{{CausalKG}}: {{Causal Knowledge Graph Explainability Using Interventional}} and {{Counterfactual Reasoning}}},
  shorttitle = {{{CausalKG}}},
  author = {Jaimini, Utkarshani and Sheth, Amit},
  year = {2022},
  month = jan,
  journal = {IEEE Internet Computing},
  volume = {26},
  number = {1},
  pages = {43--50},
  issn = {1089-7801, 1941-0131},
  doi = {10.1109/MIC.2021.3133551},
  urldate = {2025-04-17},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {TLDR: The human mind has an innate understanding of causality, which develops a causal model of the world, which learns with fewer data points, makes inferences, and contemplates counterfactual scenarios.},
  timestamp = {2025-04-17T15:25:33Z}
}

@article{jannati2024validation,
  title = {Validation of {{Clinical Decision Support Recommendations}} of a {{Digital Cognitive Assessment}} by {{Expert Cognitive Neurologists}}},
  author = {Jannati, Ali and Toro-Serey, Claudio and Gomes-Osman, Joyce Rios and Morrow, William Isaiah and Ciesla, Marissa C and Banks, Russell and Bates, David and Tobyne, Sean and Showalter, John and Pascual-Leone, Alvaro},
  year = {2024},
  month = dec,
  journal = {Alzheimer's \& Dementia},
  volume = {20},
  number = {S2},
  pages = {e092040},
  issn = {1552-5260, 1552-5279},
  doi = {10.1002/alz.092040},
  urldate = {2025-04-06},
  abstract = {Abstract                            Background               Disease-modifying treatments for Alzheimer's disease highlight the need for early detection of cognitive decline. However, most primary care providers do not currently perform routine cognitive testing, in part due to a lack of time and resources to administer and interpret the tests. Brief, self-scoring, and sensitive digital cognitive assessments, such as the Linus Health Core Cognitive Evaluation (CCE)--which includes the Digital Clock and Recall (DCR) and the Life and Health Questionnaire (LHQ)--can automatically provide medically-informed recommendations that can address this need. Here we evaluate the clinical appropriateness of the recommendations generated by this clinical decision support (CDS) tool to guide the diagnosis of cognitive impairment by primary care providers (PCPs).                                         Method               The CDS tool uses data from the CCE to list potential medical concerns and recommend pathways toward formal diagnosis and/or care. We conducted a retrospective expert-review study in June 2023 to evaluate the nine CDS pathways for patients aged 55 and above. Experts were five board-certified cognitive neurologists affiliated with academic institutions. We calculated the median ratings (on a scale of 1 to 9, where 9 is high appropriateness) for the nine CDS pathways across raters. A rating of 7 or above was deemed clinically appropriate.                                         Result               All 7 pathways related to cognitive impairment received a clinically appropriate rating (median = 7, SD = 0.3, range=7-8). Pathways below the appropriateness threshold included the one for Green DCR scores (i.e., cognitively unimpaired; median = 6, SD = 0.87) and a preliminary Lecanemab eligibility pathway (median = 5, SD = 1.10).                                         Conclusion               Pathways and the cognition-related recommendations generated by the CDS tool of the Linus Health CCE are clinically appropriate, as rated by this initial survey of board-certified, academic cognitive neurologists. The findings indicate the clinical utility of digital cognitive assessments such as the CCE for guiding the PCPs' approach to diagnosis and management of patients with cognitive impairment.},
  langid = {english},
  annotation = {TLDR: The findings indicate the clinical utility of digital cognitive assessments such as the CCE for guiding the PCPs' approach to diagnosis and management of patients with cognitive impairment.},
  timestamp = {2025-04-16T07:57:43Z}
}

@misc{jethani2022fastshap,
  title = {{{FastSHAP}}: {{Real-Time Shapley Value Estimation}}},
  shorttitle = {{{FastSHAP}}},
  author = {Jethani, Neil and Sudarshan, Mukund and Covert, Ian and Lee, Su-In and Ranganath, Rajesh},
  year = {2022},
  month = mar,
  number = {arXiv:2107.07436},
  eprint = {2107.07436},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.07436},
  urldate = {2025-07-05},
  abstract = {Shapley values are widely used to explain black-box models, but they are costly to calculate because they require many model evaluations. We introduce FastSHAP, a method for estimating Shapley values in a single forward pass using a learned explainer model. FastSHAP amortizes the cost of explaining many inputs via a learning approach inspired by the Shapley value's weighted least squares characterization, and it can be trained using standard stochastic gradient optimization. We compare FastSHAP to existing estimation approaches, revealing that it generates high-quality explanations with orders of magnitude speedup.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-07-05T11:59:15Z}
}

@article{ji2024construction,
  title = {Construction and Interpretation of Machine Learning-Based Prognostic Models for Survival Prediction among Intestinal-Type and Diffuse-Type Gastric Cancer Patients},
  author = {Ji, Kunxiang and Shi, Lei and Feng, Yan and Wang, Linna and Guo, HuanNan and Li, Hui and Xing, Jiacheng and Xia, Siyu and Xu, Boran and Liu, Eryu and Zheng, YanDan and Li, Chunfeng and Liu, Mingyang},
  year = {2024},
  month = oct,
  journal = {World Journal of Surgical Oncology},
  volume = {22},
  number = {1},
  pages = {275},
  issn = {1477-7819},
  doi = {10.1186/s12957-024-03550-y},
  urldate = {2025-08-11},
  abstract = {Gastric cancer is one of the most common malignant tumors worldwide, with high incidence and mortality rates, and it has a complex etiology and complex pathological features. Depending on the tumor type, gastric cancer can be classified as intestinal-type and diffuse-type gastric cancer, each with distinct pathogenic mechanisms and clinical presentations. In recent years, machine learning techniques have been widely applied in the medical field, offering new perspectives for the diagnosis, treatment, and prognosis of gastric cancer patients.},
  keywords = {Diffuse-type,Gastric cancer,Intestinal-type,Machine learning,Prognosis},
  annotation = {TLDR: Machine learning shows great potential in predicting survival outcomes of gastric cancer patients, providing strong support for the development of personalized treatment plans.},
  timestamp = {2025-08-11T03:48:47Z}
}

@article{jian2024predicting,
  title = {Predicting Progression-Free Survival in Patients with Epithelial Ovarian Cancer Using an Interpretable Random Forest Model},
  author = {Jian, Lian and Chen, Xiaoyan and Hu, Pingsheng and Li, Handong and Fang, Chao and Wang, Jing and Wu, Nayiyuan and Yu, Xiaoping},
  year = {2024},
  month = aug,
  journal = {Heliyon},
  volume = {10},
  number = {15},
  pages = {e35344},
  issn = {24058440},
  doi = {10.1016/j.heliyon.2024.e35344},
  urldate = {2025-08-11},
  langid = {english},
  annotation = {TLDR: The Shapley additive explanation-based interpretation of the prognostic model enables clinicians to understand the reasoning behind predictions better and predicts progression-free survival in patients with epithelial ovarian cancer using clinical variables and radiomics features.},
  timestamp = {2025-08-11T07:05:04Z}
}

@article{jiang2019clinical,
  title = {A Clinical Decision Support System Learned from Data to Personalize Treatment Recommendations towards Preventing Breast Cancer Metastasis},
  author = {Jiang, Xia and Wells, Alan and Brufsky, Adam and Neapolitan, Richard},
  year = {2019},
  journal = {PloS One},
  volume = {14},
  number = {3},
  pages = {e0213292},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0213292},
  abstract = {OBJECTIVE: A Clinical Decision Support System (CDSS) that can amass Electronic Health Record (EHR) and other patient data holds promise to provide accurate classification and guide treatment choices. Our objective is to develop the Decision Support System for Making Personalized Assessments and Recommendations Concerning Breast Cancer Patients (DPAC), which is a CDSS learned from data that recommends the optimal treatment decisions based on a patient's features. METHOD: We developed a Bayesian network architecture called Causal Modeling with Internal Layers (CAMIL), and an algorithm called Treatment Feature Interactions (TFI), which learns from data the interactions needed in a CAMIL model. Using the TFI algorithm, we learned interactions for six treatments from the LSDS-5YDM dataset. We created a CAMIL model using these interactions, resulting in a DPAC which recommends treatments towards preventing 5-year breast cancer metastasis. RESULTS: In a 5-fold cross-validation analysis, we compared the probability of being metastasis free in 5 years for patients who made decisions recommended by DPAC to those who did not. These probabilities are (the probability for those making the decisions appears first): chemotherapy (.938, .872); breast/chest wall radiation (.939, .902); nodal field radiation (.940, .784); antihormone (.941, .906); HER2 inhibitors (.934, .880); neadjuvant therapy (.931, .837). In an application of DPAC to the independent METABRIC dataset, the probabilities for chemotherapy were (.845, .788). DISCUSSION: Patients who took the advice of DPAC had, as a group, notably better outcomes than those who did not. We conclude that DPAC is effective at amassing and analyzing data towards treatment recommendations. Some of the findings in DPAC are controversial. For example, DPAC says that chemotherapy increases the chances of metastasis for many node negative patients. This controversy shows the importance of developing a conclusive version of DPAC to ensure we provide patients with the best patient-specific treatment recommendations.},
  langid = {english},
  pmcid = {PMC6407919},
  pmid = {30849111},
  keywords = {Adolescent,Adult,Algorithms,Bayes Theorem,Breast Neoplasms,Child,Child Preschool,Combined Modality Therapy,Decision Support Systems Clinical,Female,Humans,Infant,Infant Newborn,Middle Aged,Models Theoretical,Neoplasm Metastasis,Practice Guidelines as Topic,Precision Medicine,Young Adult},
  annotation = {TLDR: The Decision Support System for Making Personalized Assessments and Recommendations Concerning Breast Cancer Patients (DPAC), which is a CDSS learned from data that recommends the optimal treatment decisions based on a patient's features, is developed.},
  timestamp = {2025-05-14T02:39:26Z}
}

@misc{jiang2021fusion,
  title = {Fusion of Medical Imaging and Electronic Health Records with Attention and Multi-Head Machanisms},
  author = {Jiang, Cheng and Chen, Yihao and Chang, Jianbo and Feng, Ming and Wang, Renzhi and Yao, Jianhua},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2112.11710},
  urldate = {2025-06-13},
  abstract = {Doctors often make diagonostic decisions based on patient's image scans, such as magnetic resonance imaging (MRI), and patient's electronic health records (EHR) such as age, gender, blood pressure and so on. Despite a lot of automatic methods have been proposed for either image or text analysis in computer vision or natural language research areas, much fewer studies have been developed for the fusion of medical image and EHR data for medical problems. Among existing early or intermediate fusion methods, concatenation of features from both modalities is still a mainstream. For a better exploiting of image and EHR data, we propose a multi-modal attention module which use EHR data to help the selection of important regions during image feature extraction process conducted by traditional CNN. Moreover, we propose to incorporate multi-head machnism to gated multimodal unit (GMU) to make it able to parallelly fuse image and EHR features in different subspaces. With the help of the two modules, existing CNN architecture can be enhanced using both modalities. Experiments on predicting Glasgow outcome scale (GOS) of intracerebral hemorrhage patients and classifying Alzheimer's Disease showed the proposed method can automatically focus on task-related areas and achieve better results by making better use of image and EHR features.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  timestamp = {2025-06-13T07:04:45Z}
}

@article{jiang2023health,
  ids = {jiang2023},
  title = {Health System-Scale Language Models Are All-Purpose Prediction Engines},
  author = {Jiang, Lavender Yao and Liu, Xujin Chris and Nejatian, Nima Pour and {Nasir-Moin}, Mustafa and Wang, Duo and Abidin, Anas and Eaton, Kevin and Riina, Howard Antony and Laufer, Ilya and Punjabi, Paawan and Miceli, Madeline and Kim, Nora C. and Orillac, Cordelia and Schnurman, Zane and Livia, Christopher and Weiss, Hannah and Kurland, David and Neifert, Sean and Dastagirzada, Yosef and Kondziolka, Douglas and Cheung, Alexander T. M. and Yang, Grace and Cao, Ming and Flores, Mona and Costa, Anthony B. and Aphinyanaphongs, Yindalon and Cho, Kyunghyun and Oermann, Eric Karl},
  year = {2023},
  month = jul,
  journal = {Nature},
  volume = {619},
  number = {7969},
  pages = {357--362},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06160-y},
  urldate = {2025-04-03},
  abstract = {Physicians make critical time-constrained decisions every day. Clinical predictive models can help physicians and administrators make decisions by forecasting clinical and operational events. Existing structured data-based clinical predictive models have limited use in everyday practice owing to complexity in data processing, as well as model development and deployment1--3. Here we show that unstructured clinical notes from the electronic health record can enable the training of clinical language models, which can be used as all-purpose clinical predictive engines with low-resistance development and deployment. Our approach leverages recent advances in natural language processing4,5 to train a large language model for medical language (NYUTron) and subsequently fine-tune it across a wide range of clinical and operational predictive tasks. We evaluated our approach within our health system for five such tasks: 30-day all-cause readmission prediction, in-hospital mortality prediction, comorbidity index prediction, length of stay prediction, and insurance denial prediction. We show that NYUTron has an area under the curve (AUC) of 78.7--94.9\%, with an improvement of 5.36--14.7\% in the AUC compared with traditional models. We additionally demonstrate the benefits of pretraining with clinical text, the potential for increasing generalizability to different sites through fine-tuning and the full deployment of our system in a prospective, single-arm trial. These results show the potential for using clinical language models in medicine to read alongside physicians and provide guidance at the point of care.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational science,Translational research},
  annotation = {TLDR: The potential for using clinical language models in medicine to read alongside physicians and provide guidance at the point of care is shown, as a clinical language model trained on unstructured clinical notes from the electronic health record enhances prediction of clinical and operational events.},
  timestamp = {2025-04-03T12:35:03Z}
}

@article{jiang2024autosurv,
  title = {Autosurv: Interpretable Deep Learning Framework for Cancer Survival Analysis Incorporating Clinical and Multi-Omics Data},
  shorttitle = {Autosurv},
  author = {Jiang, Lindong and Xu, Chao and Bai, Yuntong and Liu, Anqi and Gong, Yun and Wang, Yu-Ping and Deng, Hong-Wen},
  year = {2024},
  month = jan,
  journal = {npj Precision Oncology},
  volume = {8},
  number = {1},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {2397-768X},
  doi = {10.1038/s41698-023-00494-6},
  urldate = {2025-07-25},
  abstract = {AbstractAccurate prognosis for cancer patients can provide critical information for optimizing treatment plans and improving life quality. Combining omics data and demographic/clinical information can offer a more comprehensive view of cancer prognosis than using omics or clinical data alone and can also reveal the underlying disease mechanisms at the molecular level. In this study, we developed and validated a deep learning framework to extract information from high-dimensional gene expression and miRNA expression data and conduct prognosis prediction for breast cancer and ovarian-cancer patients using multiple independent multi-omics datasets. Our model achieved significantly better prognosis prediction than the current machine learning and deep learning approaches in various settings. Moreover, an interpretation method was applied to tackle the ``black-box'' nature of deep neural networks and we identified features (i.e., genes, miRNA, demographic/clinical variables) that were important to distinguish predicted high- and low-risk patients. The significance of the identified features was partially supported by previous studies.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  annotation = {TLDR: A deep learning framework was developed and validated to extract information from high-dimensional gene expression and miRNA expression data and conduct prognosis prediction for breast cancer and ovarian-cancer patients using multiple independent multi-omics datasets and achieved significantly better prognosis prediction than the current machine learning and deep learning approaches in various settings.},
  timestamp = {2025-07-25T13:52:06Z}
}

@misc{jiang2025knowledgea,
  title = {A {{Knowledge Distillation-Based Approach}} to {{Enhance Transparency}} of {{Classifier Models}}},
  author = {Jiang, Yuchen and Zhao, Xinyuan and Wu, Yihang and Chaddad, Ahmad},
  year = {2025},
  month = feb,
  number = {arXiv:2502.15959},
  eprint = {2502.15959},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.15959},
  urldate = {2025-04-10},
  abstract = {With the rapid development of artificial intelligence (AI), especially in the medical field, the need for its explainability has grown. In medical image analysis, a high degree of transparency and model interpretability can help clinicians better understand and trust the decision-making process of AI models. In this study, we propose a Knowledge Distillation (KD)-based approach that aims to enhance the transparency of the AI model in medical image analysis. The initial step is to use traditional CNN to obtain a teacher model and then use KD to simplify the CNN architecture, retain most of the features of the data set, and reduce the number of network layers. It also uses the feature map of the student model to perform hierarchical analysis to identify key features and decision-making processes. This leads to intuitive visual explanations. We selected three public medical data sets (brain tumor, eye disease, and Alzheimer's disease) to test our method. It shows that even when the number of layers is reduced, our model provides a remarkable result in the test set and reduces the time required for the interpretability analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {TLDR: A Knowledge Distillation-based approach that aims to enhance the transparency of the AI model in medical image analysis by using KD to simplify the CNN architecture, retain most of the features of the data set, and reduce the number of network layers.},
  timestamp = {2025-04-10T14:02:17Z}
}

@article{jiao2024causal,
  title = {Causal {{Inference Meets Deep Learning}}: {{A Comprehensive Survey}}},
  shorttitle = {Causal {{Inference Meets Deep Learning}}},
  author = {Jiao, Licheng and Wang, Yuhan and Liu, Xu and Li, Lingling and Liu, Fang and Ma, Wenping and Guo, Yuwei and Chen, Puhua and Yang, Shuyuan and Hou, Biao},
  year = {2024},
  month = jan,
  journal = {Research},
  volume = {7},
  pages = {0467},
  issn = {2639-5274},
  doi = {10.34133/research.0467},
  urldate = {2025-04-17},
  abstract = {Deep learning relies on learning from extensive data to generate prediction results. This approach may inadvertently capture spurious correlations within the data, leading to models that lack interpretability and robustness. Researchers have developed more profound and stable causal inference methods based on cognitive neuroscience. By replacing the correlation model with a stable and interpretable causal model, it is possible to mitigate the misleading nature of spurious correlations and overcome the limitations of model calculations. In this survey, we provide a comprehensive and structured review of causal inference methods in deep learning. Brain-like inference ideas are discussed from a brain-inspired perspective, and the basic concepts of causal learning are introduced. The article describes the integration of causal inference with traditional deep learning algorithms and illustrates its application to large model tasks as well as specific modalities in deep learning. The current limitations of causal inference and future research directions are discussed. Moreover, the commonly used benchmark datasets and the corresponding download links are summarized.},
  langid = {english},
  annotation = {TLDR: The article describes the integration of causal inference with traditional deep learning algorithms and illustrates its application to large model tasks as well as specific modalities in deep learning.},
  timestamp = {2025-04-17T15:33:27Z}
}

@article{jin2022guidelines,
  title = {Guidelines and {{Evaluation}} of {{Clinical Explainable AI}} in {{Medical Image Analysis}}},
  author = {Jin, Weina and Li, Xiaoxiao and Fatehi, Mostafa and Hamarneh, Ghassan},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2202.10553},
  urldate = {2025-06-13},
  abstract = {Explainable artificial intelligence (XAI) is essential for enabling clinical users to get informed decision support from AI and comply with evidence-based medical practice. Applying XAI in clinical settings requires proper evaluation criteria to ensure the explanation technique is both technically sound and clinically useful, but specific support is lacking to achieve this goal. To bridge the research gap, we propose the Clinical XAI Guidelines that consist of five criteria a clinical XAI needs to be optimized for. The guidelines recommend choosing an explanation form based on Guideline 1 (G1) Understandability and G2 Clinical relevance. For the chosen explanation form, its specific XAI technique should be optimized for G3 Truthfulness, G4 Informative plausibility, and G5 Computational efficiency. Following the guidelines, we conducted a systematic evaluation on a novel problem of multi-modal medical image explanation with two clinical tasks, and proposed new evaluation metrics accordingly. Sixteen commonly-used heatmap XAI techniques were evaluated and found to be insufficient for clinical use due to their failure in G3 and G4. Our evaluation demonstrated the use of Clinical XAI Guidelines to support the design and evaluation of clinically viable XAI.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {92C55 92C50 68T45 68T01,Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV),Machine Learning (cs.LG)},
  timestamp = {2025-06-13T12:05:34Z}
}

@article{jin2023can,
  title = {Can Large Language Models Infer Causation from Correlation?},
  author = {Jin, Zhijing and Liu, Jiarui and Lyu, Zhiheng and Poff, Spencer and Sachan, Mrinmaya and Mihalcea, Rada and Diab, Mona and Sch{\"o}lkopf, Bernhard},
  year = {2023},
  journal = {arXiv preprint arXiv:2306.05836},
  eprint = {2306.05836},
  archiveprefix = {arXiv},
  timestamp = {2025-03-23T10:31:16Z}
}

@article{jin2023guidelines,
  ids = {jin2023guidelinesa},
  title = {Guidelines and Evaluation of Clinical Explainable {{AI}} in Medical Image Analysis},
  author = {Jin, Weina and Li, Xiaoxiao and Fatehi, Mostafa and Hamarneh, Ghassan},
  year = {2023},
  journal = {Medical image analysis},
  volume = {84},
  pages = {102684},
  publisher = {Elsevier},
  issn = {13618415},
  doi = {10.1016/j.media.2022.102684},
  urldate = {2025-05-02},
  abstract = {Explainable artificial intelligence (XAI) is essential for enabling clinical users to get informed decision support from AI and comply with evidence-based medical practice. Applying XAI in clinical settings requires proper evaluation criteria to ensure the explanation technique is both technically sound and clinically useful, but specific support is lacking to achieve this goal. To bridge the research gap, we propose the Clinical XAI Guidelines that consist of five criteria a clinical XAI needs to be optimized for. The guidelines recommend choosing an explanation form based on Guideline 1 (G1) Understandability and G2 Clinical relevance. For the chosen explanation form, its specific XAI technique should be optimized for G3 Truthfulness, G4 Informative plausibility, and G5 Computational efficiency. Following the guidelines, we conducted a systematic evaluation on a novel problem of multi-modal medical image explanation with two clinical tasks, and proposed new evaluation metrics accordingly. Sixteen commonly-used heatmap XAI techniques were evaluated and found to be insufficient for clinical use due to their failure in G3 and G4. Our evaluation demonstrated the use of Clinical XAI Guidelines to support the design and evaluation of clinically viable XAI.},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  annotation = {TLDR: The use of Clinical XAI Guidelines to support the design and evaluation of clinically viable XAI techniques and proposed new evaluation metrics accordingly are demonstrated.},
  timestamp = {2025-05-02T13:42:22Z}
}

@misc{jin2024impact,
  title = {The {{Impact}} of {{Reasoning Step Length}} on {{Large Language Models}}},
  author = {Jin, Mingyu and Yu, Qinkai and Shu, Dong and Zhao, Haiyan and Hua, Wenyue and Meng, Yanda and Zhang, Yongfeng and Du, Mengnan},
  year = {2024},
  month = jun,
  number = {arXiv:2401.04925},
  eprint = {2401.04925},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.04925},
  urldate = {2025-03-03},
  abstract = {Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences. The code is available at https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  timestamp = {2025-03-03T08:02:33Z}
}

@misc{johansson2023generalization,
  title = {Generalization {{Bounds}} and {{Representation Learning}} for {{Estimation}} of {{Potential Outcomes}} and {{Causal Effects}}},
  author = {Johansson, Fredrik D. and Shalit, Uri and Kallus, Nathan and Sontag, David},
  year = {2023},
  month = jul,
  number = {arXiv:2001.07426},
  eprint = {2001.07426},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.07426},
  urldate = {2025-03-24},
  abstract = {Practitioners in diverse fields such as healthcare, economics and education are eager to apply machine learning to improve decision making. The cost and impracticality of performing experiments and a recent monumental increase in electronic record keeping has brought attention to the problem of evaluating decisions based on non-experimental observational data. This is the setting of this work. In particular, we study estimation of individual-level causal effects, such as a single patient's response to alternative medication, from recorded contexts, decisions and outcomes. We give generalization bounds on the error in estimated effects based on distance measures between groups receiving different treatments, allowing for sample re-weighting. We provide conditions under which our bound is tight and show how it relates to results for unsupervised domain adaptation. Led by our theoretical results, we devise representation learning algorithms that minimize our bound, by regularizing the representation's induced treatment group distance, and encourage sharing of information between treatment groups. We extend these algorithms to simultaneously learn a weighted representation to further reduce treatment group distances. Finally, an experimental evaluation on real and synthetic data shows the value of our proposed representation architecture and regularization scheme.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-24T08:08:12Z}
}

@article{johnson2016mimiciii,
  title = {{{MIMIC-III}}, a Freely Accessible Critical Care Database},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
  year = {2016},
  month = may,
  journal = {Scientific Data},
  volume = {3},
  number = {1},
  pages = {160035},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.35},
  urldate = {2025-04-09},
  abstract = {MIMIC-III (`Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Diagnosis,Health care,Medical research,Outcomes research,Prognosis},
  timestamp = {2025-04-09T02:53:48Z}
}

@article{johnson2018generalizability,
  title = {Generalizability of Predictive Models for Intensive Care Unit Patients},
  author = {Johnson, Alistair E. W. and Pollard, T. and Naumann, Tristan},
  year = {2018},
  month = nov,
  journal = {ArXiv},
  urldate = {2025-04-04},
  abstract = {A large volume of research has considered the creation of predictive models for clinical data; however, much existing literature reports results using only a single source of data. In this work, we evaluate the performance of models trained on the publicly-available eICU Collaborative Research Database. We show that cross-validation using many distinct centers provides a reasonable estimate of model performance in new centers. We further show that a single model trained across centers transfers well to distinct hospitals, even compared to a model retrained using hospital-specific data. Our results motivate the use of multi-center datasets for model development and highlight the need for data sharing among hospitals to maximize model performance.},
  timestamp = {2025-04-04T02:58:35Z}
}

@article{joseph2022automated,
  title = {Automated Data Extraction of Electronic Medical Records: {{Validity}} of Data Mining to Construct Research Databases for Eligibility in Gastroenterological Clinical Trials},
  shorttitle = {Automated Data Extraction of Electronic Medical Records},
  author = {Joseph, Nora and Lindblad, Ida and Zaker, Sara and Elfversson, Sharareh and Albinzon, Maria and {\O}deg{\aa}rd, {\O}yvind and Hantler, Li and Hellstr{\"o}m, Per M.},
  year = {2022},
  journal = {Upsala Journal of Medical Sciences},
  volume = {127},
  issn = {2000-1967},
  doi = {10.48101/ujms.v127.8260},
  abstract = {BACKGROUND: Electronic medical records (EMRs) are adopted for storing patient-related healthcare information. Using data mining techniques, it is possible to make use of and derive benefit from this massive amount of data effectively. We aimed to evaluate validity of data extracted by the Customized eXtraction Program (CXP). METHODS: The CXP extracts and structures data in rapid standardised processes. The CXP was programmed to extract TNF{$\alpha$}-native active ulcerative colitis (UC) patients from EMRs using defined International Classification of Disease-10 (ICD-10) codes. Extracted data were read in parallel with manual assessment of the EMR to compare with CXP-extracted data. RESULTS: From the complete EMR set, 2,802 patients with code K51 (UC) were extracted. Then, CXP extracted 332 patients according to inclusion and exclusion criteria. Of these, 97.5\% were correctly identified, resulting in a final set of 320 cases eligible for the study. When comparing CXP-extracted data against manually assessed EMRs, the recovery rate was 95.6-101.1\% over the years with 96.1\% weighted average sensitivity. CONCLUSION: Utilisation of the CXP software can be considered as an effective way to extract relevant EMR data without significant errors. Hence, by extracting from EMRs, CXP accurately identifies patients and has the capacity to facilitate research studies and clinical trials by finding patients with the requested code as well as funnel down itemised individuals according to specified inclusion and exclusion criteria. Beyond this, medical procedures and laboratory data can rapidly be retrieved from the EMRs to create tailored databases of extracted material for immediate use in clinical trials.},
  langid = {english},
  pmcid = {PMC8809051},
  pmid = {35173908},
  keywords = {Big data,data analytics,data extraction,data mining,Data Mining,Databases Factual,Electronic Health Records,electronic medical records,Humans,International Classification of Diseases},
  timestamp = {2025-04-09T11:55:34Z}
}

@misc{joshi2019realistic,
  title = {Towards {{Realistic Individual Recourse}} and {{Actionable Explanations}} in {{Black-Box Decision Making Systems}}},
  author = {Joshi, Shalmali and Koyejo, Oluwasanmi and Vijitbenjaronk, Warut and Kim, Been and Ghosh, Joydeep},
  year = {2019},
  month = jul,
  number = {arXiv:1907.09615},
  eprint = {1907.09615},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.09615},
  urldate = {2025-05-18},
  abstract = {Machine learning based decision making systems are increasingly affecting humans. An individual can suffer an undesirable outcome under such decision making systems (e.g. denied credit) irrespective of whether the decision is fair or accurate. Individual recourse pertains to the problem of providing an actionable set of changes a person can undertake in order to improve their outcome. We propose a recourse algorithm that models the underlying data distribution or manifold. We then provide a mechanism to generate the smallest set of changes that will improve an individual's outcome. This mechanism can be easily used to provide recourse for any differentiable machine learning based decision making system. Further, the resulting algorithm is shown to be applicable to both supervised classification and causal decision making systems. Our work attempts to fill gaps in existing fairness literature that have primarily focused on discovering and/or algorithmically enforcing fairness constraints on decision making systems. This work also provides an alternative approach to generating counterfactual explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-05-18T12:50:25Z}
}

@article{joskowicz2025artificial,
  title = {Artificial {{Intelligence}} Interpretation of Chest Radiographs in Intensive Care. {{Ready}} for Prime Time?},
  author = {Joskowicz, Leo and Beil, Michael and Sviri, Sigal},
  year = {2025},
  month = jan,
  journal = {Intensive Care Medicine},
  volume = {51},
  number = {1},
  pages = {154--156},
  issn = {1432-1238},
  doi = {10.1007/s00134-024-07725-9},
  urldate = {2025-03-03},
  langid = {english},
  timestamp = {2025-03-03T07:57:46Z}
}

@article{joyce2023explainable,
  title = {Explainable Artificial Intelligence for Mental Health through Transparency and Interpretability for Understandability},
  author = {Joyce, Dan W. and Kormilitzin, Andrey and Smith, Katharine A. and Cipriani, Andrea},
  year = {2023},
  month = jan,
  journal = {npj Digital Medicine},
  volume = {6},
  number = {1},
  pages = {1--7},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00751-9},
  urldate = {2025-03-03},
  abstract = {The literature on artificial intelligence (AI) or machine learning (ML) in mental health and psychiatry lacks consensus on what ``explainability'' means. In the more general XAI (eXplainable AI) literature, there has been some convergence on explainability meaning model-agnostic techniques that augment a complex model (with internal mechanics intractable for human understanding) with a simpler model argued to deliver results that humans can comprehend. Given the differing usage and intended meaning of the term ``explainability'' in AI and ML, we propose instead to approximate model/algorithm explainability by understandability defined as a function of transparency and interpretability. These concepts are easier to articulate, to ``ground'' in our understanding of how algorithms and models operate and are used more consistently in the literature. We describe the TIFU (Transparency and Interpretability For Understandability) framework and examine how this applies to the landscape of AI/ML in mental health research. We argue that the need for understandablity is heightened in psychiatry because data describing the syndromes, outcomes, disorders and signs/symptoms possess probabilistic relationships to each other---as do the tentative aetiologies and multifactorial social- and psychological-determinants of disorders. If we develop and deploy AI/ML models, ensuring human understandability of the inputs, processes and outputs of these models is essential to develop trustworthy systems fit for deployment.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Health care},
  annotation = {TLDR: It is argued that the need for understandablity is heightened in psychiatry because data describing the syndromes, outcomes, disorders and signs/symptoms possess probabilistic relationships to each other---as do the tentative aetiologies and multifactorial social- and psychological-determinants of disorders.},
  timestamp = {2025-03-14T11:09:47Z}
}

@article{jumper2021highly,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2025-05-04},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50~years9. Despite recent progress10--14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology},
  annotation = {TLDR: This work validated an entirely redesigned version of the neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods.},
  timestamp = {2025-05-04T13:48:26Z}
}

@article{kalyakulina2022disease,
  title = {Disease Classification for Whole-Blood {{DNA}} Methylation: {{Meta-analysis}}, Missing Values Imputation, and {{XAI}}},
  shorttitle = {Disease Classification for Whole-Blood {{DNA}} Methylation},
  author = {Kalyakulina, Alena and Yusipov, Igor and Bacalini, Maria Giulia and Franceschi, Claudio and Vedunova, Maria and Ivanchenko, Mikhail},
  year = {2022},
  month = oct,
  journal = {GigaScience},
  volume = {11},
  pages = {giac097},
  issn = {2047-217X},
  doi = {10.1093/gigascience/giac097},
  abstract = {BACKGROUND: DNA methylation has a significant effect on gene expression and can be associated with various diseases. Meta-analysis of available DNA methylation datasets requires development of a specific workflow for joint data processing. RESULTS: We propose a comprehensive approach of combined DNA methylation datasets to classify controls and patients. The solution includes data harmonization, construction of machine learning classification models, dimensionality reduction of models, imputation of missing values, and explanation of model predictions by explainable artificial intelligence (XAI) algorithms. We show that harmonization can improve classification accuracy by up to 20\% when preprocessing methods of the training and test datasets are different. The best accuracy results were obtained with tree ensembles, reaching above 95\% for Parkinson's disease. Dimensionality reduction can substantially decrease the number of features, without detriment to the classification accuracy. The best imputation methods achieve almost the same classification accuracy for data with missing values as for the original data. XAI approaches have allowed us to explain model predictions from both populational and individual perspectives. CONCLUSIONS: We propose a methodologically valid and comprehensive approach to the classification of healthy individuals and patients with various diseases based on whole-blood DNA methylation data using Parkinson's disease and schizophrenia as examples. The proposed algorithm works better for the former pathology, characterized by a complex set of symptoms. It allows to solve data harmonization problems for meta-analysis of many different datasets, impute missing values, and build classification models of small dimensionality.},
  langid = {english},
  pmcid = {PMC9718659},
  pmid = {36259657},
  keywords = {Algorithms,Artificial Intelligence,data harmonization,DNA methylation,DNA Methylation,explainable artificial intelligence,Humans,machine learning,Machine Learning,Parkinson Disease},
  annotation = {TLDR: A methodologically valid and comprehensive approach to the classification of healthy individuals and patients with various diseases based on whole blood DNA methylation data using Parkinson's disease and schizophrenia as examples is proposed.},
  timestamp = {2025-07-25T07:38:03Z}
}

@article{kamal2021alzheimers,
  title = {Alzheimer's {{Patient Analysis Using Image}} and {{Gene Expression Data}} and {{Explainable-AI}} to {{Present Associated Genes}}},
  author = {Kamal, Md. Sarwar and Northcote, Aden and Chowdhury, Linkon and Dey, Nilanjan and Crespo, Rub{\'e}n Gonz{\'a}lez and {Herrera-Viedma}, Enrique},
  year = {2021},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  volume = {70},
  pages = {1--7},
  issn = {1557-9662},
  doi = {10.1109/TIM.2021.3107056},
  urldate = {2025-05-17},
  abstract = {There are more than 10 million new cases of Alzheimer's patients worldwide each year, which means there is a new case every 3.2 s. Alzheimer's disease (AD) is a progressive neurodegenerative disease and various machine learning (ML) and image processing methods have been used to detect it. In this study, we used ML methods to classify AD using image and gene expression data. First, SpinalNet and convolutional neural network (CNN) were used to classify AD from MRI images. Then we used microarray gene expression data to classify the diseases using k-nearest neighbors (KNN), support vector classifier (SVC), and Xboost classifiers. Previous approaches used only either images or gene expression, while we used both data together and also explained the results using trustworthy methods. it was difficult to understand how the classifiers predicted the diseases and genes. It would be useful if the results of these classifiers could be explained in a trustworthy way. To establish trustworthy predictive modeling, we introduced an explainable artificial intelligence (XAI) method. The XAI approach we used here is local interpretable model-agnostic explanations (LIME) for a simple human interpretation. LIME interprets how genes were predicted and which genes are particularly responsible for an AD patient. The accuracy of CNN is 97.6\%, which is 10.96\% higher than the SpinlNet approach. When analyzing gene expression data, SVC provides higher accuracy than other approaches. LIME shows how genes were selected for a particular AD patient and the most important genes for that patient were determined from the gene expression data.},
  langid = {american},
  keywords = {Alzheimer's disease,Convolutional neural network (CNN),Convolutional neural networks,Diseases,Gene expression,gene expression measurements,k-nearest neighbors (KNN),local interpretable model-agnostic explanations (LIMEs),Magnetic resonance imaging,Neurons,SpinalNet,support vector classifier (SVC),Training,Xboost},
  annotation = {TLDR: To establish trustworthy predictive modeling, an explainable artificial intelligence (XAI) method is introduced that shows how genes were selected for a particular AD patient and the most important genes for that patient were determined from the gene expression data.},
  timestamp = {2025-05-17T12:01:51Z}
}

@article{karim2021deep,
  title = {Deep Learning-Based Clustering Approaches for Bioinformatics},
  author = {Karim, Md Rezaul and Beyan, Oya and Zappa, Achille and Costa, Ivan G and {Rebholz-Schuhmann}, Dietrich and Cochez, Michael and Decker, Stefan},
  year = {2021},
  month = jan,
  journal = {Briefings in Bioinformatics},
  volume = {22},
  number = {1},
  pages = {393--415},
  publisher = {Oxford University Press (OUP)},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbz170},
  urldate = {2025-07-26},
  abstract = {AbstractClustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  timestamp = {2025-07-26T08:16:52Z}
}

@article{karim2023explainable,
  title = {Explainable {{AI}} for {{Bioinformatics}}: {{Methods}}, {{Tools}} and {{Applications}}},
  shorttitle = {Explainable {{AI}} for {{Bioinformatics}}},
  author = {Karim, Md Rezaul and Islam, Tanhim and Shajalal, Md and Beyan, Oya and Lange, Christoph and Cochez, Michael and {Rebholz-Schuhmann}, Dietrich and Decker, Stefan},
  year = {2023},
  month = sep,
  journal = {Briefings in Bioinformatics},
  volume = {24},
  number = {5},
  pages = {bbad236},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbad236},
  urldate = {2025-05-14},
  abstract = {Abstract             Artificial intelligence (AI) systems utilizing deep neural networks and machine learning (ML) algorithms are widely used for solving critical problems in bioinformatics, biomedical informatics and precision medicine. However, complex ML models that are often perceived as opaque and black-box methods make it difficult to understand the reasoning behind their decisions. This lack of transparency can be a challenge for both end-users and decision-makers, as well as AI developers. In sensitive areas such as healthcare, explainability and accountability are not only desirable properties but also legally required for AI systems that can have a significant impact on human lives. Fairness is another growing concern, as algorithmic decisions should not show bias or discrimination towards certain groups or individuals based on sensitive attributes. Explainable AI (XAI) aims to overcome the opaqueness of black-box models and to provide transparency in how AI systems make decisions. Interpretable ML models can explain how they make predictions and identify factors that influence their outcomes. However, the majority of the state-of-the-art interpretable ML methods are domain-agnostic and have evolved from fields such as computer vision, automated reasoning or statistics, making direct application to bioinformatics problems challenging without customization and domain adaptation. In this paper, we discuss the importance of explainability and algorithmic transparency in the context of bioinformatics. We provide an overview of model-specific and model-agnostic interpretable ML methods and tools and outline their potential limitations. We discuss how existing interpretable ML methods can be customized and fit to bioinformatics research problems. Further, through case studies in bioimaging, cancer genomics and text mining, we demonstrate how XAI methods can improve transparency and decision fairness. Our review aims at providing valuable insights and serving as a starting point for researchers wanting to enhance explainability and decision transparency while solving bioinformatics problems. GitHub: https://github.com/rezacsedu/XAI-for-bioinformatics.},
  copyright = {https://academic.oup.com/pages/standard-publication-reuse-rights},
  langid = {english},
  timestamp = {2025-05-14T10:31:59Z}
}

@inproceedings{karras2019style,
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year = {2019},
  pages = {4401--4410},
  timestamp = {2025-05-18T13:02:00Z}
}

@misc{ke2020learning,
  title = {Learning {{Neural Causal Models}} from {{Unknown Interventions}}},
  author = {Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Sch{\"o}lkopf, Bernhard and Mozer, Michael C. and Pal, Chris and Bengio, Yoshua},
  year = {2020},
  month = aug,
  number = {arXiv:1910.01075},
  eprint = {1910.01075},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.01075},
  urldate = {2025-05-17},
  abstract = {Promising results have driven a recent surge of interest in continuous optimization methods for Bayesian network structure learning from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained from observational data alone. Interventional data provides much richer information about the underlying data-generating process. However, the extension and application of methods designed for observational data to include interventions is not straightforward and remains an open problem. In this paper we provide a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data. The proposed method is even applicable in the challenging and realistic case that the identity of the intervened upon variable is unknown. We examine the proposed method in the setting of graph recovery both de novo and from a partially-known edge set. We establish strong benchmark results on several structure learning tasks, including structure recovery of both synthetic graphs as well as standard graphs from the Bayesian Network Repository.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-05-17T08:46:27Z}
}

@book{kelly2013,
  title = {Smart Machines: {{IBM}}'s Watson and the Era of Cognitive Computing},
  author = {Kelly, John E. and Hamm, Steve},
  year = {2013},
  publisher = {Columbia University Press},
  timestamp = {2025-09-06T00:42:24Z}
}

@article{keyl2022multimodal,
  title = {Multimodal Survival Prediction in Advanced Pancreatic Cancer Using Machine Learning},
  author = {Keyl, J. and Kasper, S. and Wiesweg, M. and G{\"o}tze, J. and Sch{\"o}nrock, M. and Sinn, M. and Berger, A. and Nasca, E. and Kostbade, K. and Schumacher, B. and Markus, P. and Albers, D. and Treckmann, J. and Schmid, K. W. and Schildhaus, H.-U. and Siveke, J. T. and Schuler, M. and Kleesiek, J.},
  year = {2022},
  month = oct,
  journal = {ESMO open},
  volume = {7},
  number = {5},
  pages = {100555},
  issn = {2059-7029},
  doi = {10.1016/j.esmoop.2022.100555},
  abstract = {BACKGROUND: Existing risk scores appear insufficient to assess the individual survival risk of patients with advanced pancreatic ductal adenocarcinoma (PDAC) and do not take advantage of the variety of parameters that are collected during clinical care. METHODS: In this retrospective study, we built a random survival forest model from clinical data of 203 patients with advanced PDAC. The parameters were assessed before initiation of systemic treatment and included age, CA19-9, C-reactive protein, metastatic status, neutrophil-to-lymphocyte ratio and total serum protein level. Separate models including imaging and molecular parameters were built for subgroups. RESULTS: Over the entire cohort, a model based on clinical parameters achieved a c-index of 0.71. Our approach outperformed the American Joint Committee on Cancer (AJCC) staging system and the modified Glasgow Prognostic Score (mGPS) in the identification of high- and low-risk subgroups. Inclusion of the KRAS p.G12D mutational status could further improve the prediction, whereas radiomics data of the primary tumor only showed little benefit. In an external validation cohort of PDAC patients with liver metastases, our model achieved a c-index of 0.67 (mGPS: 0.59). CONCLUSIONS: The combination of multimodal data and machine-learning algorithms holds potential for personalized prognostication in advanced PDAC already at diagnosis.},
  langid = {english},
  pmcid = {PMC9588888},
  pmid = {35988455},
  keywords = {Adenocarcinoma,C-Reactive Protein,CA-19-9 Antigen,computed tomography,genetics,Humans,machine learning,Machine Learning,Neoplasm Staging,pancreatic cancer,Pancreatic Neoplasms,prognosis,Prognosis,Proto-Oncogene Proteins p21(ras),Retrospective Studies,survival analysis},
  annotation = {TLDR: A random survival forest model based on clinical parameters and machine-learning algorithms holds potential for personalized prognostication in advanced PDAC already at diagnosis and outperformed the American Joint Committee on Cancer staging system and the modified Glasgow Prognostic Score.},
  timestamp = {2025-05-16T03:16:23Z}
}

@article{keyl2022patient,
  title = {Patient-Level Proteomic Network Prediction by Explainable Artificial Intelligence},
  author = {Keyl, Philipp and Bockmayr, Michael and Heim, Daniel and Dernbach, Gabriel and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Klauschen, Frederick},
  year = {2022},
  journal = {NPJ Precision Oncology},
  volume = {6},
  number = {1},
  pages = {35},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2025-05-15T15:04:19Z}
}

@article{keyl2023singlecell,
  title = {Single-Cell Gene Regulatory Network Prediction by Explainable {{AI}}},
  author = {Keyl, Philipp and Bischoff, Philip and Dernbach, Gabriel and Bockmayr, Michael and Fritz, Rebecca and Horst, David and Bl{\"u}thgen, Nils and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Klauschen, Frederick},
  year = {2023},
  month = feb,
  journal = {Nucleic Acids Research},
  volume = {51},
  number = {4},
  pages = {e20-e20},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkac1212},
  urldate = {2025-05-16},
  abstract = {Abstract             The molecular heterogeneity of cancer cells contributes to the often partial response to targeted therapies and relapse of disease due to the escape of resistant cell populations. While single-cell sequencing has started to improve our understanding of this heterogeneity, it offers a mostly descriptive view on cellular types and states. To obtain more functional insights, we propose scGeneRAI, an explainable deep learning approach that uses layer-wise relevance propagation (LRP) to infer gene regulatory networks from static single-cell RNA sequencing data for individual cells. We benchmark our method with synthetic data and apply it to single-cell RNA sequencing data of a cohort of human lung cancers. From the predicted single-cell networks our approach reveals characteristic network patterns for tumor cells and normal epithelial cells and identifies subnetworks that are observed only in (subgroups of) tumor cells of certain patients. While current state-of-the-art methods are limited by their ability to only predict average networks for cell populations, our approach facilitates the reconstruction of networks down to the level of single cells which can be utilized to characterize the heterogeneity of gene regulation within and across tumors.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  pmcid = {PMC9976884},
  pmid = {36629274},
  keywords = {Deep Learning,Gene Expression Regulation,Gene Regulatory Networks,Humans,Lung Neoplasms,Neoplasms,Single-Cell Gene Expression Analysis},
  timestamp = {2025-05-16T09:13:47Z}
}

@inproceedings{khater2023explainable,
  title = {Explainable {{AI}} for {{Breast Cancer Detection}}: {{A LIME-Driven Approach}}},
  shorttitle = {Explainable {{AI}} for {{Breast Cancer Detection}}},
  booktitle = {2023 16th {{International Conference}} on {{Developments}} in {{eSystems Engineering}} ({{DeSE}})},
  author = {Khater, Tarek and Hussain, Abir and Mahmoud, Soliman and Yasen, Salwa},
  year = {2023},
  month = dec,
  pages = {540--545},
  publisher = {IEEE},
  address = {Istanbul, Turkiye},
  doi = {10.1109/DeSE60595.2023.10469341},
  urldate = {2025-04-06},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-8134-4},
  annotation = {TLDR: This study uses Local Interpretable Model-Agnostic Explanations to explain how the machine-learning model accurately classifies breast cancer cases as either Benign or Malignant, and provides valuable insights into the decision-making process of the model, making it more interpretable and useful for breast cancer diagnosis.},
  timestamp = {2025-04-06T09:52:04Z}
}

@article{kiani2022understanding,
  title = {Towards {{Understanding Human Functional Brain Development With Explainable Artificial Intelligence}}: {{Challenges}} and {{Perspectives}}},
  shorttitle = {Towards {{Understanding Human Functional Brain Development With Explainable Artificial Intelligence}}},
  author = {Kiani, Mehrin and {Andreu-Perez}, Javier and Hagras, Hani and Rigato, Silvia and Filippetti, Maria Laura},
  year = {2022},
  month = feb,
  journal = {Comp. Intell. Mag.},
  volume = {17},
  number = {1},
  pages = {16--33},
  issn = {1556-603X},
  doi = {10.1109/MCI.2021.3129956},
  urldate = {2025-04-17},
  abstract = {The last decades have seen significant advancements in non-invasive neuroimaging technologies that have been increasingly adopted to examine human brain development. However, these improvements have not necessarily been followed by more sophisticated data analysis measures that are able to explain the mechanisms underlying functional brain development. For example, the shift from univariate (single area in the brain) to multivariate (multiple areas in brain) analysis paradigms is of significance as it allows investigations into the interactions between different brain regions. However, despite the potential of multivariate analysis to shed light on the interactions between developing brain regions, artificial intelligence (AI) techniques applied render the analysis non-explainable. The purpose of this paper is to understand the extent to which current state-of-the-art AI techniques can inform functional brain development. In addition, a review of which AI techniques are more likely to explain their learning based on the processes of brain development as defined by developmental cognitive neuroscience (DCN) frameworks is also undertaken. This work also proposes that eXplainable AI (XAI) may provide viable methods to investigate functional brain development as hypothesized by DCN frameworks.},
  langid = {american},
  annotation = {TLDR: It is proposed that eXplainable AI (XAI) may provide viable methods to investigate functional brain development as hypothesized by developmental cognitive neuroscience (DCN) frameworks.},
  timestamp = {2025-04-17T12:00:51Z}
}

@misc{kiciman2024causal,
  title = {Causal {{Reasoning}} and {{Large Language Models}}: {{Opening}} a {{New Frontier}} for {{Causality}}},
  shorttitle = {Causal {{Reasoning}} and {{Large Language Models}}},
  author = {K{\i}c{\i}man, Emre and Ness, Robert and Sharma, Amit and Tan, Chenhao},
  year = {2024},
  month = aug,
  number = {arXiv:2305.00050},
  eprint = {2305.00050},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.00050},
  urldate = {2025-03-23},
  abstract = {The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a "behavorial" study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97\%, 13 points gain), counterfactual reasoning task (92\%, 20 points gain) and event causality (86\% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Methodology},
  annotation = {TLDR: Across a wide range of tasks, it is found that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods.},
  timestamp = {2025-03-23T10:36:15Z}
}

@article{kim2017interpretability,
  title = {Interpretability {{Beyond Feature Attribution}}: {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  shorttitle = {Interpretability {{Beyond Feature Attribution}}},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  year = {2017},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1711.11279},
  urldate = {2025-09-05},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (stat.ML)},
  timestamp = {2025-09-05T12:11:20Z}
}

@article{kim2021dynamic,
  title = {The Dynamic, Combinatorial Cis-Regulatory Lexicon of Epidermal Differentiation},
  author = {Kim, Daniel S. and Risca, Viviana I. and Reynolds, David L. and Chappell, James and Rubin, Adam J. and Jung, Namyoung and Donohue, Laura K. H. and {Lopez-Pajares}, Vanessa and Kathiria, Arwa and Shi, Minyi and Zhao, Zhixin and Deep, Harsh and Sharmin, Mahfuza and Rao, Deepti and Lin, Shin and Chang, Howard Y. and Snyder, Michael P. and Greenleaf, William J. and Kundaje, Anshul and Khavari, Paul A.},
  year = {2021},
  month = nov,
  journal = {Nature Genetics},
  volume = {53},
  number = {11},
  pages = {1564--1576},
  issn = {1546-1718},
  doi = {10.1038/s41588-021-00947-3},
  abstract = {Transcription factors bind DNA sequence motif vocabularies in cis-regulatory elements (CREs) to modulate chromatin state and gene expression during cell state transitions. A quantitative understanding of how motif lexicons influence dynamic regulatory activity has been elusive due to the combinatorial nature of the cis-regulatory code. To address this, we undertook multiomic data profiling of chromatin and expression dynamics across epidermal differentiation to identify 40,103 dynamic CREs associated with 3,609 dynamically expressed genes, then applied an interpretable deep-learning framework to model the cis-regulatory logic of chromatin accessibility. This analysis framework identified cooperative DNA sequence rules in dynamic CREs regulating synchronous gene modules with diverse roles in skin differentiation. Massively parallel reporter assay analysis validated temporal dynamics and cooperative cis-regulatory logic. Variants linked to human polygenic skin disease were enriched in these time-dependent combinatorial motif rules. This integrative approach shows the combinatorial cis-regulatory lexicon of epidermal differentiation and represents a general framework for deciphering the organizational principles of the cis-regulatory code of dynamic gene regulation.},
  langid = {english},
  pmcid = {PMC8763320},
  pmid = {34650237},
  keywords = {Cell Differentiation,Chromatin,Epidermis,Epigenome,Gene Expression Regulation,Genes Reporter,Genome-Wide Association Study,Humans,Keratinocytes,Models Genetic,Neural Networks Computer,Regulatory Elements Transcriptional,Skin Diseases,Transcription Factors},
  annotation = {TLDR: An interpretable deep-learning framework interprets multiomic data across epidermal differentiation and represents a general framework for deciphering the organizational principles of the cis-regulatory code of dynamic gene regulation, identifying cooperative DNA sequence rules that regulate gene modules.},
  timestamp = {2025-07-05T11:51:15Z}
}

@article{kim2025mdagents,
  title = {Mdagents: {{An}} Adaptive Collaboration of Llms for Medical Decision-Making},
  author = {Kim, Yubin and Park, Chanwoo and Jeong, Hyewon and Chan, Yik Siu and Xu, Xuhai and McDuff, Daniel and Lee, Hyeonhoon and Ghassemi, Marzyeh and Breazeal, Cynthia and Park, Hae and others},
  year = {2025},
  journal = {Advances in Neural Information Processing Systems},
  volume = {37},
  pages = {79410--79452},
  timestamp = {2025-03-03T12:47:34Z}
}

@article{kiseleva2022transparency,
  title = {Transparency of {{AI}} in Healthcare as a Multilayered System of Accountabilities: Between Legal Requirements and Technical Limitations},
  author = {Kiseleva, Anastasiya and Kotzinos, Dimitris and De Hert, Paul},
  year = {2022},
  journal = {Frontiers in artificial intelligence},
  volume = {5},
  pages = {879603},
  publisher = {Frontiers Media SA},
  timestamp = {2025-04-15T13:54:35Z}
}

@misc{klaassen2024doublemldeep,
  title = {{{DoubleMLDeep}}: {{Estimation}} of {{Causal Effects}} with {{Multimodal Data}}},
  shorttitle = {{{DoubleMLDeep}}},
  author = {Klaassen, Sven and {Teichert-Kluge}, Jan and Bach, Philipp and Chernozhukov, Victor and Spindler, Martin and Vijaykumar, Suhas},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01785},
  eprint = {2402.01785},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01785},
  urldate = {2025-03-23},
  abstract = {This paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. We propose a neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model. An additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. The proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. Our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Economics - Econometrics,Statistics - Machine Learning,Statistics - Methodology},
  annotation = {TLDR: A neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model is proposed that is adapted to the double machine learning (DML) framework.},
  timestamp = {2025-03-23T07:01:55Z}
}

@article{klauschen2024explainable,
  title = {Toward {{Explainable Artificial Intelligence}} for {{Precision Pathology}}},
  author = {Klauschen, Frederick and Dippel, Jonas and Keyl, Philipp and Jurmeister, Philipp and Bockmayr, Michael and Mock, Andreas and Buchstab, Oliver and Alber, Maximilian and Ruff, Lukas and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert},
  year = {2024},
  month = jan,
  journal = {Annual Review of Pathology: Mechanisms of Disease},
  volume = {19},
  number = {Volume 19, 2024},
  pages = {541--570},
  publisher = {Annual Reviews},
  issn = {1553-4006, 1553-4014},
  doi = {10.1146/annurev-pathmechdis-051222-113147},
  urldate = {2025-09-17},
  abstract = {The rapid development of precision medicine in recent years has started to challenge diagnostic pathology with respect to its ability to analyze histological images and increasingly large molecular profiling data in a quantitative, integrative, and standardized way. Artificial intelligence (AI) and, more precisely, deep learning technologies have recently demonstrated the potential to facilitate complex data analysis tasks, including clinical, histological, and molecular data for disease classification; tissue biomarker quantification; and clinical outcome prediction. This review provides a general introduction to AI and describes recent developments with a focus on applications in diagnostic pathology and beyond. We explain limitations including the black-box character of conventional AI and describe solutions to make machine learning decisions more transparent with so-called explainable AI. The purpose of the review is to foster a mutual understanding of both the biomedical and the AI side. To that end, in addition to providing an overview of the relevant foundations in pathology and machine learning, we present worked-through examples for a better practical understanding of what AI can achieve and how it should be done.},
  langid = {english},
  annotation = {TLDR: This review provides a general introduction to AI and describes recent developments with a focus on applications in diagnostic pathology and beyond and explains limitations including the black-box character of conventional AI and describe solutions to make machine learning decisions more transparent with so-called explainable AI.},
  timestamp = {2025-09-17T07:57:32Z}
}

@article{kline2022multimodal,
  title = {Multimodal Machine Learning in Precision Health: {{A}} Scoping Review},
  shorttitle = {Multimodal Machine Learning in Precision Health},
  author = {Kline, Adrienne and Wang, Hanyin and Li, Yikuan and Dennis, Saya and Hutch, Meghan and Xu, Zhenxing and Wang, Fei and Cheng, Feixiong and Luo, Yuan},
  year = {2022},
  month = nov,
  journal = {npj Digital Medicine},
  volume = {5},
  number = {1},
  pages = {171},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00712-8},
  urldate = {2025-08-08},
  abstract = {Abstract             Machine learning is frequently being leveraged to tackle problems in the health sector including utilization for clinical decision-support. Its use has historically been focused on single modal data. Attempts to improve prediction and mimic the multimodal nature of clinical expert decision-making has been met in the biomedical field of machine learning by fusing disparate data. This review was conducted to summarize the current studies in this field and identify topics ripe for future research. We conducted this review in accordance with the PRISMA extension for Scoping Reviews to characterize multi-modal data fusion in health. Search strings were established and used in databases: PubMed, Google Scholar, and IEEEXplore from 2011 to 2021. A final set of 128 articles were included in the analysis. The most common health areas utilizing multi-modal methods were neurology and oncology. Early fusion was the most common data merging strategy. Notably, there was an improvement in predictive performance when using data fusion. Lacking from the papers were clear clinical deployment strategies, FDA-approval, and analysis of how using multimodal approaches from diverse sub-populations may improve biases and healthcare disparities. These findings provide a summary on multimodal data fusion as applied to health diagnosis/prognosis problems. Few papers compared the outputs of a multimodal approach with a unimodal prediction. However, those that did achieved an average increase of 6.4\% in predictive accuracy. Multi-modal machine learning, while more robust in its estimations over unimodal methods, has drawbacks in its scalability and the time-consuming nature of information concatenation.},
  langid = {english},
  annotation = {TLDR: Multi-modal machine learning, while more robust in its estimations over unimodal methods, has drawbacks in its scalability and the time-consuming nature of information concatenation.},
  timestamp = {2025-08-08T10:26:34Z}
}

@article{knapic2021explainable,
  title = {Explainable {{Artificial Intelligence}} for {{Human Decision Support System}} in the {{Medical Domain}}},
  author = {Knapi{\v c}, Samanta and Malhi, Avleen and Saluja, Rohit and Fr{\"a}mling, Kary},
  year = {2021},
  month = sep,
  journal = {Machine Learning and Knowledge Extraction},
  volume = {3},
  number = {3},
  pages = {740--770},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-4990},
  doi = {10.3390/make3030037},
  urldate = {2025-03-27},
  abstract = {In this paper, we present the potential of Explainable Artificial Intelligence methods for decision support in medical image analysis scenarios. Using three types of explainable methods applied to the same medical image data set, we aimed to improve the comprehensibility of the decisions provided by the Convolutional Neural Network (CNN). In vivo gastral images obtained by a video capsule endoscopy (VCE) were the subject of visual explanations, with the goal of increasing health professionals' trust in black-box predictions. We implemented two post hoc interpretable machine learning methods, called Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP), and an alternative explanation approach, the Contextual Importance and Utility (CIU) method. The produced explanations were assessed by human evaluation. We conducted three user studies based on explanations provided by LIME, SHAP and CIU. Users from different non-medical backgrounds carried out a series of tests in a web-based survey setting and stated their experience and understanding of the given explanations. Three user groups (n = 20, 20, 20) with three distinct forms of explanations were quantitatively analyzed. We found that, as hypothesized, the CIU-explainable method performed better than both LIME and SHAP methods in terms of improving support for human decision-making and being more transparent and thus understandable to users. Additionally, CIU outperformed LIME and SHAP by generating explanations more rapidly. Our findings suggest that there are notable differences in human decision-making between various explanation support settings. In line with that, we present three potential explainable methods that, with future improvements in implementation, can be generalized to different medical data sets and can provide effective decision support to medical experts.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {explainable artificial intelligence,human decision support,image recognition,medical image analyses},
  annotation = {TLDR: The CIU-explainable method performed better than both LIME and SHAP methods in terms of improving support for human decision-making and being more transparent and thus understandable to users, and CIU outperformed LIMEand SHAP by generating explanations more rapidly.},
  timestamp = {2025-03-27T03:20:52Z}
}

@misc{koch2021deep,
  title = {Deep {{Learning}} for {{Causal Inference}}},
  author = {Koch, Bernard and Sainburg, Tim and Geraldo, Pablo and Jiang, Song and Sun, Yizhou and Foster, Jacob G.},
  year = {2021},
  month = oct,
  publisher = {OSF},
  doi = {10.31235/osf.io/aeszf},
  urldate = {2025-03-22},
  abstract = {This primer systematizes the emerging literature on causal inference using deep neural networks under the potential outcomes framework. It provides an intuitive introduction on building and optimizing custom deep learning models and shows how to adapt them to estimate/predict heterogeneous treatment effects. It also discusses ongoing work to extend causal inference to settings where confounding is non-linear, time-varying, or encoded in text, networks, and images. To maximize accessibility, we also introduce prerequisite concepts from causal inference and deep learning. The primer differs from other treatments of deep learning and causal inference in its sharp focus on observational causal estimation, its extended exposition of key algorithms, and its detailed tutorials for implementing, training, and selecting among deep estimators in Tensorflow 2 and PyTorch available at https://github.com/kochbj/Deep-Learning-for-Causal-Inference.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {causal inference,deep learning,machine learning,potential outcomes,review,selection on observables,survey},
  timestamp = {2025-03-22T08:30:13Z}
}

@article{kodipalli2025explainable,
  title = {Explainable {{AI-based}} Feature Importance Analysis for Ovarian Cancer Classification with Ensemble Methods},
  author = {Kodipalli, Ashwini and Devi, V. Susheela and Guruvare, Shyamala and Ismail, Taha},
  year = {2025},
  month = mar,
  journal = {Frontiers in Public Health},
  volume = {13},
  pages = {1479095},
  issn = {2296-2565},
  doi = {10.3389/fpubh.2025.1479095},
  urldate = {2025-08-11},
  abstract = {Introduction               Ovarian Cancer (OC) is one of the leading causes of cancer deaths among women. Despite recent advances in the medical field, such as surgery, chemotherapy, and radiotherapy interventions, there are only marginal improvements in the diagnosis of OC using clinical parameters, as the symptoms are very non-specific at the early stage. Owing to advances in computational algorithms, such as ensemble machine learning, it is now possible to identify complex patterns in clinical parameters. However, these complex patterns do not provide deeper insights into prediction and diagnosis. Explainable artificial intelligence (XAI) models, such as LIME and SHAP Kernels, can provide insights into the decision-making process of ensemble models, thus increasing their applicability.                                         Methods               The main aim of this study is to design a computer-aided diagnostic system that accurately classifies and detects ovarian cancer. To achieve this objective, a three-stage ensemble model and a game-theoretic approach based on SHAP values were built to evaluate and visualize the results, thus analyzing the important features responsible for prediction.                                         Results and Discussion                                The results demonstrate the efficacy of the proposed model with an accuracy of 98.66\%. The proposed model's consistency and advantages are compared with single classifiers. The SHAP values of the proposed model are validated using conventional statistical methods such as the                 p                 -test and Cohen's                 d                 -test to highlight the efficacy of the proposed method. To further validate the ranking of the features, we compared the                 p                 -values and Cohen's                 d                 -values of the top five and bottom five features. The study proposed and validated an AI-based method for the detection, diagnosis, and prognosis of OC using multi-modal real-life data, which mimics the move of a clinician approach with a demonstration of high performance. The proposed strategy can lead to reliable, accurate, and consistent AI solutions for the detection and management of OC with higher patient experience and outcomes at low cost, low morbidity, and low mortality. This can be beneficial for millions of women living in resource-constrained and challenging economies.},
  annotation = {TLDR: The study proposed and validated an AI-based method for the detection, diagnosis, and prognosis of OC using multi-modal real-life data, which mimics the move of a clinician approach with a demonstration of high performance.},
  timestamp = {2025-08-11T08:50:58Z}
}

@inproceedings{koh2020concept,
  title = {Concept Bottleneck Models},
  booktitle = {International Conference on Machine Learning},
  author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  year = {2020},
  pages = {5338--5348},
  publisher = {PMLR},
  timestamp = {2025-09-06T02:38:03Z}
}

@misc{kokhlikyan2020captum,
  title = {Captum: {{A}} Unified and Generic Model Interpretability Library for {{PyTorch}}},
  author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and {Reblitz-Richardson}, Orion},
  year = {2020},
  eprint = {2009.07896},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  timestamp = {2025-04-12T13:17:17Z}
}

@article{koo2021global,
  title = {Global Importance Analysis: {{An}} Interpretability Method to Quantify Importance of Genomic Features in Deep Neural Networks},
  shorttitle = {Global Importance Analysis},
  author = {Koo, Peter K. and Majdandzic, Antonio and Ploenzke, Matthew and Anand, Praveen and Paul, Steffan B.},
  year = {2021},
  month = may,
  journal = {PLoS computational biology},
  volume = {17},
  number = {5},
  pages = {e1008925},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008925},
  abstract = {Deep neural networks have demonstrated improved performance at predicting the sequence specificities of DNA- and RNA-binding proteins compared to previous methods that rely on k-mers and position weight matrices. To gain insights into why a DNN makes a given prediction, model interpretability methods, such as attribution methods, can be employed to identify motif-like representations along a given sequence. Because explanations are given on an individual sequence basis and can vary substantially across sequences, deducing generalizable trends across the dataset and quantifying their effect size remains a challenge. Here we introduce global importance analysis (GIA), a model interpretability method that quantifies the population-level effect size that putative patterns have on model predictions. GIA provides an avenue to quantitatively test hypotheses of putative patterns and their interactions with other patterns, as well as map out specific functions the network has learned. As a case study, we demonstrate the utility of GIA on the computational task of predicting RNA-protein interactions from sequence. We first introduce a convolutional network, we call ResidualBind, and benchmark its performance against previous methods on RNAcompete data. Using GIA, we then demonstrate that in addition to sequence motifs, ResidualBind learns a model that considers the number of motifs, their spacing, and sequence context, such as RNA secondary structure and GC-bias.},
  langid = {english},
  pmcid = {PMC8118286},
  pmid = {33983921},
  keywords = {Computational Biology,Deep Learning,Genomics,Humans,Neural Networks Computer},
  annotation = {TLDR: GIA provides an avenue to quantitatively test hypotheses of putative patterns and their interactions with other patterns, as well as map out specific functions the network has learned, and is demonstrated on the computational task of predicting RNA-protein interactions from sequence.},
  timestamp = {2025-07-05T12:12:29Z}
}

@article{kothari2024explainable,
  title = {An Explainable {{AI-assisted}} Web Application in Cancer Drug Value Prediction},
  author = {Kothari, Sonali and Sharma, Shivanandana and Shejwal, Sanskruti and Kazi, Aqsa and D'Silva, Michela and Karthikeyan, M.},
  year = {2024},
  month = jun,
  journal = {MethodsX},
  volume = {12},
  pages = {102696},
  issn = {22150161},
  doi = {10.1016/j.mex.2024.102696},
  urldate = {2025-08-08},
  langid = {english},
  annotation = {TLDR: This research attempts to create a framework where a user can upload a test case and receive forecasts with explanations, all in a portable PDF report, and presents a viable route for customizing treatments and improving patient outcomes in oncology by combining XAI with a large dataset.},
  timestamp = {2025-08-08T08:01:21Z}
}

@inproceedings{kuang2017,
  title = {Estimating Treatment Effect in the Wild via Differentiated Confounder Balancing},
  booktitle = {23rd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Kuang, Kun and Cui, Peng and Li, Bo and Jiang, Meng and Yang, Shiqiang},
  year = {2017},
  pages = {265--274},
  publisher = {ACM},
  timestamp = {2025-03-20T04:01:33Z}
}

@article{kumarans2024explainable,
  title = {Explainable Lung Cancer Classification with Ensemble Transfer Learning of {{VGG16}}, {{Resnet50}} and {{InceptionV3}} Using Grad-Cam},
  author = {Kumaran S, Yogesh and Jeya, J. Jospin and R, Mahesh T and Khan, Surbhi Bhatia and Alzahrani, Saeed and Alojail, Mohammed},
  year = {2024},
  month = jul,
  journal = {BMC Medical Imaging},
  volume = {24},
  number = {1},
  pages = {176},
  issn = {1471-2342},
  doi = {10.1186/s12880-024-01345-x},
  urldate = {2025-05-06},
  abstract = {Abstract             Medical imaging stands as a critical component in diagnosing various diseases, where traditional methods often rely on manual interpretation and conventional machine learning techniques. These approaches, while effective, come with inherent limitations such as subjectivity in interpretation and constraints in handling complex image features. This research paper proposes an integrated deep learning approach utilizing pre-trained models---VGG16, ResNet50, and InceptionV3---combined within a unified framework to improve diagnostic accuracy in medical imaging. The method focuses on lung cancer detection using images resized and converted to a uniform format to optimize performance and ensure consistency across datasets. Our proposed model leverages the strengths of each pre-trained network, achieving a high degree of feature extraction and robustness by freezing the early convolutional layers and fine-tuning the deeper layers. Additionally, techniques like SMOTE and Gaussian Blur are applied to address class imbalance, enhancing model training on underrepresented classes. The model's performance was validated on the IQ-OTH/NCCD lung cancer dataset, which was collected from the Iraq-Oncology Teaching Hospital/National Center for Cancer Diseases over a period of three months in fall 2019. The proposed model achieved an accuracy of 98.18\%, with precision and recall rates notably high across all classes. This improvement highlights the potential of integrated deep learning systems in medical diagnostics, providing a more accurate, reliable, and efficient means of disease detection.},
  langid = {english},
  annotation = {TLDR: An integrated deep learning approach utilizing pre-trained models---VGG16, ResNet50, and InceptionV3---combined within a unified framework to improve diagnostic accuracy in medical imaging, achieving a high degree of feature extraction and robustness.},
  timestamp = {2025-05-06T04:01:26Z}
}

@article{kunzel2019metalearners,
  title = {Metalearners for Estimating Heterogeneous Treatment Effects Using Machine Learning},
  author = {K{\"u}nzel, S{\"o}ren R and Sekhon, Jasjeet S and Bickel, Peter J and Yu, Bin},
  year = {2019},
  journal = {Proceedings of the national academy of sciences},
  volume = {116},
  number = {10},
  pages = {4156--4165},
  publisher = {National Academy of Sciences},
  timestamp = {2025-03-20T08:12:50Z}
}

@article{kunzel2019metalearnersa,
  title = {Meta-Learners for {{Estimating Heterogeneous Treatment Effects}} Using {{Machine Learning}}},
  author = {K{\"u}nzel, S{\"o}ren R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
  year = {2019},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {10},
  eprint = {1706.03461},
  primaryclass = {math},
  pages = {4156--4165},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1804597116},
  urldate = {2025-04-13},
  abstract = {There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of meta-algorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the Conditional Average Treatment Effect (CATE) function. Meta-algorithms build on base algorithms---such as Random Forests (RF), Bayesian Additive Regression Trees (BART) or neural networks---to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a new meta-algorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other, and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the meta-learners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our new X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology,Statistics - Statistics Theory},
  annotation = {TLDR: A metalearner, the X-learner, is proposed, which can adapt to structural properties, such as the smoothness and sparsity of the underlying treatment effect, and is shown to be easy to use and to produce results that are interpretable.},
  timestamp = {2025-04-13T07:16:27Z}
}

@article{kuo2009intuition,
  title = {Intuition and Deliberation: Two Systems for Strategizing in the Brain},
  author = {Kuo, Wen-Jui and Sj{\"o}str{\"o}m, Tomas and Chen, Yu-Ping and Wang, Yen-Hsiang and Huang, Chen-Ying},
  year = {2009},
  journal = {Science},
  volume = {324},
  number = {5926},
  pages = {519--522},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2025-03-23T03:23:40Z}
}

@article{ladyzynski2022dynamic,
  title = {Dynamic {{Bayesian}} Networks for Prediction of Health Status and Treatment Effect in Patients with Chronic Lymphocytic Leukemia},
  author = {Ladyzynski, Piotr and Molik, Maria and Foltynski, Piotr},
  year = {2022},
  month = feb,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {1811},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-05813-8},
  urldate = {2025-05-04},
  abstract = {Chronic lymphocytic leukemia (CLL) is the most common blood cancer in adults. The course of CLL and patients' response to treatment are varied. This variability makes it difficult to select the most appropriate treatment regimen and predict the progression of the disease. This work was aimed at developing and validating dynamic Bayesian networks (DBNs) to predict changes of the health status of patients with CLL and progression of the disease over time. Two DBNs were developed and implemented i.e. Health Status Network (HSN) and Treatment Effect Network (TEN). Based on the literature data and expert knowledge we identified relationships linking the most important factors influencing the health status and treatment effects in patients with CLL. The developed networks, and in particular TEN, were able to predict probability of survival in patients with CLL, which was in line with the survival data collected in large medical registries. The networks can be used to personalize the predictions, taking into account a priori knowledge concerning a particular patient with CLL. The proposed approach can serve as a basis for the development of artificial intelligence systems that facilitate the choice of treatment that maximizes the chances of survival in patients with CLL.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Biomedical engineering,Translational research},
  annotation = {TLDR: The developed networks were able to predict probability of survival in patients with CLL, which was in line with the survival data collected in large medical registries, and can serve as a basis for the development of artificial intelligence systems that facilitate the choice of treatment that maximizes the chances of survival.},
  timestamp = {2025-05-04T01:26:11Z}
}

@article{lagemann2023deep,
  title = {Deep Learning of Causal Structures in High Dimensions under Data Limitations},
  author = {Lagemann, Kai and Lagemann, Christian and Taschler, Bernd and Mukherjee, Sach},
  year = {2023},
  month = nov,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {11},
  pages = {1306--1316},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00744-z},
  urldate = {2025-04-18},
  abstract = {Causal learning is a key challenge in scientific artificial intelligence as it allows researchers to go beyond purely correlative or predictive analyses towards learning underlying cause-and-effect relationships, which are important for scientific understanding as well as for a wide range of downstream tasks. Here, motivated by emerging biomedical questions, we propose a deep neural architecture for learning causal relationships between variables from a combination of high-dimensional data and prior causal knowledge. We combine convolutional and graph neural networks within a causal risk framework to provide an approach that is demonstrably effective under the conditions of high dimensionality, noise and data limitations that are characteristic of many applications, including in large-scale biology. In experiments, we find that the proposed learners can effectively identify novel causal relationships across thousands of variables. Results include extensive (linear and nonlinear) simulations (where the ground truth is known and can be directly compared against), as well as real biological examples where the models are applied to high-dimensional molecular data and their outputs compared against entirely unseen validation experiments. These results support the notion that deep learning approaches can be used to learn causal networks at large scale.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational models,Machine learning},
  annotation = {TLDR: A deep neural network approach combining convolutional and graph models intended for causal learning in high-dimensional biomedical problems is introduced, which supports the notion that deep learning approaches can be used to learn causal networks at large scale.},
  timestamp = {2025-04-18T03:19:07Z}
}

@inproceedings{laguna2023explimeable,
  title = {{{ExpLIMEable}}: {{A Visual Analytics Approach}} for {{Exploring LIME}}},
  shorttitle = {{{ExpLIMEable}}},
  booktitle = {2023 {{Workshop}} on {{Visual Analytics}} in {{Healthcare}} ({{VAHC}})},
  author = {Laguna, Sonia and Heidenreich, Julian N. and Sun, Jiugeng and Cetin, Nil{\"u}fer and {Al-Hazwani}, Ibrahim and Schlegel, Udo and Cheng, Furui and {El-Assady}, Mennatallah},
  year = {2023},
  month = oct,
  pages = {27--33},
  publisher = {IEEE},
  address = {Melbourne, Australia},
  doi = {10.1109/VAHC60858.2023.00011},
  urldate = {2025-04-06},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-3024-3},
  annotation = {TLDR: The developed application assists machine learning developers in understanding the limitations of LIME and its sensitivity to different parameters, as well as the doctors in providing an explanation to machine learning models, enabling more informed decision-making, with the ultimate goal of improving its robustness and explanation quality.},
  timestamp = {2025-04-06T09:54:44Z}
}

@misc{lake2016building,
  title = {Building {{Machines That Learn}} and {{Think Like People}}},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2016},
  number = {arXiv:1604.00289},
  eprint = {1604.00289},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1604.00289},
  urldate = {2025-09-05},
  abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  archiveprefix = {arXiv},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),Neural and Evolutionary Computing (cs.NE),Statistics - Machine Learning},
  timestamp = {2025-09-05T16:00:57Z}
}

@article{lakiotaki2023automated,
  title = {Automated Machine Learning for Genome Wide Association Studies},
  author = {Lakiotaki, Kleanthi and Papadovasilakis, Zaharias and Lagani, Vincenzo and Fafalios, Stefanos and Charonyktakis, Paulos and Tsagris, Michail and Tsamardinos, Ioannis},
  year = {2023},
  month = sep,
  journal = {Bioinformatics (Oxford, England)},
  volume = {39},
  number = {9},
  pages = {btad545},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btad545},
  abstract = {MOTIVATION: Genome-wide association studies (GWAS) present several computational and statistical challenges for their data analysis, including knowledge discovery, interpretability, and translation to clinical practice. RESULTS: We develop, apply, and comparatively evaluate an automated machine learning (AutoML) approach, customized for genomic data that delivers reliable predictive and diagnostic models, the set of genetic variants that are important for predictions (called a biosignature), and an estimate of the out-of-sample predictive power. This AutoML approach discovers variants with higher predictive performance compared to standard GWAS methods, computes an individual risk prediction score, generalizes to new, unseen data, is shown to better differentiate causal variants from other highly correlated variants, and enhances knowledge discovery and interpretability by reporting multiple equivalent biosignatures. AVAILABILITY AND IMPLEMENTATION: Code for this study is available at: https://github.com/mensxmachina/autoML-GWAS. JADBio offers a free version at: https://jadbio.com/sign-up/. SNP data can be downloaded from the EGA repository (https://ega-archive.org/). PRS data are found at: https://www.aicrowd.com/challenges/opensnp-height-prediction. Simulation data to study population structure can be found at: https://easygwas.ethz.ch/data/public/dataset/view/1/.},
  langid = {english},
  pmcid = {PMC10562960},
  pmid = {37672022},
  keywords = {Computer Simulation,Genome-Wide Association Study,Humans,Machine Learning,Phenotype,Polymorphism Single Nucleotide},
  annotation = {TLDR: This AutoML approach discovers variants with higher predictive performance compared to standard GWAS methods, computes an individual risk prediction score, generalizes to new, unseen data, is shown to better differentiate causal variants from other highly correlated variants, and enhances knowledge discovery and interpretability.},
  timestamp = {2025-04-17T15:09:28Z}
}

@article{lal2025quantumenhanced,
  title = {Quantum-Enhanced Intelligent System for Personalized Adaptive Radiotherapy Dose Estimation},
  author = {Lal, Radhey and Singh, Rajiv Kumar and Nishad, Dinesh Kumar and Khalid, Saifullah},
  year = {2025},
  month = jun,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {19919},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-05673-y},
  urldate = {2025-08-09},
  abstract = {Abstract             This research introduces a novel quantum-enhanced intelligent system tailored for personalized adaptive radiotherapy dose estimation. The system efficiently models radiation transport and predicts patient-specific dose distributions by integrating quantum algorithms, deep learning, and Monte Carlo simulations. Quantum-enhanced Monte Carlo simulations, employing algorithms such as Harrow-Hassidim-Lloyd (HHL) and Variational Quantum Eigensolver (VQE), achieve computational speedups of 8--15 times compared to classical methods while maintaining high accuracy. The deep learning architecture leverages convolutional and recurrent neural networks to capture complex anatomical and dosimetric patterns. Validation on simulated datasets demonstrates a 50--70\% reduction in mean absolute error and 2--3\% improvements in gamma index metrics compared to conventional approaches. Dose-volume histogram analysis further highlights enhanced Dice coefficients and reduced Hausdorff distances. These advancements underscore the potential for precise, efficient, and clinically relevant dose estimations, paving the way for improved outcomes in personalized adaptive radiotherapy.},
  langid = {english},
  annotation = {TLDR: A novel quantum-enhanced intelligent system tailored for personalized adaptive radiotherapy dose estimation that efficiently models radiation transport and predicts patient-specific dose distributions by integrating quantum algorithms, deep learning, and Monte Carlo simulations is introduced.},
  timestamp = {2025-08-09T15:05:08Z}
}

@article{lamy2019explainable,
  title = {Explainable Artificial Intelligence for Breast Cancer: {{A}} Visual Case-Based Reasoning Approach},
  shorttitle = {Explainable Artificial Intelligence for Breast Cancer},
  author = {Lamy, Jean-Baptiste and Sekar, Boomadevi and Guezennec, Gilles and Bouaud, Jacques and S{\'e}roussi, Brigitte},
  year = {2019},
  month = mar,
  journal = {Artificial Intelligence in Medicine},
  volume = {94},
  pages = {42--53},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2019.01.001},
  urldate = {2025-03-24},
  abstract = {Case-Based Reasoning (CBR) is a form of analogical reasoning in which the solution for a (new) query case is determined using a database of previous known cases with their solutions. Cases similar to the query are retrieved from the database, and then their solutions are adapted to the query. In medicine, a case usually corresponds to a patient and the problem consists of classifying the patient in a class of diagnostic or therapy. Compared to ``black box'' algorithms such as deep learning, the responses of CBR systems can be justified easily using the similar cases as examples. However, this possibility is often under-exploited and the explanations provided by most CBR systems are limited to the display of the similar cases. In this paper, we propose a CBR method that can be both executed automatically as an algorithm and presented visually in a user interface for providing visual explanations or for visual reasoning. After retrieving similar cases, a visual interface displays quantitative and qualitative similarities between the query and the similar cases, so as one can easily classify the query through visual reasoning, in a fully explainable manner. It combines a quantitative approach (visualized by a scatter plot based on Multidimensional Scaling in polar coordinates, preserving distances involving the query) and a qualitative approach (set visualization using rainbow boxes). We applied this method to breast cancer management. We showed on three public datasets that our qualitative method has a classification accuracy comparable to k-Nearest Neighbors algorithms, but is better explainable. We also tested the proposed interface during a small user study. Finally, we apply the proposed approach to a real dataset in breast cancer. Medical experts found the visual approach interesting as it explains why cases are similar through the visualization of shared patient characteristics.},
  langid = {american},
  keywords = {Breast cancer,Case-based reasoning,Data-driven decision making,Explainable Artificial Intelligence,Multidimensional Scaling,Visual explanation},
  annotation = {TLDR: This paper proposes a CBR method that can be both executed automatically as an algorithm and presented visually in a user interface for providing visual explanations or for visual reasoning, and shows that the qualitative method has a classification accuracy comparable to k-Nearest Neighbors algorithms, but is better explainable.},
  timestamp = {2025-03-24T07:52:19Z}
}

@article{lauritsen2020explainable,
  title = {Explainable Artificial Intelligence Model to Predict Acute Critical Illness from Electronic Health Records},
  author = {Lauritsen, Simon Meyer and Kristensen, Mads and Olsen, Mathias Vassard and Larsen, Morten Skaarup and Lauritsen, Katrine Meyer and J{\o}rgensen, Marianne Johansson and Lange, Jeppe and Thiesson, Bo},
  year = {2020},
  month = jul,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {3852},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17431-x},
  urldate = {2025-04-21},
  abstract = {Acute critical illness is often preceded by deterioration of routinely measured clinical parameters, e.g., blood pressure and heart rate. Early clinical prediction is typically based on manually calculated screening metrics that simply weigh these parameters, such as early warning scores (EWS). The predictive performance of EWSs yields a tradeoff between sensitivity and specificity that can lead to negative outcomes for the patient. Previous work on electronic health records (EHR) trained artificial intelligence (AI) systems offers promising results with high levels of predictive performance in relation to the early, real-time prediction of acute critical illness. However, without insight into the complex decisions by such system, clinical translation is hindered. Here, we present an explainable AI early warning score (xAI-EWS) system for early detection of acute critical illness. xAI-EWS potentiates clinical translation by accompanying a prediction with information on the EHR data explaining it.},
  copyright = {2020 The Author(s)},
  langid = {english},
  pmcid = {PMC7395744},
  pmid = {32737308},
  keywords = {Acute Disease,Acute kidney injury,Acute Kidney Injury,Acute Lung Injury,Area Under Curve,Artificial Intelligence,Blood Pressure,Computational models,Critical Illness,Early Diagnosis,Electronic Health Records,Heart Rate,Humans,Machine learning,Predictive medicine,Prognosis,Respiratory signs and symptoms,ROC Curve,Sepsis},
  timestamp = {2025-06-13T08:00:21Z}
}

@article{lawlor2008mendelian,
  title = {Mendelian Randomization: Using Genes as Instruments for Making Causal Inferences in Epidemiology},
  shorttitle = {Mendelian Randomization},
  author = {Lawlor, Debbie A. and Harbord, Roger M. and Sterne, Jonathan A. C. and Timpson, Nic and Davey Smith, George},
  year = {2008},
  month = apr,
  journal = {Statistics in Medicine},
  volume = {27},
  number = {8},
  pages = {1133--1163},
  issn = {0277-6715},
  doi = {10.1002/sim.3034},
  urldate = {2025-04-06},
  abstract = {Observational epidemiological studies suffer from many potential biases, from confounding and from reverse causation, and this limits their ability to robustly identify causal associations. Several high-profile situations exist in which randomized controlled trials of precisely the same intervention that has been examined in observational studies have produced markedly different findings. In other observational sciences, the use of instrumental variable (IV) approaches has been one approach to strengthening causal inferences in non-experimental situations. The use of germline genetic variants that proxy for environmentally modifiable exposures as instruments for these exposures is one form of IV analysis that can be implemented within observational epidemiological studies. The method has been referred to as 'Mendelian randomization', and can be considered as analogous to randomized controlled trials. This paper outlines Mendelian randomization, draws parallels with IV methods, provides examples of implementation of the approach and discusses limitations of the approach and some methods for dealing with these.},
  langid = {english},
  pmid = {17886233},
  keywords = {Bias,Causality,Epidemiologic Methods,Genetic Diseases Inborn,Genome Human,Genotype,Humans,Models Econometric,Molecular Epidemiology,Randomized Controlled Trials as Topic},
  annotation = {TLDR: The use of germline genetic variants that proxy for environmentally modifiable exposures as instruments for these exposures is one form of IV analysis that can be implemented within observational epidemiological studies and can be considered as analogous to randomized controlled trials.},
  timestamp = {2025-05-29T14:01:53Z}
}

@misc{layerwise,
  title = {Layer-{{Wise Relevance Propagation}} with {{Conservation Property}} for {{ResNet}}},
  urldate = {2025-05-17},
  howpublished = {https://arxiv.org/html/2407.09115v1},
  timestamp = {2025-05-17T13:53:50Z}
}

@article{leach1996enhancement,
  title = {Enhancement of Antitumor Immunity by {{CTLA-4}} Blockade},
  author = {Leach, D. R. and Krummel, M. F. and Allison, J. P.},
  year = {1996},
  month = mar,
  journal = {Science (New York, N.Y.)},
  volume = {271},
  number = {5256},
  pages = {1734--1736},
  issn = {0036-8075},
  doi = {10.1126/science.271.5256.1734},
  abstract = {One reason for the poor immunogenicity of many tumors may be that they cannot provide signals for CD28-mediated costimulation necessary to fully activate T cells. It has recently become apparent that CTLA-4, a second counterreceptor for the B7 family of costimulatory molecules, is a negative regulator of T cell activation. Here, in vivo administration of antibodies to CTLA-4 resulted in the rejection of tumors, including preestablished tumors. Furthermore, this rejection resulted in immunity to a secondary exposure to tumor cells. These results suggest that blockade of the inhibitory effects of CTLA-4 can allow for, and potentiate, effective immune responses against tumor cells.},
  langid = {english},
  pmid = {8596936},
  keywords = {Abatacept,Animals,Antibodies,Antigens CD,Antigens Differentiation,B7-1 Antigen,CD28 Antigens,CTLA-4 Antigen,Female,Graft Rejection,Immunoconjugates,Immunologic Memory,Lymphocyte Activation,Mice,Mice Inbred A,Mice Inbred BALB C,Neoplasm Transplantation,Neoplasms Experimental,T-Lymphocytes,Transfection,Tumor Cells Cultured},
  annotation = {TLDR: In vivo administration of antibodies to CTLA-4 resulted in the rejection of tumors, including preestablished tumors, and this rejection resulted in immunity to a secondary exposure to tumor cells, suggesting that blockade of the inhibitory effects of CTLA4 can allow for, and potentiate, effective immune responses against tumor cells.},
  timestamp = {2025-05-28T00:01:16Z}
}

@article{lee1999nmf,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D and Seung, H Sebastian},
  year = {1999},
  journal = {Nature},
  volume = {401},
  number = {6755},
  pages = {788--791},
  publisher = {Nature Publishing Group},
  timestamp = {2025-09-06T03:06:25Z}
}

@article{lee2018us,
  title = {U.{{S}}. {{Food}} and {{Drug Administration Precertification Pilot Program}} for {{Digital Health Software}}: {{Weighing}} the {{Benefits}} and {{Risks}}},
  shorttitle = {U.{{S}}. {{Food}} and {{Drug Administration Precertification Pilot Program}} for {{Digital Health Software}}},
  author = {Lee, Theodore T. and Kesselheim, Aaron S.},
  year = {2018},
  month = may,
  journal = {Annals of Internal Medicine},
  volume = {168},
  number = {10},
  pages = {730--732},
  issn = {0003-4819, 1539-3704},
  doi = {10.7326/M17-2715},
  urldate = {2025-04-06},
  copyright = {https://www.acpjournals.org/journal/aim/text-and-data-mining},
  langid = {english},
  annotation = {TLDR: Current controversies about the regulation of health-related software as a medical device and the implications of the Pre-Cert program are reviewed.},
  timestamp = {2025-04-06T13:05:59Z}
}

@misc{lee2019generation,
  title = {Generation of {{Multimodal Justification Using Visual Word Constraint Model}} for {{Explainable Computer-Aided Diagnosis}}},
  author = {Lee, Hyebin and Kim, Seong Tae and Ro, Yong Man},
  year = {2019},
  month = jun,
  number = {arXiv:1906.03922},
  eprint = {1906.03922},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.03922},
  urldate = {2025-07-25},
  abstract = {The ambiguity of the decision-making process has been pointed out as the main obstacle to applying the deep learning-based method in a practical way in spite of its outstanding performance. Interpretability could guarantee the confidence of deep learning system, therefore it is particularly important in the medical field. In this study, a novel deep network is proposed to explain the diagnostic decision with visual pointing map and diagnostic sentence justifying result simultaneously. For the purpose of increasing the accuracy of sentence generation, a visual word constraint model is devised in training justification generator. To verify the proposed method, comparative experiments were conducted on the problem of the diagnosis of breast masses. Experimental results demonstrated that the proposed deep network could explain diagnosis more accurately with various textual justifications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  timestamp = {2025-07-25T01:58:11Z}
}

@article{lee2023validation,
  title = {Validation of a {{Deep Learning Chest X-ray Interpretation Model}}: {{Integrating Large-Scale AI}} and {{Large Language Models}} for {{Comparative Analysis}} with {{ChatGPT}}},
  shorttitle = {Validation of a {{Deep Learning Chest X-ray Interpretation Model}}},
  author = {Lee, Kyu Hong and Lee, Ro Woon and Kwon, Ye Eun},
  year = {2023},
  month = dec,
  journal = {Diagnostics},
  volume = {14},
  number = {1},
  pages = {90},
  issn = {2075-4418},
  doi = {10.3390/diagnostics14010090},
  urldate = {2025-03-03},
  abstract = {This study evaluates the diagnostic accuracy and clinical utility of two artificial intelligence (AI) techniques: Kakao Brain Artificial Neural Network for Chest X-ray Reading (KARA-CXR), an assistive technology developed using large-scale AI and large language models (LLMs), and ChatGPT, a well-known LLM. The study was conducted to validate the performance of the two technologies in chest X-ray reading and explore their potential applications in the medical imaging diagnosis domain. The study methodology consisted of randomly selecting 2000 chest X-ray images from a single institution's patient database, and two radiologists evaluated the readings provided by KARA-CXR and ChatGPT. The study used five qualitative factors to evaluate the readings generated by each model: accuracy, false findings, location inaccuracies, count inaccuracies, and hallucinations. Statistical analysis showed that KARA-CXR achieved significantly higher diagnostic accuracy compared to ChatGPT. In the `Acceptable' accuracy category, KARA-CXR was rated at 70.50\% and 68.00\% by two observers, while ChatGPT achieved 40.50\% and 47.00\%. Interobserver agreement was moderate for both systems, with KARA at 0.74 and GPT4 at 0.73. For `False Findings', KARA-CXR scored 68.00\% and 68.50\%, while ChatGPT scored 37.00\% for both observers, with high interobserver agreements of 0.96 for KARA and 0.97 for GPT4. In `Location Inaccuracy' and `Hallucinations', KARA-CXR outperformed ChatGPT with significant margins. KARA-CXR demonstrated a non-hallucination rate of 75\%, which is significantly higher than ChatGPT's 38\%. The interobserver agreement was high for KARA (0.91) and moderate to high for GPT4 (0.85) in the hallucination category. In conclusion, this study demonstrates the potential of AI and large-scale language models in medical imaging and diagnostics. It also shows that in the chest X-ray domain, KARA-CXR has relatively higher accuracy than ChatGPT.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  timestamp = {2025-03-03T07:55:24Z}
}

@article{lemsara2020pathme,
  title = {{{PathME}}: Pathway Based Multi-Modal Sparse Autoencoders for Clustering of Patient-Level Multi-Omics Data},
  shorttitle = {{{PathME}}},
  author = {Lemsara, Amina and Ouadfel, Salima and Fr{\"o}hlich, Holger},
  year = {2020},
  month = apr,
  journal = {BMC Bioinformatics},
  volume = {21},
  number = {1},
  pages = {146},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-3465-2},
  urldate = {2025-07-25},
  abstract = {Recent years have witnessed an increasing interest in multi-omics data, because these data allow for better understanding complex diseases such as cancer on a molecular system level. In addition, multi-omics data increase the chance to robustly identify molecular patient sub-groups and hence open the door towards a better personalized treatment of diseases. Several methods have been proposed for unsupervised clustering of multi-omics data. However, a number of challenges remain, such as the magnitude of features and the large difference in dimensionality across different omics data sources.},
  keywords = {Deep learning,Multi-omics,Patient clustering},
  annotation = {TLDR: A suggested multi-modal sparse denoising autoencoder framework coupled with sparse non-negative matrix factorization approach allows for an effective and interpretable integration of multi-omics data on pathway level while addressing the high dimensional character of omics data.},
  timestamp = {2025-07-25T13:46:00Z}
}

@article{lexical_innovation,
  title = {Lexical Innovation},
  author = {{contributors}, Wikipedia},
  year = {2025},
  journal = {Wikipedia, The Free Encyclopedia},
  timestamp = {2025-09-06T00:36:49Z}
}

@article{li2020timephased,
  title = {A {{Time-Phased Machine Learning Model}} for {{Real-Time Prediction}} of {{Sepsis}} in {{Critical Care}}},
  author = {Li, Xiang and Xu, Xiao and Xie, Fei and Xu, Xian and Sun, Yuyao and Liu, Xiaoshuang and Jia, Xiaoyu and Kang, Yanni and Xie, Lixin and Wang, Fei and Xie, Guotong},
  year = {2020},
  month = oct,
  journal = {Critical Care Medicine},
  volume = {48},
  number = {10},
  pages = {e884-e888},
  issn = {1530-0293},
  doi = {10.1097/CCM.0000000000004494},
  abstract = {OBJECTIVES: As a life-threatening condition, sepsis is one of the major public health issues worldwide. Early prediction can improve sepsis outcomes with appropriate interventions. With the PhysioNet/Computing in Cardiology Challenge 2019, we aimed to develop and validate a machine learning algorithm with high prediction performance and clinical interpretability for prediction of sepsis onset during critical care in real-time. DESIGN: Retrospective observational cohort study. SETTING: The dataset was collected from three ICUs in three different U.S. hospitals. Two of them were publicly available for model development (offline) and one was used for testing (online). PATIENTS: Forty-thousand three-hundred thirty-six ICU patients from the two model development databases and 24,819 from the test database. There are up to 40 hourly-recorded clinical variables for each ICU stay. The Sepsis-3 criteria were used to confirm sepsis onset. INTERVENTIONS: None. MEASUREMENTS AND MAIN RESULTS: Three-hundred twelve features were constructed hourly as the input of our proposed Time-phAsed machine learning model for Sepsis Prediction. Time-phAsed machine learning model for Sepsis Prediction first estimates the likelihood of sepsis onset for each hour of an ICU stay in the following 6 hours, and then makes a binary prediction with three time-phased cutoff values. On the internal validation set, the utility score (official challenge measurement) achieved by Time-phAsed machine learning model for Sepsis Prediction was 0.430. On the test set, the utility score reached was 0.354. Furthermore, Time-phAsed machine learning model for Sepsis Prediction provides an intuitive way to illustrate the impact of the input features on the outcome prediction, which makes it clinically interpretable. CONCLUSIONS: The proposed Time-phAsed machine learning model for Sepsis Prediction model is accurate and interpretable for real-time prediction of sepsis onset in critical care, which holds great potential for further evaluation in prospective studies.},
  langid = {english},
  pmid = {32931194},
  keywords = {Critical Care,Early Diagnosis,Humans,Intensive Care Units,Machine Learning,Reproducibility of Results,Retrospective Studies,Sepsis,Severity of Illness Index,Time Factors,Vital Signs},
  annotation = {TLDR: The proposed Time-phAsed machine learning model for Sepsis Prediction model is accurate and interpretable for real-time prediction of sepsis onset in critical care, which holds great potential for further evaluation in prospective studies.},
  timestamp = {2025-05-01T08:54:16Z}
}

@misc{li2021causalbert,
  title = {{{CausalBERT}}: {{Injecting Causal Knowledge Into Pre-trained Models}} with {{Minimal Supervision}}},
  shorttitle = {{{CausalBERT}}},
  author = {Li, Zhongyang and Ding, Xiao and Liao, Kuo and Qin, Bing and Liu, Ting},
  year = {2021},
  month = aug,
  number = {arXiv:2107.09852},
  eprint = {2107.09852},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.09852},
  urldate = {2025-04-06},
  abstract = {Recent work has shown success in incorporating pre-trained models like BERT to improve NLP systems. However, existing pre-trained models lack of causal knowledge which prevents today's NLP systems from thinking like humans. In this paper, we investigate the problem of injecting causal knowledge into pre-trained models. There are two fundamental problems: 1) how to collect various granularities of causal pairs from unstructured texts; 2) how to effectively inject causal knowledge into pre-trained models. To address these issues, we extend the idea of CausalBERT from previous studies, and conduct experiments on various datasets to evaluate its effectiveness. In addition, we adopt a regularization-based method to preserve the already learned knowledge with an extra regularization term while injecting causal knowledge. Extensive experiments on 7 datasets, including four causal pair classification tasks, two causal QA tasks and a causal inference task, demonstrate that CausalBERT captures rich causal knowledge and outperforms all pre-trained models-based state-of-the-art methods, achieving a new causal inference benchmark.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computation and Language},
  timestamp = {2025-04-06T09:39:23Z}
}

@inproceedings{li2021dual,
  title = {Dual-Stream {{Multiple Instance Learning Network}} for {{Whole Slide Image Classification}} with {{Self-supervised Contrastive Learning}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Bin and Li, Yin and Eliceiri, Kevin W.},
  year = {2021},
  month = jun,
  pages = {14313--14323},
  publisher = {IEEE},
  address = {Nashville, TN, USA},
  doi = {10.1109/CVPR46437.2021.01409},
  urldate = {2025-04-13},
  abstract = {We address the challenging problem of whole slide image (WSI) classification. WSIs have very high resolutions and usually lack localized annotations. WSI classification can be cast as a multiple instance learning (MIL) problem when only slide-level labels are available. We propose a MILbased method for WSI classification and tumor detection that does not require localized annotations. Our method has three major components. First, we introduce a novel MIL aggregator that models the relations of the instances in a dual-stream architecture with trainable distance measurement. Second, since WSIs can produce large or unbalanced bags that hinder the training of MIL models, we propose to use self-supervised contrastive learning to extract good representations for MIL and alleviate the issue of prohibitive memory cost for large bags. Third, we adopt a pyramidal fusion mechanism for multiscale WSI features, and further improve the accuracy of classification and localization. Our model is evaluated on two representative WSI datasets. The classification accuracy of our model compares favorably to fully-supervised methods, with less than 2\% accuracy gap across datasets. Our results also outperform all previous MIL-based methods. Additional benchmark results on standard MIL datasets further demonstrate the superior performance of our MIL aggregator on general MIL problems.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-6654-4509-2},
  langid = {english},
  timestamp = {2025-04-13T07:07:37Z}
}

@misc{li2022emerginga,
  title = {Emerging {{Artificial Intelligence Applications}} in {{Spatial Transcriptomics Analysis}}},
  author = {Li, Yijun and Stanojevic, Stefan and Garmire, Lana X.},
  year = {2022},
  month = mar,
  number = {arXiv:2203.09664},
  eprint = {2203.09664},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.09664},
  urldate = {2025-04-17},
  abstract = {Spatial transcriptomics (ST) has advanced significantly in the last few years. Such advancement comes with the urgent need for novel computational methods to handle the unique challenges of ST data analysis. Many artificial intelligence (AI) methods have been developed to utilize various machine learning and deep learning techniques for computational ST analysis. This review provides a comprehensive and up-to-date survey of current AI methods for ST analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Genomics},
  timestamp = {2025-04-17T11:42:11Z}
}

@article{li2022survey,
  title = {A {{Survey}} of {{Data-Driven}} and {{Knowledge-Aware eXplainable AI}}},
  author = {Li, Xiao-Hui and Cao, Caleb Chen and Shi, Yuhan and Bai, Wei and Gao, Han and Qiu, Luyu and Wang, Cong and Gao, Yuanyuan and Zhang, Shenjia and Xue, Xun and Chen, Lei},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {1},
  pages = {29--49},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2020.2983930},
  urldate = {2025-03-29},
  abstract = {We are witnessing a fast development of Artificial Intelligence (AI), but it becomes dramatically challenging to explain AI models in the past decade. ``Explanation'' has a flexible philosophical concept of ``satisfying the subjective curiosity for causal information'', driving a wide spectrum of methods being invented and/or adapted from many aspects and communities, including machine learning, visual analytics, human-computer interaction and so on. Nevertheless, from the view-point of data and knowledge engineering (DKE), a best explaining practice that is cost-effective in terms of extra intelligence acquisition should exploit the causal information and explaining scenarios which is hidden richly in the data itself. In the past several years, there are plenty of works contributing in this line but there is a lack of a clear taxonomy and systematic review of the current effort. To this end, we propose this survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics. Specifically, we categorize methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated. Furthermore, in the light of practice, we provide survey of state-of-art evaluation metrics and deployed explanation applications in industrial practice.},
  keywords = {algorithms,Data models,Data visualization,deep learning,eXplainable AI (XAI),Feature extraction,knowledge-base,metrics,Predictive models,Task analysis,Taxonomy},
  annotation = {TLDR: A survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics, and categorizing methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated.},
  timestamp = {2025-03-29T02:55:52Z}
}

@article{li2023,
  title = {Emergent World Representations: {{Exploring}} a Sequence Model Trained on a Synthetic Task},
  author = {Li, Kevin and Darrell, Trevor and Mordatch, Igor},
  year = {2023},
  journal = {Transactions on Machine Learning Research},
  timestamp = {2025-09-06T00:42:24Z}
}

@misc{li2023balancing,
  title = {Balancing {{Privacy Protection}} and {{Interpretability}} in {{Federated Learning}}},
  author = {Li, Zhe and Chen, Honglong and Ni, Zhichen and Shao, Huajie},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.08044},
  urldate = {2025-04-04},
  abstract = {Federated learning (FL) aims to collaboratively train the global model in a distributed manner by sharing the model parameters from local clients to a central server, thereby potentially protecting users' private information. Nevertheless, recent studies have illustrated that FL still suffers from information leakage as adversaries try to recover the training data by analyzing shared parameters from local clients. To deal with this issue, differential privacy (DP) is adopted to add noise to the gradients of local models before aggregation. It, however, results in the poor performance of gradient-based interpretability methods, since some weights capturing the salient region in feature map will be perturbed. To overcome this problem, we propose a simple yet effective adaptive differential privacy (ADP) mechanism that selectively adds noisy perturbations to the gradients of client models in FL. We also theoretically analyze the impact of gradient perturbation on the model interpretability. Finally, extensive experiments on both IID and Non-IID data demonstrate that the proposed ADP can achieve a good trade-off between privacy and interpretability in FL.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Cryptography and Security (cs.CR),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: This work proposes a simple yet effective adaptive differential privacy (ADP) mechanism that selectively adds noisy perturbations to the gradients of client models in FL and theoretically analyzes the impact of gradient perturbation on the model interpretability.},
  timestamp = {2025-04-04T06:37:26Z}
}

@misc{li2023causal,
  title = {Causal {{Recurrent Variational Autoencoder}} for {{Medical Time Series Generation}}},
  author = {Li, Hongming and Yu, Shujian and Principe, Jose},
  year = {2023},
  month = jan,
  number = {arXiv:2301.06574},
  eprint = {2301.06574},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.06574},
  urldate = {2025-05-04},
  abstract = {We propose causal recurrent variational autoencoder (CR-VAE), a novel generative model that is able to learn a Granger causal graph from a multivariate time series x and incorporates the underlying causal mechanism into its data generation process. Distinct to the classical recurrent VAEs, our CR-VAE uses a multi-head decoder, in which the \$p\$-th head is responsible for generating the \$p\$-th dimension of \${\textbackslash}mathbf\{x\}\$ (i.e., \${\textbackslash}mathbf\{x\}{\textasciicircum}p\$). By imposing a sparsity-inducing penalty on the weights (of the decoder) and encouraging specific sets of weights to be zero, our CR-VAE learns a sparse adjacency matrix that encodes causal relations between all pairs of variables. Thanks to this causal matrix, our decoder strictly obeys the underlying principles of Granger causality, thereby making the data generating process transparent. We develop a two-stage approach to train the overall objective. Empirically, we evaluate the behavior of our model in synthetic data and two real-world human brain datasets involving, respectively, the electroencephalography (EEG) signals and the functional magnetic resonance imaging (fMRI) data. Our model consistently outperforms state-of-the-art time series generative models both qualitatively and quantitatively. Moreover, it also discovers a faithful causal graph with similar or improved accuracy over existing Granger causality-based causal inference methods. Code of CR-VAE is publicly available at https://github.com/hongmingli1995/CR-VAE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  annotation = {TLDR: The proposed causal recurrent variational autoencoder (CR-VAE), a novel generative model that is able to learn a Granger causal graph from a multivariate time series x and incorporates the underlying causal mechanism into its data generation process, consistently outperforms state-of theart time series generative models both qualitatively and quantitatively.},
  timestamp = {2025-05-04T09:38:03Z}
}

@article{li2023deep,
  title = {Deep {{Learning Attention Mechanism}} in {{Medical Image Analysis}}: {{Basics}} and {{Beyonds}}},
  shorttitle = {Deep {{Learning Attention Mechanism}} in {{Medical Image Analysis}}},
  author = {Li, Xiang and Li, Minglei and Yan, Pengfei and Li, Guanyi and Jiang, Yuchen and Luo, Hao and Yin, Shen},
  year = {2023},
  month = mar,
  journal = {International Journal of Network Dynamics and Intelligence},
  pages = {93--116},
  issn = {2653-6226},
  doi = {10.53941/ijndi0201006},
  urldate = {2025-05-04},
  abstract = {Survey/review study Deep Learning Attention Mechanism in Medical Image Analysis: Basics and Beyonds Xiang Li~1, Minglei Li~1, Pengfei Yan~1, Guanyi Li~1, Yuchen Jiang~1, Hao Luo~1,*, and Shen Yin~2 1~Department of Control Science and Engineering, Harbin Institute of Technology, Harbin 150001, China 2~Department of Mechanical and Industrial Engineering, Faculty of Engineering, Norwegian University of Science and Technology, Trondheim 7034, Norway *~Correspondence:~hao.luo@hit.edu.cn ~ ~ Received: 16 October 2022 Accepted: 25 November 2022 Published:~27~March~2023 ~ Abstract:~With the improvement of hardware computing power and the development of deep learning algorithms, a revolution of "artificial intelligence (AI) + medical image" is taking place. Benefiting from diversified modern medical measurement equipment, a large number of medical images will be produced in the clinical process. These images improve the diagnostic accuracy of doctors, but also increase the labor burden of doctors. Deep learning technology is expected to realize an auxiliary diagnosis and improve diagnostic efficiency. At present, the method of deep learning technology combined with attention mechanism is a research hotspot and has achieved state-of-the-art results in many medical image tasks. This paper reviews the deep learning attention methods in medical image analysis. A comprehensive literature survey is first conducted to analyze the keywords and literature. Then, we introduce the development and technical characteristics of the attention mechanism. For its application in medical image analysis, we summarize the related methods in medical image classification, segmentation, detection, and enhancement. The remaining challenges, potential solutions, and future research directions are also discussed.},
  copyright = {Copyright (c) 2023 by the authors.},
  langid = {english},
  annotation = {TLDR: The method of deep learning technology combined with attention mechanism is a research hotspot and has achieved state-of-the-art results in many medical image tasks.},
  timestamp = {2025-05-04T04:15:01Z}
}

@inproceedings{li2023probiore,
  title = {{{ProBioRE}}: {{A Framework}} for {{Biomedical Causal Relation Extraction Based}} on {{Dual-head Prompt}} and {{Prototypical Network}}},
  shorttitle = {{{ProBioRE}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Li, Lishuang and Ning, Wanting},
  year = {2023},
  month = dec,
  pages = {2071--2074},
  publisher = {IEEE},
  address = {Istanbul, Turkiye},
  doi = {10.1109/BIBM58861.2023.10385919},
  urldate = {2025-04-03},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-3748-8},
  annotation = {TLDR: This work proposes a framework for biomedical event causal relation extraction based on dual-head prompt augmentation and prototypical network filtering and validate the effectiveness of the method by comparing the framework with existing methods.},
  timestamp = {2025-04-03T02:46:06Z}
}

@article{li2024association,
  title = {Association between Inflammation Markers and All-Cause Mortality in Critical Ill Patients with Atrial Fibrillation: {{Analysis}} of the {{Multi-Parameter Intelligent Monitoring}} in {{Intensive Care}} ({{MIMIC-IV}}) Database},
  shorttitle = {Association between Inflammation Markers and All-Cause Mortality in Critical Ill Patients with Atrial Fibrillation},
  author = {Li, Qian and Nie, Jian and Cao, Miaomiao and Luo, Chaodi and Sun, Chaofeng},
  year = {2024},
  month = apr,
  journal = {IJC Heart \& Vasculature},
  volume = {51},
  pages = {101372},
  issn = {2352-9067},
  doi = {10.1016/j.ijcha.2024.101372},
  urldate = {2025-04-09},
  abstract = {Background Inflammation is related to cardiovascular disease. Among the many inflammatory markers, neutrophil-to-lymphocyte ratio (NLR), platelet-to-lymphocyte ratio (PLR), and systemic immune-inflammatory index (SII) were considered as novel predictors for atherosclerosis outcomes. We aimed to investigate the impact of these inflammatory markers on the prognosis of patients with atrial fibrillation (AF). Methods We obtained data on AF patients from the Medical Information Mart for Intensive Care (MIMIC)-IV database. These patients were classified into two groups based on their survival status within 30~days. Then, they were divided into three groups based on the tertile of baseline NLR, PLR, and SII, respectively. We comprehensively explored the relationship between those inflammatory indicators and all-cause mortality in patients with AF by Kaplan-Meier analysis, multivariate Cox regression analysis, receiver operating characteristic (ROC) analyses, restricted cubic spline regression (RCS), and subgroup analysis. Results A total of 4562 patients with AF were included. Statistically significant differences were found between survivor and non-survivor groups for NLR, PLR and SII. Patients in the high tertile of the NLR had a higher mortality rate than those in the low and intermediate tertiles, as did patients in the PLR and the SII. NLR, PLR and SII were independently associated with increased risk of all-cause mortality. RCS showed that the 30-day and 365-day risk of death were linearly associated with increases in NLR, PLR, and SII, respectively. Conclusion NLR, PLR, and SII have the potential to be used as indicators for stratifying the risk of mortality in critically ill patients with AF.},
  keywords = {Atrial fibrillation,Inflammation marker,Neutrophil-to-lymphocyte ratio,Platelet-to-lymphocyte ratio,Systemic immune-inflammatory index},
  timestamp = {2025-04-09T02:34:32Z}
}

@article{li2024cisepsis,
  title = {{{CISepsis}}: A Causal Inference Framework for Early Sepsis Detection},
  shorttitle = {{{CISepsis}}},
  author = {Li, Qiang and Li, Dongchen and Jiao, He and Wu, Zhenhua and Nie, Weizhi},
  year = {2024},
  month = nov,
  journal = {Frontiers in Cellular and Infection Microbiology},
  volume = {14},
  pages = {1488130},
  issn = {2235-2988},
  doi = {10.3389/fcimb.2024.1488130},
  urldate = {2025-05-20},
  abstract = {Introduction               The early prediction of sepsis based on machine learning or deep learning has achieved good results.Most of the methods use structured data stored in electronic medical records, but the pathological characteristics of sepsis involve complex interactions between multiple physiological systems and signaling pathways, resulting in mixed structured data. Some researchers will introduce unstructured data when also introduce confounders. These confounders mask the direct causality of sepsis, leading the model to learn misleading correlations. Finally, it affects the generalization ability, robustness, and interpretability of the model.                                         Methods               To address this challenge, we propose an early sepsis prediction approach based on causal inference which can remove confounding effects and capture causal relationships. First, we analyze the relationship between each type of observation, confounder, and label to create a causal structure diagram. To eliminate the effects of different confounders separately, the methods of back-door adjustment and instrumental variable are used. Specifically, we learn the confounder and an instrumental variable based on mutual information from various observed data and eliminate the influence of the confounder by optimizing mutual information. We use back-door adjustment to eliminate the influence of confounders in clinical notes and static indicators on the true causal effect.                                         Results               Our method, named CISepsis, was validated on the MIMIC-IV dataset. Compared to existing state-of-the-art early sepsis prediction models such as XGBoost, LSTM, and MGP-AttTCN, our method demonstrated a significant improvement in AUC. Specifically, our model achieved AUC values of 0.921, 0.920, 0.919, 0.923, 0.924, 0.926, and 0.926 at the 6, 5, 4, 3, 2, 1, and 0 time points, respectively. Furthermore, the effectiveness of our method was confirmed through ablation experiments.                                         Discussion               Our method, based on causal inference, effectively removes the influence of confounding factors, significantly improving the predictive accuracy of the model. Compared to traditional methods, this adjustment allows for a more accurate capture of the true causal effects of sepsis, thereby enhancing the model's generalizability, robustness, and interpretability. Future research will explore the impact of specific indicators or treatment interventions on sepsis using counterfactual adjustments in causal inference, as well as investigate the potential clinical application of our method.},
  langid = {english},
  keywords = {Algorithms,back-door intervention,causal inference,Deep Learning,Early Diagnosis,Electronic Health Records,Humans,instrumental variable,Machine Learning,MIMIC-IV,sepsis,Sepsis},
  annotation = {TLDR: This method, based on causal inference, effectively removes the influence of confounding factors, significantly improving the predictive accuracy of the model and enhancing the model's generalizability, robustness, and interpretability.},
  timestamp = {2025-05-20T20:52:07Z}
}

@article{li2024integrated,
  title = {Integrated Image-Based Deep Learning and Language Models for Primary Diabetes Care},
  author = {Li, Jiajia and Guan, Zhouyu and Wang, Jing and Cheung, Carol Y. and Zheng, Yingfeng and Lim, Lee-Ling and Lim, Cynthia Ciwei and Ruamviboonsuk, Paisan and Raman, Rajiv and Corsino, Leonor and {Echouffo-Tcheugui}, Justin B. and Luk, Andrea O. Y. and Chen, Li Jia and Sun, Xiaodong and Hamzah, Haslina and Wu, Qiang and Wang, Xiangning and Liu, Ruhan and Wang, Ya Xing and Chen, Tingli and Zhang, Xiao and Yang, Xiaolong and Yin, Jun and Wan, Jing and Du, Wei and Quek, Ten Cheer and Goh, Jocelyn Hui Lin and Yang, Dawei and Hu, Xiaoyan and Nguyen, Truong X. and Szeto, Simon K. H. and Chotcomwongse, Peranut and Malek, Rachid and Normatova, Nargiza and Ibragimova, Nilufar and Srinivasan, Ramyaa and Zhong, Pingting and Huang, Wenyong and Deng, Chenxin and Ruan, Lei and Zhang, Cuntai and Zhang, Chenxi and Zhou, Yan and Wu, Chan and Dai, Rongping and Koh, Sky Wei Chee and Abdullah, Adina and Hee, Nicholas Ken Yoong and Tan, Hong Chang and Liew, Zhong Hong and Tien, Carolyn Shan-Yeu and Kao, Shih Ling and Lim, Amanda Yuan Ling and Mok, Shao Feng and Sun, Lina and Gu, Jing and Wu, Liang and Li, Tingyao and Cheng, Di and Wang, Zheyuan and Qin, Yiming and Dai, Ling and Meng, Ziyao and Shu, Jia and Lu, Yuwei and Jiang, Nan and Hu, Tingting and Huang, Shan and Huang, Gengyou and Yu, Shujie and Liu, Dan and Ma, Weizhi and Guo, Minyi and Guan, Xinping and Yang, Xiaokang and Bascaran, Covadonga and Cleland, Charles R. and Bao, Yuqian and Ekinci, Elif I. and Jenkins, Alicia and Chan, Juliana C. N. and Bee, Yong Mong and Sivaprasad, Sobha and Shaw, Jonathan E. and Sim{\'o}, Rafael and Keane, Pearse A. and Cheng, Ching-Yu and Tan, Gavin Siew Wei and Jia, Weiping and Tham, Yih-Chung and Li, Huating and Sheng, Bin and Wong, Tien Yin},
  year = {2024},
  month = oct,
  journal = {Nature Medicine},
  volume = {30},
  number = {10},
  pages = {2886--2896},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-024-03139-8},
  urldate = {2025-04-12},
  abstract = {Primary diabetes care and diabetic retinopathy (DR) screening persist as major public health challenges due to a shortage of trained primary care physicians (PCPs), particularly in low-resource settings. Here, to bridge the gaps, we developed an integrated image--language system (DeepDR-LLM), combining a large language model (LLM module) and image-based deep learning (DeepDR-Transformer), to provide individualized diabetes management recommendations to PCPs. In a retrospective evaluation, the LLM module demonstrated comparable performance to PCPs and endocrinology residents when tested in English and outperformed PCPs and had comparable performance to endocrinology residents in Chinese. For identifying referable DR, the average PCP's accuracy was 81.0\% unassisted and 92.3\% assisted by DeepDR-Transformer. Furthermore, we performed a single-center real-world prospective study, deploying DeepDR-LLM. We compared diabetes management adherence of patients under the unassisted PCP arm (n\,=\,397) with those under the PCP+DeepDR-LLM arm (n\,=\,372). Patients with newly diagnosed diabetes in the PCP+DeepDR-LLM arm showed better self-management behaviors throughout follow-up (P\,{$<$}\,0.05). For patients with referral DR, those in the PCP+DeepDR-LLM arm were more likely to adhere to DR referrals (P\,{$<$}\,0.01). Additionally, DeepDR-LLM deployment improved the quality and empathy level of management recommendations. Given its multifaceted performance, DeepDR-LLM holds promise as a digital solution for enhancing primary diabetes care and DR screening.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Diabetes,Diabetes complications},
  annotation = {TLDR: An integrated image--language system (DeepDR-LLM), combining a large language model (LLM module) and image-based deep learning (DeepDR-Transformer) to provide individualized diabetes management recommendations to PCPs, holds promise as a digital solution for enhancing primary diabetes care and DR screening.},
  timestamp = {2025-04-12T06:07:40Z}
}

@article{li2024machine,
  title = {Machine {{Learning}} and {{Mendelian Randomization Reveal Molecular Mechanisms}} and {{Causal Relationships}} of {{Immune}}-{{Related Biomarkers}} in {{Periodontitis}}},
  author = {Li, Yuan and Zhang, Bolun and Li, Dengke and Zhang, Yu and Xue, Yang and Hu, Kaijin},
  editor = {Tsuji, Fumio},
  year = {2024},
  month = jan,
  journal = {Mediators of Inflammation},
  volume = {2024},
  number = {1},
  pages = {9983323},
  issn = {0962-9351, 1466-1861},
  doi = {10.1155/mi/9983323},
  urldate = {2025-06-13},
  abstract = {This study aimed to investigate the molecular mechanisms of periodontitis and identify key immune-related biomarkers using machine learning and Mendelian randomization (MR). Differentially expressed gene (DEG) analysis was performed on periodontitis datasets GSE16134 and GSE10334 from the Gene Expression Omnibus (GEO) database, followed by weighted gene co-expression network analysis (WGCNA) to identify relevant gene modules. Various machine learning algorithms were utilized to construct predictive models, highlighting core genes, while MR assessed the causal relationships between these genes and periodontitis. Additionally, immune infiltration analysis and single-cell sequencing were employed to explore the roles of key genes in immunity and their expression across different cell types. The integration of machine learning, MR, and single-cell sequencing represents a novel approach that significantly enhances our understanding of the immune dynamics and gene interactions in periodontitis. The study identified 682 significant DEGs, with WGCNA revealing seven gene modules associated with periodontitis and 471 core candidate genes. Among the 113 machine learning algorithms tested, XGBoost was the most effective in identifying periodontitis samples, leading to the selection of 19 core genes. MR confirmed significant causal relationships between CD93, CD69, and CXCL6 and periodontitis. Further analysis showed that these genes were correlated with various immune cells and exhibited specific expression patterns in periodontitis tissues. The findings suggest that CD93, CD69, and CXCL6 are closely related to the progression of periodontitis, with MR confirming their causal links to the disease. These genes have potential applications in the diagnosis and treatment of periodontitis, offering new insights into the disease's molecular mechanisms and providing valuable resources for precision medicine approaches in periodontitis management. Limitations of this study include the demographic and sample size constraints of the datasets, which may impact the generalizability of the findings. Future research is needed to validate these biomarkers in larger, diverse cohorts and to investigate their functional roles in the pathogenesis of periodontitis.},
  langid = {english},
  annotation = {TLDR: The findings suggest that CD93, CD69, and CXCL6 are closely related to the progression of periodontitis, with MR confirming their causal links to the disease.},
  timestamp = {2025-06-13T03:40:05Z}
}

@misc{li2024timeaware,
  title = {Time-Aware {{Heterogeneous Graph Transformer}} with {{Adaptive Attention Merging}} for {{Health Event Prediction}}},
  author = {Li, Shibo and Cheng, Hengliang and Li, Weihua},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2404.14815},
  urldate = {2025-06-13},
  abstract = {The widespread application of Electronic Health Records (EHR) data in the medical field has led to early successes in disease risk prediction using deep learning methods. These methods typically require extensive data for training due to their large parameter sets. However, existing works do not exploit the full potential of EHR data. A significant challenge arises from the infrequent occurrence of many medical codes within EHR data, limiting their clinical applicability. Current research often lacks in critical areas: 1) incorporating disease domain knowledge; 2) heterogeneously learning disease representations with rich meanings; 3) capturing the temporal dynamics of disease progression. To overcome these limitations, we introduce a novel heterogeneous graph learning model designed to assimilate disease domain knowledge and elucidate the intricate relationships between drugs and diseases. This model innovatively incorporates temporal data into visit-level embeddings and leverages a time-aware transformer alongside an adaptive attention mechanism to produce patient representations. When evaluated on two healthcare datasets, our approach demonstrated notable enhancements in both prediction accuracy and interpretability over existing methodologies, signifying a substantial advancement towards personalized and proactive healthcare management.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: A novel heterogeneous graph learning model designed to assimilate disease domain knowledge and elucidate the intricate relationships between drugs and diseases is introduced, which innovatively incorporates temporal data into visit-level embeddings and leverages a time-aware transformer alongside an adaptive attention mechanism to produce patient representations.},
  timestamp = {2025-06-13T07:48:51Z}
}

@article{li2025interpretable,
  title = {Interpretable Deep Learning of Single-Cell and Epigenetic Data Reveals Novel Molecular Insights in Aging},
  author = {Li, Zhi-Peng and Du, Zhaozhen and Huang, De-Shuang and Teschendorff, Andrew E.},
  year = {2025},
  month = feb,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {5048},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-89646-1},
  abstract = {Deep learning (DL) and explainable artificial intelligence (XAI) have emerged as powerful machine-learning tools to identify complex predictive data patterns in a spatial or temporal domain. Here, we consider the application of DL and XAI to large omic datasets, in order to study biological aging at the molecular level. We develop an advanced multi-view graph-level representation learning (MGRL) framework that integrates prior biological network information, to build molecular aging clocks at cell-type resolution, which we subsequently interpret using XAI. We apply this framework to one of the largest single-cell transcriptomic datasets encompassing over a million immune cells from 981 donors, revealing a ribosomal gene subnetwork, whose expression correlates with age independently of cell-type. Application of the same DL-XAI framework to DNA methylation data of sorted monocytes reveals an epigenetically deregulated inflammatory response pathway whose activity increases with age. We show that the ribosomal module and inflammatory pathways would not have been discovered had we used more standard machine-learning methods. In summary, the computational deep learning framework presented here illustrates how deep learning when combined with explainable AI tools, can reveal novel biological insights into the complex process of aging.},
  langid = {american},
  pmcid = {PMC11814351},
  pmid = {39934290},
  keywords = {Aged,Aging,AI,Deep Learning,Deep-learning,DNA Methylation,Epigenesis Genetic,Epigenetics,Epigenomics,Explainable AI,Graph-level representation learning,Humans,Single-cell,Single-Cell Analysis,Transcriptome},
  annotation = {TLDR: An advanced multi-view graph-level representation learning (MGRL) framework is developed that integrates prior biological network information, to build molecular aging clocks at cell-type resolution, which are interpreted using XAI.},
  timestamp = {2025-07-24T10:31:55Z}
}

@article{liang2024advances,
  title = {Advances in {{Causal Inference Methods}} for {{Biological Network Analysis}}},
  author = {Liang, Kaiwen and Lin, Jiefu},
  year = {2024},
  journal = {Computational Molecular Biology},
  issn = {1927-5587},
  doi = {10.5376/cmb.2024.14.0010},
  urldate = {2025-04-17},
  timestamp = {2025-04-17T11:30:09Z}
}

@article{liang2024interpretable,
  title = {Interpretable {{Spatial Gradient Analysis}} for {{Spatial Transcriptomics Data}}},
  author = {Liang, Qingnan and Soto, Luisa Solis and Haymaker, Cara and Chen, Ken},
  year = {2024},
  month = mar,
  journal = {bioRxiv: The Preprint Server for Biology},
  pages = {2024.03.19.585725},
  issn = {2692-8205},
  doi = {10.1101/2024.03.19.585725},
  abstract = {Cellular anatomy and signaling vary across niches, which can induce gradated gene expressions in subpopulations of cells. Such spatial transcriptomic gradient (STG) makes a significant source of intratumor heterogeneity and can influence tumor invasion, progression, and response to treatment. Here we report Local Spatial Gradient Inference (LSGI), a computational framework that systematically identifies spatial locations with prominent, interpretable STGs from spatial transcriptomic (ST) data. To achieve so, LSGI scrutinizes each sliding window employing non-negative matrix factorization (NMF) combined with linear regression. With LSGI, we demonstrated the identification of spatially proximal yet opposite directed pathway gradients in a glioblastoma dataset. We further applied LSGI to 87 tumor ST datasets reported from nine published studies and identified both pan-cancer and tumor-type specific pathways with gradated expression patterns, such as epithelial mesenchymal transition, MHC complex, and hypoxia. The local gradients were further categorized according to their association to tumor-TME (tumor microenvironment) interface, highlighting the pathways related to spatial transcriptional intratumoral heterogeneity. We conclude that LSGI enables highly interpretable STG analysis which can reveal novel insights in tumor biology from the increasingly reported tumor ST datasets.},
  langid = {english},
  pmcid = {PMC10983986},
  pmid = {38562886},
  annotation = {TLDR: Local Spatial Gradient Inference (LSGI) is reported, a computational framework that systematically identifies spatial locations with prominent, interpretable STGs from spatial transcriptomic (ST) data that can reveal novel insights in tumor biology from the increasingly reported tumor ST datasets.},
  timestamp = {2025-04-21T13:05:20Z}
}

@article{liao2025xscpae,
  title = {X-{{scPAE}}: {{An}} Explainable Deep Learning Model for Embryonic Lineage Allocation Prediction Based on Single-Cell Transcriptomics Revealing Key Genes in Embryonic Cell Development},
  shorttitle = {X-{{scPAE}}},
  author = {Liao, Kai and Yan, Bowei and Ding, Ziyin and Huang, Jian and Fan, Xiaodan and Wu, Shanshan and Chen, Changshui and Li, Haibo},
  year = {2025},
  month = apr,
  journal = {Computers in Biology and Medicine},
  volume = {188},
  pages = {109787},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2025.109787},
  urldate = {2025-04-16},
  abstract = {In single-cell transcriptomics research, accurately predicting cell lineage allocation and identifying differences between lineages are crucial for understanding cell differentiation processes and reducing early pregnancy miscarriages in humans. This paper introduces an explainable PCA-based deep learning attention autoencoder model, X-scPAE (eXplained Single Cell PCA - Attention Auto Encoder), which is built on the Counterfactual Gradient Attribution (CGA) algorithm. The model is designed to predict lineage allocation in human and mouse single-cell transcriptomic data, while identifying and interpreting gene expression differences across lineages to extract key genes. It first reduces dimensionality using Principal Component Analysis (PCA) and ranks the importance of principal components. An autoencoder is then employed for feature extraction, integrating an attention mechanism to capture interactions between features. Finally, the Counterfactual Gradient Attribution algorithm calculates the importance of each feature. The model achieved an accuracy of 0.945 on the test set and 0.977 on the validation set, with other metrics such as F1-score, Precision, and Recall all reaching 0.94. It significantly outperformed both baseline algorithms (XGBoost, SVM, RF, and LR) and advanced approaches like F-Score-SVM, CV2-LR, scChrBin, and TripletCell. Notably, the explainability analysis uncovered key lineage predictor genes for both humans and mice and identified crucial genes distinguishing between developmental stages and lineages. A logistic regression model built using the extracted key genes still achieved an AUROC of 0.92, surpassing the performance of other feature extraction methods, including F-Score, CV2, PCA, random feature selection, and the interpretability method Shapley. Lastly, ablation studies demonstrated the effectiveness of each model component.},
  langid = {american},
  annotation = {TLDR: An explainable PCA-based deep learning attention autoencoder model, X-scPAE (eXplained Single Cell PCA - Attention Auto Encoder), which is built on the Counterfactual Gradient Attribution (CGA) algorithm is introduced.},
  timestamp = {2025-04-16T09:13:13Z}
}

@article{lim2024reporting,
  title = {Reporting Guidelines for Precision Medicine Research of Clinical Relevance: The {{BePRECISE}} Checklist},
  shorttitle = {Reporting Guidelines for Precision Medicine Research of Clinical Relevance},
  author = {Lim, Siew S. and {Semnani-Azad}, Zhila and Morieri, Mario L. and Ng, Ashley H. and Ahmad, Abrar and Fitipaldi, Hugo and Boyle, Jacqueline and Collin, Christian and Dennis, John M. and Langenberg, Claudia and Loos, Ruth J. F. and Morrison, Melinda and Ramsay, Michele and Sanyal, Arun J. and Sattar, Naveed and Hivert, Marie-France and Gomez, Maria F. and Merino, Jordi and Tobias, Deirdre K. and Trenell, Michael I. and Rich, Stephen S. and Sargent, Jennifer L. and Franks, Paul W.},
  year = {2024},
  month = jul,
  journal = {Nature Medicine},
  volume = {30},
  number = {7},
  pages = {1874--1881},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-024-03033-3},
  urldate = {2025-08-12},
  abstract = {Precision medicine should aspire to reduce error and improve accuracy in medical and health recommendations by comparison with contemporary practice, while maintaining safety and cost-effectiveness. The etiology, clinical manifestation and prognosis of diseases such as obesity, diabetes, cardiovascular disease, kidney disease and fatty liver disease are heterogeneous. Without standardized reporting, this heterogeneity, combined with the diversity of research tools used in precision medicine studies, makes comparisons across studies and implementation of the findings challenging. Specific recommendations for reporting precision medicine research do not currently exist. The BePRECISE (Better Precision-data Reporting of Evidence from Clinical Intervention Studies \& Epidemiology) consortium, comprising 23 experts in precision medicine, cardiometabolic diseases, statistics, editorial and lived experience, conducted a scoping review and participated in a modified Delphi and nominal group technique process to develop guidelines for reporting precision medicine research. The BePRECISE checklist comprises 23 items organized into 5 sections that align with typical sections of a scientific publication. A specific section about health equity serves to encourage precision medicine research to be inclusive of individuals and communities that are traditionally under-represented in clinical research and/or underserved by health systems. Adoption of BePRECISE by investigators, reviewers and editors will facilitate and accelerate equitable clinical implementation of precision medicine.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Medical research,Translational research},
  annotation = {TLDR: The BePRECISE consortium, comprising 23 experts in precision medicine, cardiometabolic diseases, statistics, editorial and lived experience, conducted a scoping review and participated in a modified Delphi and nominal group technique process to develop guidelines for reporting precision medicine research.},
  timestamp = {2025-08-12T02:00:41Z}
}

@inproceedings{lin2021what,
  title = {What {{Do You See}}? {{Evaluation}} of {{Explainable Artificial Intelligence}} ({{XAI}}) {{Interpretability}} through {{Neural Backdoors}}},
  shorttitle = {What {{Do You See}}?},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Lin, Yi-Shan and Lee, Wen-Chuan and Celik, Z. Berkay},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {1027--1035},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3447548.3467213},
  urldate = {2025-02-02},
  abstract = {EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.},
  isbn = {978-1-4503-8332-5},
  annotation = {TLDR: This paper proposes backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations and discovers six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region.},
  timestamp = {2025-02-02T09:36:39Z}
}

@article{linardatos2020explainable,
  title = {Explainable {{AI}}: {{A Review}} of {{Machine Learning Interpretability Methods}}},
  shorttitle = {Explainable {{AI}}},
  author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  year = {2020},
  month = dec,
  journal = {Entropy},
  volume = {23},
  number = {1},
  pages = {18},
  issn = {1099-4300},
  doi = {10.3390/e23010018},
  urldate = {2025-04-21},
  abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into ``black box'' approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  timestamp = {2025-04-21T06:10:45Z}
}

@article{littman2023scing,
  title = {{{SCING}}: {{Inference}} of Robust, Interpretable Gene Regulatory Networks from Single Cell and Spatial Transcriptomics},
  shorttitle = {{{SCING}}},
  author = {Littman, Russell and Cheng, Michael and Wang, Ning and Peng, Chao and Yang, Xia},
  year = {2023},
  month = jul,
  journal = {iScience},
  volume = {26},
  number = {7},
  pages = {107124},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2023.107124},
  abstract = {Gene regulatory network (GRN) inference is an integral part of understanding physiology and disease. Single cell/nuclei RNA-seq (scRNA-seq/snRNA-seq) data has been used to elucidate cell-type GRNs; however, the accuracy and speed of current scRNAseq-based GRN approaches are suboptimal. Here, we present Single Cell INtegrative Gene regulatory network inference (SCING), a gradient boosting and mutual information-based approach for identifying robust GRNs from scRNA-seq, snRNA-seq, and spatial transcriptomics data. Performance evaluation using Perturb-seq datasets, held-out data, and the mouse cell atlas combined with the DisGeNET database demonstrates the improved accuracy and biological interpretability of SCING compared to existing methods. We applied SCING to the entire mouse single cell atlas, human Alzheimer's disease (AD), and mouse AD spatial transcriptomics. SCING GRNs reveal unique disease subnetwork modeling capabilities, have intrinsic capacity to correct for batch effects, retrieve disease relevant genes and pathways, and are informative on spatial specificity of disease pathogenesis.},
  langid = {english},
  pmcid = {PMC10331489},
  pmid = {37434694},
  keywords = {Biocomputational method,Machine learning,Transcriptomics},
  annotation = {TLDR: Performance evaluation using Perturb-seq datasets, held-out data, and the mouse cell atlas combined with the DisGeNET database demonstrates the improved accuracy and biological interpretability of SCING compared to existing methods.},
  timestamp = {2025-04-17T11:48:59Z}
}

@article{liu2013application,
  title = {The {{Application}} of {{Decision Tree Algorithm}} in {{Medical Field Based}} on {{Cloud Platform}}},
  author = {Liu, Zhi Yuan and Peng, Jian Xi and Yang, Yuan Kai},
  year = {2013},
  month = aug,
  journal = {Applied Mechanics and Materials},
  volume = {347--350},
  pages = {3397--3402},
  issn = {1662-7482},
  doi = {10.4028/www.scientific.net/AMM.347-350.3397},
  urldate = {2025-06-09},
  abstract = {In the medical system, medical record is summarized, analyzed and predicted a patient seizure type and incidence. With the development of information technology, a large amount of data is to be processed. Traditional analysis algorithm could not be effectively processed to obtain the best predictive results as the data increasing. Decision tree algorithm based on cloud platform is used to record, analyze and predict patients medical data in this paper. A large number of experimental results show that distributed decision tree algorithm proposed in this paper is efficient and could complete prediction work in medical system. The algorithm has good expansibility, its very suitable for large-scale and multitude medical data process.},
  copyright = {https://www.scientific.net/PolicyAndEthics/PublishingPolicies},
  annotation = {TLDR: Decision tree algorithm based on cloud platform is used to record, analyze and predict patients medical data and a large number of experimental results show that distributed decision tree algorithm is efficient and could complete prediction work in medical system.},
  timestamp = {2025-06-09T13:36:39Z}
}

@inproceedings{liu2014propagation,
  ids = {liu2014propagationa},
  title = {Propagation Graph Fusion for Multi-Modal Medical Content-Based Retrieval},
  booktitle = {2014 13th International Conference on Control Automation Robotics \& Vision ({{ICARCV}})},
  author = {Liu, Sidong and Liu, Siqi and Pujol, Sonia and Kikinis, Ron and Feng, Dagan and Cai, Weidong},
  year = {2014},
  pages = {849--854},
  publisher = {IEEE},
  doi = {10.1109/ICARCV.2014.7064415},
  urldate = {2025-03-28},
  abstract = {Medical content-based retrieval (MCBR) plays an important role in computer aided diagnosis and clinical decision support. Multi-modal imaging data have been increasingly used in MCBR, as they could provide more insights of the diseases and complement the deficiencies of single-modal data. However, it is very challenging to fuse data in different modalities since they have different physical fundamentals and large value range variations. In this study, we propose a novel Propagation Graph Fusion (PGF) framework for multi-modal medical data retrieval. PGF models the subjects' relationships in single modalities using the directed propagation graphs, and then fuses the graphs into a single graph by summing up the edge weights. Our proposed PGF method could reduce the large inter-modality and inter-subject variations, and can be solved efficiently using the PageRank algorithm. We test the proposed method on a public medical database with 331 subjects using features extracted from two imaging modalities, PET and MRI. The preliminary results show that our PGF method could enhance multi-modal retrieval and modestly outperform the state-of-the-art single-modal and multi-modal retrieval methods.},
  keywords = {Diseases,Feature extraction,Magnetic resonance imaging,Medical diagnostic imaging,Positron emission tomography},
  timestamp = {2025-03-28T09:21:08Z}
}

@misc{liu2018improving,
  title = {Improving the {{Interpretability}} of {{Deep Neural Networks}} with {{Knowledge Distillation}}},
  author = {Liu, Xuan and Wang, Xiaoguang and Matwin, Stan},
  year = {2018},
  month = dec,
  number = {arXiv:1812.10924},
  eprint = {1812.10924},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.10924},
  urldate = {2025-03-15},
  abstract = {Deep Neural Networks have achieved huge success at a wide spectrum of applications from language modeling, computer vision to speech recognition. However, nowadays, good performance alone is not sufficient to satisfy the needs of practical deployment where interpretability is demanded for cases involving ethics and mission critical applications. The complex models of Deep Neural Networks make it hard to understand and reason the predictions, which hinders its further progress. To tackle this problem, we apply the Knowledge Distillation technique to distill Deep Neural Networks into decision trees in order to attain good performance and interpretability simultaneously. We formulate the problem at hand as a multi-output regression problem and the experiments demonstrate that the student model achieves significantly better accuracy performance (about 1{\textbackslash}\% to 5{\textbackslash}\%) than vanilla decision trees at the same level of tree depth. The experiments are implemented on the TensorFlow platform to make it scalable to big datasets. To the best of our knowledge, we are the first to distill Deep Neural Networks into vanilla decision trees on multi-class datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-15T07:48:18Z}
}

@article{liu2018quantitative,
  title = {Quantitative Analysis of Breast Cancer Diagnosis Using a Probabilistic Modelling Approach},
  author = {Liu, Shuo and Zeng, Jinshu and Gong, Huizhou and Yang, Hongqin and Zhai, Jia and Cao, Yi and Liu, Junxiu and Luo, Yuling and Li, Yuhua and Maguire, Liam and Ding, Xuemei},
  year = {2018},
  month = jan,
  journal = {Computers in Biology and Medicine},
  volume = {92},
  pages = {168--175},
  issn = {00104825},
  doi = {10.1016/j.compbiomed.2017.11.014},
  urldate = {2025-04-06},
  langid = {english},
  annotation = {TLDR: This study suggested that, in terms of ultrasound data, cell shape is the most significant feature for breast cancer diagnosis, and the resistance index presents a strong probabilistic dependency on blood signals.},
  timestamp = {2025-04-16T08:23:33Z}
}

@misc{liu2020when,
  title = {When Deep Learning Meets Causal Inference: A Computational Framework for Drug Repurposing from Real-World Data},
  shorttitle = {When Deep Learning Meets Causal Inference},
  author = {Liu, Ruoqi and Wei, Lai and Zhang, Ping},
  year = {2020},
  month = jul,
  number = {arXiv:2007.10152},
  eprint = {2007.10152},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.10152},
  urldate = {2025-05-04},
  abstract = {Drug repurposing is an effective strategy to identify new uses for existing drugs, providing the quickest possible transition from bench to bedside. Existing methods for drug repurposing that mainly focus on pre-clinical information may exist translational issues when applied to human beings. Real world data (RWD), such as electronic health records and insurance claims, provide information on large cohorts of users for many drugs. Here we present an efficient and easily-customized framework for generating and testing multiple candidates for drug repurposing using a retrospective analysis of RWDs. Building upon well-established causal inference and deep learning methods, our framework emulates randomized clinical trials for drugs present in a large-scale medical claims database. We demonstrate our framework in a case study of coronary artery disease (CAD) by evaluating the effect of 55 repurposing drug candidates on various disease outcomes. We achieve 6 drug candidates that significantly improve the CAD outcomes but not have been indicated for treating CAD, paving the way for drug repurposing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications},
  timestamp = {2025-05-04T13:25:28Z}
}

@article{liu2021face,
  title = {Face {{Image Publication Based}} on {{Differential Privacy}}},
  author = {Liu, Chao and Yang, Jing and Zhao, Weinan and Zhang, Yining and Li, Jingyou and Mu, Chunmiao},
  editor = {Chen, Chi-Hua},
  year = {2021},
  month = jan,
  journal = {Wireless Communications and Mobile Computing},
  volume = {2021},
  number = {1},
  pages = {6680701},
  issn = {1530-8669, 1530-8677},
  doi = {10.1155/2021/6680701},
  urldate = {2025-04-04},
  abstract = {As an information carrier, face images contain abundant sensitive information. Due to its natural weak privacy, direct publishing may divulge privacy. Anonymization Technology and Data Encryption Technology are limited by the background knowledge and attack means of attackers, which cannot completely content the needs of face image privacy protection. Therefore, this paper proposes a face image publishing SWP (sliding window publication) algorithm, which satisfies the differential privacy. Firstly, the SWP translates the image gray matrix into a one-dimensional ordered data stream by using image segmentation technology. The purpose of this step is to transform the image privacy protection problem into the data stream privacy protection problem. Then, the sliding window model is used to model the data flow. By comparing the similarity of data in adjacent sliding windows, the privacy budget is dynamically allocated, and Laplace noise is added. In SWP, the data in the sliding window comes from the image. To present the image features contained in the data more comprehensively and use the privacy budget more reasonably, this paper proposes a fusion similarity measurement EM (exact mechanism) mechanism and a dynamic privacy budget allocation DA (dynamic allocation) mechanism. Also, for further improving the usability of human face images and reducing the impact of noise, a sort-SWP algorithm based on the SWP method is proposed in the paper. Through the analysis, it can be seen that ordered input can further improve the usability of the SWP algorithm, but direct sorting of data will destroy the               {$\varepsilon$}               -differential privacy. Therefore, this paper proposes a sorting method-               SAS               method, which satisfies the               {$\varepsilon$}               -differential privacy; SAS obtain an initial sort by using an exponential mechanism firstly. And then an approximate correct sort is obtained by using the Annealing algorithm to optimize the initial sort. Compared with LAP algorithm and SWP algorithm, the average accuracy rate of sort-SWP algorithm in ORL, Yale is increased by 56.63\% and 21.55\%, the recall rate is increased by 6.85\% and 3.32\%, and F1-sroce is improved by 55.62\% and 16.55\%.},
  langid = {english},
  annotation = {TLDR: A sorting method-SAS method, which satisfies the differential privacy; SAS obtain an initial sort by using an exponential mechanism firstly and then an approximate correct sort is obtained by using the Annealing algorithm to optimize the initial sort.},
  timestamp = {2025-04-04T06:30:22Z}
}

@misc{liu2021feddg,
  title = {{{FedDG}}: {{Federated Domain Generalization}} on {{Medical Image Segmentation}} via {{Episodic Learning}} in {{Continuous Frequency Space}}},
  shorttitle = {{{FedDG}}},
  author = {Liu, Quande and Chen, Cheng and Qin, Jing and Dou, Qi and Heng, Pheng-Ann},
  year = {2021},
  month = mar,
  number = {arXiv:2103.06030},
  eprint = {2103.06030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.06030},
  urldate = {2025-05-21},
  abstract = {Federated learning allows distributed medical institutions to collaboratively learn a shared prediction model with privacy protection. While at clinical deployment, the models trained in federated learning can still suffer from performance drop when applied to completely unseen hospitals outside the federation. In this paper, we point out and solve a novel problem setting of federated domain generalization (FedDG), which aims to learn a federated model from multiple distributed source domains such that it can directly generalize to unseen target domains. We present a novel approach, named as Episodic Learning in Continuous Frequency Space (ELCFS), for this problem by enabling each client to exploit multi-source data distributions under the challenging constraint of data decentralization. Our approach transmits the distribution information across clients in a privacy-protecting way through an effective continuous frequency space interpolation mechanism. With the transferred multi-source distributions, we further carefully design a boundary-oriented episodic learning paradigm to expose the local learning to domain distribution shifts and particularly meet the challenges of model generalization in medical image segmentation scenario. The effectiveness of our method is demonstrated with superior performance over state-of-the-arts and in-depth ablation experiments on two medical image segmentation tasks. The code is available at "https://github.com/liuquande/FedDG-ELCFS".},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  timestamp = {2025-05-21T02:30:25Z}
}

@misc{liu2021kernelized,
  title = {Kernelized {{Heterogeneous Risk Minimization}}},
  author = {Liu, Jiashuo and Hu, Zheyuan and Cui, Peng and Li, Bo and Shen, Zheyan},
  year = {2021},
  month = oct,
  journal = {arXiv.org},
  urldate = {2025-04-12},
  abstract = {The ability to generalize under distributional shifts is essential to reliable machine learning, while models optimized with empirical risk minimization usually fail on non-\$i.i.d\$ testing data. Recently, invariant learning methods for out-of-distribution (OOD) generalization propose to find causally invariant relationships with multi-environments. However, modern datasets are frequently multi-sourced without explicit source labels, rendering many invariant learning methods inapplicable. In this paper, we propose Kernelized Heterogeneous Risk Minimization (KerHRM) algorithm, which achieves both the latent heterogeneity exploration and invariant learning in kernel space, and then gives feedback to the original neural network by appointing invariant gradient direction. We theoretically justify our algorithm and empirically validate the effectiveness of our algorithm with extensive experiments.},
  howpublished = {https://arxiv.org/abs/2110.12425v1},
  langid = {english},
  timestamp = {2025-04-12T07:35:41Z}
}

@article{liu2022does,
  title = {Does {{AI}} Explainability Affect Physicians' Intention to Use {{AI}}?},
  author = {Liu, Chung-Feng and Chen, Zhih-Cherng and Kuo, Szu-Chen and Lin, Tzu-Chi},
  year = {2022},
  month = dec,
  journal = {International Journal of Medical Informatics},
  volume = {168},
  pages = {104884},
  issn = {1872-8243},
  doi = {10.1016/j.ijmedinf.2022.104884},
  abstract = {BACKGROUND: Artificial Intelligence (AI) is increasingly being developed to support clinical decisions for better health service quality, but the adoption of AI in hospitals is not as popular as expected. A possible reason is that the unclear AI explainability (XAI) affects the physicians' consideration of adopting the model. PURPOSE: To propose and validate an innovative conceptual model aimed at exploring physicians' intention to use AI with XAI as an antecedent variable of technology trust (TT) and perceived value (PV). METHODS: A questionnaire survey was conducted to collect data from physicians of three hospitals in Taiwan. Structural equation modeling (SEM) was used to validate the proposed model and test the hypotheses. RESULTS: A total of 295 valid questionnaires were collected. The research results showed that physicians expressed a high intention to use AI. The XAI was found to be of great importance and had a significant impact both on AI TT and PV. We also observed that TT in AI had a significant impact on PV. Moreover, physicians' PV and TT in AI had a significant impact on their behavioral intention to use AI (BI). However, XAI's impact on BI cannot be proved. CONCLUSIONS: The conceptual model developed in this study provides empirical evidence that could be used as guidelines to effectively explore physicians' intention to use medical AI from the antecedent of XAI. Our findings contribute crucial AI-human interaction insights in health care studies.},
  langid = {english},
  pmid = {36228415},
  keywords = {AI explainability (XAI),Artificial Intelligence,Artificial intelligence (AI),Attitude of Health Personnel,Behavioral intention,Humans,Intention,Perceived value,Physician,Physicians,Surveys and Questionnaires,Technology trust},
  annotation = {TLDR: The conceptual model developed in this study provides empirical evidence that could be used as guidelines to effectively explore physicians' intention to use medical AI from the antecedent of XAI.},
  timestamp = {2025-05-28T00:51:21Z}
}

@article{liu2022explainable,
  title = {Explainable Deep Transfer Learning Model for Disease Risk Prediction Using High-Dimensional Genomic Data},
  author = {Liu, Long and Meng, Qingyu and Weng, Cherry and Lu, Qing and Wang, Tong and Wen, Yalu},
  editor = {Li, Wei},
  year = {2022},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {7},
  pages = {e1010328},
  publisher = {Public Library of Science (PLoS)},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010328},
  urldate = {2025-07-24},
  abstract = {Building an accurate disease risk prediction model is an essential step in the modern quest for precision medicine. While high-dimensional genomic data provides valuable data resources for the investigations of disease risk, their huge amount of noise and complex relationships between predictors and outcomes have brought tremendous analytical challenges. Deep learning model is the state-of-the-art methods for many prediction tasks, and it is a promising framework for the analysis of genomic data. However, deep learning models generally suffer from the curse of dimensionality and the lack of biological interpretability, both of which have greatly limited their applications. In this work, we have developed a deep neural network (DNN) based prediction modeling framework. We first proposed a group-wise feature importance score for feature selection, where genes harboring genetic variants with both linear and non-linear effects are efficiently detected. We then designed an explainable transfer-learning based DNN method, which can directly incorporate information from feature selection and accurately capture complex predictive effects. The proposed DNN-framework is biologically interpretable, as it is built based on the selected predictive genes. It is also computationally efficient and can be applied to genome-wide data. Through extensive simulations and real data analyses, we have demonstrated that our proposed method can not only efficiently detect predictive features, but also accurately predict disease risk, as compared to many existing methods.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  timestamp = {2025-07-24T10:44:17Z}
}

@article{liu2023explainable,
  title = {An Explainable Knowledge Distillation Method with {{XGBoost}} for {{ICU}} Mortality Prediction},
  author = {Liu, Mucan and Guo, Chonghui and Guo, Sijia},
  year = {2023},
  month = jan,
  journal = {Computers in Biology and Medicine},
  volume = {152},
  pages = {106466},
  issn = {00104825},
  doi = {10.1016/j.compbiomed.2022.106466},
  urldate = {2025-04-04},
  langid = {english},
  annotation = {TLDR: An explainable Knowledge Distillation method with XGBoost (XGB-KD) is proposed to improve the predictive performance of X GBoost while supporting better explainability and can provide meaningful explanations for predictions.},
  timestamp = {2025-04-16T08:23:47Z}
}

@phdthesis{liu2024adaptive,
  title = {Adaptive Explainable {{AI}}: Designing User-Centric Explanation Systems for Enhanced Interaction},
  author = {Liu, Xinyi and others},
  year = {2024},
  timestamp = {2025-05-04T16:09:01Z}
}

@article{liu2025challenges,
  title = {Challenges in {{AI-driven Biomedical Multimodal Data Fusion}} and {{Analysis}}},
  author = {Liu, Junwei and Cen, Xiaoping and Yi, Chenxin and Wang, Feng-ao and Ding, Junxiang and Cheng, Jinyu and Wu, Qinhua and Gai, Baowen and Zhou, Yiwen and He, Ruikun and Gao, Feng and Li, Yixue},
  year = {2025},
  month = feb,
  journal = {Genomics, Proteomics \& Bioinformatics},
  pages = {qzaf011},
  issn = {1672-0229},
  doi = {10.1093/gpbjnl/qzaf011},
  urldate = {2025-05-03},
  abstract = {The rapid development of biological and medical examination methods has vastly expanded personal biomedical information, including molecular, cellular, image, and electronic health record datasets. Integrating this wealth of information enables precise disease diagnosis, biomarker identification, and treatment design in clinical settings. Artificial intelligence (AI) techniques, particularly deep learning models, have been extensively employed in biomedical applications, demonstrating increased precision, efficiency, and generalization. The success of the large language and vision models further significantly extends their biomedical applications. However, challenges remain in learning these multimodal biomedical datasets, such as data privacy, fusion, and model interpretation. In this review, we provided a comprehensive overview of various biomedical data modalities, multi-modal representation learning methods, and the applications of AI in biomedical data integrative analysis. Additionally, we discussed the challenges in applying these deep learning methods and how to better integrate them into biomedical scenarios. We then proposed future directions for adapting deep learning methods with model pre-training and knowledge integration to advance biomedical research and benefit their clinical applications.},
  annotation = {TLDR: This review provided a comprehensive overview of various biomedical data modalities, multi-modal representation learning methods, and the applications of AI in biomedical data integrative analysis.},
  timestamp = {2025-05-03T09:29:36Z}
}

@misc{liu2025distillation,
  title = {Beyond {{Distillation}}: {{Pushing}} the {{Limits}} of {{Medical LLM Reasoning}} with {{Minimalist Rule-Based RL}}},
  shorttitle = {Beyond {{Distillation}}},
  author = {Liu, Che and Wang, Haozhe and Pan, Jiazhen and Wan, Zhongwei and Dai, Yong and Lin, Fangzhen and Bai, Wenjia and Rueckert, Daniel and Arcucci, Rossella},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2505.17952},
  urldate = {2025-06-13},
  abstract = {Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences},
  timestamp = {2025-06-13T07:33:25Z}
}

@misc{liu2025fourier,
  title = {Fourier {{Feature Attribution}}: {{A New Efficiency Attribution Method}}},
  shorttitle = {Fourier {{Feature Attribution}}},
  author = {Liu, Zechen and Zhang, Feiyang and Song, Wei and Li, Xiang and Wei, Wei},
  year = {2025},
  month = apr,
  number = {arXiv:2504.02016},
  eprint = {2504.02016},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.02016},
  urldate = {2025-04-09},
  abstract = {The study of neural networks from the perspective of Fourier features has garnered significant attention. While existing analytical research suggests that neural networks tend to learn low-frequency features, a clear attribution method for identifying the specific learned Fourier features has remained elusive. To bridge this gap, we propose a novel Fourier feature attribution method grounded in signal decomposition theory. Additionally, we analyze the differences between game-theoretic attribution metrics for Fourier and spatial domain features, demonstrating that game-theoretic evaluation metrics are better suited for Fourier-based feature attribution. Our experiments show that Fourier feature attribution exhibits superior feature selection capabilities compared to spatial domain attribution methods. For instance, in the case of Vision Transformers (ViTs) on the ImageNet dataset, only \$8{\textbackslash}\%\$ of the Fourier features are required to maintain the original predictions for \$80{\textbackslash}\%\$ of the samples. Furthermore, we compare the specificity of features identified by our method against traditional spatial domain attribution methods. Results reveal that Fourier features exhibit greater intra-class concentration and inter-class distinctiveness, indicating their potential for more efficient classification and explainable AI algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  timestamp = {2025-04-09T03:52:08Z}
}

@article{liu2025generalist,
  title = {A Generalist Medical Language Model for Disease Diagnosis Assistance},
  author = {Liu, Xiaohong and Liu, Hao and Yang, Guoxing and Jiang, Zeyu and Cui, Shuguang and Zhang, Zhaoze and Wang, Huan and Tao, Liyuan and Sun, Yongchang and Song, Zhu and Hong, Tianpei and Yang, Jin and Gao, Tianrun and Zhang, Jiangjiang and Li, Xiaohu and Zhang, Jing and Sang, Ye and Yang, Zhao and Xue, Kanmin and Wu, Song and Zhang, Ping and Yang, Jian and Song, Chunli and Wang, Guangyu},
  year = {2025},
  month = mar,
  journal = {Nature Medicine},
  volume = {31},
  number = {3},
  pages = {932--942},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-024-03416-6},
  urldate = {2025-05-03},
  abstract = {The delivery of accurate diagnoses is crucial in healthcare and represents the gateway to appropriate and timely treatment. Although recent large language models (LLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, their effectiveness in clinical diagnosis remains unproven. Here we present MedFound, a generalist medical language model with 176 billion parameters, pre-trained on a large-scale corpus derived from diverse medical text and real-world clinical records. We further fine-tuned MedFound to learn physicians' inferential diagnosis with a self-bootstrapping strategy-based chain-of-thought approach and introduced a unified preference alignment framework to align it with standard clinical practice. Extensive experiments demonstrate that our medical LLM outperforms other baseline LLMs and specialized models in in-distribution (common diseases), out-of-distribution (external validation) and long-tailed distribution (rare diseases) scenarios across eight specialties. Further ablation studies indicate the effectiveness of key components in our medical LLM training approach. We conducted a comprehensive evaluation of the clinical applicability of LLMs for diagnosis involving artificial intelligence (AI) versus physician comparison, AI-assistance study and human evaluation framework. Our proposed framework incorporates eight clinical evaluation metrics, covering capabilities such as medical record summarization, diagnostic reasoning and risk management. Our findings demonstrate the model's feasibility in assisting physicians with disease diagnosis as part of the clinical workflow.},
  copyright = {2025 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Diagnosis},
  annotation = {TLDR: MedFound, a generalist medical language model with 176 billion parameters, is presented, pre-trained on a large-scale corpus derived from diverse medical text and real-world clinical records, demonstrating the model's feasibility in assisting physicians with disease diagnosis as part of the clinical workflow.},
  timestamp = {2025-05-03T14:10:00Z}
}

@article{liu2025improving,
  title = {Improving Explainability and Integrability of Medical {{AI}} to Promote Health Care Professional Acceptance and Use: {{Mixed}} Systematic Review},
  author = {Liu, Yushu and Liu, Chenxi and Zheng, Jianing and Xu, Chang and Wang, Dan},
  year = {2025},
  journal = {Journal of Medical Internet Research},
  volume = {27},
  pages = {e73374},
  publisher = {JMIR Publications Toronto, Canada},
  timestamp = {2025-08-13T09:25:50Z}
}

@misc{liu2025large,
  title = {Large {{Language Models}} and {{Causal Inference}} in {{Collaboration}}: {{A Comprehensive Survey}}},
  shorttitle = {Large {{Language Models}} and {{Causal Inference}} in {{Collaboration}}},
  author = {Liu, Xiaoyu and Xu, Paiheng and Wu, Junda and Yuan, Jiaxin and Yang, Yifan and Zhou, Yuhang and Liu, Fuxiao and Guan, Tianrui and Wang, Haoliang and Yu, Tong and McAuley, Julian and Ai, Wei and Huang, Furong},
  year = {2025},
  month = feb,
  number = {arXiv:2403.09606},
  eprint = {2403.09606},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.09606},
  urldate = {2025-03-23},
  abstract = {Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {TLDR: This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality.},
  timestamp = {2025-03-23T10:15:45Z}
}

@misc{liu2025largea,
  title = {Large {{Language Model Distilling Medication Recommendation Model}}},
  author = {Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Zhang, Zijian and Tian, Feng and Zheng, Yefeng},
  year = {2025},
  month = jan,
  number = {arXiv:2402.02803},
  eprint = {2402.02803},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.02803},
  urldate = {2025-05-03},
  abstract = {The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the straightforward integration of LLMs into recommender systems leads to an out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a novel output layer and a refined tuning loss function. Although LLM-based models exhibit remarkable capabilities, they are plagued by high computational costs during inference, which is impractical for the healthcare sector. To mitigate this, we have developed a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model. Extensive experiments conducted on two real-world datasets, MIMIC-III and MIMIC-IV, demonstrate that our proposed model not only delivers effective results but also is efficient. To ease the reproducibility of our experiments, we release the implementation code online.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {TLDR: This paper introduces a novel approach called Large Language Model Distilling Medication Recommendation (LEADER), which transfers the LLM's proficiency to a more compact model and develops a feature-level knowledge distillation technique, which transfers the LLM's proficiency to a more compact model.},
  timestamp = {2025-05-03T14:20:51Z}
}

@misc{liu2025ontotune,
  title = {{{OntoTune}}: {{Ontology-Driven Self-training}} for {{Aligning Large Language Models}}},
  shorttitle = {{{OntoTune}}},
  author = {Liu, Zhiqiang and Gan, Chengtao and Wang, Junjie and Zhang, Yichi and Bo, Zhongpu and Sun, Mengshu and Chen, Huajun and Zhang, Wen},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2502.05478},
  urldate = {2025-06-13},
  abstract = {Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain knowledge of LLMs, leading to fragmented understanding. Inspired by how humans connect concepts and organize knowledge through mind maps, we aim to emulate this approach by using ontology with hierarchical conceptual knowledge to reorganize LLM's domain knowledge. From this perspective, we propose an ontology-driven self-training framework called OntoTune, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology. We leverage in-context learning to identify whether the LLM has acquired the specific concept's ontology knowledge, and select the entries not yet mastered by LLM as the training set to further align the LLM with ontology. Compared to existing domain LLMs based on newly collected large-scale domain-specific corpora, our OntoTune, which relies on the existing, long-term developed ontology and LLM itself, significantly reduces data maintenance costs and offers improved generalization ability. We conduct our study in the medical domain to evaluate the effectiveness of OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology source. Experimental results demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA. Moreover, compared to the latest direct ontology injection method TaxoLLaMA, our OntoTune better preserves original knowledge of LLM. The code and data are available at https://github.com/zjukg/OntoTune.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  timestamp = {2025-06-13T07:40:53Z}
}

@article{loh2022application,
  ids = {2022application},
  title = {Application of Explainable Artificial Intelligence for Healthcare: {{A}} Systematic Review of the Last Decade (2011--2022)},
  shorttitle = {Application of Explainable Artificial Intelligence for Healthcare},
  author = {Loh, Hui Wen and Ooi, Chui Ping and Seoni, Silvia and Barua, Prabal Datta and Molinari, Filippo and Acharya, U Rajendra},
  year = {2022},
  month = nov,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {226},
  pages = {107161},
  issn = {01692607},
  doi = {10.1016/j.cmpb.2022.107161},
  urldate = {2025-05-18},
  abstract = {Artificial intelligence (AI) has branched out to various applications in healthcare, such as health services management, predictive medicine, clinical{\dots}},
  langid = {english},
  annotation = {TLDR: It is discovered that detecting abnormalities in 1D biosignals and identifying key text in clinical notes are areas that require more attention from the XAI research community, and it is hoped this is review will encourage the development of a holistic cloud system for a smart city.},
  timestamp = {2025-05-18T01:38:03Z}
}

@article{long2024deciphering,
  title = {Deciphering Spatial Domains from Spatial Multi-Omics with {{SpatialGlue}}},
  author = {Long, Yahui and Ang, Kok Siong and Sethi, Raman and Liao, Sha and Heng, Yang and {van Olst}, Lynn and Ye, Shuchen and Zhong, Chengwei and Xu, Hang and Zhang, Di and Kwok, Immanuel and Husna, Nazihah and Jian, Min and Ng, Lai Guan and Chen, Ao and Gascoigne, Nicholas R. J. and Gate, David and Fan, Rong and Xu, Xun and Chen, Jinmiao},
  year = {2024},
  month = sep,
  journal = {Nature Methods},
  volume = {21},
  number = {9},
  pages = {1658--1667},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02316-4},
  urldate = {2025-04-18},
  abstract = {Advances in spatial omics technologies now allow multiple types of data to be acquired from the same tissue slice. To realize the full potential of such data, we need spatially informed methods for data integration. Here, we introduce SpatialGlue, a graph neural network model with a dual-attention mechanism that deciphers spatial domains by intra-omics integration of spatial location and omics measurement followed by cross-omics integration. We demonstrated SpatialGlue on data acquired from different tissue types using different technologies, including spatial epigenome--transcriptome and transcriptome--proteome modalities. Compared to other methods, SpatialGlue captured more anatomical details and more accurately resolved spatial domains such as the cortex layers of the brain. Our method also identified cell types like spleen macrophage subsets located at three different zones that were not available in the original data annotations. SpatialGlue scales well with data size and can be used to integrate three modalities. Our spatial multi-omics analysis tool combines the information from complementary omics modalities to obtain a holistic view of cellular and tissue properties.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational models,Data integration},
  annotation = {TLDR: SpatialGlue, a graph neural network model with a dual-attention mechanism that deciphers spatial domains by intra-omics integration of spatial location and omics measurement followed by cross-omics integration, is introduced.},
  timestamp = {2025-04-18T03:14:39Z}
}

@misc{longoni2019resistance,
  type = {{{SSRN Scholarly Paper}}},
  title = {Resistance {{To Medical Artificial Intelligence}}},
  author = {Longoni, Chiara and Bonezzi, Andrea and Morewedge, Carey},
  year = {2019},
  month = apr,
  number = {3375716},
  eprint = {3375716},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  urldate = {2025-05-01},
  abstract = {Artificial intelligence (AI) is revolutionizing healthcare, but little is known about consumer receptivity toward AI in medicine. Consumers are reluctant to utilize healthcare provided by AI in real and hypothetical choices, separate and joint evaluations. Consumers are less likely to utilize healthcare (study 1), exhibit lower reservation prices for healthcare (study 2), are less sensitive to differences in provider performance (studies 3A-3C), and derive negative utility if a provider is automated rather than human (study 4). Uniqueness neglect, a concern that AI providers are less able than human providers to account for their unique characteristics and circumstances, drives consumer resistance to medical AI. Indeed, resistance to medical AI is stronger for consumers who perceive themselves to be more unique (study 5).~Uniqueness neglect mediates resistance to medical AI (study 6), and is eliminated when AI provides care (a) that is framed as personalized (study 7), (b) to consumers other than the self (study 8), or (c) only supports, rather than replaces, a decision made by a human healthcare provider (study 9). These findings make contributions to the psychology of automation and medical decision making, and suggest interventions to increase consumer acceptance of AI in medicine.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {artificial intelligence,automation,healthcare,medical decision making,uniqueness},
  timestamp = {2025-05-01T11:15:53Z}
}

@article{lotfollahi2019scgen,
  title = {{{scGen}} Predicts Single-Cell Perturbation Responses},
  author = {Lotfollahi, Mohammad and Wolf, F Alexander and Theis, Fabian J},
  year = {2019},
  journal = {Nature methods},
  volume = {16},
  number = {8},
  pages = {715--721},
  publisher = {Nature Publishing Group US New York},
  timestamp = {2025-05-17T07:40:25Z}
}

@article{lotsch2021explainable,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}) in {{Biomedicine}}: {{Making AI Decisions Trustworthy}} for {{Physicians}} and {{Patients}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}}) in {{Biomedicine}}},
  author = {L{\"o}tsch, J{\"o}rn and Kringel, Dario and Ultsch, Alfred},
  year = {2021},
  month = dec,
  journal = {BioMedInformatics},
  volume = {2},
  number = {1},
  pages = {1--17},
  publisher = {MDPI AG},
  issn = {2673-7426},
  doi = {10.3390/biomedinformatics2010001},
  urldate = {2025-05-01},
  abstract = {The use of artificial intelligence (AI) systems in biomedical and clinical settings can disrupt the traditional doctor--patient relationship, which is based on trust and transparency in medical advice and therapeutic decisions. When the diagnosis or selection of a therapy is no longer made solely by the physician, but to a significant extent by a machine using algorithms, decisions become nontransparent. Skill learning is the most common application of machine learning algorithms in clinical decision making. These are a class of very general algorithms (artificial neural networks, classifiers, etc.), which are tuned based on examples to optimize the classification of new, unseen cases. It is pointless to ask for an explanation for a decision. A detailed understanding of the mathematical details of an AI algorithm may be possible for experts in statistics or computer science. However, when it comes to the fate of human beings, this ``developer's explanation'' is not sufficient. The concept of explainable AI (XAI) as a solution to this problem is attracting increasing scientific and regulatory interest. This review focuses on the requirement that XAIs must be able to explain in detail the decisions made by the AI to the experts in the field.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: This review focuses on the requirement that XAIs must be able to explain in detail the decisions made by the AI to the experts in the field.},
  timestamp = {2025-05-01T12:32:01Z}
}

@article{louizos2017causal,
  title = {Causal Effect Inference with Deep Latent-Variable Models},
  author = {Louizos, Christos and Shalit, Uri and Mooij, Joris M and Sontag, David and Zemel, Richard and Welling, Max},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  timestamp = {2025-03-20T08:48:37Z}
}

@article{loyola-gonzalez2019blackbox,
  title = {Black-{{Box}} vs. {{White-Box}}: {{Understanding Their Advantages}} and {{Weaknesses From}} a {{Practical Point}} of {{View}}},
  shorttitle = {Black-{{Box}} vs. {{White-Box}}},
  author = {{Loyola-Gonz{\'a}lez}, Octavio},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {154096--154113},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2949286},
  urldate = {2025-06-10},
  abstract = {Nowadays, in the international scientific community of machine learning, there exists an enormous discussion about the use of black-box models or explainable models; especially in practical problems. On the one hand, a part of the community defends that black-box models are more accurate than explainable models in some contexts, like image preprocessing. On the other hand, there exist another part of the community alleging that explainable models are better than black-box models because they can obtain comparable results and also they can explain these results in a language close to a human expert by using patterns. In this paper, advantages and weaknesses for each approach are shown; taking into account a state-of-the-art review for both approaches, their practical applications, trends, and future challenges. This paper shows that both approaches are suitable for solving practical problems, but experts in machine learning need to understand the input data, the problem to solve, and the best way for showing the output data before applying a machine learning model. Also, we propose some ideas for fusing both, explainable and black-box, approaches to provide better solutions to experts in real-world domains. Additionally, we show one way to measure the effectiveness of the applied machine learning model by using expert opinions jointly with statistical methods. Throughout this paper, we show the impact of using explainable and black-box models on the security and medical applications.},
  keywords = {Biological neural networks,Biological system modeling,Black-box,Computational modeling,deep learning,explainable artificial intelligence,Gallium nitride,Machine learning,Mathematical model,Statistical analysis,white-box},
  annotation = {TLDR: Both explainable and black-box models are suitable for solving practical problems, but experts in machine learning need to understand the input data, the problem to solve, and the best way for showing the output data before applying a machine learning model.},
  timestamp = {2025-06-10T03:45:52Z}
}

@misc{lu2019gender,
  title = {Gender {{Bias}} in {{Neural Natural Language Processing}}},
  author = {Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
  year = {2019},
  month = may,
  number = {arXiv:1807.11714},
  eprint = {1807.11714},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.11714},
  urldate = {2025-03-20},
  abstract = {We examine whether neural natural language processing (NLP) systems reflect historical biases in training data. We define a general benchmark to quantify gender bias in a variety of neural NLP tasks. Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark datasets finds significant gender bias in how models view occupations. We then mitigate bias with CDA: a generic methodology for corpus augmentation via causal interventions that breaks associations between gendered and gender-neutral words. We empirically show that CDA effectively decreases gender bias while preserving accuracy. We also explore the space of mitigation strategies with CDA, a prior approach to word embedding debiasing (WED), and their compositions. We show that CDA outperforms WED, drastically so when word embeddings are trained. For pre-trained embeddings, the two methods can be effectively composed. We also find that as training proceeds on the original data set with gradient descent the gender bias grows as the loss reduces, indicating that the optimization encourages bias; CDA mitigates this behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  timestamp = {2025-03-20T23:12:07Z}
}

@misc{lu2023visuallanguage,
  title = {Towards a {{Visual-Language Foundation Model}} for {{Computational Pathology}}},
  author = {Lu, Ming Y. and Chen, Bowen and Williamson, Drew F. K. and Chen, Richard J. and Liang, Ivy and Ding, Tong and Jaume, Guillaume and Odintsov, Igor and Zhang, Andrew and Le, Long Phi and Gerber, Georg and Parwani, Anil V and Mahmood, Faisal},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.12914},
  urldate = {2025-06-13},
  abstract = {The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-the-art performance on histology image classification, segmentation, captioning, text-to-image and image-to-text retrieval. CONCH represents a substantial leap over concurrent visual-language pretrained systems for histopathology, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  annotation = {TLDR: This work introduces CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.},
  timestamp = {2025-06-13T06:59:09Z}
}

@article{lu2024diversify,
  title = {Diversify: {{A General Framework}} for {{Time Series Out-of-Distribution Detection}} and {{Generalization}}},
  shorttitle = {Diversify},
  author = {Lu, Wang and Wang, Jindong and Sun, Xinwei and Chen, Yiqiang and Ji, Xiangyang and Yang, Qiang and Xie, Xing},
  year = {2024},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {46},
  number = {6},
  pages = {4534--4550},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2024.3355212},
  urldate = {2025-04-04},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {TLDR: Qualitative and quantitative results demonstrate that Diversify learns more generalized features and significantly outperforms other baselines, as well as theoretical insights support the framework's validity.},
  timestamp = {2025-04-16T08:18:10Z}
}

@article{lu2024explainable,
  ids = {lu2024},
  title = {Explainable and Visualizable Machine Learning Models to Predict Biochemical Recurrence of Prostate Cancer},
  author = {Lu, Wenhao and Zhao, Lin and Wang, Shenfan and Zhang, Huiyong and Jiang, Kangxian and Ji, Jin and Chen, Shaohua and Wang, Chengbang and Wei, Chunmeng and Zhou, Rongbin and Wang, Zuheng and Li, Xiao and Wang, Fubo and Wei, Xuedong and Hou, Wenlei},
  year = {2024},
  month = sep,
  journal = {Clinical and Translational Oncology},
  volume = {26},
  number = {9},
  pages = {2369--2379},
  issn = {1699-3055},
  doi = {10.1007/s12094-024-03480-x},
  urldate = {2025-02-21},
  abstract = {Machine learning (ML) models presented an excellent performance in the prognosis prediction. However, the black box characteristic of ML models limited the clinical applications. Here, we aimed to establish explainable and visualizable ML models to predict biochemical recurrence (BCR) of prostate cancer (PCa).},
  langid = {english},
  keywords = {Biochemical recurrence,Machine learning,Prognosis,Prostate cancer,SHAP value},
  annotation = {TLDR: The authors' score system provide reference for the identification for BCR, and the crafting of a framework for making therapeutic decisions for PCa on a personalized basis, and showed that RSF model had significant advantages over all models.},
  timestamp = {2025-03-18T02:01:30Z}
}

@misc{lu2024explainablea,
  title = {Explainable {{Diagnosis Prediction}} through {{Neuro-Symbolic Integration}}},
  author = {Lu, Qiuhao and Li, Rui and Sagheb, Elham and Wen, Andrew and Wang, Jinlian and Wang, Liwei and Fan, Jungwei W. and Liu, Hongfang},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2410.01855},
  urldate = {2025-05-03},
  abstract = {Diagnosis prediction is a critical task in healthcare, where timely and accurate identification of medical conditions can significantly impact patient outcomes. Traditional machine learning and deep learning models have achieved notable success in this domain but often lack interpretability which is a crucial requirement in clinical settings. In this study, we explore the use of neuro-symbolic methods, specifically Logical Neural Networks (LNNs), to develop explainable models for diagnosis prediction. Essentially, we design and implement LNN-based models that integrate domain-specific knowledge through logical rules with learnable thresholds. Our models, particularly \$M\_\{{\textbackslash}text\{multi-pathway\}\}\$ and \$M\_\{{\textbackslash}text\{comprehensive\}\}\$, demonstrate superior performance over traditional models such as Logistic Regression, SVM, and Random Forest, achieving higher accuracy (up to 80.52{\textbackslash}\%) and AUROC scores (up to 0.8457) in the case study of diabetes prediction. The learned weights and thresholds within the LNN models provide direct insights into feature contributions, enhancing interpretability without compromising predictive power. These findings highlight the potential of neuro-symbolic approaches in bridging the gap between accuracy and explainability in healthcare AI applications. By offering transparent and adaptable diagnostic models, our work contributes to the advancement of precision medicine and supports the development of equitable healthcare solutions. Future research will focus on extending these methods to larger and more diverse datasets to further validate their applicability across different medical conditions and populations.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: This study explores the use of neuro-symbolic methods, specifically Logical Neural Networks (LNNs), to develop explainable models for diagnosis prediction, and designs and implements LNN-based models that integrate domain-specific knowledge through logical rules with learnable thresholds.},
  timestamp = {2025-05-03T15:03:00Z}
}

@article{luecken2022benchmarking,
  title = {Benchmarking Atlas-Level Data Integration in Single-Cell Genomics},
  author = {Luecken, Malte D and B{\"u}ttner, Maren and Chaichoompu, Kridsadakorn and Danese, Anna and Interlandi, Marta and M{\"u}ller, Michaela F and Strobl, Daniel C and Zappia, Luke and Dugas, Martin and {Colom{\'e}-Tatch{\'e}}, Maria and others},
  year = {2022},
  journal = {Nature methods},
  volume = {19},
  number = {1},
  pages = {41--50},
  publisher = {Nature Publishing Group US New York},
  timestamp = {2025-05-17T07:39:18Z}
}

@inproceedings{lundberg2017unified,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-03-18},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  timestamp = {2025-03-18T02:01:30Z}
}

@article{luo2020when,
  title = {When Causal Inference Meets Deep Learning},
  author = {Luo, Yunan and Peng, Jian and Ma, Jianzhu},
  year = {2020},
  month = aug,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {8},
  pages = {426--427},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0218-x},
  urldate = {2025-03-22},
  abstract = {Bayesian networks can capture causal relations, but learning such a network from data is NP-hard. Recent work has made it possible to approximate this problem as a continuous optimization task that can be solved efficiently with well-established numerical techniques.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Data mining,Machine learning},
  annotation = {TLDR: This work has made it possible to approximate Bayesian networks as a continuous optimization task that can be solved efficiently with well-established numerical techniques.},
  timestamp = {2025-03-22T08:53:16Z}
}

@misc{luo2024harvard,
  title = {Harvard {{Glaucoma Fairness}}: {{A Retinal Nerve Disease Dataset}} for {{Fairness Learning}} and {{Fair Identity Normalization}}},
  shorttitle = {Harvard {{Glaucoma Fairness}}},
  author = {Luo, Yan and Tian, Yu and Shi, Min and Pasquale, Louis R. and Shen, Lucy Q. and Zebardast, Nazlee and Elze, Tobias and Wang, Mengyu},
  year = {2024},
  month = mar,
  number = {arXiv:2306.09264},
  eprint = {2306.09264},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.09264},
  urldate = {2025-05-01},
  abstract = {Fairness (also known as equity interchangeably) in machine learning is important for societal well-being, but limited public datasets hinder its progress. Currently, no dedicated public medical datasets with imaging data for fairness learning are available, though minority groups suffer from more health issues. To address this gap, we introduce Harvard Glaucoma Fairness (Harvard-GF), a retinal nerve disease dataset with both 2D and 3D imaging data and balanced racial groups for glaucoma detection. Glaucoma is the leading cause of irreversible blindness globally with Blacks having doubled glaucoma prevalence than other races. We also propose a fair identity normalization (FIN) approach to equalize the feature importance between different identity groups. Our FIN approach is compared with various the-state-of-the-art fairness learning methods with superior performance in the racial, gender, and ethnicity fairness tasks with 2D and 3D imaging data, which demonstrate the utilities of our dataset Harvard-GF for fairness learning. To facilitate fairness comparisons between different models, we propose an equity-scaled performance measure, which can be flexibly used to compare all kinds of performance metrics in the context of fairness. The dataset and code are publicly accessible via {\textbackslash}url\{https://ophai.hms.harvard.edu/datasets/harvard-glaucoma-fairness-3300-samples/\}.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  timestamp = {2025-05-01T15:23:24Z}
}

@article{lupton2018some,
  title = {Some Ethical and Legal Consequences of the Application of Artificial Intelligence in the Field of Medicine},
  author = {Lupton, Michael},
  year = {2018},
  journal = {Trends Med},
  volume = {18},
  number = {4},
  pages = {100147},
  timestamp = {2025-04-16T02:35:23Z}
}

@misc{lvliyuan2023lunliai,
  title = {{{lunliAI}} \_\_},
  shorttitle = {Ai},
  author = {{lvliyuan}},
  year = {2023},
  month = oct,
  journal = {keji},
  urldate = {2025-05-05},
  abstract = {},
  howpublished = {https://www.gov.cn/zhengce/zhengceku/202310/content\_6908045.htm},
  langid = {american},
  timestamp = {2025-05-05T02:44:58Z}
}

@article{ma2014measuring,
  title = {Measuring the {{Effect}} of {{Inter-Study Variability}} on {{Estimating Prediction Error}}},
  author = {Ma, Shuyi and Sung, Jaeyun and Magis, Andrew T. and Wang, Yuliang and Geman, Donald and Price, Nathan D.},
  editor = {Hsiao, Chuhsing Kate},
  year = {2014},
  month = oct,
  journal = {PLoS ONE},
  volume = {9},
  number = {10},
  pages = {e110840},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0110840},
  urldate = {2025-04-04},
  langid = {english},
  annotation = {TLDR: By examining how fast ISV performance approaches RCV as the number of studies is increased, one can estimate when ``sufficient'' diversity has been achieved for learning a molecular signature likely to translate without significant loss of accuracy to new clinical settings.},
  timestamp = {2025-04-04T03:00:50Z}
}

@article{ma2018using,
  title = {Using Deep Learning to Model the Hierarchical Structure and Function of a Cell},
  author = {Ma, Jianzhu and Yu, Michael Ku and Fong, Samson and Ono, Keiichiro and Sage, Eric and Demchak, Barry and Sharan, Roded and Ideker, Trey},
  year = {2018},
  month = apr,
  journal = {Nature Methods},
  volume = {15},
  number = {4},
  pages = {290--298},
  issn = {1548-7105},
  doi = {10.1038/nmeth.4627},
  abstract = {Although artificial neural networks are powerful classifiers, their internal structures are hard to interpret. In the life sciences, extensive knowledge of cell biology provides an opportunity to design visible neural networks (VNNs) that couple the model's inner workings to those of real systems. Here we develop DCell, a VNN embedded in the hierarchical structure of 2,526 subsystems comprising a eukaryotic cell (http://d-cell.ucsd.edu/). Trained on several million genotypes, DCell simulates cellular growth nearly as accurately as laboratory observations. During simulation, genotypes induce patterns of subsystem activities, enabling in silico investigations of the molecular mechanisms underlying genotype-phenotype associations. These mechanisms can be validated, and many are unexpected; some are governed by Boolean logic. Cumulatively, 80\% of the importance for growth prediction is captured by 484 subsystems (21\%), reflecting the emergence of a complex phenotype. DCell provides a foundation for decoding the genetics of disease, drug resistance and synthetic life.},
  langid = {english},
  pmcid = {PMC5882547},
  pmid = {29505029},
  keywords = {Cell Physiological Phenomena,Computer Simulation,Deep Learning,Gene Expression Regulation,Genotype,Humans,Neural Networks Computer},
  annotation = {TLDR: DCell, a VNN embedded in the hierarchical structure of 2,526 subsystems comprising a eukaryotic cell, provides a foundation for decoding the genetics of disease, drug resistance and synthetic life.},
  timestamp = {2025-07-05T12:45:32Z}
}

@article{ma2022novel,
  title = {A Novel Surgical Planning System Using an {{AI}} Model to Optimize Planning of Pedicle Screw Trajectories with Highest Bone Mineral Density and Strongest Pull-out Force},
  author = {Ma, Chi and Zou, Da and Qi, Huan and Li, Chentian and Zhang, Cheng and Yang, Kedi and Zhu, Feng and Li, Weishi and Lu, William W.},
  year = {2022},
  month = apr,
  journal = {Neurosurgical Focus},
  volume = {52},
  number = {4},
  pages = {E10},
  issn = {1092-0684},
  doi = {10.3171/2022.1.FOCUS21721},
  abstract = {OBJECTIVE: The purpose of this study was to evaluate the ability of a novel artificial intelligence (AI) model in identifying optimized transpedicular screw trajectories with higher bone mineral density (BMD) as well as higher pull-out force (POF) in osteoporotic patients. METHODS: An innovative pedicle screw trajectory planning system called Bone's Trajectory was developed using a 3D graphic search and an AI-based finite element analysis model. The preoperative CT scans of 21 elderly osteoporotic patients were analyzed retrospectively. The AI model automatically calculated the number of alternative transpedicular trajectories, the trajectory BMD, and the estimated POF of L3-5. The highest BMD and highest POF of optimized trajectories were recorded and compared with AO standard trajectories. RESULTS: The average patient age and average BMD of the vertebral bodies were 69.6 {\textpm} 7.8 years and 55.9 {\textpm} 17.1 mg/ml, respectively. On both sides of L3-5, the optimized trajectories showed significantly higher BMD and POF than the AO standard trajectories (p {$<$} 0.05). On average, the POF of optimized trajectory screws showed at least a 2.0-fold increase compared with AO trajectory screws. CONCLUSIONS: The novel AI model performs well in enabling the selection of optimized transpedicular trajectories with higher BMD and POF than the AO standard trajectories.},
  langid = {english},
  pmid = {35364575},
  keywords = {Aged,artificial intelligence,Artificial Intelligence,Bone Density,Humans,osteoporosis,Pedicle Screws,Retrospective Studies,screw trajectory,Spinal Fusion,surgical planning},
  annotation = {TLDR: The novel AI model performs well in enabling the selection of optimized transpedicular trajectories with higher BMD and POF than the AO standard trajectories.},
  timestamp = {2025-05-04T01:09:30Z}
}

@article{maceachern2021machine,
  title = {Machine Learning for Precision Medicine},
  author = {MacEachern, Sarah J. and Forkert, Nils D.},
  year = {2021},
  month = apr,
  journal = {Genome},
  volume = {64},
  number = {4},
  pages = {416--425},
  issn = {0831-2796, 1480-3321},
  doi = {10.1139/gen-2020-0131},
  urldate = {2025-05-27},
  abstract = {Precision medicine is an emerging approach to clinical research and patient care that focuses on understanding and treating disease by integrating multi-modal or multi-omics data from an individual to make patient-tailored decisions. With the large and complex datasets generated using precision medicine diagnostic approaches, novel techniques to process and understand these complex data were needed. At the same time, computer science has progressed rapidly to develop techniques that enable the storage, processing, and analysis of these complex datasets, a feat that traditional statistics and early computing technologies could not accomplish. Machine learning, a branch of artificial intelligence, is a computer science methodology that aims to identify complex patterns in data that can be used to make predictions or classifications on new unseen data or for advanced exploratory data analysis. Machine learning analysis of precision medicine's multi-modal data allows for broad analysis of large datasets and ultimately a greater understanding of human health and disease. This review focuses on machine learning utilization for precision medicine's ``big data'', in the context of genetics, genomics, and beyond.},
  copyright = {http://www.nrcresearchpress.com/page/about/CorporateTextAndDataMining},
  langid = {english},
  annotation = {TLDR: This review focuses on machine learning utilization for precision medicine's "big data", in the context of genetics, genomics, and beyond.},
  timestamp = {2025-05-27T23:46:43Z}
}

@article{magesh2020explainable,
  title = {An {{Explainable Machine Learning Model}} for {{Early Detection}} of {{Parkinson}}'s {{Disease}} Using {{LIME}} on {{DaTSCAN Imagery}}},
  author = {Magesh, Pavan Rajkumar and Myloth, Richard Delwin and Tom, Rijo Jackson},
  year = {2020},
  month = nov,
  journal = {Computers in Biology and Medicine},
  volume = {126},
  pages = {104041},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2020.104041},
  urldate = {2025-03-26},
  abstract = {Parkinson's Disease (PD) is a degenerative and progressive neurological condition. Early diagnosis can improve treatment for patients and is performed through dopaminergic imaging techniques like the SPECT DaTSCAN. In this study, we propose a machine learning model that accurately classifies any given DaTSCAN as having Parkinson's disease or not, in addition to providing a plausible reason for the prediction. This kind of reasoning is done through the use of visual indicators generated using Local Interpretable Model-Agnostic Explainer (LIME) methods. DaTSCANs were drawn from the Parkinson's Progression Markers Initiative database and trained on a CNN (VGG16) using transfer learning, yielding an accuracy of 95.2\%, a sensitivity of 97.5\%, and a specificity of 90.9\%. Keeping model interpretability of paramount importance, especially in the healthcare field, this study utilises LIME explanations to distinguish PD from non-PD, using visual superpixels on the DaTSCANs. It could be concluded that the proposed system, in union with its measured interpretability and accuracy may effectively aid medical workers in the early diagnosis of Parkinson's Disease.},
  keywords = {Computer-aided diagnosis,Convolutional neural network,Explainable AI,Interpretability,Parkinson's disease},
  annotation = {TLDR: It could be concluded that the proposed system, in union with its measured interpretability and accuracy may effectively aid medical workers in the early diagnosis of Parkinson's Disease.},
  timestamp = {2025-03-26T14:45:14Z}
}

@article{mahbooba2021explainable,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}) to {{Enhance Trust Management}} in {{Intrusion Detection Systems Using Decision Tree Model}}},
  author = {Mahbooba, Basim and Timilsina, Mohan and Sahal, Radhya and Serrano, Martin},
  year = {2021},
  journal = {Complexity},
  volume = {2021},
  number = {1},
  pages = {6634811},
  issn = {1099-0526},
  doi = {10.1155/2021/6634811},
  urldate = {2025-05-04},
  abstract = {Despite the growing popularity of machine learning models in the cyber-security applications (e.g., an intrusion detection system (IDS)), most of these models are perceived as a black-box. The eXplainable Artificial Intelligence (XAI) has become increasingly important to interpret the machine learning models to enhance trust management by allowing human experts to understand the underlying data evidence and causal reasoning. According to IDS, the critical role of trust management is to understand the impact of the malicious data to detect any intrusion in the system. The previous studies focused more on the accuracy of the various classification algorithms for trust in IDS. They do not often provide insights into their behavior and reasoning provided by the sophisticated algorithm. Therefore, in this paper, we have addressed XAI concept to enhance trust management by exploring the decision tree model in the area of IDS. We use simple decision tree algorithms that can be easily read and even resemble a human approach to decision-making by splitting the choice into many small subchoices for IDS. We experimented with this approach by extracting rules in a widely used KDD benchmark dataset. We also compared the accuracy of the decision tree approach with the other state-of-the-art algorithms.},
  copyright = {Copyright {\copyright} 2021 Basim Mahbooba et al.},
  langid = {english},
  annotation = {TLDR: This paper addresses XAI concept to enhance trust management by exploring the decision tree model in the area of IDS by using simple decision tree algorithms that can be easily read and even resemble a human approach to decision-making by splitting the choice into many small subchoices for IDS.},
  timestamp = {2025-05-04T00:23:31Z}
}

@article{mahmoud2025explainable,
  title = {Explainable {{AI}} for {{Prognostic Factor Identification}} in {{Colorectal Cancer}}: {{An Electronic Health Records Analysis}}},
  shorttitle = {Explainable {{AI}} for {{Prognostic Factor Identification}} in {{Colorectal Cancer}}},
  author = {Mahmoud, Amena},
  year = {2025},
  month = jul,
  journal = {International Journal of Mathematics, Statistics, and Computer Science},
  volume = {3},
  pages = {457--475},
  issn = {2704-1069, 2704-1077},
  doi = {10.59543/ijmscs.v3i.15123},
  urldate = {2025-08-11},
  abstract = {Colorectal cancer (CRC) prognosis remains challenging due to the disease's heterogeneity and the complex interplay of clinical, demographic, and molecular factors. This study leverages explainable artificial intelligence (XAI) and electronic health records (EHRs) to develop interpretable machine learning models for prognostic factor identification in CRC. Using a retrospective cohort of 8,247 patients, we extracted 1,247 features from EHRs, including demographic, laboratory, treatment, and natural language processing (NLP)-derived data. After rigorous feature selection, six machine learning models were evaluated, with XGBoost achieving the highest performance (C-index: 0.798, 95\% CI: 0.785--0.811), significantly outperforming traditional Cox models (C-index: 0.742) and established prognostic scores. SHAP and LIME analyses identified both established prognostic factors (e.g., TNM stage, age) and novel predictors, such as temporal albumin trends and neutrophil-to-lymphocyte ratio (NLR), which accounted for 40\% of the top prognostic features. Clinical validation by oncology experts confirmed the relevance and biological plausibility of these findings. The study demonstrates that XAI-enhanced models can improve prognostic accuracy while providing transparent, actionable insights, bridging the gap between complex machine learning outputs and clinical decision-making. These results highlight the potential of integrating comprehensive EHR data with XAI to advance precision oncology in CRC care.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  annotation = {TLDR: It is demonstrated that XAI-enhanced models can improve prognostic accuracy while providing transparent, actionable insights, bridging the gap between complex machine learning outputs and clinical decision-making.},
  timestamp = {2025-08-11T08:33:45Z}
}

@article{malec2023causal,
  title = {Causal Feature Selection Using a Knowledge Graph Combining Structured Knowledge from the Biomedical Literature and Ontologies: {{A}} Use Case Studying Depression as a Risk Factor for {{Alzheimer}}'s Disease},
  shorttitle = {Causal Feature Selection Using a Knowledge Graph Combining Structured Knowledge from the Biomedical Literature and Ontologies},
  author = {Malec, Scott A. and Taneja, Sanya B. and Albert, Steven M. and Elizabeth Shaaban, C. and Karim, Helmet T. and Levine, Arthur S. and Munro, Paul and Callahan, Tiffany J. and Boyce, Richard D.},
  year = {2023},
  month = jun,
  journal = {Journal of Biomedical Informatics},
  volume = {142},
  pages = {104368},
  issn = {1532-0480},
  doi = {10.1016/j.jbi.2023.104368},
  abstract = {BACKGROUND: Causal feature selection is essential for estimating effects from observational data. Identifying confounders is a crucial step in this process. Traditionally, researchers employ content-matter expertise and literature review to identify confounders. Uncontrolled confounding from unidentified confounders threatens validity, conditioning on intermediate variables (mediators) weakens estimates, and conditioning on common effects (colliders) induces bias. Additionally, without special treatment, erroneous conditioning on variables combining roles introduces bias. However, the vast literature is growing exponentially, making it infeasible to assimilate this knowledge. To address these challenges, we introduce a novel knowledge graph (KG) application enabling causal feature selection by combining computable literature-derived knowledge with biomedical ontologies. We present a use case of our approach specifying a causal model for estimating the total causal effect of depression on the risk of developing Alzheimer's disease (AD) from observational data. METHODS: We extracted computable knowledge from a literature corpus using three machine reading systems and inferred missing knowledge using logical closure operations. Using a KG framework, we mapped the output to target terminologies and combined it with ontology-grounded resources. We translated epidemiological definitions of confounder, collider, and mediator into queries for searching the KG and summarized the roles played by the identified variables. We compared the results with output from a complementary method and published observational studies and examined a selection of confounding and combined role variables in-depth. RESULTS: Our search identified 128 confounders, including 58 phenotypes, 47 drugs, 35 genes, 23 collider, and 16 mediator phenotypes. However, only 31 of the 58 confounder phenotypes were found to behave exclusively as confounders, while the remaining 27 phenotypes played other roles. Obstructive sleep apnea emerged as a potential novel confounder for depression and AD. Anemia exemplified a variable playing combined roles. CONCLUSION: Our findings suggest combining machine reading and KG could augment human expertise for causal feature selection. However, the complexity of causal feature selection for depression with AD highlights the need for standardized field-specific databases of causal variables. Further work is needed to optimize KG search and transform the output for human consumption.},
  langid = {english},
  pmcid = {PMC10355339},
  pmid = {37086959},
  keywords = {Alzheimer Disease,Alzheimer's disease,Causal modeling,Causality,Depression,Feature selection,Humans,Knowledge graphs,Knowledge representation management or engineering,Pattern Recognition Automated,Risk Factors},
  timestamp = {2025-04-17T15:24:25Z}
}

@incollection{manhaeve2021chapter,
  title = {Chapter 7. {{Neuro-Symbolic AI}} = {{Neural}} + {{Logical}} + {{Probabilistic AI}}},
  booktitle = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  author = {Manhaeve, Robin and Marra, Giuseppe and Demeester, Thomas and Duman{\v c}i{\'c}, Sebastijan and Kimmig, Angelika and De Raedt, Luc},
  editor = {Hitzler, Pascal and Sarker, Md Kamruzzaman},
  year = {2021},
  month = dec,
  publisher = {IOS Press},
  doi = {10.3233/FAIA210354},
  urldate = {2025-06-13},
  abstract = {There is a broad consensus that both learning and reasoning are essential to achieve true artificial intelligence. This has put the quest for neural-symbolic artificial intelligence (NeSy) high on the research agenda. In the past decade, neural networks have caused great advances in the field of machine learning. Conversely, the two most prominent frameworks for reasoning are logic and probability. While in the past they were studied by separate communities, a significant number of researchers has been working towards their integration, cf. the area of statistical relational artificial intelligence (StarAI). Generally, NeSy systems integrate logic with neural networks. However, probability theory has already been integrated with both logic (cf. StarAI) and neural networks. It therefore makes sense to consider the integration of logic, neural networks and probabilities. In this chapter, we first consider these three base paradigms separately. Then, we look at the well established integrations, NeSy and StarAI. Next, we consider the integration of all three paradigms as Neural Probabilistic Logic Programming, and exemplify it with the DeepProbLog framework. Finally, we discuss the limitations of the state of the art, and consider future directions based on the parallels between StarAI and NeSy.},
  isbn = {978-1-64368-244-0 978-1-64368-245-7},
  timestamp = {2025-06-13T04:16:04Z}
}

@misc{mao2017modeling,
  title = {Modeling {{Enhancer-Promoter Interactions}} with {{Attention-Based Neural Networks}}},
  author = {Mao, Weiguang and Kostka, Dennis and Chikina, Maria},
  year = {2017},
  month = nov,
  publisher = {Bioinformatics},
  doi = {10.1101/219667},
  urldate = {2025-07-05},
  abstract = {Abstract                        Background             Gene regulatory sequences play critical roles in ensuring tightly controlled RNA expression patterns that are essential in a large variety of biological processes. Specifically, enhancer sequences drive expression of their target genes, and the availability of genome-wide maps of enhancer-promoter interactions has opened up the possibility to use machine learning approaches to extract and interpret features that define these interactions in different biological contexts.                                   Methods                            Inspired by machine translation models we develop an attention-based neural network model, EPIANN, to predict enhancer-promoter interactions based on DNA sequences. Codes and data are available at               https://github.com/wgmao/EPIANN               .                                                Results             Our approach accurately predicts enhancer-promoter interactions across six cell lines. In addition, our method generates pairwise attention scores at the sequence level, which specify how short regions in the enhancer and promoter pair-up to drive the interaction prediction. This allows us to identify over-represented transcription factors (TF) binding sites and TF-pair interactions in the context of enhancer function.},
  archiveprefix = {Bioinformatics},
  langid = {english},
  annotation = {TLDR: An attention-based neural network model, EPIANN, is developed to predict enhancer-promoter interactions based on DNA sequences, which allows us to identify over-represented transcription factors (TF) binding sites and TF-pair interactions in the context of enhancer function.},
  timestamp = {2025-07-05T06:12:51Z}
}

@misc{mao2019neurosymbolic,
  title = {The {{Neuro-Symbolic Concept Learner}}: {{Interpreting Scenes}}, {{Words}}, and {{Sentences From Natural Supervision}}},
  shorttitle = {The {{Neuro-Symbolic Concept Learner}}},
  author = {Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B. and Wu, Jiajun},
  year = {2019},
  month = apr,
  number = {arXiv:1904.12584},
  eprint = {1904.12584},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.12584},
  urldate = {2025-03-17},
  abstract = {We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  timestamp = {2025-03-17T08:38:53Z}
}

@inproceedings{mao2023xgboostenhanced,
  title = {{{XGBoost-Enhanced Prediction}} and {{Interpretation}} of {{Heart Disease Using SHAP Values}}},
  booktitle = {2023 4th {{International Conference}} on {{Computer}}, {{Big Data}} and {{Artificial Intelligence}} ({{ICCBD}}+{{AI}})},
  author = {Mao, QunYi and Jiang, Wangdong and Sun, Guang and Zeng, DaiWei},
  year = {2023},
  month = dec,
  pages = {738--742},
  publisher = {IEEE},
  address = {Guiyang, China},
  doi = {10.1109/ICCBD-AI62252.2023.00134},
  urldate = {2025-06-12},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-7323-3},
  annotation = {TLDR: A classification model for heart disease prediction, underpinned by the XGBoost algorithm and informed by authentic medical datasets from the Kaggle platform is introduced, exhibiting strong predictive capabilities and interpretability.},
  timestamp = {2025-06-12T15:41:54Z}
}

@article{mao2025assessments,
  title = {Assessments of Lung Nodules by an Artificial Intelligence Chatbot Using Longitudinal {{CT}} Images},
  author = {Mao, Yuqiang and Xu, Nan and Wu, Yanan and Wang, Lu and Wang, Hongtao and He, Qianqian and Zhao, Tianqi and Ma, Shuangchun and Zhou, Meihong and Jin, Hongjie and Pei, Dongmei and Zhang, Lina and Song, Jiangdian},
  year = {2025},
  month = mar,
  journal = {Cell Reports Medicine},
  volume = {6},
  number = {3},
  publisher = {Elsevier},
  issn = {2666-3791},
  doi = {10.1016/j.xcrm.2025.101988},
  urldate = {2025-04-04},
  langid = {english},
  pmid = {40043704},
  keywords = {computed tomography,deep learning,diagnosis,feature detection,follow-up,GPT,large language model,lung cancer,lung nodule,malignancy},
  annotation = {TLDR: GPT-4o could capture dynamic changes in lung nodules across longitudinal follow-up CT images, thus providing high-quality radiological evidence to assist in clinical management, and could capture changes in nodule features with a median Likert score of 4.17.},
  timestamp = {2025-04-04T00:40:24Z}
}

@misc{mao2025neurosymbolic,
  title = {Neuro-{{Symbolic Concepts}}},
  author = {Mao, Jiayuan and Tenenbaum, Joshua B. and Wu, Jiajun},
  year = {2025},
  month = may,
  number = {arXiv:2505.06191},
  eprint = {2505.06191},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.06191},
  urldate = {2025-05-28},
  abstract = {This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  timestamp = {2025-05-28T00:33:44Z}
}

@misc{marcus2020next,
  title = {The {{Next Decade}} in {{AI}}: {{Four Steps Towards Robust Artificial Intelligence}}},
  shorttitle = {The {{Next Decade}} in {{AI}}},
  author = {Marcus, Gary},
  year = {2020},
  month = feb,
  number = {arXiv:2002.06177},
  eprint = {2002.06177},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.06177},
  urldate = {2025-09-01},
  abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  timestamp = {2025-09-01T12:05:31Z}
}

@misc{marcus2020nexta,
  title = {The {{Next Decade}} in {{AI}}: {{Four Steps Towards Robust Artificial Intelligence}}},
  shorttitle = {The {{Next Decade}} in {{AI}}},
  author = {Marcus, Gary},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2002.06177},
  urldate = {2025-09-05},
  abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,I.2; I.2.6,Machine Learning (cs.LG)},
  timestamp = {2025-09-05T16:15:27Z}
}

@article{marey2024explainability,
  title = {Explainability, Transparency and Black Box Challenges of {{AI}} in Radiology: Impact on Patient Care in Cardiovascular Radiology},
  shorttitle = {Explainability, Transparency and Black Box Challenges of {{AI}} in Radiology},
  author = {Marey, Ahmed and Arjmand, Parisa and Alerab, Ameerh Dana Sabe and Eslami, Mohammad Javad and Saad, Abdelrahman M. and Sanchez, Nicole and Umair, Muhammad},
  year = {2024},
  month = sep,
  journal = {Egyptian Journal of Radiology and Nuclear Medicine},
  volume = {55},
  number = {1},
  pages = {183},
  issn = {2090-4762},
  doi = {10.1186/s43055-024-01356-2},
  urldate = {2025-03-03},
  abstract = {The integration of artificial intelligence (AI) in cardiovascular imaging has revolutionized the field, offering significant advancements in diagnostic accuracy and clinical efficiency. However, the complexity and opacity of AI models, particularly those involving machine learning (ML) and deep learning (DL), raise critical legal and ethical concerns due to their "black box" nature. This manuscript addresses these concerns by providing a comprehensive review of AI technologies in cardiovascular imaging, focusing on the challenges and implications of the black box phenomenon. We begin by outlining the foundational concepts of AI, including ML and DL, and their applications in cardiovascular imaging. The manuscript delves into the "black box" issue, highlighting the difficulty in understanding and explaining AI decision-making processes. This lack of transparency poses significant challenges for clinical acceptance and ethical deployment. The discussion then extends to the legal and ethical implications of AI's opacity. The need for explicable AI systems is underscored, with an emphasis on the ethical principles of beneficence and non-maleficence. The manuscript explores potential solutions such as explainable AI (XAI) techniques, which aim to provide insights into AI decision-making without sacrificing performance. Moreover, the impact of AI explainability on clinical decision-making and patient outcomes is examined. The manuscript argues for the development of hybrid models that combine interpretability with the advanced capabilities of black box systems. It also advocates for enhanced education and training programs for healthcare professionals to equip them with the necessary skills to utilize AI effectively. Patient involvement and informed consent are identified as critical components for the ethical deployment of AI in healthcare. Strategies for improving patient understanding and engagement with AI technologies are discussed, emphasizing the importance of transparent communication and education. Finally, the manuscript calls for the establishment of standardized regulatory frameworks and policies to address the unique challenges posed by AI in healthcare. By fostering interdisciplinary collaboration and continuous monitoring, the medical community can ensure the responsible integration of AI into cardiovascular imaging, ultimately enhancing patient care and clinical outcomes.},
  langid = {english},
  keywords = {Artificial intelligence,Artificial Intelligence,Black box phenomenon,Cardiovascular imaging,Clinical decision-making,Deep learning,Ethical implications,Explainable AI,Machine learning,Medical Ethics,Patient outcomes,Regulatory frameworks},
  annotation = {TLDR: This manuscript argues for the development of hybrid models that combine interpretability with the advanced capabilities of black box systems, and advocates for enhanced education and training programs for healthcare professionals to equip them with the necessary skills to utilize AI effectively.},
  timestamp = {2025-03-03T09:38:34Z}
}

@article{markus2021role,
  ids = {role2021},
  title = {The Role of Explainability in Creating Trustworthy Artificial Intelligence for Health Care: {{A}} Comprehensive Survey of the Terminology, Design Choices, and Evaluation Strategies},
  shorttitle = {The Role of Explainability in Creating Trustworthy Artificial Intelligence for Health Care},
  author = {Markus, Aniek F. and Kors, Jan A. and Rijnbeek, Peter R.},
  year = {2021},
  month = jan,
  journal = {Journal of Biomedical Informatics},
  volume = {113},
  pages = {103655},
  publisher = {Academic Press},
  issn = {15320464},
  doi = {10.1016/j.jbi.2020.103655},
  urldate = {2025-03-03},
  abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. La{\dots}},
  langid = {english},
  annotation = {TLDR: It is concluded that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care.},
  timestamp = {2025-03-05T02:16:22Z}
}

@article{mashekova2024importance,
  title = {The Importance of Explainable Artificial Intelligence Based Medical Diagnosis},
  author = {Mashekova, Aigerim and Zarikas, Vasilios and Zhao, Yong and Ng, Eddie Yin Kwee},
  year = {2024},
  journal = {Clinical and Experimental Obstetrics \& Gynecology},
  volume = {51},
  number = {12},
  pages = {268},
  publisher = {IMR Press},
  timestamp = {2025-04-15T14:49:52Z}
}

@article{mathew2025recent,
  ids = {danielenemonamathew2025recent},
  title = {Recent {{Emerging Techniques}} in {{Explainable Artificial Intelligence}} to {{Enhance}} the {{Interpretable}} and {{Understanding}} of {{AI Models}} for {{Human}}},
  author = {Mathew, Daniel Enemona and Ebem, Deborah Uzoamaka and Ikegwu, Anayo Chukwu and Ukeoma, Pamela Eberechukwu and Dibiaezue, Ngozi Fidelia},
  year = {2025},
  month = feb,
  journal = {Neural Processing Letters},
  volume = {57},
  number = {1},
  pages = {16},
  issn = {1573-773X},
  doi = {10.1007/s11063-025-11732-2},
  urldate = {2025-05-04},
  abstract = {Recent advancements in Explainable Artificial Intelligence (XAI) aim to bridge the gap between complex artificial intelligence (AI) models and human understanding, fostering trust and usability in AI systems. However, challenges persist in comprehensively interpreting these models, hindering their widespread adoption. This study addresses these challenges by exploring recently emerging techniques in XAI. The primary problem addressed is the lack of transparency and interpretability in AI models to humanity for institution-wide use, which undermines user trust and inhibits their integration into critical decision-making processes. Through an in-depth review, this study identifies the objectives of enhancing the interpretability of AI models and improving human understanding of their decision-making processes. Various methodological approaches, including post-hoc explanations, model transparency methods, and interactive visualization techniques, are investigated to elucidate AI model behaviours. We further present techniques and methods to make AI models more interpretable and understandable to humans including their strengths and weaknesses to demonstrate promising advancements in model interpretability, facilitating better comprehension of complex AI systems by humans. In addition, we provide the application of XAI in local use cases. Challenges, solutions, and open research directions were highlighted to clarify these compelling XAI utilization challenges. The implications of this research are profound, as enhanced interpretability fosters trust in AI systems across diverse applications, from healthcare to finance. By empowering users to understand and scrutinize AI decisions, these techniques pave the way for more responsible and accountable AI deployment.},
  langid = {english},
  keywords = {AI models,AI use cases,Artificial intelligence,Artificial Intelligence,Explainable AI,Interpretable AI,Review},
  annotation = {S2ID: 4ed5429cf635af02d36bf9ba69ed3cf61845dc4e},
  timestamp = {2025-05-04T00:20:54Z}
}

@misc{megas2024celcomen,
  title = {Celcomen: Spatial Causal Disentanglement for Single-Cell and Tissue Perturbation Modeling},
  shorttitle = {Celcomen},
  author = {Megas, Stathis and Chen, Daniel G. and Polanski, Krzysztof and Eliasof, Moshe and Schonlieb, Carola-Bibiane and Teichmann, Sarah A.},
  year = {2024},
  month = sep,
  number = {arXiv:2409.05804},
  eprint = {2409.05804},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.05804},
  urldate = {2025-04-17},
  abstract = {Celcomen leverages a mathematical causality framework to disentangle intra- and inter- cellular gene regulation programs in spatial transcriptomics and single-cell data through a generative graph neural network. It can learn gene-gene interactions, as well as generate post-perturbation counterfactual spatial transcriptomics, thereby offering access to experimentally inaccessible samples. We validated its disentanglement, identifiability, and counterfactual prediction capabilities through simulations and in clinically relevant human glioblastoma, human fetal spleen, and mouse lung cancer samples. Celcomen provides the means to model disease and therapy induced changes allowing for new insights into single-cell spatially resolved tissue responses relevant to human health.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Tissues and Organs},
  annotation = {TLDR: Celcomen provides the means to model disease and therapy induced changes allowing for new insights into single-cell spatially resolved tissue responses relevant to human health.},
  timestamp = {2025-04-17T11:59:50Z}
}

@article{mendez2025roadmap,
  title = {A Roadmap to Precision Medicine through Post-Genomic Electronic Medical Records},
  author = {Mendez, Kevin M. and Reinke, Stacey N. and Kelly, Rachel S. and Chen, Qingwen and Su, Mark and McGeachie, Michael and Weiss, Scott and Broadhurst, David I. and {Lasky-Su}, Jessica A.},
  year = {2025},
  month = feb,
  journal = {Nature Communications},
  volume = {16},
  number = {1},
  pages = {1700},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-025-56442-4},
  urldate = {2025-05-24},
  abstract = {The promise of integrating Electronic Medical Records (EMR) and genetic data for precision medicine has largely fallen short due to its omission of environmental context over time. Post-genomic data can bridge this gap by capturing the real-time dynamic relationship between underlying genetics and the environment. This perspective highlights the pivotal role of integrating EMR and post-genomics for personalized health, reflecting on lessons from past efforts, and outlining a roadmap of challenges and opportunities that must be addressed to realize the potential of precision medicine.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Genetics,Health care},
  annotation = {TLDR: This perspective highlights the pivotal role of integrating EMR and post-genomics for personalized health, reflecting on lessons from past efforts, and outlining a roadmap of challenges and opportunities that must be addressed to realize the potential of precision medicine.},
  timestamp = {2025-05-24T08:44:17Z}
}

@misc{mercier2022time,
  title = {Time to {{Focus}}: {{A Comprehensive Benchmark Using Time Series Attribution Methods}}},
  shorttitle = {Time to {{Focus}}},
  author = {Mercier, Dominique and Bhatt, Jwalin and Dengel, Andreas and Ahmed, Sheraz},
  year = {2022},
  month = feb,
  number = {arXiv:2202.03759},
  eprint = {2202.03759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.03759},
  urldate = {2025-04-09},
  abstract = {In the last decade neural network have made huge impact both in industry and research due to their ability to extract meaningful features from imprecise or complex data, and by achieving super human performance in several domains. However, due to the lack of transparency the use of these networks is hampered in the areas with safety critical areas. In safety-critical areas, this is necessary by law. Recently several methods have been proposed to uncover this black box by providing interpreation of predictions made by these models. The paper focuses on time series analysis and benchmark several state-of-the-art attribution methods which compute explanations for convolutional classifiers. The presented experiments involve gradient-based and perturbation-based attribution methods. A detailed analysis shows that perturbation-based approaches are superior concerning the Sensitivity and occlusion game. These methods tend to produce explanations with higher continuity. Contrarily, the gradient-based techniques are superb in runtime and Infidelity. In addition, a validation the dependence of the methods on the trained model, feasible application domains, and individual characteristics is attached. The findings accentuate that choosing the best-suited attribution method is strongly correlated with the desired use case. Neither category of attribution methods nor a single approach has shown outstanding performance across all aspects.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Artificial Intelligence},
  timestamp = {2025-04-09T03:37:48Z}
}

@article{mertes2022ganterfactual,
  title = {{{GANterfactual}}---{{Counterfactual Explanations}} for {{Medical Non-experts Using Generative Adversarial Learning}}},
  author = {Mertes, Silvan and Huber, Tobias and Weitz, Katharina and Heimerl, Alexander and Andr{\'e}, Elisabeth},
  year = {2022},
  month = apr,
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  pages = {825565},
  issn = {2624-8212},
  doi = {10.3389/frai.2022.825565},
  urldate = {2025-05-03},
  abstract = {With the ongoing rise of machine learning, the need for methods for explaining decisions made by artificial intelligence systems is becoming a more and more important topic. Especially for image classification tasks, many state-of-the-art tools to explain such classifiers rely on visual highlighting of important areas of the input data. Contrary, counterfactual explanation systems try to enable a counterfactual reasoning by modifying the input image in a way such that the classifier would have made a different prediction. By doing so, the users of counterfactual explanation systems are equipped with a completely different kind of explanatory information. However, methods for generating realistic counterfactual explanations for image classifiers are still rare. Especially in medical contexts, where relevant information often consists of textural and structural information, high-quality counterfactual images have the potential to give meaningful insights into decision processes. In this work, we present               GANterfactual               , an approach to generate such counterfactual image explanations based on adversarial image-to-image translation techniques. Additionally, we conduct a user study to evaluate our approach in an exemplary medical use case. Our results show that, in the chosen medical use-case, counterfactual explanations lead to significantly better results regarding mental models, explanation satisfaction, trust, emotions, and self-efficacy than two state-of-the art systems that work with saliency maps, namely LIME and LRP.},
  annotation = {TLDR: GANterfactual, an approach to generate counterfactual image explanations based on adversarial image-to-image translation techniques, shows that it leads to significantly better results regarding mental models, explanation satisfaction, trust, emotions, and self-efficacy than two state-of-the art systems that work with saliency maps, namely LIME and LRP.},
  timestamp = {2025-05-03T12:51:13Z}
}

@article{meske2022explainable,
  title = {Explainable {{Artificial Intelligence}}: {{Objectives}}, {{Stakeholders}}, and {{Future Research Opportunities}}},
  shorttitle = {Explainable {{Artificial Intelligence}}},
  author = {Meske, Christian and Bunde, Enrico and Schneider, Johannes and Gersch, Martin},
  year = {2022},
  month = jan,
  journal = {Information Systems Management},
  volume = {39},
  number = {1},
  pages = {53--63},
  issn = {1058-0530, 1934-8703},
  doi = {10.1080/10580530.2020.1849465},
  urldate = {2025-04-21},
  langid = {english},
  annotation = {TLDR: This research note describes exemplary risks of black-box AI, the consequent need for explainability, and previous research on Explainable AI (XAI) in information systems research.},
  timestamp = {2025-04-21T07:33:03Z}
}

@article{mesko2017role,
  title = {The Role of Artificial Intelligence in Precision Medicine},
  author = {Mesko, Bertalan},
  year = {2017},
  month = sep,
  journal = {Expert Review of Precision Medicine and Drug Development},
  volume = {2},
  number = {5},
  pages = {239--241},
  issn = {2380-8993},
  doi = {10.1080/23808993.2017.1380516},
  urldate = {2025-05-13},
  langid = {english},
  annotation = {TLDR: Artificial intelligence is the key technology that can bring this opportunity to everyday practice in medicine toward prevention, personalization, and precision.},
  timestamp = {2025-05-13T11:19:41Z}
}

@article{metta2024towards,
  title = {Towards Transparent Healthcare: Advancing Local Explanation Methods in Explainable Artificial Intelligence},
  author = {Metta, Carlo and Beretta, Andrea and Pellungrini, Roberto and Rinzivillo, Salvatore and Giannotti, Fosca},
  year = {2024},
  journal = {Bioengineering},
  volume = {11},
  number = {4},
  pages = {369},
  publisher = {MDPI},
  timestamp = {2025-04-16T02:01:08Z}
}

@misc{meulemeester2023calculating,
  title = {Calculating and {{Visualizing Counterfactual Feature Importance Values}}},
  author = {Meulemeester, Bjorge and Oliveira, Raphael Mazzine Barbosa De and Martens, David},
  year = {2023},
  month = jun,
  number = {arXiv:2306.06506},
  eprint = {2306.06506},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.06506},
  urldate = {2025-04-12},
  abstract = {Despite the success of complex machine learning algorithms, mostly justified by an outstanding performance in prediction tasks, their inherent opaque nature still represents a challenge to their responsible application. Counterfactual explanations surged as one potential solution to explain individual decision results. However, two major drawbacks directly impact their usability: (1) the isonomic view of feature changes, in which it is not possible to observe {\textbackslash}textit\{how much\} each modified feature influences the prediction, and (2) the lack of graphical resources to visualize the counterfactual explanation. We introduce Counterfactual Feature (change) Importance (CFI) values as a solution: a way of assigning an importance value to each feature change in a given counterfactual explanation. To calculate these values, we propose two potential CFI methods. One is simple, fast, and has a greedy nature. The other, coined CounterShapley, provides a way to calculate Shapley values between the factual-counterfactual pair. Using these importance values, we additionally introduce three chart types to visualize the counterfactual explanations: (a) the Greedy chart, which shows a greedy sequential path for prediction score increase up to predicted class change, (b) the CounterShapley chart, depicting its respective score in a simple and one-dimensional chart, and finally (c) the Constellation chart, which shows all possible combinations of feature changes, and their impact on the model's prediction score. For each of our proposed CFI methods and visualization schemes, we show how they can provide more information on counterfactual explanations. Finally, an open-source implementation is offered, compatible with any counterfactual explanation generator algorithm. Code repository at: https://github.com/ADMAntwerp/CounterPlots},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  annotation = {TLDR: This work introduces Counterfactual Feature (change) Importance values as a solution: a way of assigning an importance value to each feature change in a given counterfactual explanation, and proposes two potential CFI methods and visualization schemes.},
  timestamp = {2025-04-12T07:42:24Z}
}

@article{michoel2023causal,
  title = {Causal Inference in Drug Discovery and Development},
  author = {Michoel, Tom and Zhang, Jitao David},
  year = {2023},
  month = oct,
  journal = {Drug Discovery Today},
  volume = {28},
  number = {10},
  eprint = {2209.14664},
  primaryclass = {q-bio},
  pages = {103737},
  issn = {13596446},
  doi = {10.1016/j.drudis.2023.103737},
  urldate = {2025-05-04},
  abstract = {To discover new drugs is to seek and to prove causality. As an emerging approach leveraging human knowledge and creativity, data, and machine intelligence, causal inference holds the promise of reducing cognitive bias and improving decision making in drug discovery. While it has been applied across the value chain, the concepts and practice of causal inference remain obscure to many practitioners. This article offers a non-technical introduction to causal inference, reviews its recent applications, and discusses opportunities and challenges of adopting the causal language in drug discovery and development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Applications},
  annotation = {TLDR: A nontechnical introduction to causal inference is offered, its recent applications are reviewed, and opportunities and challenges of adopting the causal language in drug discovery and development are discussed.},
  timestamp = {2025-05-14T10:48:23Z}
}

@misc{microsoft2019econml,
  title = {{{EconML}}: A Python Package for {{ML-based}} Heterogeneous Treatment Effects Estimation},
  author = {Research, Microsoft},
  year = {2019},
  howpublished = {GitHub repository},
  version = {0.x},
  timestamp = {2025-03-20T10:18:18Z}
}

@article{mienye2024survey,
  title = {A Survey of Explainable Artificial Intelligence in Healthcare: {{Concepts}}, Applications, and Challenges},
  author = {Mienye, Ibomoiye Domor and Obaido, George and Jere, Nobert and Mienye, Ebikella and Aruleba, Kehinde and Emmanuel, Ikiomoye Douglas and Ogbuokiri, Blessing},
  year = {2024},
  journal = {Informatics in Medicine Unlocked},
  pages = {101587},
  publisher = {Elsevier},
  timestamp = {2025-04-15T14:18:14Z}
}

@article{might2022why,
  title = {Why Rare Disease Needs Precision Medicine-and Precision Medicine Needs Rare Disease},
  author = {Might, Matthew and Crouse, Andrew B.},
  year = {2022},
  month = feb,
  journal = {Cell Reports. Medicine},
  volume = {3},
  number = {2},
  pages = {100530},
  issn = {2666-3791},
  doi = {10.1016/j.xcrm.2022.100530},
  abstract = {With one in ten suffering from one of 10,000 rare diseases, precision medicine opens a path toward identifying therapies for rare patients. Conversely, it is rare patients-through their collective experience and the knowledge captured in their genetics-who open the path toward identifying therapies for common patients.},
  langid = {english},
  pmcid = {PMC8861960},
  pmid = {35243424},
  keywords = {Humans,Knowledge,Precision Medicine,Rare Diseases},
  annotation = {TLDR: With one in ten suffering from one of 10,000 rare diseases, precision medicine opens a path toward identifying therapies for rare patients.},
  timestamp = {2025-05-28T00:05:54Z}
}

@inproceedings{mikolov2013,
  title = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {Proceedings of {{ICLR}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  timestamp = {2025-09-06T00:42:24Z}
}

@article{minh2022explainable,
  title = {Explainable Artificial Intelligence: A Comprehensive Review},
  shorttitle = {Explainable Artificial Intelligence},
  author = {Minh, Dang and Wang, H. Xiang and Li, Y. Fen and Nguyen, Tan N.},
  year = {2022},
  month = jun,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {5},
  pages = {3503--3568},
  issn = {1573-7462},
  doi = {10.1007/s10462-021-10088-y},
  urldate = {2025-02-02},
  abstract = {Thanks to the exponential growth in computing power and vast amounts of data, artificial intelligence (AI) has witnessed remarkable developments in recent years, enabling it to be ubiquitously adopted in our daily lives. Even though AI-powered systems have brought competitive advantages, the black-box nature makes them lack transparency and prevents them from explaining their decisions. This issue has motivated the introduction of explainable artificial intelligence (XAI), which promotes AI algorithms that can show their internal process and explain how they made decisions. The number of XAI research has increased significantly in recent years, but there lacks a unified and comprehensive review of the latest XAI progress. This review aims to bridge the gap by discovering the critical perspectives of the rapidly growing body of research associated with XAI. After offering the readers a solid XAI background, we analyze and review various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-modeling explainability. We also pay attention to the current methods that dedicate to interpret and analyze deep learning methods. In addition, we systematically discuss various XAI challenges, such as the trade-off between the performance and the explainability, evaluation methods, security, and policy. Finally, we show the standard approaches that are leveraged to deal with the mentioned challenges.},
  langid = {english},
  keywords = {Artificial Intelligence,Black-box models,Deep learning,Explainable artificial intelligence,Interpretability,Machine learning},
  annotation = {TLDR: This review aims to bridge the gap by discovering the critical perspectives of the rapidly growing body of research associated with XAI by analyzing and reviewing various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-modelling explainability.},
  timestamp = {2025-02-02T09:08:26Z}
}

@inproceedings{mittelstadt2019explaining,
  title = {Explaining Explanations in {{AI}}},
  booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
  year = {2019},
  pages = {279--288},
  timestamp = {2025-03-29T12:06:50Z}
}

@misc{mohammed2025developing,
  title = {Developing an {{Artificial Intelligence Tool}} for {{Personalized Breast Cancer Treatment Plans}} Based on the {{NCCN Guidelines}}},
  author = {Mohammed, Abdul M. and Mansoor, Iqtidar and Blythe, Sarah and Trujillo, Dennis},
  year = {2025},
  month = jan,
  number = {arXiv:2502.15698},
  eprint = {2502.15698},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.15698},
  urldate = {2025-04-04},
  abstract = {Cancer treatments require personalized approaches based on a patient's clinical condition, medical history, and evidence-based guidelines. The National Comprehensive Cancer Network (NCCN) provides frequently updated, complex guidelines through visuals like flowcharts and diagrams, which can be time consuming for oncologists to stay current with treatment protocols. This study presents an AI (Artificial Intelligence)-driven methodology to accurately automate treatment regimens following NCCN guidelines for breast cancer patients. We proposed two AI-driven methods: Agentic-RAG (Retrieval-Augmented Generation) and Graph-RAG. Agentic-RAG used a three-step Large Language Model (LLM) process to select clinical titles from NCCN guidelines, retrieve matching JSON content, and iteratively refine recommendations based on insufficiency checks. Graph-RAG followed a Microsoft-developed framework with proprietary prompts, where JSON data was converted to text via an LLM, summarized, and mapped into graph structures representing key treatment relationships. Final recommendations were generated by querying relevant graph summaries. Both were evaluated using a set of patient descriptions, each with four associated questions. As shown in Table 1, Agentic RAG achieved a 100\% adherence (24/24) with no hallucinations or incorrect treatments. Graph-RAG had 95.8\% adherence (23/24) with one incorrect treatment and no hallucinations. Chat GPT-4 showed 91.6\% adherence (22/24) with two wrong treatments and no hallucinations. Both Agentic RAG and Graph-RAG provided detailed treatment recommendations with accurate references to relevant NCCN document page numbers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval},
  annotation = {TLDR: An AI (Artificial Intelligence)-driven methodology to accurately automate treatment regimens following NCCN guidelines for breast cancer patients is presented and two AI-driven methods are proposed: Agentic-RAG (Retrieval-Augmented Generation) and Graph-RAG.},
  timestamp = {2025-04-04T02:39:29Z}
}

@book{molnar2020interpretable,
  title = {Interpretable Machine Learning},
  author = {Molnar, Christoph},
  year = {2020},
  publisher = {Lulu. com},
  timestamp = {2025-07-28T07:11:44Z}
}

@incollection{molnar2020interpretablea,
  title = {Interpretable {{Machine Learning}} -- {{A Brief History}}, {{State-of-the-Art}} and {{Challenges}}},
  author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
  year = {2020},
  volume = {1323},
  eprint = {2010.09337},
  primaryclass = {stat},
  pages = {417--431},
  doi = {10.1007/978-3-030-65965-3_28},
  urldate = {2025-08-29},
  abstract = {We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods, and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-08-29T08:15:50Z}
}

@article{moncada-torres2021explainable,
  title = {Explainable Machine Learning Can Outperform {{Cox}} Regression Predictions and Provide Insights in Breast Cancer Survival},
  author = {{Moncada-Torres}, Arturo and {van Maaren}, Marissa C. and Hendriks, Mathijs P. and Siesling, Sabine and Geleijnse, Gijs},
  year = {2021},
  month = mar,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {6968},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-86327-7},
  urldate = {2025-05-16},
  abstract = {Cox Proportional Hazards (CPH) analysis is the standard for survival analysis in oncology. Recently, several machine learning (ML) techniques have been adapted for this task. Although they have shown to yield results at least as good as classical methods, they are often disregarded because of their lack of transparency and little to no explainability, which are key for their adoption in clinical settings. In this paper, we used data from the Netherlands Cancer Registry of 36,658~non-metastatic breast cancer patients to compare the performance of CPH with ML techniques (Random Survival Forests, Survival Support Vector Machines, and Extreme Gradient Boosting [XGB]) in predicting survival using the \$\$c\$\$-index. We demonstrated that in our dataset, ML-based models can perform at least as good as the classical CPH regression (\$\$c\$\$-index~\$\${\textbackslash}sim {\textbackslash},0.63\$\$), and in the case of XGB even better (\$\$c\$\$-index~\$\${\textbackslash}sim 0.73\$\$). Furthermore, we used Shapley Additive Explanation (SHAP) values to explain the models' predictions. We concluded that the difference in performance can be attributed to XGB's ability to model nonlinearities and complex interactions. We also investigated the impact of specific features on the models' predictions as well as their corresponding insights. Lastly, we showed that explainable ML can generate explicit knowledge of how models make their predictions, which is crucial in increasing the trust and adoption of innovative ML techniques in oncology and healthcare overall.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Breast cancer,Computer science,Scientific data},
  annotation = {TLDR: It is shown that explainable ML can generate explicit knowledge of how models make their predictions, which is crucial in increasing the trust and adoption of innovative ML techniques in oncology and healthcare overall.},
  timestamp = {2025-05-16T07:01:43Z}
}

@article{monteiro2022explainable,
  title = {Explainable Deep Drug-Target Representations for Binding Affinity Prediction},
  author = {Monteiro, Nelson R. C. and Sim{\~o}es, Carlos J. V. and {\'A}vila, Henrique V. and Abbasi, Maryam and Oliveira, Jos{\'e} L. and Arrais, Joel P.},
  year = {2022},
  month = jun,
  journal = {BMC bioinformatics},
  volume = {23},
  number = {1},
  pages = {237},
  issn = {1471-2105},
  doi = {10.1186/s12859-022-04767-y},
  abstract = {BACKGROUND: Several computational advances have been achieved in the drug discovery field, promoting the identification of novel drug-target interactions and new leads. However, most of these methodologies have been overlooking the importance of providing explanations to the decision-making process of deep learning architectures. In this research study, we explore the reliability of convolutional neural networks (CNNs) at identifying relevant regions for binding, specifically binding sites and motifs, and the significance of the deep representations extracted by providing explanations to the model's decisions based on the identification of the input regions that contributed the most to the prediction. We make use of an end-to-end deep learning architecture to predict binding affinity, where CNNs are exploited in their capacity to automatically identify and extract discriminating deep representations from 1D sequential and structural data. RESULTS: The results demonstrate the effectiveness of the deep representations extracted from CNNs in the prediction of drug-target interactions. CNNs were found to identify and extract features from regions relevant for the interaction, where the weight associated with these spots was in the range of those with the highest positive influence given by the CNNs in the prediction. The end-to-end deep learning model achieved the highest performance both in the prediction of the binding affinity and on the ability to correctly distinguish the interaction strength rank order when compared to baseline approaches. CONCLUSIONS: This research study validates the potential applicability of an end-to-end deep learning architecture in the context of drug discovery beyond the confined space of proteins and ligands with determined 3D structure. Furthermore, it shows the reliability of the deep representations extracted from the CNNs by providing explainability to the decision-making process.},
  langid = {english},
  pmcid = {PMC9204982},
  pmid = {35715734},
  keywords = {Binding affinity,Binding Sites,Convolutional neural network,Drug-target interaction,Explainable deep learning,Neural Networks Computer,Plant Extracts,Proteins,Reproducibility of Results},
  annotation = {TLDR: This research study validates the potential applicability of an end-to-end deep learning architecture in the context of drug discovery beyond the confined space of proteins and ligands with determined 3D structure and shows the reliability of the deep representations extracted from the CNNs by providing explainability to the decision-making process.},
  timestamp = {2025-07-25T12:52:50Z}
}

@inproceedings{moody1996database,
  title = {A Database to Support Development and Evaluation of Intelligent Intensive Care Monitoring},
  booktitle = {Computers in {{Cardiology}} 1996},
  author = {Moody, G.B. and Mark, R.G.},
  year = {1996},
  month = sep,
  pages = {657--660},
  issn = {0276-6547},
  doi = {10.1109/CIC.1996.542622},
  urldate = {2025-04-09},
  abstract = {Development and evaluation of automated decision support systems requires large amounts of well-characterized, reproducible test data. The MIMIC (Multi-parameter Intelligent Monitoring for Intensive Care) Database is intended to meet these needs. The database, currently nearing completion, will include 100 patient records, each typically containing between 24 and 48 hours of continuous data recorded from patient monitors in the medical, surgical, and cardiac intensive care units of Boston's Beth Israel Hospital. Each record will be accompanied by detailed clinical data derived from the patient's medical record and from the hospital's on-line medical information systems. We select patients to record from those likely to be hemodynamically unstable during the planned recording period. We expect to complete the selection of the recordings to be included in the database by the end of 1996, and to make the database available to other researchers shortly thereafter.},
  keywords = {Algorithm design and analysis,Automatic testing,Biomedical monitoring,Cardiology,Decision support systems,Deductive databases,Hospitals,Medical diagnostic imaging,Patient monitoring,System testing},
  annotation = {TLDR: The MIMIC (Multi-parameter Intelligent Monitoring for Intensive Care) Database is intended to meet the needs of automated decision support systems and to make the database available to other researchers shortly thereafter.},
  timestamp = {2025-04-09T02:45:04Z}
}

@article{moraffah2021causal,
  title = {Causal Inference for Time Series Analysis: Problems, Methods and Evaluation},
  shorttitle = {Causal Inference for Time Series Analysis},
  author = {Moraffah, Raha and Sheth, Paras and Karami, Mansooreh and Bhattacharya, Anchit and Wang, Qianru and Tahir, Anique and Raglin, Adrienne and Liu, Huan},
  year = {2021},
  month = dec,
  journal = {Knowledge and Information Systems},
  volume = {63},
  number = {12},
  pages = {3041--3085},
  issn = {0219-3116},
  doi = {10.1007/s10115-021-01621-0},
  urldate = {2025-05-04},
  abstract = {Time series data are a collection of chronological observations which are generated by several domains such as medical and financial fields. Over the years, different tasks such as classification, forecasting and clustering have been proposed to analyze this type of data. Time series data have been also used to study the effect of interventions overtime. Moreover, in many fields of science, learning the causal structure of dynamic systems and time series data is considered an interesting task which plays an important role in scientific discoveries. Estimating the effect of an intervention and identifying the causal relations from the data can be performed via causal inference. Existing surveys on time series discuss traditional tasks such as classification and forecasting or explain the details of the approaches proposed to solve a specific task. In this paper, we focus on two causal inference tasks, i.e., treatment effect estimation and causal discovery for time series data and provide a comprehensive review of the approaches in each task. Furthermore, we curate a list of commonly used evaluation metrics and datasets for each task and provide an in-depth insight. These metrics and datasets can serve as benchmark for research in the field.},
  langid = {english},
  keywords = {Causal benchmarking,Causal discovery,Causal effect estimation,Causal evaluation,Causal inference,Granger causality,Structural causal models,Time series},
  annotation = {TLDR: This paper focuses on two causal inference tasks, i.e., treatment effect estimation and causal discovery for time series data and provides a comprehensive review of the approaches in each task and curates a list of commonly used evaluation metrics and datasets for each task.},
  timestamp = {2025-05-04T09:30:41Z}
}

@inproceedings{mothilal2020explaining,
  title = {Explaining {{Machine Learning Classifiers}} through {{Diverse Counterfactual Explanations}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
  year = {2020},
  month = jan,
  eprint = {1905.07697},
  primaryclass = {cs},
  pages = {607--617},
  doi = {10.1145/3351095.3372850},
  urldate = {2025-04-13},
  abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {TLDR: This work proposes a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes, and provides metrics that enable comparison ofcounterfactual-based methods to other local explanation methods.},
  timestamp = {2025-04-13T07:14:37Z}
}

@article{mu2024deepenhancerppo,
  title = {{{DeepEnhancerPPO}}: {{An Interpretable Deep Learning Approach}} for {{Enhancer Classification}}},
  shorttitle = {{{DeepEnhancerPPO}}},
  author = {Mu, Xuechen and Huang, Zhenyu and Chen, Qiufen and Shi, Bocheng and Xu, Long and Xu, Ying and Zhang, Kai},
  year = {2024},
  month = dec,
  journal = {International Journal of Molecular Sciences},
  volume = {25},
  number = {23},
  pages = {12942},
  publisher = {MDPI AG},
  issn = {1422-0067},
  doi = {10.3390/ijms252312942},
  urldate = {2025-07-24},
  abstract = {Enhancers are short genomic segments located in non-coding regions of the genome that play a critical role in regulating the expression of target genes. Despite their importance in transcriptional regulation, effective methods for classifying enhancer categories and regulatory strengths remain limited. To address this challenge, we propose a novel end-to-end deep learning architecture named DeepEnhancerPPO. The model integrates ResNet and Transformer modules to extract local, hierarchical, and long-range contextual features. Following feature fusion, we employ Proximal Policy Optimization (PPO), a reinforcement learning technique, to reduce the dimensionality of the fused features, retaining the most relevant features for downstream classification tasks. We evaluate the performance of DeepEnhancerPPO from multiple perspectives, including ablation analysis, independent tests, assessment of PPO's contribution to performance enhancement, and interpretability of the classification results. Each module positively contributes to the overall performance, with ResNet and PPO being the most significant contributors. Overall, DeepEnhancerPPO demonstrates superior performance on independent datasets compared to other models, outperforming the second-best model by 6.7\% in accuracy for enhancer category classification. The model consistently ranks among the top five classifiers out of 25 for enhancer strength classification without requiring re-optimization of the hyperparameters and ranks as the second-best when the hyperparameters are refined. This indicates that the DeepEnhancerPPO framework is highly robust for enhancer classification. Additionally, the incorporation of PPO enhances the interpretability of the classification results.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: The proposed DeepEnhancerPPO framework consistently ranks among the top five classifiers out of 25 for enhancer strength classification without requiring re-optimization of the hyperparameters and ranks as the second-best when the hyperparameters are refined.},
  timestamp = {2025-07-24T12:47:56Z}
}

@article{muehlematter2023fdacleared,
  title = {{{FDA-cleared}} Artificial Intelligence and Machine Learning-Based Medical Devices and Their 510(k) Predicate Networks},
  author = {Muehlematter, Urs J. and Bluethgen, Christian and Vokinger, Kerstin N.},
  year = {2023},
  month = sep,
  journal = {The Lancet Digital Health},
  volume = {5},
  number = {9},
  pages = {e618-e626},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(23)00126-7},
  urldate = {2025-04-12},
  langid = {english},
  pmid = {37625896},
  annotation = {TLDR: To improve patient care, a stronger focus should be placed on the distinctive characteristics of AI/ML when defining substantial equivalence between a new AI/ ML-based medical device and predicate devices.},
  timestamp = {2025-04-12T03:48:38Z}
}

@article{muhammad2024unveiling,
  ids = {2024unveiling},
  title = {Unveiling the Black Box: A Systematic Review of {{Explainable Artificial Intelligence}} in Medical Image Analysis},
  author = {Muhammad, Dost and Bendechache, Malika},
  year = {2024},
  journal = {Computational and structural biotechnology journal},
  publisher = {Elsevier},
  timestamp = {2025-05-19T01:53:50Z}
}

@misc{multimodal,
  title = {Multimodal Artificial Intelligence Models for Radiology {\textbar} {{BJR}}{\textbar}{{Artificial Intelligence}} {\textbar} {{Oxford Academic}}},
  urldate = {2025-04-12},
  howpublished = {https://academic.oup.com/bjrai/article/2/1/ubae017/7959794},
  timestamp = {2025-04-12T07:25:07Z}
}

@misc{multiomics,
  title = {Multi-Omics Disease Module Detection with an Explainable {{Greedy Decision Forest}} {\textbar} {{Scientific Reports}}},
  urldate = {2025-05-16},
  howpublished = {https://www.nature.com/articles/s41598-022-21417-8},
  timestamp = {2025-05-16T08:39:39Z}
}

@article{muralidharan2024scoping,
  title = {A Scoping Review of Reporting Gaps in {{FDA-approved AI}} Medical Devices},
  author = {Muralidharan, Vijaytha and Adewale, Boluwatife Adeleye and Huang, Caroline J. and Nta, Mfon Thelma and Ademiju, Peter Oluwaduyilemi and Pathmarajah, Pirunthan and Hang, Man Kien and Adesanya, Oluwafolajimi and Abdullateef, Ridwanullah Olamide and Babatunde, Abdulhammed Opeyemi and Ajibade, Abdulquddus and Onyeka, Sonia and Cai, Zhou Ran and Daneshjou, Roxana and Olatunji, Tobi},
  year = {2024},
  month = oct,
  journal = {npj Digital Medicine},
  volume = {7},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-024-01270-x},
  urldate = {2025-04-04},
  abstract = {Machine learning and artificial intelligence (AI/ML) models in healthcare may exacerbate health biases. Regulatory oversight is critical in evaluating the safety and effectiveness of AI/ML devices in clinical settings. We conducted a scoping review on the 692 FDA-approved AI/ML-enabled medical devices approved from 1995-2023 to examine transparency, safety reporting, and sociodemographic representation. Only 3.6\% of approvals reported race/ethnicity, 99.1\% provided no socioeconomic data. 81.6\% did not report the age of study subjects. Only 46.1\% provided comprehensive detailed results of performance studies; only 1.9\% included a link to a scientific publication with safety and efficacy data. Only 9.0\% contained a prospective study for post-market surveillance. Despite the growing number of market-approved medical devices, our data shows that FDA reporting data remains inconsistent. Demographic and socioeconomic characteristics are underreported, exacerbating the risk of algorithmic bias and health disparity.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Diagnostic markers,Health policy,Health services,Medical imaging},
  annotation = {TLDR: Despite the growing number of market-approved medical devices, the data shows that FDA reporting data remains inconsistent, and Demographic and socioeconomic characteristics are underreported, exacerbating the risk of algorithmic bias and health disparity.},
  timestamp = {2025-04-04T02:45:58Z}
}

@article{murdoch2019definitions,
  ids = {murdoch2019interpretable},
  title = {Definitions, Methods, and Applications in Interpretable Machine Learning},
  shorttitle = {Interpretable Machine Learning},
  author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and {Abbasi-Asl}, Reza and Yu, Bin},
  year = {2019},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {44},
  pages = {22071--22080},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1900654116},
  urldate = {2025-06-15},
  abstract = {Significance             The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work.           ,              Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  annotation = {TLDR: This work defines interpretability in the context of machine learning and introduces the predictive, descriptive, relevant (PDR) framework for discussing interpretations, and introduces 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy.},
  timestamp = {2025-06-15T15:07:31Z}
}

@book{murphy2002concepts,
  title = {The Big Book of Concepts},
  author = {Murphy, Gregory L.},
  year = {2002},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  isbn = {9780262632739},
  timestamp = {2025-09-06T03:45:14Z}
}

@article{nagy2024interpretable,
  title = {Interpretable {{Dropout Prediction}}: {{Towards XAI-Based Personalized Intervention}}},
  shorttitle = {Interpretable {{Dropout Prediction}}},
  author = {Nagy, Marcell and Molontay, Roland},
  year = {2024},
  month = jun,
  journal = {International Journal of Artificial Intelligence in Education},
  volume = {34},
  number = {2},
  pages = {274--300},
  issn = {1560-4306},
  doi = {10.1007/s40593-023-00331-8},
  urldate = {2025-02-02},
  abstract = {Student drop-out is one of the most burning issues in STEM higher education, which induces considerable social and economic costs. Using machine learning tools for the early identification of students at risk of dropping out has gained a lot of interest recently. However, there has been little discussion on dropout prediction using interpretable machine learning (IML) and explainable artificial intelligence (XAI) tools.In this work, using the data of a large public Hungarian university, we demonstrate how IML and XAI tools can support educational stakeholders in dropout prediction. We show that complex machine learning models -- such as the CatBoost classifier -- can efficiently identify at-risk students relying solely on pre-enrollment achievement measures, however, they lack interpretability. Applying IML tools, such as permutation importance (PI), partial dependence plot (PDP), LIME, and SHAP values, we demonstrate how the predictions can be explained both globally and locally. Explaining individual predictions opens up great opportunities for personalized intervention, for example by offering the right remedial courses or tutoring sessions. Finally, we present the results of a user study that evaluates whether higher education stakeholders find these tools interpretable and useful.},
  langid = {english},
  keywords = {Artificial Intelligence,Digital Education and Educational Technology,Dropout prediction,Educational data science,Explainable AI,Higher education,Interpretable machine learning},
  annotation = {TLDR: This work demonstrates how IML and XAI tools can support educational stakeholders in dropout prediction and presents the results of a user study that evaluates whether higher education stakeholders find these tools interpretable and useful.},
  timestamp = {2025-02-26T14:48:57Z}
}

@article{naik2024current,
  title = {Current {{Status}} and {{Future Directions}}: {{The Application}} of {{Artificial Intelligence}}/{{Machine Learning}} for {{Precision Medicine}}},
  shorttitle = {Current {{Status}} and {{Future Directions}}},
  author = {Naik, Kunal and Goyal, Rahul K. and Foschini, Luca and Chak, Choi Wai and Thielscher, Christian and Zhu, Hao and Lu, James and Leh{\'a}r, Joseph and Pacanoswki, Michael A. and Terranova, Nadia and Mehta, Neha and Korsbo, Niklas and Fakhouri, Tala and Liu, Qi and Gobburu, Jogarao},
  year = {2024},
  month = apr,
  journal = {Clinical Pharmacology and Therapeutics},
  volume = {115},
  number = {4},
  pages = {673--686},
  issn = {1532-6535},
  doi = {10.1002/cpt.3152},
  abstract = {Technological innovations, such as artificial intelligence (AI) and machine learning (ML), have the potential to expedite the goal of precision medicine, especially when combined with increased capacity for voluminous data from multiple sources and expanded therapeutic modalities; however, they also present several challenges. In this communication, we first discuss the goals of precision medicine, and contextualize the use of AI in precision medicine by showcasing innovative applications (e.g., prediction of tumor growth and overall survival, biomarker identification using biomedical images, and identification of patient population for clinical practice) which were presented during the February 2023 virtual public workshop entitled "Application of Artificial Intelligence and Machine Learning for Precision Medicine," hosted by the US Food and Drug Administration (FDA) and University of Maryland Center of Excellence in Regulatory Science and Innovation (M-CERSI). Next, we put forward challenges brought about by the multidisciplinary nature of AI, particularly highlighting the need for AI to be trustworthy. To address such challenges, we subsequently note practical approaches, viz., differential privacy, synthetic data generation, and federated learning. The proposed strategies - some of which are highlighted presentations from the workshop - are for the protection of personal information and intellectual property. In addition, methods such as the risk-based management approach and the need for an agile regulatory ecosystem are discussed. Finally, we lay out a call for action that includes sharing of data and algorithms, development of regulatory guidance documents, and pooling of expertise from a broad-spectrum of stakeholders to enhance the application of AI in precision medicine.},
  langid = {english},
  pmid = {38103204},
  keywords = {Algorithms,Artificial Intelligence,Humans,Machine Learning,Precision Medicine},
  annotation = {TLDR: The goals of precision medicine are discussed, and the use of AI in precision medicine is contextualized by showcasing innovative applications, and challenges brought about by the multidisciplinary nature of AI are put forward.},
  timestamp = {2025-05-13T11:20:40Z}
}

@article{naithani2021precision,
  title = {Precision Medicine: {{Concept}} and Tools},
  shorttitle = {Precision Medicine},
  author = {Naithani, Nardeep and Sinha, Sharmila and Misra, Pratibha and Vasudevan, Biju and Sahu, Rajesh},
  year = {2021},
  month = jul,
  journal = {Medical Journal, Armed Forces India},
  volume = {77},
  number = {3},
  pages = {249--257},
  issn = {0377-1237},
  doi = {10.1016/j.mjafi.2021.06.021},
  abstract = {Precision medicine is the new age medicine and refers to tailoring treatments to a subpopulation who have a common susceptibility to a particular disease or similar response to a particular drug. Although the concept existed even during the times of Sir William Osler, it was given a shot in the arm with the Precision Medicine Initiative launched by Barack Obama in 2015. The main tools of precision medicine are Big data, artificial intelligence, the various omics, pharmaco-omics, environmental and social factors~and the integration of these with preventive and population medicine. Big data can be acquired from electronic health records of patients and includes various biomarkers (clinical and omics based), laboratory and radiological investigations and these can be analysed through machine learning by various complex flowcharts setting up an algorithm for the management of specific subpopulations. So, there is a move away from the traditional "one size fits all" treatment to precision-based medicine. Research in "omics" has increased in leaps and bounds and advancements have included the fields of genomics, epigenomics, proteomics, transcriptomics, metabolomics~and microbiomics. Pharmaco-omics has also come to the forefront with development of new drugs and suiting a particular drug to a particular subpopulation, thus avoiding their prescription to non-responders, preventing unwanted adverse effects and proving economical in the long run. Environmental, social~and behavioural factors are as important or in fact more important than genetic factors in most complex diseases~and managing these factors form~an important part of precision medicine. Finally integrating precision with preventive and public health makes "precision medicine" a complete final product which will change the way medicine will be practised in future.},
  langid = {english},
  pmcid = {PMC8282508},
  pmid = {34305276},
  keywords = {Big data,Epigenetics,Omics,Precision medicine,Preventive medicine},
  annotation = {TLDR: Integrating precision with preventive and public health makes "precision medicine" a complete final product which will change the way medicine will be practised in future.},
  timestamp = {2025-05-27T14:29:01Z}
}

@inproceedings{najjar2023feature,
  title = {Feature {{Attribution Methods}} for {{Multivariate Time-Series Explainability}} in {{Remote Sensing}}},
  booktitle = {{{IGARSS}} 2023 - 2023 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  author = {Najjar, Hiba and Helber, Patrick and Bischke, Benjamin and Habelitz, Peter and Sanchez, Cristhian and Mena, Francisco and Miranda, Miro and Pathak, Deepak and Siddamsetty, Jayanth and Arenas, Diego and Vollmer, Michaela and Charfuelan, Marcela and Nuske, Marlon and Dengel, Andreas},
  year = {2023},
  month = jul,
  pages = {5014--5017},
  issn = {2153-7003},
  doi = {10.1109/IGARSS52108.2023.10282120},
  urldate = {2025-04-09},
  abstract = {Numerous remote sensing applications rely on temporal satellite data, and Deep learning models are increasingly being used for such tasks. Nevertheless, these models operate as black boxes, lacking transparency and understandability. We address this gap by using explainable AI on an agricultural task. Specifically, we trained a recurrent neural network on individual pixels from multispectral time-series of Sentinel-2 satellite images to predict crop yield. We then applied nine feature attribution methods on a sample of the dataset and computed the spectral and temporal contributions to the final individual predictions. The aggregated results were evaluated qualitatively and quantitatively. Results suggest that LIME and Shapley sampling value methods performed best on the quantitative scores, followed by GradientShap. Most backpropagation-based techniques had highly inconsistent scores across the explained data points. Finally, to guide remote sensing practitioners in using Explainable AI on similar datasets, we further discuss some selection criteria to be considered.},
  langid = {american},
  keywords = {crop yield,Data models,Deep learning,explainable AI,feature attribution,multivariate time-series,pixel-wise prediction,Recurrent neural networks,remote sensing,Satellite images,Satellites,Sensitivity,Sensors,Sentinel-2},
  annotation = {TLDR: This work trained a recurrent neural network on individual pixels from multispectral time-series of Sentinel-2 satellite images to predict crop yield, and applied nine feature attribution methods on a sample of the dataset and computed the spectral and temporal contributions to the final individual predictions.},
  timestamp = {2025-04-09T03:23:13Z}
}

@misc{narendra2018explaining,
  title = {Explaining {{Deep Learning Models}} Using {{Causal Inference}}},
  author = {Narendra, Tanmayee and Sankaran, Anush and Vijaykeerthy, Deepak and Mani, Senthil},
  year = {2018},
  month = nov,
  number = {arXiv:1811.04376},
  eprint = {1811.04376},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.04376},
  urldate = {2025-03-20},
  abstract = {Although deep learning models have been successfully applied to a variety of tasks, due to the millions of parameters, they are becoming increasingly opaque and complex. In order to establish trust for their widespread commercial use, it is important to formalize a principled framework to reason over these models. In this work, we use ideas from causal inference to describe a general framework to reason over CNN models. Specifically, we build a Structural Causal Model (SCM) as an abstraction over a specific aspect of the CNN. We also formulate a method to quantitatively rank the filters of a convolution layer according to their counterfactual importance. We illustrate our approach with popular CNN architectures such as LeNet5, VGG19, and ResNet32.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-20T21:54:45Z}
}

@article{national2011toward,
  title = {Toward Precision Medicine: Building a Knowledge Network for Biomedical Research and a New Taxonomy of Disease},
  author = {Council, National Research and {on Earth}, Division and Studies, Life and {on Life Sciences}, Board and {on A Framework for Developing a New Taxonomy of Disease}, Committee},
  year = {2011},
  publisher = {National Academies Press},
  timestamp = {2025-05-13T13:07:37Z}
}

@misc{nazi2024large,
  title = {Large Language Models in Healthcare and Medical Domain: {{A}} Review. {{Informatics}}, 11, 57},
  author = {Nazi, {\relax ZA} and Peng, W},
  year = {2024},
  timestamp = {2025-04-16T01:54:50Z}
}

@inproceedings{NEURIPS2020_ecb287ff,
  title = {On Completeness-Aware Concept-Based Explanations in Deep Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yeh, Chih-Kuan and Kim, Been and Arik, Sercan and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {20554--20565},
  publisher = {Curran Associates, Inc.},
  timestamp = {2025-05-18T11:52:39Z}
}

@inproceedings{NEURIPS2022_63d3bae2,
  title = {Weakly Supervised Representation Learning with Sparse Perturbations},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ahuja, Kartik and Hartford, Jason S and Bengio, Yoshua},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {15516--15528},
  publisher = {Curran Associates, Inc.},
  timestamp = {2025-05-17T09:13:45Z}
}

@inproceedings{NEURIPS2022_7a8fa138,
  title = {Large-Scale Differentiable Causal Discovery of Factor Graphs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Lopez, Romain and Huetter, Jan-Christian and Pritchard, Jonathan and Regev, Aviv},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {19290--19303},
  publisher = {Curran Associates, Inc.},
  timestamp = {2025-05-17T06:56:01Z}
}

@inproceedings{NEURIPS2022_aa933b5a,
  title = {Predicting Cellular Responses to Novel Drug Perturbations at a Single-Cell Resolution},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hetzel, Leon and Boehm, Simon and Kilbertus, Niki and G{\"u}nnemann, Stephan and {lotfollahi}, mohammad and Theis, Fabian},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {26711--26722},
  publisher = {Curran Associates, Inc.},
  timestamp = {2025-05-17T07:17:42Z}
}

@inproceedings{NEURIPS2024_765871e7,
  title = {{{HEALNet}}: {{Multimodal}} Fusion for Heterogeneous Biomedical Data},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hemker, Konstantin and Simidjievski, Nikola and Jamnik, Mateja},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  year = {2024},
  volume = {37},
  pages = {64479--64498},
  publisher = {Curran Associates, Inc.},
  timestamp = {2025-06-13T07:03:06Z}
}

@article{newsham2024early,
  ids = {newsham2024earlya},
  title = {Early Detection and Diagnosis of Cancer with Interpretable Machine Learning to Uncover Cancer-Specific {{DNA}} Methylation Patterns},
  author = {Newsham, Izzy and Sendera, Marcin and Jammula, Sri Ganesh and Samarajiwa, Shamith A.},
  year = {2024},
  journal = {Biology Methods \& Protocols},
  volume = {9},
  number = {1},
  pages = {bpae028},
  issn = {2396-8923},
  doi = {10.1093/biomethods/bpae028},
  urldate = {2025-07-23},
  abstract = {Cancer, a collection of more than two hundred different diseases, remains a leading cause of morbidity and mortality worldwide. Usually detected at the advanced stages of disease, metastatic cancer accounts for 90\% of cancer-associated deaths. Therefore, the early detection of cancer, combined with current therapies, would have a significant impact on survival and treatment of various cancer types. Epigenetic changes such as DNA methylation are some of the early events underlying carcinogenesis. Here, we report on an interpretable machine learning model that can classify 13 cancer types as well as non-cancer tissue samples using only DNA methylome data, with 98.2\% accuracy. We utilize the features identified by this model to develop EMethylNET, a robust model consisting of an XGBoost model that provides information to a deep neural network that can generalize to independent data sets. We also demonstrate that the methylation-associated genomic loci detected by the classifier are associated with genes, pathways and networks involved in cancer, providing insights into the epigenomic regulation of carcinogenesis.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  pmcid = {PMC11186673},
  pmid = {38903861},
  annotation = {TLDR: An interpretable machine learning model is reported that can classify 13 cancer types as well as non-cancer tissue samples using only DNA methylome data, with 98.2\% accuracy, and it is demonstrated that the methylation-associated genomic loci detected by the classifier are associated with genes, pathways and networks involved in cancer, providing insights into the epigenomic regulation of carcinogenesis.},
  timestamp = {2025-07-25T07:35:21Z}
}

@article{nguyen2020counterfactual,
  title = {Counterfactual Clinical Prediction Models Could Help to Infer Individualized Treatment Effects in Randomized Controlled Trials-{{An}} Illustration with the {{International Stroke Trial}}},
  author = {Nguyen, Tri-Long and Collins, Gary S. and Landais, Paul and Le Manach, Yannick},
  year = {2020},
  month = sep,
  journal = {Journal of Clinical Epidemiology},
  volume = {125},
  pages = {47--56},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2020.05.022},
  abstract = {OBJECTIVE: Causal treatment effects are estimated at the population level in randomized controlled trials, while clinical decision is often to be made at the individual level in practice. We aim to show how clinical prediction models used under a counterfactual framework may help to infer individualized treatment effects. STUDY DESIGN AND SETTING: As an illustrative example, we reanalyze the International Stroke Trial. This large, multicenter trial enrolled 19,435 adult patients with suspected acute ischemic stroke from 36 countries, and reported a modest average benefit of aspirin (vs. no aspirin) on a composite outcome of death or dependency at 6~months. We derive and validate multivariable logistic regression models that predict the patient counterfactual risks of outcome with and without aspirin, conditionally on 23 predictors. RESULTS: The counterfactual prediction models display good performance in terms of calibration and discrimination (validation c-statistics: 0.798 and 0.794). Comparing the counterfactual predicted risks on an absolute difference scale, we show that aspirin-despite an average benefit-may increase the risk of death or dependency at 6~months (compared with the control) in a quarter of stroke patients. CONCLUSIONS: Counterfactual prediction models could help researchers and clinicians (i) infer individualized treatment effects and (ii) better target patients who may benefit from treatments.},
  langid = {english},
  pmid = {32464321},
  keywords = {Aged,Aged 80 and over,Aspirin,Causal inference,Clinical Decision Rules,Clinical prediction models,Counterfactual framework,Female,Heparin,Heterogeneity of treatment effect,Humans,Ischemic Stroke,Logistic Models,Male,Models Theoretical,Multicenter Studies as Topic,Precision Medicine,Randomized controlled trial,Randomized Controlled Trials as Topic},
  annotation = {TLDR: It is shown that Aspirin - despite an average benefit - may increase the risk of death or dependency at 6 months (compared to the control) in a quarter of stroke patients.},
  timestamp = {2025-05-21T01:33:13Z}
}

@article{nicholson2020constructing,
  title = {Constructing Knowledge Graphs and Their Biomedical Applications},
  author = {Nicholson, David N. and Greene, Casey S.},
  year = {2020},
  journal = {Computational and Structural Biotechnology Journal},
  volume = {18},
  pages = {1414--1428},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2020.05.017},
  abstract = {Knowledge graphs can support many biomedical applications. These graphs represent biomedical concepts and relationships in the form of nodes and edges. In this review, we discuss how these graphs are constructed and applied with a particular focus on how machine learning approaches are changing these processes. Biomedical knowledge graphs have often been constructed by integrating databases that were populated by experts via manual curation, but we are now seeing a more robust use of automated systems. A number of techniques are used to represent knowledge graphs, but often machine learning methods are used to construct a low-dimensional representation that can support many different applications. This representation is designed to preserve a knowledge graph's local and/or global structure. Additional machine learning methods can be applied to this representation to make predictions within genomic, pharmaceutical, and clinical domains. We frame our discussion first around knowledge graph construction and then around unifying representational learning techniques and unifying applications. Advances in machine learning for biomedicine are creating new opportunities across many domains, and we note potential avenues for future work with knowledge graphs that appear particularly promising.},
  langid = {english},
  pmcid = {PMC7327409},
  pmid = {32637040},
  keywords = {knowledge graphs,Lterature review,Machine learning,Natural language processing,Network embeddings,Text mining},
  annotation = {TLDR: This review discusses how knowledge graphs are constructed and applied with a particular focus on how machine learning approaches are changing these processes, and notes potential avenues for future work with knowledge graphs that appear particularly promising.},
  timestamp = {2025-04-17T15:16:40Z}
}

@misc{nie2020quasioracle,
  title = {Quasi-{{Oracle Estimation}} of {{Heterogeneous Treatment Effects}}},
  author = {Nie, Xinkun and Wager, Stefan},
  year = {2020},
  month = aug,
  number = {arXiv:1712.04912},
  eprint = {1712.04912},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.04912},
  urldate = {2025-03-20},
  abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: In both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},
  timestamp = {2025-03-20T08:14:03Z}
}

@article{nikolaev2009retracted,
  title = {{{RETRACTED ARTICLE}}: {{APP}} Binds {{DR6}} to Trigger Axon Pruning and Neuron Death via Distinct Caspases},
  author = {Nikolaev, Anatoly and McLaughlin, Todd and O'Leary, Dennis DM and {Tessier-Lavigne}, Marc},
  year = {2009},
  journal = {Nature},
  volume = {457},
  number = {7232},
  pages = {981--989},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2025-04-16T06:39:55Z}
}

@article{nilforoshan2023zero,
  title = {Zero-Shot Causal Learning},
  author = {Nilforoshan, Hamed and Moor, Michael and Roohani, Yusuf and Chen, Yining and {\v S}urina, Anja and Yasunaga, Michihiro and Oblak, Sara and Leskovec, Jure},
  year = {2023},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {6862--6901},
  timestamp = {2025-03-23T11:17:40Z}
}

@article{noor2025unveiling,
  title = {Unveiling {{Explainable AI}} in {{Healthcare}}: {{Current Trends}}, {{Challenges}}, and {{Future Directions}}},
  shorttitle = {Unveiling {{Explainable}}},
  author = {Noor, Abdul Aziz and Manzoor, Awais and Mazhar Qureshi, Muhammad Deedahwar and Qureshi, M. Atif and Rashwan, Wael},
  year = {2025},
  month = jun,
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {15},
  number = {2},
  pages = {e70018},
  issn = {1942-4787, 1942-4795},
  doi = {10.1002/widm.70018},
  urldate = {2025-08-11},
  abstract = {ABSTRACT             This overview investigates the evolution and current landscape of eXplainable Artificial Intelligence (XAI) in healthcare, highlighting its implications for researchers, technology developers, and policymakers. Following the PRISMA protocol, we analyzed 89 publications from January 2000 to June 2024, spanning 19 medical domains, with a focus on Neurology and Cancer as the most studied areas. Various data types are reviewed, including tabular data, medical imaging, and clinical text, offering a comprehensive perspective on XAI applications. Key findings identify significant gaps, such as the limited availability of public datasets, suboptimal data preprocessing techniques, insufficient feature selection and engineering, and the limited utilization of multiple XAI methods. Additionally, the lack of standardized XAI evaluation metrics and practical obstacles in integrating XAI systems into clinical workflows are emphasized. We provide actionable recommendations, including the design of explainability-centric models, the application of diverse and multiple XAI methods, and the fostering of interdisciplinary collaboration. These strategies aim to guide researchers in building robust AI models, assist technology developers in creating intuitive and user-friendly AI tools, and inform policymakers in establishing effective regulations. Addressing these gaps will promote the development of transparent, reliable, and user-centred AI systems in healthcare, ultimately improving decision-making and patient outcomes.},
  langid = {english},
  annotation = {TLDR: This overview investigates the evolution and current landscape of eXplainable Artificial Intelligence (XAI) in healthcare, highlighting its implications for researchers, technology developers, and policymakers and provides actionable recommendations, including the design of explainability-centric models, the application of diverse and multiple XAI methods, and the fostering of interdisciplinary collaboration.},
  timestamp = {2025-08-13T09:18:40Z}
}

@incollection{Notovich2023,
  title = {Explainable Artificial Intelligence ({{XAI}}): {{Motivation}}, Terminology, and Taxonomy},
  booktitle = {Machine Learning for Data Science Handbook: {{Data}} Mining and Knowledge Discovery Handbook},
  author = {Notovich, Aviv and {Chalutz-Ben Gal}, Hila and {Ben-Gal}, Irad},
  editor = {Rokach, Lior and Maimon, Oded and Shmueli, Erez},
  year = {2023},
  pages = {971--985},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-24628-9_41},
  abstract = {Deep learning algorithms and deep neural networks (DNNs) have become extremely popular due to their high-performance accuracy in complex fields, such as image and text classification, speech understanding, document segmentation, credit scoring, and facial recognition. As a result of the highly nonlinear structure of deep learning algorithms, these networks are hard to interpret; thus, it is not clear how the models reach their conclusions and therefore, they are often considered black-box models. The poor transparency of these models is a major drawback despite their effectiveness. In addition, recent regulations such as the General Data Protection Regulation (GDPR), require that, in many cases, an explanation will be provided whenever the learning model may affect a person's life. For example, in autonomous vehicle applications, methods for visualizing, explaining, and interpreting deep learning models that analyze driver behavior and the road environment have become standard. Explainable artificial intelligence (XAI) or interpretable machine learning (IML) programs aim to enable a suite of methods and techniques that produce more explainable models while maintaining a high level of output accuracy [1--4]. These programs enable human users to better understand, trust, and manage the emerging generation of artificially intelligent systems [4].},
  isbn = {978-3-031-24628-9},
  timestamp = {2025-06-10T03:52:01Z}
}

@article{novakovsky2023obtaining,
  title = {Obtaining Genetics Insights from Deep Learning via Explainable Artificial Intelligence},
  author = {Novakovsky, Gherman and Dexter, Nick and Libbrecht, Maxwell W. and Wasserman, Wyeth W. and Mostafavi, Sara},
  year = {2023},
  month = feb,
  journal = {Nature Reviews. Genetics},
  volume = {24},
  number = {2},
  pages = {125--137},
  issn = {1471-0064},
  doi = {10.1038/s41576-022-00532-2},
  urldate = {2025-05-02},
  abstract = {Artificial intelligence (AI) models based on deep learning now represent the state of the art for making functional predictions in genomics research. However, the underlying basis on which predictive models make such predictions is often unknown. For genomics researchers, this missing explanatory information would frequently be of greater value than the predictions themselves, as it can enable new insights into genetic processes. We review progress in the emerging area of explainable AI (xAI), a field with the potential to empower life science researchers to gain mechanistic insights into complex deep learning models. We discuss and categorize approaches for model interpretation, including an intuitive understanding of how each approach works and their underlying assumptions and limitations in the context of typical high-throughput biological datasets.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  pmid = {36192604},
  keywords = {Artificial Intelligence,Deep Learning,Gene regulation,Genomics,Machine learning,Statistical methods},
  annotation = {TLDR: Advances in deep learning approaches in genomics are described, whereby researchers are moving beyond the typical `black box' nature of models to obtain biological insights through explainable artificial intelligence (xAI).},
  timestamp = {2025-07-05T00:50:11Z}
}

@article{noviandy2024explainable,
  title = {Explainable {{Artificial Intelligence}} in {{Medical Imaging}}: {{A Case Study}} on {{Enhancing Lung Cancer Detection}} through {{CT Images}}},
  shorttitle = {Explainable {{Artificial Intelligence}} in {{Medical Imaging}}},
  author = {Noviandy, Teuku Rizky and Maulana, Aga and Zulfikar, Teuku and Rusyana, Asep and Enitan, Seyi Samson and Idroes, Rinaldi},
  year = {2024},
  month = may,
  journal = {Indonesian Journal of Case Reports},
  volume = {2},
  number = {1},
  pages = {6--14},
  issn = {3025-3578},
  doi = {10.60084/ijcr.v2i1.150},
  urldate = {2025-05-04},
  abstract = {This study tackles the pressing challenge of lung cancer detection, the foremost cause of cancer-related mortality worldwide, hindered by late detection and diagnostic limitations. Aiming to improve early detection rates and diagnostic reliability, we propose an approach integrating Deep Convolutional Neural Networks (DCNN) with Explainable Artificial Intelligence (XAI) techniques, specifically focusing on the Residual Network (ResNet) architecture and Gradient-weighted Class Activation Mapping (Grad-CAM). Utilizing a dataset of 1,000 CT scans, categorized into normal, non-cancerous, and three types of lung cancer images, we adapted the ResNet50 model through transfer learning and fine-tuning for enhanced specificity in lung cancer subtype detection. Our methodology demonstrated the modified ResNet50 model's effectiveness, significantly outperforming the original architecture in accuracy (91.11\%), precision (91.66\%), sensitivity (91.11\%), specificity (96.63\%), and F1-score (91.10\%). The inclusion of Grad-CAM provided insightful visual explanations for the model's predictions, fostering transparency and trust in computer-assisted diagnostics. The study highlights the potential of combining DCNN with XAI to advance lung cancer detection, suggesting future research should expand dataset diversity and explore multimodal data integration for broader applicability and improved diagnostic capabilities.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0},
  annotation = {TLDR: The study highlights the potential of combining DCNN with XAI to advance lung cancer detection, suggesting future research should expand dataset diversity and explore multimodal data integration for broader applicability and improved diagnostic capabilities.},
  timestamp = {2025-05-04T02:54:48Z}
}

@article{null2019all,
  title = {The ``{{All}} of {{Us}}'' {{Research Program}}},
  author = {{null}, null},
  year = {2019},
  month = aug,
  journal = {New England Journal of Medicine},
  volume = {381},
  number = {7},
  pages = {668--676},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMsr1809937},
  urldate = {2025-05-28},
  abstract = {Knowledge gained from observational cohort studies has dramatically advanced the prevention and treatment of diseases. Many of these cohorts, however, are small, lack diversity, or do not provide comprehensive phenotype data. The All of Us Research Program plans to enroll a diverse group of at least 1 million persons in the United States in order to accelerate biomedical research and improve health. The program aims to make the research results accessible to participants, and it is developing new approaches to generate, access, and make data broadly available to approved researchers. All of Us opened for enrollment in May 2018 and currently enrolls participants 18 years of age or older from a network of more than 340 recruitment sites. Elements of the program protocol include health questionnaires, electronic health records (EHRs), physical measurements, the use of digital health technology, and the collection and analysis of biospecimens. As of July 2019, more than 175,000 participants had contributed biospecimens. More than 80\% of these participants are from groups that have been historically underrepresented in biomedical research. EHR data on more than 112,000 participants from 34 sites have been collected. The All of Us data repository should permit researchers to take into account individual differences in lifestyle, socioeconomic factors, environment, and biologic characteristics in order to advance precision diagnosis, prevention, and treatment. The U.S. research cohort All of Us was announced in 2015 and launched last year. With more than one fifth of the target enrollment now completed, the investigators report on progress and challenges.},
  annotation = {TLDR: The All of Us data repository should permit researchers to take into account individual differences in lifestyle, socioeconomic factors, environment, and biologic characteristics in order to advance precision diagnosis, prevention, and treatment.},
  timestamp = {2025-05-28T00:13:15Z}
}

@misc{nvidia_clara_medical_devices,
  title = {{{NVIDIA}} Clara for Medical Devices - {{AI}} Computing Platform},
  author = {{NVIDIA}},
  timestamp = {2025-04-12T13:34:43Z}
}

@misc{nye2021improving,
  title = {Improving {{Coherence}} and {{Consistency}} in {{Neural Sequence Models}} with {{Dual-System}}, {{Neuro-Symbolic Reasoning}}},
  author = {Nye, Maxwell and Tessler, Michael Henry and Tenenbaum, Joshua B. and Lake, Brenden M.},
  year = {2021},
  month = dec,
  number = {arXiv:2107.02794},
  eprint = {2107.02794},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.02794},
  urldate = {2025-09-01},
  abstract = {Human reasoning can often be understood as an interplay between two systems: the intuitive and associative ("System 1") and the deliberative and logical ("System 2"). Neural sequence models -- which have been increasingly successful at performing complex, structured tasks -- exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent. In this work, we seek a lightweight, training-free means of improving existing System 1-like sequence models by adding System 2-inspired logical reasoning. We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  timestamp = {2025-09-01T12:19:36Z}
}

@book{ogden1923,
  title = {The Meaning of Meaning},
  author = {Ogden, C. K. and Richards, I. A.},
  year = {1923},
  publisher = {Harcourt, Brace \& World},
  timestamp = {2025-09-06T00:42:24Z}
}

@article{oh2024integrating,
  title = {Integrating Predictive Modeling and Causal Inference for Advancing Medical Science},
  author = {Oh, Tae Ryom},
  year = {2024},
  journal = {Childhood Kidney Diseases},
  volume = {28},
  number = {3},
  pages = {93--98},
  publisher = {Korean Society of Pediatric Nephrology},
  timestamp = {2025-04-15T16:20:56Z}
}

@misc{oikarinen2023labelfree,
  title = {Label-{{Free Concept Bottleneck Models}}},
  author = {Oikarinen, Tuomas and Das, Subhro and Nguyen, Lam M. and Weng, Tsui-Wei},
  year = {2023},
  month = jun,
  number = {arXiv:2304.06129},
  eprint = {2304.06129},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.06129},
  urldate = {2025-09-05},
  abstract = {Concept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to human-understandable concepts. However, existing CBMs and their variants have two crucial limitations: first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable - we present the first CBM scaled to ImageNet, efficient - creating a CBM takes only a few hours even for very large datasets, and automated - training it for a new dataset requires minimal human effort. Our code is available at https://github.com/Trustworthy-ML-Lab/Label-free-CBM. Finally, in Appendix B we conduct a large scale user evaluation of the interpretability of our method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {TLDR: This work proposes Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy.},
  timestamp = {2025-09-05T12:38:37Z}
}

@article{okada2023explainable,
  title = {Explainable Artificial Intelligence in Emergency Medicine: An Overview},
  author = {Okada, Yohei and Ning, Yilin and Ong, Marcus Eng Hock},
  year = {2023},
  journal = {Clinical and Experimental Emergency Medicine},
  volume = {10},
  number = {4},
  pages = {354},
  timestamp = {2025-04-15T14:56:57Z}
}

@article{olah2017feature,
  title = {Feature {{Visualization}}},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  year = {2017},
  month = nov,
  journal = {Distill},
  volume = {2},
  number = {11},
  pages = {10.23915/distill.00007},
  issn = {2476-0757},
  doi = {10.23915/distill.00007},
  urldate = {2025-09-02},
  timestamp = {2025-09-02T02:40:28Z}
}

@article{olah2020zoom,
  title = {Zoom {{In}}: {{An Introduction}} to {{Circuits}}},
  shorttitle = {Zoom {{In}}},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  year = {2020},
  month = mar,
  journal = {Distill},
  volume = {5},
  number = {3},
  pages = {10.23915/distill.00024.001},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.001},
  urldate = {2025-09-02},
  timestamp = {2025-09-02T02:40:28Z}
}

@article{olier2023causal,
  title = {Causal Inference and Observational Data},
  author = {Olier, Ivan and Zhan, Yiqiang and Liang, Xiaoyu and Volovici, Victor},
  year = {2023},
  month = oct,
  journal = {BMC Medical Research Methodology},
  volume = {23},
  number = {1},
  pages = {227, s12874-023-02058-5},
  issn = {1471-2288},
  doi = {10.1186/s12874-023-02058-5},
  urldate = {2025-03-20},
  abstract = {Abstract             Observational studies using causal inference frameworks can provide a feasible alternative to randomized controlled trials. Advances in statistics, machine learning, and access to big data facilitate unraveling complex causal relationships from observational data across healthcare, social sciences, and other fields. However, challenges like evaluating models and bias amplification remain.},
  langid = {english},
  annotation = {TLDR: Advances in statistics, machine learning, and access to big data facilitate unraveling complex causal relationships from observational data across healthcare, social sciences, and other fields, but challenges like evaluating models and bias amplification remain.},
  timestamp = {2025-03-20T21:57:24Z}
}

@misc{ong2025regulatory,
  title = {Regulatory {{Science Innovation}} for {{Generative AI}} and {{Large Language Models}} in {{Health}} and {{Medicine}}: {{A Global Call}} for {{Action}}},
  shorttitle = {Regulatory {{Science Innovation}} for {{Generative AI}} and {{Large Language Models}} in {{Health}} and {{Medicine}}},
  author = {Ong, Jasmine Chiat Ling and Ning, Yilin and Liu, Mingxuan and Ma, Yian and Liang, Zhao and Singh, Kuldev and Chang, Robert T and Vogel, Silke and Lim, John CW and Tan, Iris Siu Kwan and Freyer, Oscar and Gilbert, Stephen and Bitterman, Danielle S and Liu, Xiaoxuan and Denniston, Alastair K and Liu, Nan},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2502.07794},
  urldate = {2025-06-13},
  abstract = {The integration of generative AI (GenAI) and large language models (LLMs) in healthcare presents both unprecedented opportunities and challenges, necessitating innovative regulatory approaches. GenAI and LLMs offer broad applications, from automating clinical workflows to personalizing diagnostics. However, the non-deterministic outputs, broad functionalities and complex integration of GenAI and LLMs challenge existing medical device regulatory frameworks, including the total product life cycle (TPLC) approach. Here we discuss the constraints of the TPLC approach to GenAI and LLM-based medical device regulation, and advocate for global collaboration in regulatory science research. This serves as the foundation for developing innovative approaches including adaptive policies and regulatory sandboxes, to test and refine governance in real-world settings. International harmonization, as seen with the International Medical Device Regulators Forum, is essential to manage implications of LLM on global health, including risks of widening health inequities driven by inherent model biases. By engaging multidisciplinary expertise, prioritizing iterative, data-driven approaches, and focusing on the needs of diverse populations, global regulatory science research enables the responsible and equitable advancement of LLM innovations in healthcare.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computers and Society (cs.CY),FOS: Computer and information sciences},
  annotation = {TLDR: The constraints of the TPLC approach to GenAI and LLM-based medical device regulation are discussed, the foundation for developing innovative approaches including adaptive policies and regulatory sandboxes are developed, to test and refine governance in real-world settings.},
  timestamp = {2025-06-13T12:03:07Z}
}

@misc{openai2024learning,
  title = {Learning to Reason with Llms},
  author = {{OpenAI}},
  year = {2024b},
  timestamp = {2025-03-19T08:14:19Z}
}

@article{ortigossa2024explainable,
  title = {{{EXplainable Artificial Intelligence}} ({{XAI}})---{{From Theory}} to {{Methods}} and {{Applications}}},
  author = {Ortigossa, Evandro S. and Gon{\c c}alves, Thales and Nonato, Luis Gustavo},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {80799--80846},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3409843},
  urldate = {2025-05-28},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  annotation = {TLDR: The theoretical foundations of Explainable Artificial Intelligence (XAI) are provided, clarifying diffuse definitions and identifying research objectives, challenges, and future research lines related to turning opaque machine learning outputs into more transparent decisions.},
  timestamp = {2025-05-28T02:53:56Z}
}

@article{osullivan2022explainable,
  title = {Explainable Artificial Intelligence ({{XAI}}): Closing the Gap between Image Analysis and Navigation in Complex Invasive Diagnostic Procedures},
  shorttitle = {Explainable Artificial Intelligence ({{XAI}})},
  author = {O'Sullivan, S. and Janssen, M. and Holzinger, Andreas and Nevejans, Nathalie and Eminaga, O. and Meyer, C. P. and Miernik, Arkadiusz},
  year = {2022},
  month = may,
  journal = {World Journal of Urology},
  volume = {40},
  number = {5},
  pages = {1125--1134},
  issn = {1433-8726},
  doi = {10.1007/s00345-022-03930-7},
  urldate = {2025-05-04},
  abstract = {Cystoscopy is the gold standard for initial macroscopic assessments of the human urinary bladder to rule out (or diagnose) bladder cancer (BCa). Despite having guidelines, cystoscopic findings are diverse and often challenging to classify.  The extent of the false negatives and false positives in cystoscopic diagnosis is currently unknown. We suspect that there is a certain degree of under-diagnosis (like the failure to detect malignant tumours) and over-diagnosis (e.g. sending the patient for unnecessary transurethral resection of bladder tumors with anesthesia) that put the patient at risk.},
  langid = {english},
  keywords = {Autonomous endoscopy,Autonomous surgery,Bladder cancer detection,Bladder cancer diagnosis,Cystoscopy,Robotic endoscopy},
  annotation = {TLDR: XAI robot-assisted cystoscopes would help to overcome the risks/flaws of conventional Cystoscopy and represent a surgical parallel to `Autonomous Driving' (where a standard requires a human supervisor to remain in the `vehicle').},
  timestamp = {2025-05-04T01:53:52Z}
}

@inproceedings{p20243d,
  title = {{{3D Grad-CAM}} in {{Lung Cancer Images}} Using {{Deep Learning Techniques}}},
  booktitle = {Proceedings of the 1st {{International Conference}} on {{Artificial Intelligence}}, {{Communication}}, {{IoT}}, {{Data Engineering}} and {{Security}}, {{IACIDS}} 2023, 23-25 {{November}} 2023, {{Lavasa}}, {{Pune}}, {{India}}},
  author = {P, Bhavani and Pl, Chithra},
  year = {2024},
  month = mar,
  urldate = {2025-05-04},
  abstract = {Medical image processing approach play a vital role in 3D Convolutional Neural Networks (CNN) using Grad-CAM (Gradient-Weighted Class Activation Mapping) techniques. This proposed 3D Grad-CAM architecture identifies the part of the tumor in the input image and utilizes the Gradient Class Activation},
  isbn = {978-1-63190-457-8},
  timestamp = {2025-05-04T00:10:15Z}
}

@article{page2021prisma,
  title = {The {{PRISMA}} 2020 Statement: An Updated Guideline for Reporting Systematic Reviews},
  shorttitle = {The {{PRISMA}} 2020 Statement},
  author = {Page, Matthew J and McKenzie, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M and Hr{\'o}bjartsson, Asbj{\o}rn and Lalu, Manoj M and Li, Tianjing and Loder, Elizabeth W and {Mayo-Wilson}, Evan and McDonald, Steve and McGuinness, Luke A and Stewart, Lesley A and Thomas, James and Tricco, Andrea C and Welch, Vivian A and Whiting, Penny and Moher, David},
  year = {2021},
  month = mar,
  journal = {BMJ},
  pages = {n71},
  issn = {1756-1833},
  doi = {10.1136/bmj.n71},
  urldate = {2025-05-18},
  langid = {english},
  timestamp = {2025-05-18T01:54:00Z}
}

@misc{pahde2023reveal,
  title = {Reveal to {{Revise}}: {{An Explainable AI Life Cycle}} for {{Iterative Bias Correction}} of {{Deep Models}}},
  shorttitle = {Reveal to {{Revise}}},
  author = {Pahde, Frederik and Dreyer, Maximilian and Samek, Wojciech and Lapuschkin, Sebastian},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2303.12641},
  urldate = {2025-05-28},
  abstract = {State-of-the-art machine learning models often learn spurious correlations embedded in the training data. This poses risks when deploying these models for high-stake decision-making, such as in medical applications like skin cancer detection. To tackle this problem, we propose Reveal to Revise (R2R), a framework entailing the entire eXplainable Artificial Intelligence (XAI) life cycle, enabling practitioners to iteratively identify, mitigate, and (re-)evaluate spurious model behavior with a minimal amount of human interaction. In the first step (1), R2R reveals model weaknesses by finding outliers in attributions or through inspection of latent concepts learned by the model. Secondly (2), the responsible artifacts are detected and spatially localized in the input data, which is then leveraged to (3) revise the model behavior. Concretely, we apply the methods of RRR, CDEP and ClArC for model correction, and (4) (re-)evaluate the model's performance and remaining sensitivity towards the artifact. Using two medical benchmark datasets for Melanoma detection and bone age estimation, we apply our R2R framework to VGG, ResNet and EfficientNet architectures and thereby reveal and correct real dataset-intrinsic artifacts, as well as synthetic variants in a controlled setting. Completing the XAI life cycle, we demonstrate multiple R2R iterations to mitigate different biases. Code is available on https://github.com/maxdreyer/Reveal2Revise.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  annotation = {TLDR: The Reveal to Revise (R2R) framework, a framework entailing the entire eXplainable Artificial Intelligence (XAI) life cycle, enabling practitioners to iteratively identify, mitigate, and (re-)evaluate spurious model behavior with a minimal amount of human interaction is proposed.},
  timestamp = {2025-05-28T06:59:57Z}
}

@article{palaniappan2024global,
  title = {Global {{Regulatory Frameworks}} for the {{Use}} of {{Artificial Intelligence}} ({{AI}}) in the {{Healthcare Services Sector}}},
  author = {Palaniappan, Kavitha and Lin, Elaine Yan Ting and Vogel, Silke},
  year = {2024},
  month = feb,
  journal = {Healthcare (Basel, Switzerland)},
  volume = {12},
  number = {5},
  pages = {562},
  issn = {2227-9032},
  doi = {10.3390/healthcare12050562},
  abstract = {The healthcare sector is faced with challenges due to a shrinking healthcare workforce and a rise in chronic diseases that are worsening with demographic and epidemiological shifts. Digital health interventions that include artificial intelligence (AI) are being identified as some of the potential solutions to these challenges. The ultimate aim of these AI systems is to improve the patient's health outcomes and satisfaction, the overall population's health, and the well-being of healthcare professionals. The applications of AI in healthcare services are vast and are expected to assist, automate, and augment several healthcare services. Like any other emerging innovation, AI in healthcare also comes with its own risks and requires regulatory controls. A review of the literature was undertaken to study the existing regulatory landscape for AI in the healthcare services sector in developed nations. In the global regulatory landscape, most of the regulations for AI revolve around Software as a Medical Device (SaMD) and are regulated under digital health products. However, it is necessary to note that the current regulations may not suffice as AI-based technologies are capable of working autonomously, adapting their algorithms, and improving their performance over time based on the new real-world data that they have encountered. Hence, a global regulatory convergence for AI in healthcare, similar to the voluntary AI code of conduct that is being developed by the US-EU Trade and Technology Council, would be beneficial to all nations, be it developing or developed.},
  langid = {english},
  pmcid = {PMC10930608},
  pmid = {38470673},
  keywords = {artificial intelligence,healthcare services,regulatory frameworks},
  annotation = {TLDR: A global regulatory convergence for AI in healthcare, similar to the voluntary AI code of conduct that is being developed by the US-EU Trade and Technology Council, would be beneficial to all nations, be it developing or developed.},
  timestamp = {2025-04-16T03:09:16Z}
}

@article{palatnikdesousa2019local,
  title = {Local {{Interpretable Model-Agnostic Explanations}} for {{Classification}} of {{Lymph Node Metastases}}},
  author = {Palatnik De Sousa, Iam and Maria Bernardes Rebuzzi Vellasco, Marley and Costa Da Silva, Eduardo},
  year = {2019},
  month = jul,
  journal = {Sensors},
  volume = {19},
  number = {13},
  pages = {2969},
  issn = {1424-8220},
  doi = {10.3390/s19132969},
  urldate = {2025-05-03},
  abstract = {An application of explainable artificial intelligence on medical data is presented. There is an increasing demand in machine learning literature for such explainable models in health-related applications. This work aims to generate explanations on how a Convolutional Neural Network (CNN) detects tumor tissue in patches extracted from histology whole slide images. This is achieved using the ``locally-interpretable model-agnostic explanations'' methodology. Two publicly-available convolutional neural networks trained on the Patch Camelyon Benchmark are analyzed. Three common segmentation algorithms are compared for superpixel generation, and a fourth simpler parameter-free segmentation algorithm is proposed. The main characteristics of the explanations are discussed, as well as the key patterns identified in true positive predictions. The results are compared to medical annotations and literature and suggest that the CNN predictions follow at least some aspects of human expert knowledge.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: This work aims to generate explanations on how a Convolutional Neural Network detects tumor tissue in patches extracted from histology whole slide images using the ``locally-interpretable model-agnostic explanations'' methodology.},
  timestamp = {2025-05-03T05:24:32Z}
}

@article{palm2025leveraging,
  title = {Leveraging Electronic Medical Records to Evaluate a Computerized Decision Support System for Staphylococcus Bacteremia},
  author = {Palm, Julia and Alaid, Ssuhir and Ammon, Danny and Brandes, Julian and D{\"u}rschmid, Andreas and Fischer, Claudia and Fortmann, Jonas and Friebel, Kristin and Geihs, Sarah and Hartig, Anne-Kathrin and He, Donghui and Heidel, Andrew J. and Hetfeld, Petra and Ihle, Roland and Kahle, Suzanne and Koi, Verena and Konik, Margarethe and Kretzschmann, Frauke and Kruse, Henner and Lippmann, Norman and L{\"u}bbert, Christoph and Marx, Gernot and Mikolajczyk, Rafael and Mlocek, Anne and Moritz, Stefan and M{\"u}ller, Christoph and M{\"u}ller, Susanne and P{\'e}rez Garriga, Ariadna and {Phan-Vogtmann}, Lo An and Pietzner, Diana and Pletz, Mathias W. and Popp, Mario and Rebenstorff, Maike and Renz, Jonas and Ri{\ss}ner, Florian and R{\"o}hrig, Rainer and Saleh, Kutaiba and Sch{\"o}nherr, Sebastian G. and Spreckelsen, Cord and Stempel, Anja and Stolz, Abel and Thomas, Eric and Thon, Susanne and Tiller, Daniel and Uschmann, Sebastian and Wendt, Sebastian and Wendt, Thomas and Winnekens, Philipp and Witzke, Oliver and Hagel, Stefan and Scherag, Andr{\'e}},
  year = {2025},
  month = mar,
  journal = {npj Digital Medicine},
  volume = {8},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-025-01569-3},
  urldate = {2025-05-03},
  abstract = {Infectious disease specialists (IDS) improve outcomes of patients with Staphylococcus bacteremia, but immediate IDS access is not always guaranteed. We investigated whether a care-integrated computerized decision support system (CDSS) can safely enhance the standard of care (SOC) for these patients. We conducted a multicenter, noninferiority, interventional stepped-wedge cluster randomized controlled trial relying on the data integration centers at five university hospitals. By this means, electronic medical records can be used for part of the trial documentation. We analyzed 5056 patients from 134 wards (Staphylococcus aureus (SAB): n\,=\,812, coagulase-negative staphylococci (CoNS): n\,=\,4244) and found that the CDSS was noninferior to the SOC for hospital mortality in all patients. Noninferiority regarding the 90-day mortality/relapse in SAB patients was not observed and there was no evidence for differences in vancomycin usage among CoNS patients. Despite low reported usage, physicians rated the CDSS's usability favorably. Trial registration: drks.de; Identifier: DRKS00014320; Registration Date: 2019-05-06.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Bacterial infection,Medical research},
  annotation = {TLDR: It was found that the CDSS was noninferior to the SOC for hospital mortality in all patients and despite low reported usage, physicians rated the CDSS's usability favorably.},
  timestamp = {2025-05-03T08:21:44Z}
}

@misc{pan2025coat,
  title = {{{CoAT}}: {{Chain-of-Associated-Thoughts Framework}} for {{Enhancing Large Language Models Reasoning}}},
  shorttitle = {{{CoAT}}},
  author = {Pan, Jianfeng and Deng, Senyou and Huang, Shaomang},
  year = {2025},
  month = feb,
  number = {arXiv:2502.02390},
  eprint = {2502.02390},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.02390},
  urldate = {2025-02-09},
  abstract = {Research on LLM technologies is rapidly emerging, with most of them employing a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. To validate the effectiveness of our framework, we conducted extensive experiments across a range of generative and reasoning tasks. These experiments demonstrated that our framework outperforms conventional inference processes on accuracy, coherence, and diversity. The framework's ability to iteratively expand its search space while retaining contextually relevant information results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {TLDR: The novel Chain-of-Associated-Thoughts (CoAT) framework is developed, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory', that significantly expands the LLM search space.},
  timestamp = {2025-04-01T12:58:20Z}
}

@inproceedings{panigutti2020doctor,
  title = {Doctor {{XAI}}: An Ontology-Based Approach to Black-Box Sequential Data Classification Explanations},
  shorttitle = {Doctor {{XAI}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Panigutti, Cecilia and Perotti, Alan and Pedreschi, Dino},
  year = {2020},
  month = jan,
  series = {{{FAT}}* '20},
  pages = {629--639},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3351095.3372855},
  urldate = {2025-03-28},
  abstract = {Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.},
  isbn = {978-1-4503-6936-7},
  langid = {american},
  annotation = {TLDR: This paper focuses on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit, and shows how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.},
  timestamp = {2025-03-29T03:03:31Z}
}

@misc{papakostas2024deep,
  title = {Deep {{Learning}} for {{Causal Inference}}: {{A Comparison}} of {{Architectures}} for {{Heterogeneous Treatment Effect Estimation}}},
  shorttitle = {Deep {{Learning}} for {{Causal Inference}}},
  author = {Papakostas, Demetrios and Herren, Andrew and Hahn, P. Richard and Castillo, Francisco},
  year = {2024},
  month = may,
  number = {arXiv:2405.03130},
  eprint = {2405.03130},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.03130},
  urldate = {2025-03-22},
  abstract = {Causal inference has gained much popularity in recent years, with interests ranging from academic, to industrial, to educational, and all in between. Concurrently, the study and usage of neural networks has also grown profoundly (albeit at a far faster rate). What we aim to do in this blog write-up is demonstrate a Neural Network causal inference architecture. We develop a fully connected neural network implementation of the popular Bayesian Causal Forest algorithm, a state of the art tree based method for estimating heterogeneous treatment effects. We compare our implementation to existing neural network causal inference methodologies, showing improvements in performance in simulation settings. We apply our method to a dataset examining the effect of stress on sleep.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-22T09:21:35Z}
}

@article{park2020enhancing,
  title = {Enhancing the Interpretability of Transcription Factor Binding Site Prediction Using Attention Mechanism},
  author = {Park, Sungjoon and Koh, Yookyung and Jeon, Hwisang and Kim, Hyunjae and Yeo, Yoonsun and Kang, Jaewoo},
  year = {2020},
  month = aug,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {13413},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-70218-4},
  abstract = {Transcription factors (TFs) regulate the gene expression of their target genes by binding to the regulatory sequences of target genes (e.g., promoters and enhancers). To fully understand gene regulatory mechanisms, it is crucial to decipher the relationships between TFs and DNA sequences. Moreover, studies such as GWAS and eQTL have verified that most disease-related variants exist in non-coding regions, and highlighted the necessity to identify such variants that cause diseases by interrupting TF binding mechanisms. To do this, it is necessary to build a prediction model that precisely predicts the binding relationships between TFs and DNA sequences. Recently, deep learning based models have been proposed and have shown competitive results on a transcription factor binding site prediction task. However, it is difficult to interpret the prediction results obtained from the previous models. In addition, the previous models assumed all the sequence regions in the input DNA sequence have the same importance for predicting TF-binding, although sequence regions containing TF-binding-associated signals such as TF-binding motifs should be captured more than other regions. To address these challenges, we propose TBiNet, an attention based interpretable deep neural network for predicting transcription factor binding sites. Using the attention mechanism, our method is able to assign more importance on the actual TF binding sites in the input DNA sequence. TBiNet outperforms the current state-of-the-art methods (DeepSea and DanQ) quantitatively in the TF-DNA binding prediction task. Moreover, TBiNet is more effective than the previous models in discovering known TF-binding motifs.},
  langid = {english},
  pmcid = {PMC7414127},
  pmid = {32770026},
  keywords = {Base Sequence,Binding Sites,Computational Biology,Deep Learning,Forecasting,Gene Expression Regulation,Models Theoretical,Neural Networks Computer,Protein Binding,Transcription Factors},
  annotation = {TLDR: TBiNet is an attention based interpretable deep neural network for predicting transcription factor binding sites, which outperforms the current state-of-the-art methods quantitatively in the TF-DNA binding prediction task and is more effective than the previous models in discovering known TF-binding motifs.},
  timestamp = {2025-07-05T05:59:48Z}
}

@article{park2023performance,
  title = {A Performance Evaluation of Drug Response Prediction Models for Individual Drugs},
  author = {Park, Aron and Lee, Yeeun and Nam, Seungyoon},
  year = {2023},
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {11911},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2025-08-08T09:43:32Z}
}

@misc{pathai,
  title = {{{PathAI}} {\textbar} {{Pathology Transformed}}},
  urldate = {2025-05-23},
  abstract = {At PathAI, we're dedicated to improving patient outcomes with AI-powered pathology and meaningful collaboration with biopharma, laboratories and clinicians.},
  howpublished = {https://www.pathai.com/},
  langid = {english},
  timestamp = {2025-05-23T02:00:28Z}
}

@misc{patient,
  title = {Patient and {{Caregiver Perceptions}} of an {{Interface Design}} to {{Communicate Artificial Intelligence}}--{{Based Prognosis}} for {{Patients With Advanced Solid Tumors}} {\textbar} {{JCO Clinical Cancer Informatics}}},
  urldate = {2025-04-12},
  howpublished = {https://ascopubs.org/doi/10.1200/CCI.23.00187},
  timestamp = {2025-04-12T04:15:25Z}
}

@misc{patricio2022explainable,
  title = {Explainable {{Deep Learning Methods}} in {{Medical Image Classification}}: {{A Survey}}},
  shorttitle = {Explainable {{Deep Learning Methods}} in {{Medical Image Classification}}},
  author = {Patr{\'i}cio, Cristiano and Neves, Jo{\~a}o C. and Teixeira, Lu{\'i}s F.},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.04766},
  urldate = {2025-05-28},
  abstract = {The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation-based methods. Finally, the major challenges in applying XAI to medical imaging and the future research directions on the topic are also discussed.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV),Machine Learning (cs.LG)},
  annotation = {TLDR: A thorough survey of XAI applied to medical diagnosis, including visual, textual, and example-based explanation methods, including a performance comparison among a set of report generation-based methods is provided.},
  timestamp = {2025-05-28T07:56:23Z}
}

@article{patricio2024explainable,
  title = {Explainable {{Deep Learning Methods}} in {{Medical Image Classification}}: {{A Survey}}},
  shorttitle = {Explainable {{Deep Learning Methods}} in {{Medical Image Classification}}},
  author = {Patr{\'i}cio, Cristiano and Neves, Jo{\~a}o C. and Teixeira, Lu{\'i}s F.},
  year = {2024},
  month = apr,
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {4},
  pages = {1--41},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3625287},
  urldate = {2025-05-28},
  abstract = {The remarkable success of deep learning has prompted interest in its application to medical imaging diagnosis. Even though state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box nature of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations. In addition, we include a performance comparison among a set of report generation--based methods. Finally, the major challenges in applying XAI to medical imaging and the future research directions on the topic are discussed.},
  langid = {english},
  annotation = {TLDR: A thorough survey of XAI applied to medical imaging diagnosis, including visual, textual, example-based and concept-based explanation methods, including a performance comparison among a set of report generation--based methods is provided.},
  timestamp = {2025-05-28T07:52:39Z}
}

@article{paulus2020predictably,
  title = {Predictably Unequal: Understanding and Addressing Concerns That Algorithmic Clinical Prediction May Increase Health Disparities},
  author = {Paulus, Jessica K and Kent, David M},
  year = {2020},
  journal = {NPJ digital medicine},
  volume = {3},
  number = {1},
  pages = {99},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2025-05-13T12:14:14Z}
}

@article{payrovnaziri2020explainable,
  title = {Explainable Artificial Intelligence Models Using Real-World Electronic Health~Record Data: A Systematic Scoping Review},
  shorttitle = {Explainable Artificial Intelligence Models Using Real-World Electronic Health~Record Data},
  author = {Payrovnaziri, Seyedeh Neelufar and Chen, Zhaoyi and {Rengifo-Moreno}, Pablo and Miller, Tim and Bian, Jiang and Chen, Jonathan H and Liu, Xiuwen and He, Zhe},
  year = {2020},
  month = jul,
  journal = {Journal of the American Medical Informatics Association},
  volume = {27},
  number = {7},
  pages = {1173--1185},
  issn = {1527-974X},
  doi = {10.1093/jamia/ocaa053},
  urldate = {2025-03-29},
  abstract = {To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions.We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges.Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (N\,=\,13), intrinsically interpretable models (N\,=\,9), data dimensionality reduction (N\,=\,8), attention mechanism (N\,=\,7), and feature interaction and importance (N\,=\,5).XAI evaluation is an open issue that requires~a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view.Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.},
  annotation = {TLDR: It is found that XAI evaluation in medicine has not been adequately and formally practiced, andple opportunities exist to advance XAI research in medicine.},
  timestamp = {2025-03-29T02:34:39Z}
}

@book{Pearl2000-PEACMR,
  title = {Causality: {{Models}}, Reasoning and Inference},
  author = {Pearl, Judea},
  year = {2000},
  publisher = {Cambridge University Press},
  address = {New York},
  timestamp = {2025-05-29T13:57:25Z}
}

@book{pearl2009causality,
  title = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  publisher = {Cambridge university press},
  timestamp = {2025-06-08T03:47:42Z}
}

@book{pearl2014probabilistic,
  title = {Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference},
  author = {Pearl, Judea},
  year = {2014},
  publisher = {Elsevier},
  timestamp = {2025-03-19T12:22:14Z}
}

@inproceedings{pearl2018theoretical,
  ids = {10.1145/3159652.3176182},
  title = {Theoretical {{Impediments}} to {{Machine Learning With Seven Sparks}} from the {{Causal Revolution}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Pearl, Judea},
  year = {2018},
  month = feb,
  series = {Wsdm '18},
  pages = {3--3},
  publisher = {ACM},
  address = {Marina Del Rey CA USA},
  doi = {10.1145/3159652.3176182},
  urldate = {2025-05-13},
  abstract = {Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.},
  isbn = {978-1-4503-5581-0},
  langid = {english},
  keywords = {keynote talk},
  annotation = {TLDR: This paper presents a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference, to demonstrate the essential role of such models.},
  timestamp = {2025-05-13T12:11:56Z}
}

@book{pearl2020book,
  title = {The Book of Why: The New Science of Cause and Effect},
  shorttitle = {The Book of Why},
  author = {Pearl, Judea and Mackenzie, Dana},
  year = {2020},
  edition = {First trade paperback edition},
  publisher = {Basic Books},
  address = {New York},
  isbn = {978-0-465-09760-9 978-1-5416-9896-3},
  langid = {english},
  timestamp = {2025-03-22T07:22:08Z}
}

@incollection{pearl2022causal,
  title = {Causal Diagrams for Empirical Research (with {{Discussions}})},
  booktitle = {Probabilistic and Causal Inference: {{The}} Works of {{Judea Pearl}}},
  author = {Pearl, Judea},
  year = {2022},
  pages = {255--316},
  timestamp = {2025-05-29T13:59:56Z}
}

@article{perchik2023artificial,
  title = {Artificial {{Intelligence Literacy}}: {{Developing}} a {{Multi-institutional Infrastructure}} for {{AI Education}}},
  shorttitle = {Artificial {{Intelligence Literacy}}},
  author = {Perchik, J. D. and Smith, A. D. and Elkassem, A. A. and Park, J. M. and Rothenberg, S. A. and Tanwar, M. and Yi, P. H. and Sturdivant, A. and Tridandapani, S. and Sotoudeh, H.},
  year = {2023},
  month = jul,
  journal = {Academic Radiology},
  volume = {30},
  number = {7},
  pages = {1472--1480},
  issn = {1076-6332},
  doi = {10.1016/j.acra.2022.10.002},
  urldate = {2025-04-12},
  abstract = {Rationale and Objectives To evaluate the effectiveness of an artificial intelligence (AI) in radiology literacy course on participants from nine radiology residency programs in the Southeast and Mid-Atlantic United States. Materials and Methods A week-long AI in radiology course was developed and included participants from nine radiology residency programs in the Southeast and Mid-Atlantic United States. Ten 30 minutes lectures utilizing a remote learning format covered basic AI terms and methods, clinical applications of AI in radiology by four different subspecialties, and special topics lectures on the economics of AI, ethics of AI, algorithm bias, and medicolegal implications of AI in medicine. A proctored hands-on clinical AI session allowed participants to directly use an FDA cleared AI-assisted viewer and reporting system for advanced cancer. Pre- and post-course electronic surveys were distributed to assess participants' knowledge of AI terminology and applications and interest in AI education. Results There were an average of 75 participants each day of the course (range: 50--120). Nearly all participants reported a lack of sufficient exposure to AI in their radiology training (96.7\%, 90/93). Mean participant score on the pre-course AI knowledge evaluation was 8.3/15, with a statistically significant increase to 10.1/15 on the post-course evaluation (p= 0.04). A majority of participants reported an interest in continued AI in radiology education in the future (78.6\%, 22/28). Conclusion A multi-institutional AI in radiology literacy course successfully improved AI education of participants, with the majority of participants reporting a continued interest in AI in radiology education in the future.},
  keywords = {Artificial intelligence,Machine learning,Resident education},
  annotation = {TLDR: A multi-institutional AI in radiology literacy course successfully improved AI education of participants, with the majority of participants reporting a continued interest in AI in Radiology education in the future.},
  timestamp = {2025-04-12T07:18:28Z}
}

@misc{peters2014causal,
  title = {Causal {{Discovery}} with {{Continuous Additive Noise Models}}},
  author = {Peters, Jonas and Mooij, Joris and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2014},
  month = apr,
  number = {arXiv:1309.6779},
  eprint = {1309.6779},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1309.6779},
  urldate = {2025-03-22},
  abstract = {We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (Regression with Subsequent Independence Test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  timestamp = {2025-03-22T06:22:33Z}
}

@book{peters2017elements,
  title = {Elements of Causal Inference: Foundations and Learning Algorithms},
  author = {Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  publisher = {The MIT Press},
  timestamp = {2025-03-20T22:17:47Z}
}

@article{pfeifer2022multi,
  title = {Multi-Omics Disease Module Detection with an Explainable {{Greedy Decision Forest}}},
  author = {Pfeifer, Bastian and Baniecki, Hubert and Saranti, Anna and Biecek, Przemyslaw and Holzinger, Andreas},
  year = {2022},
  journal = {Scientific reports},
  volume = {12},
  number = {1},
  pages = {16857},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2025-05-16T08:35:12Z}
}

@misc{pfeifer2023explaining,
  title = {Explaining and Visualizing Black-Box Models through Counterfactual Paths},
  author = {Pfeifer, Bastian and Krzyzinski, Mateusz and Baniecki, Hubert and Saranti, Anna and Holzinger, Andreas and Biecek, Przemyslaw},
  year = {2023},
  month = aug,
  number = {arXiv:2307.07764},
  eprint = {2307.07764},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.07764},
  urldate = {2025-04-12},
  abstract = {Explainable AI (XAI) is an increasingly important area of machine learning research, which aims to make black-box models transparent and interpretable. In this paper, we propose a novel approach to XAI that uses the so-called counterfactual paths generated by conditional permutations of features. The algorithm measures feature importance by identifying sequential permutations of features that most influence changes in model predictions. It is particularly suitable for generating explanations based on counterfactual paths in knowledge graphs incorporating domain knowledge. Counterfactual paths introduce an additional graph dimension to current XAI methods in both explaining and visualizing black-box models. Experiments with synthetic and medical data demonstrate the practical applicability of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {TLDR: A novel approach to XAI that uses counterfactual paths generated by conditional permutations to provide a more intuitive and interpretable explanation for the model's behaviour than traditional feature weighting methods and can help identify and mitigate biases in the model.},
  timestamp = {2025-04-12T07:43:17Z}
}

@misc{pham2022accurate,
  title = {An {{Accurate}} and {{Explainable Deep Learning System Improves Interobserver Agreement}} in the {{Interpretation}} of {{Chest Radiograph}}},
  author = {Pham, Hieu H. and Nguyen, Ha Q. and Nguyen, Hieu T. and Le, Linh T. and Khanh, Lam},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03545},
  eprint = {2208.03545},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.03545},
  urldate = {2025-03-03},
  abstract = {Recent artificial intelligence (AI) algorithms have achieved radiologist-level performance on various medical classification tasks. However, only a few studies addressed the localization of abnormal findings from CXR scans, which is essential in explaining the image-level classification to radiologists. We introduce in this paper an explainable deep learning system called VinDr-CXR that can classify a CXR scan into multiple thoracic diseases and, at the same time, localize most types of critical findings on the image. VinDr-CXR was trained on 51,485 CXR scans with radiologist-provided bounding box annotations. It demonstrated a comparable performance to experienced radiologists in classifying 6 common thoracic diseases on a retrospective validation set of 3,000 CXR scans, with a mean area under the receiver operating characteristic curve (AUROC) of 0.967 (95\% confidence interval [CI]: 0.958-0.975). The VinDr-CXR was also externally validated in independent patient cohorts and showed its robustness. For the localization task with 14 types of lesions, our free-response receiver operating characteristic (FROC) analysis showed that the VinDr-CXR achieved a sensitivity of 80.2\% at the rate of 1.0 false-positive lesion identified per scan. A prospective study was also conducted to measure the clinical impact of the VinDr-CXR in assisting six experienced radiologists. The results indicated that the proposed system, when used as a diagnosis supporting tool, significantly improved the agreement between radiologists themselves with an increase of 1.5\% in mean Fleiss' Kappa. We also observed that, after the radiologists consulted VinDr-CXR's suggestions, the agreement between each of them and the system was remarkably increased by 3.3\% in mean Cohen's Kappa.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  timestamp = {2025-03-03T08:32:09Z}
}

@article{phillips2021four,
  title = {Four Principles of Explainable Artificial Intelligence},
  author = {Phillips, P Jonathon and Phillips, P Jonathon and Hahn, Carina A and Fontana, Peter C and Yates, Amy N and Greene, Kristen and Broniatowski, David A and Przybocki, Mark A},
  year = {2021},
  publisher = {{US Department of Commerce, National Institute of Standards and Technology}},
  timestamp = {2025-04-15T14:33:05Z}
}

@article{piccininni2020directed,
  title = {Directed Acyclic Graphs and Causal Thinking in Clinical Risk Prediction Modeling},
  author = {Piccininni, Marco and Konigorski, Stefan and Rohmann, Jessica L and Kurth, Tobias},
  year = {2020},
  journal = {BMC medical research methodology},
  volume = {20},
  pages = {1--9},
  publisher = {Springer},
  timestamp = {2025-05-03T12:20:10Z}
}

@inproceedings{pmlr-v119-rieger20a,
  title = {Interpretations Are Useful: {{Penalizing}} Explanations to Align Neural Networks with Prior Knowledge},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Rieger, Laura and Singh, Chandan and Murdoch, William and Yu, Bin},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {8116--8126},
  publisher = {PMLR},
  abstract = {For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective. Too often, the litany of proposed explainable deep learning methods stop at the first step, providing practitioners with insight into a model, but no way to act on it. In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods to increase the predictive accuracy of a deep learning model. In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by inserting domain knowledge into the model via explanations. We demonstrate the ability of CDEP to increase performance on an array of toy and real datasets.},
  timestamp = {2025-05-17T10:35:48Z}
}

@inproceedings{pmlr-v213-lopez23a,
  title = {Learning Causal Representations of Single Cells via Sparse Mechanism Shift Modeling},
  booktitle = {Proceedings of the Second Conference on Causal Learning and Reasoning},
  author = {Lopez, Romain and Tagasovska, Natasa and Ra, Stephen and Cho, Kyunghyun and Pritchard, Jonathan and Regev, Aviv},
  editor = {{van der Schaar}, Mihaela and Zhang, Cheng and Janzing, Dominik},
  year = {2023},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {213},
  pages = {662--691},
  publisher = {PMLR},
  abstract = {Latent variable models such as the Variational Auto-Encoder (VAE) have become a go-to tool for analyzing biological data, especially in the field of single-cell genomics. One remaining challenge is the interpretability of latent variables as biological processes that define a cell's identity. Outside of biological applications, this problem is commonly referred to as learning disentangled representations. Although several disentanglement-promoting variants of the VAE were introduced, and applied to single-cell genomics data, this task has been shown to be infeasible from independent and identically distributed measurements, without additional structure. Instead, recent methods propose to leverage non-stationary data, as well as the sparse mechanism shift assumption in order to learn disentangled representations with a causal semantic. Here, we extend the application of these methodological advances to the analysis of single-cell genomics data with genetic or chemical perturbations. More precisely, we propose a deep generative model of single-cell gene expression data for which each perturbation is treated as a stochastic intervention targeting an unknown, but sparse, subset of latent variables. We benchmark these methods on simulated single-cell data to evaluate their performance at latent units recovery, causal target identification and out-of-domain generalization. Finally, we apply those approaches to two real-world large-scale gene perturbation data sets and find that models that exploit the sparse mechanism shift hypothesis surpass contemporary methods on a transfer learning task. We implement our new model and benchmarks using the scvi-tools library, and release it as open-source software at {$<$}a href="https://github.com/Genentech/sVAE"{$>$}https://github.com/Genentech/sVAE{$<$}/a{$>$}.},
  timestamp = {2025-05-17T09:09:43Z}
}

@inproceedings{pmlr-v70-sundararajan17a,
  ids = {sundararajan2017axiomatic},
  title = {Axiomatic Attribution for Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  editor = {Precup, Doina and Teh, Yee Whye},
  year = {2017},
  month = aug,
  series = {Proceedings of Machine Learning Research},
  volume = {70},
  pages = {3319--3328},
  publisher = {PMLR},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  timestamp = {2025-07-21T07:41:57Z}
}

@inproceedings{pmlr-v80-kim18d,
  title = {Interpretability beyond Feature Attribution: {{Quantitative}} Testing with Concept Activation Vectors ({{TCAV}})},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and {sayres}, Rory},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {2668--2677},
  publisher = {PMLR},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of ``zebra'' is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  timestamp = {2025-05-18T12:29:22Z}
}

@misc{poesia2022synchromesh,
  title = {Synchromesh: {{Reliable}} Code Generation from Pre-Trained Language Models},
  shorttitle = {Synchromesh},
  author = {Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
  year = {2022},
  month = jan,
  number = {arXiv:2201.11227},
  eprint = {2201.11227},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11227},
  urldate = {2025-02-13},
  abstract = {Large pre-trained language models have been used to generate code,providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation. Synchromesh comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, Synchromesh feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  timestamp = {2025-02-13T00:01:07Z}
}

@article{polaka2010decision,
  title = {Decision {{Tree Classifiers}} in {{Bioinformatics}}},
  author = {Polaka, Inese and Tom, Igor and Borisov, Arkady},
  year = {2010},
  month = jan,
  journal = {Scientific Journal of Riga Technical University. Computer Sciences},
  volume = {42},
  number = {1},
  pages = {118--123},
  issn = {1407-7493},
  doi = {10.2478/v10143-010-0052-4},
  urldate = {2025-06-12},
  abstract = {Decision Tree Classifiers in Bioinformatics             This paper presents a literature review of articles related to the use of decision tree classifiers in gene microarray data analysis published in the last ten years. The main focus is on researches solving the cancer classification problem using single decision tree classifiers (algorithms C4.5 and CART) and decision tree forests (e.g. random forests) showing strengths and weaknesses of the proposed methodologies when compared to other popular classification methods. The article also touches the use of decision tree classifiers in gene selection.},
  annotation = {TLDR: A literature review of articles related to the use of decision tree classifiers in gene microarray data analysis published in the last ten years showing strengths and weaknesses of the proposed methodologies when compared to other popular classification methods.},
  timestamp = {2025-06-12T14:16:42Z}
}

@inproceedings{pope2019explainability,
  title = {Explainability {{Methods}} for {{Graph Convolutional Neural Networks}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Pope, Phillip E. and Kolouri, Soheil and Rostami, Mohammad and Martin, Charles E. and Hoffmann, Heiko},
  year = {2019},
  pages = {10772--10781},
  urldate = {2025-03-28},
  timestamp = {2025-03-28T11:51:50Z}
}

@article{porebski2022evaluation,
  title = {Evaluation of Fuzzy Membership Functions for Linguistic Rule-Based Classifier Focused on Explainability, Interpretability and Reliability},
  author = {Porebski, Sebastian},
  year = {2022},
  journal = {Expert systems with applications},
  volume = {199},
  pages = {117116},
  publisher = {Elsevier},
  timestamp = {2025-04-16T02:04:59Z}
}

@misc{posthoc,
  title = {Post-Hoc vs Ante-Hoc Explanations: {{xAI}} Design Guidelines for Data Scientists - {{ScienceDirect}}},
  urldate = {2025-03-29},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S1389041724000378},
  timestamp = {2025-03-29T13:54:27Z}
}

@misc{prabhu2018prototypical,
  title = {Prototypical {{Clustering Networks}} for {{Dermatological Disease Diagnosis}}},
  author = {Prabhu, Viraj and Kannan, Anitha and Ravuri, Murali and Chablani, Manish and Sontag, David and Amatriain, Xavier},
  year = {2018},
  month = nov,
  number = {arXiv:1811.03066},
  eprint = {1811.03066},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.03066},
  urldate = {2025-04-13},
  abstract = {We consider the problem of image classification for the purpose of aiding doctors in dermatological diagnosis. Dermatological diagnosis poses two major challenges for standard off-the-shelf techniques: First, the data distribution is typically extremely long tailed. Second, intra-class variability is often large. To address the first issue, we formulate the problem as low-shot learning, where once deployed, a base classifier must rapidly generalize to diagnose novel conditions given very few labeled examples. To model diverse classes effectively, we propose Prototypical Clustering Networks (PCN), an extension to Prototypical Networks that learns a mixture of prototypes for each class. Prototypes are initialized for each class via clustering and refined via an online update scheme. Classification is performed by measuring similarity to a weighted combination of prototypes within a class, where the weights are the inferred cluster responsibilities. We demonstrate the strengths of our approach in effective diagnosis on a realistic dataset of dermatological conditions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  timestamp = {2025-04-13T06:58:22Z}
}

@article{pramudito2024explainable,
  title = {Explainable Artificial Intelligence ({{XAI}}) to Find Optimal in-Silico Biomarkers for Cardiac Drug Toxicity Evaluation},
  author = {Pramudito, Muhammad Adnan and Fuadah, Yunendah Nur and Qauli, Ali Ikhsanul and Marcellinus, Aroli and Lim, Ki Moo},
  year = {2024},
  month = oct,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {24045},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-71169-w},
  abstract = {The Comprehensive In-vitro Proarrhythmia Assay (CiPA) initiative aims to refine the assessment of drug-induced torsades de pointes (TdP) risk, utilizing computational models to predict cardiac drug toxicity. Despite advancements in machine learning applications for this purpose, the specific contribution of in-silico biomarkers to toxicity risk levels has yet to be thoroughly elucidated. This study addresses this gap by implementing explainable artificial intelligence (XAI) to illuminate the impact of individual biomarkers in drug toxicity prediction. We employed the Markov chain Monte Carlo method to generate a detailed dataset for 28 drugs, from which twelve in-silico biomarkers of 12 drugs were computed to train various machine learning models, including Artificial Neural Networks (ANN), Support Vector Machines (SVM), Random Forests (RF), XGBoost, K-Nearest Neighbors (KNN), and Radial Basis Function (RBF) networks. Our study's innovation is leveraging XAI, mainly through the SHAP (SHapley Additive exPlanations) method, to dissect and quantify the contributions of biomarkers across these models. Furthermore, the model performance was evaluated using the test set from 16 drugs. We found that the ANN model coupled with the eleven most influential in-silico biomarkers namely    dVm dt  repol  ,  dVm dt  max  , APD 90 , APD 50 , APD tri  , CaD 90 , CaD 50 , Ca tri  , Ca Diastole  , q I n w a r d , a n d q N e t  showed the highest classification performance among all classifiers with Area Under the Curve (AUC) scores of 0.92 for predicting high-risk, 0.83 for intermediate-risk, and 0.98 for low-risk drugs. We also found that the optimal in silico biomarkers selected based on SHAP analysis may be different for various classification models. However, we also found that the biomarker selection only sometimes improved the performance; therefore, evaluating various classifiers is still essential to obtain the desired classification performance. Our proposed method could provide a systematic way to assess the best classifier with the optimal in-silico biomarkers for predicting the TdP risk of drugs, thereby advancing the field of cardiac safety evaluations.},
  langid = {english},
  pmcid = {PMC11473646},
  pmid = {39402077},
  keywords = {Artificial Intelligence,Biomarkers,Computer Simulation,Drug-Related Side Effects and Adverse Reactions,Humans,Machine Learning,Neural Networks Computer,Support Vector Machine,Torsades de Pointes},
  annotation = {TLDR: The proposed method could provide a systematic way to assess the best classifier with the optimal in-silico biomarkers for predicting the TdP risk of drugs, thereby advancing the field of cardiac safety evaluations.},
  timestamp = {2025-08-08T09:28:40Z}
}

@misc{precision,
  title = {Precision Medicine: Recent Advances, Current Challenges and Future Perspectives},
  shorttitle = {Precision Medicine},
  urldate = {2025-05-24},
  abstract = {Personalized medicine (precision medicine) is an evolving field that comprises medical interventions tailored to individuals or groups of patients. It is designed to facilitate enhanced screening and earlier disease detection, more precise disease diagnosis, and improved treatment.{$<$}br/{$><$}br/{$>$}Personalized medicine allows patients to receive specific therapies that work best for them aiming for more effective treatment, better outcomes, safer clinical managements and more efficient health systems.{$<$}br/{$><$}br/{$>$}The diversity of pharmacological mechanisms and targets can permit therapies to efficiently target particular clinical patients. The integration of expertise from basic molecular research to clinical and diagnostic interventions, including regulatory, ethical and policy aspects is much needed to better understand peculiar patient responses to treatments.{$<$}br/{$><$}br/{$>$}Despite the number of targeted drugs has grown in the last decade, most of the current therapeutic approaches are obtained from large scale studies. Efforts are needed to determine medical treatments that best work for each patient, especially for those unresponsive to traditional treatments. In this regards, knowledge of the targets and the molecular mechanisms that might benefit particular populations is of crucial importance. There is demand for evidence demonstrating the clinical value of personalized treatments once integrated into health systems.{$<$}br/{$><$}br/{$>$}This Research Topic aims to shed a light on the most r...},
  howpublished = {https://www.frontiersin.org/research-topics/54481/precision-medicine-recent-advances-current-challenges-and-future-perspectives/magazine},
  langid = {english},
  timestamp = {2025-05-24T10:27:00Z}
}

@misc{predicting,
  title = {Predicting Sepsis Onset Using a Machine Learned Causal Probabilistic Network Algorithm Based on Electronic Health Records Data {\textbar} {{Scientific Reports}}},
  urldate = {2025-05-03},
  howpublished = {https://www.nature.com/articles/s41598-023-38858-4?utm\_source=chatgpt.com},
  timestamp = {2025-05-03T08:11:47Z}
}

@inproceedings{prisacariu2024skin,
  title = {Skin {{Cancer Diagnosis Using CNN}} with {{Attention Mechanisms Based}} on {{Grad-CAM}}},
  booktitle = {2024 28th {{International Conference}} on {{System Theory}}, {{Control}} and {{Computing}} ({{ICSTCC}})},
  author = {Pris{\u a}cariu, Ana-Maria and Ferariu, Lavinia},
  year = {2024},
  month = oct,
  pages = {107--112},
  publisher = {IEEE},
  address = {Sinaia, Romania},
  doi = {10.1109/ICSTCC62912.2024.10744754},
  urldate = {2025-06-12},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-6429-3},
  annotation = {TLDR: Attention mechanisms that exploit Grad-Classification Activation Maps (Grad-CAMs) resulting from previously trained models, for an effective transfer of knowledge to the attention weights are introduced.},
  timestamp = {2025-06-12T16:03:12Z}
}

@article{prosperi2020causal,
  ids = {prosperi2020causala},
  title = {Causal Inference and Counterfactual Prediction in Machine Learning for Actionable Healthcare},
  author = {Prosperi, Mattia and Guo, Yi and Sperrin, Matt and Koopman, James S and Min, Jae S and He, Xing and Rich, Shannan and Wang, Mo and Buchan, Iain E and Bian, Jiang},
  year = {2020},
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {7},
  pages = {369--375},
  publisher = {Nature Publishing Group UK London},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0197-y},
  urldate = {2025-05-04},
  abstract = {Big data, high-performance computing, and (deep) machine learning are increasingly becoming key to precision medicine---from identifying disease risks and taking preventive measures, to making diagnoses and personalizing treatment for individuals. Precision medicine, however, is not only about predicting risks and outcomes, but also about weighing interventions. Interventional clinical predictive models require the correct specification of cause and effect, and the calculation of so-called counterfactuals, that is, alternative scenarios. In biomedical research, observational studies are commonly affected by confounding and selection bias. Without robust assumptions, often requiring a priori domain knowledge, causal inference is not feasible. Data-driven prediction models are often mistakenly used to draw causal effects, but neither their parameters nor their predictions necessarily have a causal interpretation. Therefore, the premise that data-driven prediction models lead to trustable decisions/interventions for precision medicine is questionable. When pursuing intervention modelling, the bio-health informatics community needs to employ causal approaches and learn causal structures. Here we discuss how target trials (algorithmic emulation of randomized studies), transportability (the licence to transfer causal effects from one population to another) and prediction invariance (where a true causal model is contained in the set of all prediction models whose accuracy does not vary across different settings) are linchpins to developing and testing intervention models.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Education,Machine learning,Outcomes research,Research management},
  annotation = {TLDR: How target trials, transportability, and prediction invariance are linchpins to developing and testing intervention models and a true causal model is contained in the set of all prediction models whose accuracy does not vary across different settings is discussed.},
  timestamp = {2025-10-07T08:38:34Z}
}

@article{prosz2024biologically,
  ids = {prosz2024biologicallya},
  title = {Biologically Informed Deep Learning for Explainable Epigenetic Clocks},
  author = {Prosz, Aurel and Pipek, Orsolya and B{\"o}rcs{\"o}k, Judit and Palla, Gergely and Szallasi, Zoltan and Spisak, Sandor and Csabai, Istv{\'a}n},
  year = {2024},
  month = jan,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {1306},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-50495-5},
  urldate = {2025-06-13},
  abstract = {Abstract             Ageing is often characterised by progressive accumulation of damage, and it is one of the most important risk factors for chronic disease development. Epigenetic mechanisms including DNA methylation could functionally contribute to organismal aging, however the key functions and biological processes may govern ageing are still not understood. Although age predictors called epigenetic clocks can accurately estimate the biological age of an individual based on cellular DNA methylation, their models have limited ability to explain the prediction algorithm behind and underlying key biological processes controlling ageing. Here we present XAI-AGE, a biologically informed, explainable deep neural network model for accurate biological age prediction across multiple tissue types. We show that XAI-AGE outperforms the first-generation age predictors and achieves similar results to deep learning-based models, while opening up the possibility to infer biologically meaningful insights of the activity of pathways and other abstract biological processes directly from the model.},
  langid = {english},
  keywords = {Algorithms,Deep Learning,DNA Methylation,Epigenesis Genetic},
  annotation = {TLDR: XAI-AGE is presented, a biologically informed, explainable deep neural network model for accurate biological age prediction across multiple tissue types and achieves similar results to deep learning-based models, while opening up the possibility to infer biologically meaningful insights of the activity of pathways and other abstract biological processes directly from the model.},
  timestamp = {2025-08-13T09:20:24Z}
}

@misc{puiutta2020explainable,
  title = {Explainable {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Explainable {{Reinforcement Learning}}},
  author = {Puiutta, Erika and Veith, Eric MSP},
  year = {2020},
  month = may,
  number = {arXiv:2005.06247},
  eprint = {2005.06247},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.06247},
  urldate = {2025-03-02},
  abstract = {Explainable Artificial Intelligence (XAI), i.e., the development of more transparent and interpretable AI models, has gained increased traction over the last few years. This is due to the fact that, in conjunction with their growth into powerful and ubiquitous tools, AI models exhibit one detrimential characteristic: a performance-transparency trade-off. This describes the fact that the more complex a model's inner workings, the less clear it is how its predictions or decisions were achieved. But, especially considering Machine Learning (ML) methods like Reinforcement Learning (RL) where the system learns autonomously, the necessity to understand the underlying reasoning for their decisions becomes apparent. Since, to the best of our knowledge, there exists no single work offering an overview of Explainable Reinforcement Learning (XRL) methods, this survey attempts to address this gap. We give a short summary of the problem, a definition of important terms, and offer a classification and assessment of current XRL methods. We found that a) the majority of XRL methods function by mimicking and simplifying a complex model instead of designing an inherently simple one, and b) XRL (and XAI) methods often neglect to consider the human side of the equation, not taking into account research from related fields like psychology or philosophy. Thus, an interdisciplinary effort is needed to adapt the generated explanations to a (non-expert) human user in order to effectively progress in the field of XRL and XAI in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-02T03:21:47Z}
}

@article{qiu2024artificial,
  title = {Artificial Intelligence for Drug Discovery and Development in {{Alzheimer}}'s Disease},
  author = {Qiu, Yunguang and Cheng, Feixiong},
  year = {2024},
  month = apr,
  journal = {Current Opinion in Structural Biology},
  volume = {85},
  pages = {102776},
  issn = {0959440X},
  doi = {10.1016/j.sbi.2024.102776},
  urldate = {2025-04-16},
  langid = {english},
  annotation = {TLDR: This review summarizes the AI-driven methodologies for AD-agnostic drug discovery and development, including de novo drug design, virtual screening, and prediction of drug-target interactions, all of which have shown potentials and provides several emerging AD targets from human genetics and multi-omics findings.},
  timestamp = {2025-04-16T05:27:53Z}
}

@article{radford2018improving,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year = {2018},
  publisher = {San Francisco, CA, USA},
  timestamp = {2025-03-23T10:26:05Z}
}

@misc{radford2021learning,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.00020},
  urldate = {2025-05-03},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  timestamp = {2025-05-03T10:37:04Z}
}

@misc{rahman2024modular,
  ids = {rahman2024modulara},
  title = {Modular {{Learning}} of {{Deep Causal Generative Models}} for {{High-dimensional Causal Inference}}},
  author = {Rahman, Md Musfiqur and Kocaoglu, Murat},
  year = {2024},
  month = oct,
  number = {arXiv:2401.01426},
  eprint = {2401.01426},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01426},
  urldate = {2025-03-22},
  abstract = {Sound and complete algorithms have been proposed to compute identifiable causal queries using the causal structure and data. However, most of these algorithms assume accurate estimation of the data distribution, which is impractical for high-dimensional variables such as images. On the other hand, modern deep generative architectures can be trained to sample from high-dimensional distributions. However, training these networks are typically very costly. Thus, it is desirable to leverage pre-trained models to answer causal queries using such high-dimensional data. To address this, we propose modular training of deep causal generative models that not only makes learning more efficient, but also allows us to utilize large, pre-trained conditional generative models. To the best of our knowledge, our algorithm, Modular-DCM is the first algorithm that, given the causal structure, uses adversarial training to learn the network weights, and can make use of pre-trained models to provably sample from any identifiable causal query in the presence of latent confounders. With extensive experiments on the Colored-MNIST dataset, we demonstrate that our algorithm outperforms the baselines. We also show our algorithm's convergence on the COVIDx dataset and its utility with a causal invariant prediction problem on CelebA-HQ.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Information Theory,Statistics - Machine Learning,Statistics - Methodology},
  annotation = {TLDR: The algorithm, Modular-DCM is the first algorithm that, given the causal structure, uses adversarial training to learn the network weights, and can make use of pre-trained models to provably sample from any identifiable causal query in the presence of latent confounders.},
  timestamp = {2025-03-25T06:53:40Z}
}

@article{rai2020explainable,
  title = {Explainable {{AI}}: From Black Box to Glass Box},
  shorttitle = {Explainable {{AI}}},
  author = {Rai, Arun},
  year = {2020},
  month = jan,
  journal = {Journal of the Academy of Marketing Science},
  volume = {48},
  number = {1},
  pages = {137--141},
  issn = {1552-7824},
  doi = {10.1007/s11747-019-00710-5},
  urldate = {2025-05-17},
  langid = {english},
  annotation = {TLDR: This special issue profiles how technological innovations are exerting a transformative force on the practice and academic discipline of marketing and suggests how users with an effective explanation for the AI system's behavior can enhance their trust in the system.},
  timestamp = {2025-05-17T12:21:49Z}
}

@article{rajabi2024knowledgegraphbased,
  title = {Knowledge-Graph-Based Explainable {{AI}}: {{A}} Systematic Review},
  shorttitle = {Knowledge-Graph-Based Explainable {{AI}}},
  author = {Rajabi, Enayat and Etminani, Kobra},
  year = {2024},
  month = aug,
  journal = {Journal of Information Science},
  volume = {50},
  number = {4},
  pages = {1019--1029},
  issn = {0165-5515},
  doi = {10.1177/01655515221112844},
  abstract = {In recent years, knowledge graphs (KGs) have been widely applied in various domains for different purposes. The semantic model of KGs can represent knowledge through a hierarchical structure based on classes of entities, their properties, and their relationships. The construction of large KGs can enable the integration of heterogeneous information sources and help Artificial Intelligence (AI) systems be more explainable and interpretable. This systematic review examines a selection of recent publications to understand how KGs are currently being used in eXplainable AI systems. To achieve this goal, we design a framework and divide the use of KGs into four categories: extracting features, extracting relationships, constructing KGs, and KG reasoning. We also identify where KGs are mostly used in eXplainable AI systems (pre-model, in-model, and post-model) according to the aforementioned categories. Based on our analysis, KGs have been mainly used in pre-model XAI for feature and relation extraction. They were also utilised for inference and reasoning in post-model XAI. We found several studies that leveraged KGs to explain the XAI models in the healthcare domain.},
  langid = {english},
  pmcid = {PMC11316662},
  pmid = {39135903},
  keywords = {artificial intelligence,explainable AI,Knowledge graph,systematic review},
  annotation = {TLDR: KGs have been mainly used in pre- model XAI for feature and relation extraction and were also utilised for inference and reasoning in post-model XAI, and several studies that leveraged KGs to explain the XAI models in the healthcare domain were found.},
  timestamp = {2025-06-12T13:32:38Z}
}

@inproceedings{rajashekar2024humanalgorithmic,
  title = {Human-{{Algorithmic Interaction Using}} a {{Large Language Model-Augmented Artificial Intelligence Clinical Decision Support System}}},
  booktitle = {Proceedings of the 2024 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rajashekar, Niroop Channa and Shin, Yeo Eun and Pu, Yuan and Chung, Sunny and You, Kisung and Giuffre, Mauro and Chan, Colleen E and Saarinen, Theo and Hsiao, Allen and Sekhon, Jasjeet and Wong, Ambrose H and Evans, Leigh V and Kizilcec, Rene F. and Laine, Loren and Mccall, Terika and Shung, Dennis},
  year = {2024},
  month = may,
  series = {{{CHI}} '24},
  pages = {1--20},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3613904.3642024},
  urldate = {2025-04-02},
  abstract = {Integration of artificial intelligence (AI) into clinical decision support systems (CDSS) poses a socio-technological challenge that is impacted by usability, trust, and human-computer interaction (HCI). AI-CDSS interventions have shown limited benefit in clinical outcomes, which may be due to insufficient understanding of how health-care providers interact with AI systems. Large language models (LLMs) have the potential to enhance AI-CDSS, but haven't been studied in either simulated or real-world clinical scenarios. We present findings from a randomized controlled trial deploying AI-CDSS for the management of upper gastrointestinal bleeding (UGIB) with and without an LLM interface within realistic clinical simulations for physician and medical student participants. We find evidence that LLM augmentation improves ease-of-use, that LLM-generated responses with citations improve trust, and HCI varies based on clinical expertise. Qualitative themes from interviews suggest the perception of LLM-augmented AI-CDSS as a team-member used to confirm initial clinical intuitions and help evaluate borderline decisions.},
  isbn = {979-8-4007-0330-0},
  annotation = {TLDR: Evidence is found that LLM augmentation improves ease-of-use, that LLM-generated responses with citations improve trust, and HCI varies based on clinical expertise.},
  timestamp = {2025-04-02T14:54:59Z}
}

@article{rajpal2023xaimethylmarker,
  title = {{{XAI-MethylMarker}}: {{Explainable AI}} Approach for Biomarker Discovery for Breast Cancer Subtype Classification Using Methylation Data},
  shorttitle = {{{XAI-MethylMarker}}},
  author = {Rajpal, Sheetal and Rajpal, Ankit and Saggar, Arpita and Vaid, Ashok K. and Kumar, Virendra and Agarwal, Manoj and Kumar, Naveen},
  year = {2023},
  month = sep,
  journal = {Expert Systems with Applications},
  volume = {225},
  pages = {120130},
  publisher = {Elsevier BV},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2023.120130},
  urldate = {2025-07-23},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  timestamp = {2025-07-23T12:13:35Z}
}

@article{rane2023explainable,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}) in Healthcare: {{Interpretable Models}} for {{Clinical Decision Support}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}}) in Healthcare},
  author = {Rane, Nitin and Choudhary, Saurabh and Rane, Jayesh},
  year = {2023},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4637897},
  urldate = {2025-05-28},
  langid = {english},
  timestamp = {2025-05-28T07:16:00Z}
}

@article{reddy2022explainability,
  title = {Explainability and Artificial Intelligence in Medicine},
  author = {Reddy, Sandeep},
  year = {2022},
  month = apr,
  journal = {The Lancet Digital Health},
  volume = {4},
  number = {4},
  pages = {e214-e215},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(22)00029-2},
  urldate = {2025-05-02},
  langid = {english},
  pmid = {35337639},
  annotation = {TLDR: Criticism of explainable AI methods has been growing in recent years, but there seems to be astonishingly little scrutiny of what led to the need for explainableAI: deep learning models.},
  timestamp = {2025-05-02T14:00:48Z}
}

@article{reddy2023navigating,
  title = {Navigating the {{AI}} Revolution: The Case for Precise Regulation in Health Care},
  author = {Reddy, Sandeep},
  year = {2023},
  journal = {Journal of medical Internet research},
  volume = {25},
  pages = {e49989},
  publisher = {JMIR Publications Toronto, Canada},
  timestamp = {2025-04-16T03:07:52Z}
}

@article{reddy2025long,
  title = {The Long Road Ahead: Navigating Obstacles and Building Bridges for Clinical Integration of Artificial Intelligence Technologies},
  shorttitle = {The Long Road Ahead},
  author = {Reddy, Sandeep and Shaikh, Sameer},
  year = {2025},
  month = mar,
  journal = {Journal of Medical Artificial Intelligence},
  volume = {8},
  number = {0},
  publisher = {AME Publishing Company},
  issn = {2617-2496},
  doi = {10.21037/jmai-24-148},
  urldate = {2025-04-12},
  abstract = {The long road ahead: navigating obstacles and building bridges for clinical integration of artificial intelligence technologies},
  langid = {english},
  timestamp = {2025-04-12T04:30:25Z}
}

@misc{redirecting,
  title = {Redirecting},
  urldate = {2025-04-12},
  howpublished = {https://linkinghub.elsevier.com/retrieve/pii/S1076633222005165},
  timestamp = {2025-04-12T07:18:13Z}
}

@misc{reinhold2021structural,
  title = {A {{Structural Causal Model}} for {{MR Images}} of {{Multiple Sclerosis}}},
  author = {Reinhold, Jacob C. and Carass, Aaron and Prince, Jerry L.},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2103.03158},
  urldate = {2025-06-13},
  abstract = {Precision medicine involves answering counterfactual questions such as "Would this patient respond better to treatment A or treatment B?" These types of questions are causal in nature and require the tools of causal inference to be answered, e.g., with a structural causal model (SCM). In this work, we develop an SCM that models the interaction between demographic information, disease covariates, and magnetic resonance (MR) images of the brain for people with multiple sclerosis. Inference in the SCM generates counterfactual images that show what an MR image of the brain would look like if demographic or disease covariates are changed. These images can be used for modeling disease progression or used for image processing tasks where controlling for confounders is necessary.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Applications (stat.AP),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,FOS: Electrical engineering electronic engineering information engineering,Image and Video Processing (eess.IV),Machine Learning (cs.LG)},
  timestamp = {2025-06-13T03:24:59Z}
}

@inproceedings{Reklos2022MediCauseCR,
  ids = {reklos2022medicause},
  title = {{{MediCause}}: {{Causal}} Relation Modelling and Extraction from Medical Publications},
  shorttitle = {{{MediCause}}},
  booktitle = {{{TEXT2KG}}/{{MK}}@{{ESWC}}},
  author = {Reklos, Ioannis and {Mero{\~n}o-Pe{\~n}uela}, Albert},
  year = {2022},
  urldate = {2025-04-03},
  abstract = {Causal relations are one of the most important types of information that can be extracted from medical publications. Therefore, the automated extraction of such relations from medical text, and the development of ontologies for representing them, are active fields of research. Causal relation extraction is typically decomposed into causal sentence detection, entity recognition, and relation extraction. This study addresses the entity recognition sub-task, which remains largely unsolved since existing ontological models do not capture the various entities involved in causal relations, and datasets annotated with such entities are missing. Therefore, here we propose MediCause, an ontological model for entities involved in causal relations, and a novel dataset using it to annotate 1,202 causal sentences from existing datasets. We evaluate MediCause by training various BERT models that can recognize and label the entities in unseen texts, and we find that a BioBERT-large model fine-tuned with our dataset is the best model at this task (macro-averaged F1-score of 0.844). We also use MediCause to annotate entities in causal sentences from unseen, recent publications, and have experts evaluate them with encouraging results.},
  timestamp = {2025-04-03T02:44:53Z}
}

@inproceedings{ren2022hybrid,
  title = {A Hybrid Medical Causal Inference Platform Based on Data Lake},
  booktitle = {International Conference on Health Information Science},
  author = {Ren, Peng and Liu, Xingyue and Zheng, Shuxin and Liao, Lijun and Li, Xin and Lu, Ligong and Wang, Xia and Wang, Ruoyu and Sheng, Ming},
  year = {2022},
  pages = {136--144},
  publisher = {Springer},
  timestamp = {2025-04-15T16:23:27Z}
}

@article{ren2025smallmolecule,
  title = {A Small-Molecule {{TNIK}} Inhibitor Targets Fibrosis in Preclinical and Clinical Models},
  author = {Ren, Feng and Aliper, Alex and Chen, Jian and Zhao, Heng and Rao, Sujata and Kuppe, Christoph and Ozerov, Ivan V. and Zhang, Man and Witte, Klaus and Kruse, Chris and Aladinskiy, Vladimir and Ivanenkov, Yan and Polykovskiy, Daniil and Fu, Yanyun and Babin, Eugene and Qiao, Junwen and Liang, Xing and Mou, Zhenzhen and Wang, Hui and Pun, Frank W. and {Torres-Ayuso}, Pedro and Veviorskiy, Alexander and Song, Dandan and Liu, Sang and Zhang, Bei and Naumov, Vladimir and Ding, Xiaoqiang and Kukharenko, Andrey and Izumchenko, Evgeny and Zhavoronkov, Alex},
  year = {2025},
  month = jan,
  journal = {Nature Biotechnology},
  volume = {43},
  number = {1},
  pages = {63--75},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-024-02143-0},
  urldate = {2025-05-04},
  abstract = {Idiopathic pulmonary fibrosis (IPF) is an aggressive interstitial lung disease with a high mortality rate. Putative drug targets in IPF have failed to translate into effective therapies at the clinical level. We identify TRAF2- and NCK-interacting kinase (TNIK) as an anti-fibrotic target using a predictive artificial intelligence (AI) approach. Using AI-driven methodology, we generated INS018\_055, a small-molecule TNIK inhibitor, which exhibits desirable drug-like properties and anti-fibrotic activity across different organs in vivo through oral, inhaled or topical administration. INS018\_055 possesses anti-inflammatory effects in addition to its anti-fibrotic profile, validated in multiple in vivo studies. Its safety and tolerability as well as pharmacokinetics were validated in a randomized, double-blinded, placebo-controlled phase I clinical trial (NCT05154240) involving 78 healthy participants. A separate phase I trial in China, CTR20221542, also demonstrated comparable safety and pharmacokinetic profiles. This work was completed in roughly 18 months from target discovery to preclinical candidate nomination and demonstrates the capabilities of our generative AI-driven drug-discovery pipeline.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Drug development,Machine learning,Respiratory tract diseases},
  annotation = {TLDR: INS018\_055, a small-molecule TNIK inhibitor, is generated, which exhibits desirable drug-like properties and anti-fibrotic activity across different organs in vivo through oral, inhaled or topical administration and demonstrates the capabilities of the generative AI-driven drug-discovery pipeline.},
  timestamp = {2025-05-04T13:44:35Z}
}

@article{repetto2024precision,
  title = {Precision {{Oncology}}: 2024 in {{Review}}},
  shorttitle = {Precision {{Oncology}}},
  author = {Repetto, Matteo and Fernandez, Nicole and Drilon, Alexander and Chakravarty, Debyani},
  year = {2024},
  month = dec,
  journal = {Cancer Discovery},
  volume = {14},
  number = {12},
  pages = {2332--2345},
  issn = {2159-8290},
  doi = {10.1158/2159-8290.CD-24-1476},
  abstract = {This article discusses the specific advances made in precision oncology in 2024. We comment on the evolving nature of predictive molecular events used to select patients who will most benefit clinically from treatment. We also discuss advances in the development of strategic treatment regimens for combination therapies, rational drug design of small-molecule inhibitors, and structurally informed drug repurposing.},
  langid = {english},
  pmid = {39618285},
  keywords = {Humans,Medical Oncology,Neoplasms,Precision Medicine},
  annotation = {TLDR: Advances in the development of strategic treatment regimens for combination therapies, rational drug design of small-molecule inhibitors, and structurally informed drug repurposing are discussed.},
  timestamp = {2025-05-24T11:57:22Z}
}

@misc{rezaeian2025explainability,
  title = {Explainability and {{AI Confidence}} in {{Clinical Decision Support Systems}}: {{Effects}} on {{Trust}}, {{Diagnostic Performance}}, and {{Cognitive Load}} in {{Breast Cancer Care}}},
  shorttitle = {Explainability and {{AI Confidence}} in {{Clinical Decision Support Systems}}},
  author = {Rezaeian, Olya and Bayrak, Alparslan Emrah and Asan, Onur},
  year = {2025},
  month = jan,
  number = {arXiv:2501.16693},
  eprint = {2501.16693},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.16693},
  urldate = {2025-05-01},
  abstract = {Artificial Intelligence (AI) has demonstrated potential in healthcare, particularly in enhancing diagnostic accuracy and decision-making through Clinical Decision Support Systems (CDSSs). However, the successful implementation of these systems relies on user trust and reliance, which can be influenced by explainable AI. This study explores the impact of varying explainability levels on clinicians trust, cognitive load, and diagnostic performance in breast cancer detection. Utilizing an interrupted time series design, we conducted a web-based experiment involving 28 healthcare professionals. The results revealed that high confidence scores substantially increased trust but also led to overreliance, reducing diagnostic accuracy. In contrast, low confidence scores decreased trust and agreement while increasing diagnosis duration, reflecting more cautious behavior. Some explainability features influenced cognitive load by increasing stress levels. Additionally, demographic factors such as age, gender, and professional role shaped participants' perceptions and interactions with the system. This study provides valuable insights into how explainability impact clinicians' behavior and decision-making. The findings highlight the importance of designing AI-driven CDSSs that balance transparency, usability, and cognitive demands to foster trust and improve integration into clinical workflows.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {TLDR: The results revealed that high confidence scores substantially increased trust but also led to overreliance, reducing diagnostic accuracy, and low confidence scores decreased trust and agreement while increasing diagnosis duration, reflecting more cautious behavior.},
  timestamp = {2025-05-01T12:21:08Z}
}

@misc{ribeiro2016why,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  number = {arXiv:1602.04938},
  eprint = {1602.04938},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.04938},
  urldate = {2025-03-18},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-18T01:56:26Z}
}

@article{ribeiro2018anchors,
  title = {Anchors: {{High-Precision Model-Agnostic Explanations}}},
  shorttitle = {Anchors},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.11491},
  urldate = {2025-03-26},
  abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {interpretability},
  annotation = {TLDR: This work introduces a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions, and proposes an algorithm to efficiently compute these explanations for any black-box model with high probability guarantees.},
  timestamp = {2025-03-26T15:08:17Z}
}

@article{richens2020improving,
  title = {Improving the Accuracy of Medical Diagnosis with Causal Machine Learning},
  author = {Richens, Jonathan G. and Lee, Ciar{\'a}n M. and Johri, Saurabh},
  year = {2020},
  month = aug,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {3923},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17419-7},
  urldate = {2025-05-21},
  abstract = {Machine learning promises to revolutionize clinical decision making and diagnosis. In medical diagnosis a doctor aims to explain a patient's symptoms by determining the diseases causing them. However, existing machine learning approaches to diagnosis are purely associative, identifying diseases that are strongly correlated with a patients symptoms. We show that this inability to disentangle correlation from causation can result in sub-optimal or dangerous diagnoses. To overcome this, we reformulate diagnosis as a counterfactual inference task and derive counterfactual diagnostic algorithms. We compare our counterfactual algorithms to the standard associative algorithm and 44 doctors using a test set of clinical vignettes. While the associative algorithm achieves an accuracy placing in the top 48\% of doctors in our cohort, our counterfactual algorithm places in the top 25\% of doctors, achieving expert clinical accuracy. Our results show that causal reasoning is a vital missing ingredient for applying machine learning to medical diagnosis.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Diagnosis,Predictive medicine,Statistics},
  annotation = {TLDR: The authors reformulate diagnosis as a counterfactual inference task and derive new counterfactUAL diagnostic algorithms, showing that causal reasoning is a vital missing ingredient for applying machine learning to medical diagnosis.},
  timestamp = {2025-05-21T01:11:04Z}
}

@article{richmond2022mendelian,
  title = {Mendelian {{Randomization}}: {{Concepts}} and {{Scope}}},
  shorttitle = {Mendelian {{Randomization}}},
  author = {Richmond, Rebecca C. and Davey Smith, George},
  year = {2022},
  month = jan,
  journal = {Cold Spring Harbor Perspectives in Medicine},
  volume = {12},
  number = {1},
  pages = {a040501},
  issn = {2157-1422},
  doi = {10.1101/cshperspect.a040501},
  abstract = {Mendelian randomization (MR) is a method of studying the causal effects of modifiable exposures (i.e., potential risk factors) on health, social, and economic outcomes using genetic variants associated with the specific exposures of interest. MR provides a more robust understanding of the influence of these exposures on outcomes because germline genetic variants are randomly inherited from parents to offspring and, as a result, should not be related to potential confounding factors that influence exposure-outcome associations. The genetic variant can therefore be used as a tool to link the proposed risk factor and outcome, and to estimate this effect with less confounding and bias than conventional epidemiological approaches. We describe the scope of MR, highlighting the range of applications being made possible as genetic data sets and resources become larger and more freely available. We outline the MR approach in detail, covering concepts, assumptions, and estimation methods. We cover some common misconceptions, provide strategies for overcoming violation of assumptions, and discuss future prospects for extending the clinical applicability, methodological innovations, robustness, and generalizability of MR findings.},
  langid = {english},
  pmcid = {PMC8725623},
  pmid = {34426474},
  keywords = {Causality,Mendelian Randomization Analysis,Risk Factors},
  annotation = {TLDR: The scope of MR is described, highlighting the range of applications being made possible as genetic data sets and resources become larger and more freely available, and the MR approach is outlined in detail, covering concepts, assumptions, and estimation methods.},
  timestamp = {2025-06-13T03:04:57Z}
}

@article{rodis2024multimodal,
  title = {Multimodal {{Explainable Artificial Intelligence}}: {{A Comprehensive Review}} of {{Methodological Advances}} and {{Future Research Directions}}},
  shorttitle = {Multimodal {{Explainable Artificial Intelligence}}},
  author = {Rodis, Nikolaos and Sardianos, Christos and {Radoglou-Grammatikis}, Panagiotis and Sarigiannidis, Panagiotis and Varlamis, Iraklis and Papadopoulos, Georgios Th.},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {159794--159820},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3467062},
  urldate = {2025-04-20},
  abstract = {Despite the fact that Artificial Intelligence (AI) has boosted the achievement of remarkable results across numerous data analysis tasks, however, this is typically accompanied by a significant shortcoming in the exhibited transparency and trustworthiness of the developed systems. In order to address the latter challenge, the so-called eXplainable AI (XAI) research field has emerged, which aims, among others, at estimating meaningful explanations regarding the employed model's reasoning process. The current study focuses on systematically analyzing the recent advances in the area of Multimodal XAI (MXAI), which comprises methods that involve multiple modalities in the primary prediction and explanation tasks. In particular, the relevant AI-boosted prediction tasks and publicly available datasets used for learning/evaluating explanations in multimodal scenarios are initially described. Subsequently, a systematic and comprehensive analysis of the MXAI methods of the literature is provided, taking into account the following key criteria: a) The number of the involved modalities (in the employed AI module), b) The processing stage at which explanations are generated, and c) The type of the adopted methodology (i.e. the actual mechanism and mathematical formalization) for producing explanations. Then, a thorough analysis of the metrics used for MXAI methods' evaluation is performed. Finally, an extensive discussion regarding the current challenges and future research directions is provided.},
  keywords = {Artificial intelligence,Data models,deep learning,Deep learning,evaluation,Explainable AI,explanation,Image segmentation,multimodal explainable artificial intelligence,neural networks,Neural networks,Predictive models,Videos,Visualization},
  annotation = {TLDR: This study focuses on systematically analyzing the recent advances in the area of Multimodal XAI (MXAI), which comprises methods that involve multiple modalities in the primary prediction and explanation tasks.},
  timestamp = {2025-04-20T08:45:38Z}
}

@article{rosch1973natural,
  title = {Natural Categories},
  author = {Rosch, Eleanor},
  year = {1973},
  journal = {Cognitive Psychology},
  volume = {4},
  number = {3},
  pages = {328--350},
  publisher = {Elsevier},
  timestamp = {2025-09-05T16:41:56Z}
}

@incollection{rosch1978principles,
  title = {Principles of Categorization},
  booktitle = {Cognition and Categorization},
  author = {Rosch, Eleanor},
  editor = {Rosch, Eleanor and Lloyd, Barbara B.},
  year = {1978},
  pages = {27--48},
  publisher = {Lawrence Erlbaum},
  address = {Hillsdale, NJ},
  timestamp = {2025-09-05T16:41:56Z}
}

@article{rosenbacke2024how,
  ids = {rosenbacke2024explainable},
  title = {How {{Explainable Artificial Intelligence Can Increase}} or {{Decrease Clinicians}}' {{Trust}} in {{AI Applications}} in {{Health Care}}: {{Systematic Review}}},
  shorttitle = {How {{Explainable Artificial Intelligence Can Increase}} or {{Decrease Clinicians}}' {{Trust}} in {{AI Applications}} in {{Health Care}}},
  author = {Rosenbacke, Rikard and Melhus, {\AA}sa and McKee, Martin and Stuckler, David},
  year = {2024},
  month = oct,
  journal = {JMIR AI},
  volume = {3},
  pages = {e53207},
  issn = {2817-1705},
  doi = {10.2196/53207},
  urldate = {2025-08-11},
  abstract = {Background               Artificial intelligence (AI) has significant potential in clinical practice. However, its ``black box'' nature can lead clinicians to question its value. The challenge is to create sufficient trust for clinicians to feel comfortable using AI, but not so much that they defer to it even when it produces results that conflict with their clinical judgment in ways that lead to incorrect decisions. Explainable AI (XAI) aims to address this by providing explanations of how AI algorithms reach their conclusions. However, it remains unclear whether such explanations foster an appropriate degree of trust to ensure the optimal use of AI in clinical practice.                                         Objective               This study aims to systematically review and synthesize empirical evidence on the impact of XAI on clinicians' trust in AI-driven clinical decision-making.                                         Methods               A systematic review was conducted in accordance with PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, searching PubMed and Web of Science databases. Studies were included if they empirically measured the impact of XAI on clinicians' trust using cognition- or affect-based measures. Out of 778 articles screened, 10 met the inclusion criteria. We assessed the risk of bias using standard tools appropriate to the methodology of each paper.                                         Results               The risk of bias in all papers was moderate or moderate to high. All included studies operationalized trust primarily through cognitive-based definitions, with 2 also incorporating affect-based measures. Out of these, 5 studies reported that XAI increased clinicians' trust compared with standard AI, particularly when the explanations were clear, concise, and relevant to clinical practice. In addition, 3 studies found no significant effect of XAI on trust, and the presence of explanations does not automatically improve trust. Notably, 2 studies highlighted that XAI could either enhance or diminish trust, depending on the complexity and coherence of the provided explanations. The majority of studies suggest that XAI has the potential to enhance clinicians' trust in recommendations generated by AI. However, complex or contradictory explanations can undermine this trust. More critically, trust in AI is not inherently beneficial, as AI recommendations are not infallible. These findings underscore the nuanced role of explanation quality and suggest that trust can be modulated through the careful design of XAI systems.                                         Conclusions               Excessive trust in incorrect advice generated by AI can adversely impact clinical accuracy, just as can happen when correct advice is distrusted. Future research should focus on refining both cognitive and affect-based measures of trust and on developing strategies to achieve an appropriate balance in terms of trust, preventing both blind trust and undue skepticism. Optimizing trust in AI systems is essential for their effective integration into clinical practice.},
  langid = {english},
  annotation = {TLDR: The majority of studies suggest that XAI has the potential to enhance clinicians' trust in recommendations generated by AI, however, complex or contradictory explanations can undermine this trust.},
  timestamp = {2025-08-13T09:20:21Z}
}

@misc{rossi2024tace,
  title = {{{TACE}}: {{Tumor-Aware Counterfactual Explanations}}},
  shorttitle = {{{TACE}}},
  author = {Rossi, Eleonora Beatrice and Lopez, Eleonora and Comminiello, Danilo},
  year = {2024},
  month = sep,
  number = {arXiv:2409.13045},
  eprint = {2409.13045},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.13045},
  urldate = {2025-05-21},
  abstract = {The application of deep learning in medical imaging has significantly advanced diagnostic capabilities, enhancing both accuracy and efficiency. Despite these benefits, the lack of transparency in these AI models, often termed "black boxes," raises concerns about their reliability in clinical settings. Explainable AI (XAI) aims to mitigate these concerns by developing methods that make AI decisions understandable and trustworthy. In this study, we propose Tumor Aware Counterfactual Explanations (TACE), a framework designed to generate reliable counterfactual explanations for medical images. Unlike existing methods, TACE focuses on modifying tumor-specific features without altering the overall organ structure, ensuring the faithfulness of the counterfactuals. We achieve this by including an additional step in the generation process which allows to modify only the region of interest (ROI), thus yielding more reliable counterfactuals as the rest of the organ remains unchanged. We evaluate our method on mammography images and brain MRI. We find that our method far exceeds existing state-of-the-art techniques in quality, faithfulness, and generation speed of counterfactuals. Indeed, more faithful explanations lead to a significant improvement in classification success rates, with a 10.69\% increase for breast cancer and a 98.02\% increase for brain tumors. The code of our work is available at https://github.com/ispamm/TACE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {TLDR: This study proposes Tumor Aware Counterfactual Explanations (TACE), a framework designed to generate reliable counterfactual explanations for medical images that far exceeds existing state-of-the-art techniques in quality, faithfulness, and generation speed of counterfactuals.},
  timestamp = {2025-05-21T00:44:26Z}
}

@article{rubin1974estimating,
  title = {Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies.},
  author = {Rubin, Donald B},
  year = {1974},
  journal = {Journal of educational Psychology},
  volume = {66},
  number = {5},
  pages = {688},
  publisher = {American Psychological Association},
  timestamp = {2025-06-08T04:05:52Z}
}

@article{rubin2005causal,
  ids = {rubin2005causala},
  title = {Causal Inference Using Potential Outcomes: {{Design}}, Modeling, Decisions},
  shorttitle = {Causal {{Inference Using Potential Outcomes}}},
  author = {Rubin, Donald B},
  year = {2005},
  journal = {Journal of the American statistical Association},
  volume = {100},
  number = {469},
  pages = {322--331},
  publisher = {Taylor \& Francis},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214504000001880},
  urldate = {2025-04-06},
  langid = {english},
  timestamp = {2025-06-08T04:07:43Z}
}

@incollection{rubin2010causal,
  title = {Causal {{Inference}}},
  booktitle = {International {{Encyclopedia}} of {{Education}}},
  author = {Rubin, D.B.},
  year = {2010},
  pages = {66--71},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-08-044894-7.01313-0},
  urldate = {2025-03-20},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  isbn = {978-0-08-044894-7},
  langid = {english},
  timestamp = {2025-03-20T01:35:03Z}
}

@misc{ruipathruipath,
  title = {{{RuiPath}}},
  author = {RuiPath},
  journal = {RuiPath},
  urldate = {2025-05-04},
  howpublished = {https://ruipath.com/},
  langid = {english},
  timestamp = {2025-05-04T05:27:54Z}
}

@inproceedings{runyu2022intelligent,
  title = {An {{Intelligent And Transparent Inference}}: {{Spiking Neural Network For Causal Reasoning}}},
  shorttitle = {An {{Intelligent And Transparent Inference}}},
  booktitle = {2022 19th {{International Computer Conference}} on {{Wavelet Active Media Technology}} and {{Information Processing}} ({{ICCWAMTIP}})},
  author = {Runyu, Li and Xiaoling, Luo and Jun, Wang},
  year = {2022},
  month = dec,
  pages = {1--5},
  issn = {2576-8964},
  doi = {10.1109/ICCWAMTIP56608.2022.10016487},
  urldate = {2025-04-03},
  abstract = {In light of mining large amounts of data, artificial intelligence (AI) has learned a very strong correlation between objects. However, its limitations lie in that it can't summarize the causality between objects like human beings and it forms the blind box association mechanism. In this paper, we address these limitations and make the contributions: We propose an implementation of causal reasoning based on spiking neural network (SNN), which simulates causality using information processing with spiking activities. And the spike-timing-dependent plastic rules (STDP) is utilized as a method of causal reasoning, which is based on the topological structure of causal graph and can make the process visible. Through experiments, our model completes the inference of causal ladder proposed by Judea Pearl, and experiments prove that it can complete more complex causal reasoning under the condition of integrating multiple causality.},
  keywords = {Causal reasoning,Cognition,Correlation,Heuristic algorithms,Inference algorithms,Information processing,Media,Neural networks,Spiking neural network,STDP,Transparent inference},
  annotation = {TLDR: An implementation of causal reasoning based on spiking neural network (SNN), which simulates causality using information processing with spiking activities and the spike-timing-dependent plastic rules (STDP) is utilized as a method of causal Reasoning, which is based on the topological structure of causal graph.},
  timestamp = {2025-04-03T02:13:47Z}
}

@inproceedings{rust2024predicting,
  title = {Predicting {{Hemodynamic}} and {{Pulmonary Decompensation}} with {{Deep Neural Networks}}: {{Performance}} and {{Explainability}}},
  shorttitle = {Predicting {{Hemodynamic}} and {{Pulmonary Decompensation}} with {{Deep Neural Networks}}},
  booktitle = {2024 46th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Rust, Johannes and Mandel, Christian and Stich, Kathrin and Autexier, Serge},
  year = {2024},
  month = jul,
  pages = {1--7},
  publisher = {IEEE},
  address = {Orlando, FL, USA},
  doi = {10.1109/EMBC53108.2024.10782095},
  urldate = {2025-04-10},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-7149-9},
  timestamp = {2025-04-10T13:31:27Z}
}

@article{saarela2024recent,
  title = {Recent {{Applications}} of {{Explainable AI}} ({{XAI}}): {{A Systematic Literature Review}}},
  shorttitle = {Recent {{Applications}} of {{Explainable AI}} ({{XAI}})},
  author = {Saarela, Mirka and Podgorelec, Vili},
  year = {2024},
  month = jan,
  journal = {Applied Sciences},
  volume = {14},
  number = {19},
  pages = {8884},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app14198884},
  urldate = {2025-04-12},
  abstract = {This systematic literature review employs the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to investigate recent applications of explainable AI (XAI) over the past three years. From an initial pool of 664 articles identified through the Web of Science database, 512 peer-reviewed journal articles met the inclusion criteria---namely, being recent, high-quality XAI application articles published in English---and were analyzed in detail. Both qualitative and quantitative statistical techniques were used to analyze the identified articles: qualitatively by summarizing the characteristics of the included studies based on predefined codes, and quantitatively through statistical analysis of the data. These articles were categorized according to their application domains, techniques, and evaluation methods. Health-related applications were particularly prevalent, with a strong focus on cancer diagnosis, COVID-19 management, and medical imaging. Other significant areas of application included environmental and agricultural management, industrial optimization, cybersecurity, finance, transportation, and entertainment. Additionally, emerging applications in law, education, and social care highlight XAI's expanding impact. The review reveals a predominant use of local explanation methods, particularly SHAP and LIME, with SHAP being favored for its stability and mathematical guarantees. However, a critical gap in the evaluation of XAI results is identified, as most studies rely on anecdotal evidence or expert opinion rather than robust quantitative metrics. This underscores the urgent need for standardized evaluation frameworks to ensure the reliability and effectiveness of XAI applications. Future research should focus on developing comprehensive evaluation standards and improving the interpretability and stability of explanations. These advancements are essential for addressing the diverse demands of various application domains while ensuring trust and transparency in AI systems.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {applications,convolutional neural network,deep learning,explainable artificial intelligence,interpretable machine learning,model-agnostic explanations,post-hoc explanations},
  annotation = {TLDR: A critical gap in the evaluation of XAI results is identified, as most studies rely on anecdotal evidence or expert opinion rather than robust quantitative metrics, which underscores the urgent need for standardized evaluation frameworks to ensure the reliability and effectiveness of XAI applications.},
  timestamp = {2025-04-12T06:37:02Z}
}

@misc{sadeghi2023brief,
  title = {A {{Brief Review}} of {{Explainable Artificial Intelligence}} in {{Healthcare}}},
  author = {Sadeghi, Zahra and Alizadehsani, Roohallah and Cifci, Mehmet Akif and Kausar, Samina and Rehman, Rizwan and Mahanta, Priyakshi and Bora, Pranjal Kumar and Almasri, Ammar and Alkhawaldeh, Rami S. and Hussain, Sadiq and Alatas, Bilal and Shoeibi, Afshin and Moosaei, Hossein and Hladik, Milan and Nahavandi, Saeid and Pardalos, Panos M.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01543},
  eprint = {2304.01543},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.01543},
  urldate = {2025-05-28},
  abstract = {XAI refers to the techniques and methods for building AI applications which assist end users to interpret output and predictions of AI models. Black box AI applications in high-stakes decision-making situations, such as medical domain have increased the demand for transparency and explainability since wrong predictions may have severe consequences. Model explainability and interpretability are vital successful deployment of AI models in healthcare practices. AI applications' underlying reasoning needs to be transparent to clinicians in order to gain their trust. This paper presents a systematic review of XAI aspects and challenges in the healthcare domain. The primary goals of this study are to review various XAI methods, their challenges, and related machine learning models in healthcare. The methods are discussed under six categories: Features-oriented methods, global methods, concept models, surrogate models, local pixel-based methods, and human-centric methods. Most importantly, the paper explores XAI role in healthcare problems to clarify its necessity in safety-critical applications. The paper intends to establish a comprehensive understanding of XAI-related applications in the healthcare field by reviewing the related experimental results. To facilitate future research for filling research gaps, the importance of XAI models from different viewpoints and their limitations are investigated.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {TLDR: XAI role in healthcare problems is explored to clarify its necessity in safety-critical applications and the importance of XAI models from different viewpoints and their limitations are investigated.},
  timestamp = {2025-05-28T03:01:32Z}
}

@article{sadeghi2024review,
  title = {A Review of {{Explainable Artificial Intelligence}} in Healthcare},
  author = {Sadeghi, Zahra and Alizadehsani, Roohallah and Cifci, Mehmet Akif and Kausar, Samina and Rehman, Rizwan and Mahanta, Priyakshi and Bora, Pranjal Kumar and Almasri, Ammar and Alkhawaldeh, Rami S. and Hussain, Sadiq and Alatas, Bilal and Shoeibi, Afshin and Moosaei, Hossein and Hlad{\'i}k, Milan and Nahavandi, Saeid and Pardalos, Panos M.},
  year = {2024},
  month = aug,
  journal = {Computers and Electrical Engineering},
  volume = {118},
  pages = {109370},
  issn = {0045-7906},
  doi = {10.1016/j.compeleceng.2024.109370},
  urldate = {2025-05-28},
  abstract = {Explainable Artificial Intelligence (XAI) encompasses the strategies and methodologies used in constructing AI systems that enable end-users to comprehend and interpret the outputs and predictions made by AI models. The increasing deployment of opaque AI applications in high-stakes fields, particularly healthcare, has amplified the need for clarity and explainability. This stems from the potential high-impact consequences of erroneous AI predictions in such critical sectors. The effective integration of AI models in healthcare hinges on the capacity of these models to be both explainable and interpretable. Gaining the trust of healthcare professionals necessitates AI applications to be transparent about their decision-making processes and underlying logic. Our paper conducts a systematic review of the various facets and challenges of XAI within the healthcare realm. It aims to dissect a range of XAI methodologies and their applications in healthcare, categorizing them into six distinct groups: feature-oriented methods, global methods, concept models, surrogate models, local pixel-based methods, and human-centric approaches. Specifically, this study focuses on the significance of XAI in addressing healthcare-related challenges, underscoring its vital role in safety-critical scenarios. Our objective is to provide an exhaustive exploration of XAI's applications in healthcare, alongside an analysis of relevant experimental outcomes, thereby fostering a holistic understanding of XAI's role and potential in this critical domain.},
  keywords = {Explainable AI,Healthcare,Interpretability,Transparent AI},
  timestamp = {2025-05-28T03:08:02Z}
}

@misc{sadia2025causalged,
  title = {{{CausalGeD}}: {{Blending Causality}} and {{Diffusion}} for {{Spatial Gene Expression Generation}}},
  shorttitle = {{{CausalGeD}}},
  author = {Sadia, Rabeya Tus and Ahamed, Md Atik and Cheng, Qiang},
  year = {2025},
  month = feb,
  number = {arXiv:2502.07751},
  eprint = {2502.07751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.07751},
  urldate = {2025-04-17},
  abstract = {The integration of single-cell RNA sequencing (scRNA-seq) and spatial transcriptomics (ST) data is crucial for understanding gene expression in spatial context. Existing methods for such integration have limited performance, with structural similarity often below 60{\textbackslash}\%, We attribute this limitation to the failure to consider causal relationships between genes. We present CausalGeD, which combines diffusion and autoregressive processes to leverage these relationships. By generalizing the Causal Attention Transformer from image generation to gene expression data, our model captures regulatory mechanisms without predefined relationships. Across 10 tissue datasets, CausalGeD outperformed state-of-the-art baselines by 5- 32{\textbackslash}\% in key metrics, including Pearson's correlation and structural similarity, advancing both technical and biological insights.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Genomics},
  annotation = {TLDR: CausalGeD, which combines diffusion and autoregressive processes to leverage causal relationships between genes, and outperformed state-of-the-art baselines by 5- 32{\textbackslash}\% in key metrics, advancing both technical and biological insights.},
  timestamp = {2025-04-17T12:00:12Z}
}

@article{saeed2011multiparameter,
  title = {Multiparameter {{Intelligent Monitoring}} in {{Intensive Care II}} ({{MIMIC-II}}): {{A}} Public-Access Intensive Care Unit Database},
  shorttitle = {Multiparameter {{Intelligent Monitoring}} in {{Intensive Care II}} ({{MIMIC-II}})},
  author = {Saeed, Mohammed and Villarroel, Mauricio and Reisner, Andrew T. and Clifford, Gari and Lehman, Li-Wei and Moody, George and Heldt, Thomas and Kyaw, Tin H. and Moody, Benjamin and Mark, Roger G.},
  year = {2011},
  month = may,
  journal = {Critical care medicine},
  volume = {39},
  number = {5},
  pages = {952--960},
  issn = {0090-3493},
  doi = {10.1097/CCM.0b013e31820a92c6},
  urldate = {2025-04-09},
  abstract = {Objective We sought to develop an intensive care unit research database applying automated techniques to aggregate high-resolution diagnostic and therapeutic data from a large, diverse population of adult intensive care unit patients. This freely available database is intended to support epidemiologic research in critical care medicine and serve as a resource to evaluate new clinical decision support and monitoring algorithms. Design Data collection and retrospective analysis. Setting All adult intensive care units (medical intensive care unit, surgical intensive care unit, cardiac care unit, cardiac surgery recovery unit) at a tertiary care hospital. Patients Adult patients admitted to intensive care units between 2001 and 2007. Interventions None. Measurements and Main Results The Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II) database consists of 25,328 intensive care unit stays. The investigators collected detailed information about intensive care unit patient stays, including laboratory data, therapeutic intervention profiles such as vasoactive medication drip rates and ventilator settings, nursing progress notes, discharge summaries, radiology reports, provider order entry data, International Classification of Diseases, 9th Revision codes, and, for a subset of patients, high-resolution vital sign trends and waveforms. Data were automatically deidentified to comply with Health Insurance Portability and Accountability Act standards and integrated with relational database software to create electronic intensive care unit records for each patient stay. The data were made freely available in February 2010 through the Internet along with a detailed user's guide and an assortment of data processing tools. The overall hospital mortality rate was 11.7\%, which varied by critical care unit. The median intensive care unit length of stay was 2.2 days (interquartile range, 1.1--4.4 days). According to the primary International Classification of Diseases, 9th Revision codes, the following disease categories each comprised at least 5\% of the case records: diseases of the circulatory system (39.1\%); trauma (10.2\%); diseases of the digestive system (9.7\%); pulmonary diseases (9.0\%); infectious diseases (7.0\%); and neoplasms (6.8\%). Conclusions MIMIC-II documents a diverse and very large population of intensive care unit patient stays and contains comprehensive and detailed clinical data, including physiological waveforms and minute-by-minute trends for a subset of records. It establishes a new public-access resource for critical care research, supporting a diverse range of analytic studies spanning epidemiology, clinical decision-rule development, and electronic tool development.},
  pmcid = {PMC3124312},
  pmid = {21283005},
  annotation = {TLDR: MIMIC-II documents a diverse and very large population of intensive care unit patient stays and contains comprehensive and detailed clinical data, including physiological waveforms and minute-by-minute trends for a subset of records.},
  timestamp = {2025-04-09T02:41:34Z}
}

@article{saeed2023explainable,
  title = {Explainable {{AI}} ({{XAI}}): {{A}} Systematic Meta-Survey of Current Challenges and Future Opportunities},
  shorttitle = {Explainable {{AI}} ({{XAI}})},
  author = {Saeed, Waddah and Omlin, Christian},
  year = {2023},
  month = mar,
  journal = {Knowledge-Based Systems},
  volume = {263},
  pages = {110273},
  issn = {09507051},
  doi = {10.1016/j.knosys.2023.110273},
  urldate = {2025-08-28},
  langid = {english},
  annotation = {TLDR: A systematic meta-survey for challenges and future research directions in XAI organized in two themes based on machine learning life cycle's phases: design, development, and deployment and contributes to XAI literature by providing a guide for future exploration in the XAI area.},
  timestamp = {2025-08-28T09:23:45Z}
}

@article{sagi2020explainable,
  title = {Explainable Decision Forest: {{Transforming}} a Decision Forest into an Interpretable Tree},
  shorttitle = {Explainable Decision Forest},
  author = {Sagi, Omer and Rokach, Lior},
  year = {2020},
  month = sep,
  journal = {Information Fusion},
  volume = {61},
  pages = {124--138},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2020.03.013},
  urldate = {2025-03-28},
  abstract = {Decision forests are considered the best practice in many machine learning challenges, mainly due to their superior predictive performance. However, simple models like decision trees may be preferred over decision forests in cases in which the generated predictions must be efficient or interpretable (e.g. in insurance or health-related use cases). This paper presents a novel method for transforming a decision forest into an interpretable decision tree, which aims at preserving the predictive performance of decision forests while enabling efficient classifications that can be understood by humans. This is done by creating a set of rule conjunctions that represent the original decision forest; the conjunctions are then hierarchically organized to form a new decision tree. We evaluate the proposed method on 33 UCI datasets and show that the resulting model usually approximates the ROC AUC gained by random forest while providing an interpretable decision path for each classification.},
  keywords = {Classification Trees,Decision forest,Ensemble learning},
  annotation = {TLDR: A novel method for transforming a decision forest into an interpretable decision tree, which aims at preserving the predictive performance of decision forests while enabling efficient classifications that can be understood by humans is presented.},
  timestamp = {2025-03-28T08:57:05Z}
}

@article{saha2018machine,
  title = {A Machine Learning Approach to Radiogenomics of Breast Cancer: A Study of 922 Subjects and 529 {{DCE-MRI}} Features},
  author = {Saha, Ashirbani and Harowicz, Michael R and Grimm, Lars J and Kim, Connie E and Ghate, Sujata V and Walsh, Ruth and Mazurowski, Maciej A},
  year = {2018},
  journal = {British journal of cancer},
  volume = {119},
  number = {4},
  pages = {508--516},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2025-05-20T23:10:52Z}
}

@article{sahoo2025artificial,
  title = {Artificial {{Intelligence}} in Cancer Epigenomics: A Review on Advances in Pan-Cancer Detection and Precision Medicine},
  shorttitle = {Artificial {{Intelligence}} in Cancer Epigenomics},
  author = {Sahoo, Karishma and Lingasamy, Prakash and Khatun, Masuma and Sudhakaran, Sajitha Lulu and Salumets, Andres and Sundararajan, Vino and Modhukur, Vijayachitra},
  year = {2025},
  month = jun,
  journal = {Epigenetics \& Chromatin},
  volume = {18},
  number = {1},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1756-8935},
  doi = {10.1186/s13072-025-00595-5},
  urldate = {2025-07-23},
  abstract = {Abstract          DNA methylation is a fundamental epigenetic modification that regulates gene expression and maintains genomic stability. Consequently, DNA methylation remains a key biomarker in cancer research, playing a vital role in diagnosis, prognosis, and tailored treatment strategies. Aberrant methylation patterns enable early cancer detection and therapeutic stratification; however, their complex patterns necessitates advanced analytical tools. Recent advances in artificial intelligence (AI) and machine learning (ML), including deep learning networks and graph-based models, have revolutionized cancer epigenomics by enabling rapid, high-resolution analysis of DNA methylation profiles. Moreover, these technologies are accelerating the development of Multi-Cancer Early Detection (MCED) tests, such as GRAIL's Galleri and CancerSEEK, which improve diagnostic accuracy across diverse cancer types. In this review, we explore the synergy between AI and DNA methylation profiling to advance precision oncology. We first examine the role of DNA methylation as a biomarker in cancer, followed by an overview of DNA profiling technologies. We then assess how AI-driven approaches transform clinical practice by enabling early detection and accurate classification. Despite their promise, challenges remain, including limited sensitivity for early-stage cancers, the black-box nature of many AI algorithms, and the need for validation across diverse populations to ensure equitable implementation. Future directions include integrating multi-omics data, developing explainable AI frameworks, and addressing ethical concerns, such as data privacy and algorithmic bias. By overcoming these gaps, AI-powered epigenetic diagnostics can enable earlier detection, more effective treatments, and improved patient outcomes, globally. In summary, this review synthesizes current advancements in the field and envisions a future where AI and epigenomics converge to redefine cancer diagnostics and therapy.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  annotation = {TLDR: A future where AI and epigenomics converge to redefine cancer diagnostics and therapy is visions a future where AI and epigenomics converge to redefine cancer diagnostics and therapy.},
  timestamp = {2025-07-23T13:36:33Z}
}

@article{sahu2022sequence,
  title = {Sequence Determinants of Human Gene Regulatory Elements},
  author = {Sahu, Biswajyoti and Hartonen, Tuomo and Pihlajamaa, P{\"a}ivi and Wei, Bei and Dave, Kashyap and Zhu, Fangjie and Kaasinen, Eevi and Lidschreiber, Katja and Lidschreiber, Michael and Daub, Carsten O. and Cramer, Patrick and Kivioja, Teemu and Taipale, Jussi},
  year = {2022},
  month = mar,
  journal = {Nature Genetics},
  volume = {54},
  number = {3},
  pages = {283--294},
  issn = {1546-1718},
  doi = {10.1038/s41588-021-01009-4},
  abstract = {DNA can determine where and when genes are expressed, but the full set of sequence determinants that control gene expression is unknown. Here, we measured the transcriptional activity of DNA sequences that represent an {\textasciitilde}100 times larger sequence space than the human genome using massively parallel reporter assays (MPRAs). Machine learning models revealed that transcription factors (TFs) generally act in an additive manner with weak grammar and that most enhancers increase expression from a promoter by a mechanism that does not appear to involve specific TF-TF interactions. The enhancers themselves can be classified into three types: classical, closed chromatin and chromatin dependent. We also show that few TFs are strongly active in a cell, with most activities being similar between cell types. Individual TFs can have multiple gene regulatory activities, including chromatin opening and enhancing, promoting and determining transcription start site (TSS) activity, consistent with the view that the TF binding motif is the key atomic unit of gene expression.},
  langid = {english},
  pmcid = {PMC8920891},
  pmid = {35190730},
  keywords = {Binding Sites,Genome Human,Humans,Protein Binding,Regulatory Sequences Nucleic Acid,Transcription Factors},
  annotation = {TLDR: Analysis of massively parallel reporter assays measuring the transcriptional activity of DNA sequences indicates that most transcription factor (TF) activity is additive and does not rely on specific TF--TF interactions.},
  timestamp = {2025-07-05T12:10:07Z}
}

@article{saifullah2024privacyexplainability,
  title = {The Privacy-Explainability Trade-off: Unraveling the Impacts of Differential Privacy and Federated Learning on Attribution Methods},
  shorttitle = {The Privacy-Explainability Trade-Off},
  author = {Saifullah, Saifullah and Mercier, Dominique and Lucieri, Adriano and Dengel, Andreas and Ahmed, Sheraz},
  year = {2024},
  month = jul,
  journal = {Frontiers in Artificial Intelligence},
  volume = {7},
  pages = {1236947},
  issn = {2624-8212},
  doi = {10.3389/frai.2024.1236947},
  urldate = {2025-04-04},
  abstract = {Since the advent of deep learning (DL), the field has witnessed a continuous stream of innovations. However, the translation of these advancements into practical applications has not kept pace, particularly in safety-critical domains where artificial intelligence (AI) must meet stringent regulatory and ethical standards. This is underscored by the ongoing research in eXplainable AI (XAI) and privacy-preserving machine learning (PPML), which seek to address some limitations associated with these opaque and data-intensive models. Despite brisk research activity in both fields, little attention has been paid to their interaction. This work is the first to thoroughly investigate the effects of privacy-preserving techniques on explanations generated by common XAI methods for DL models. A detailed experimental analysis is conducted to quantify the impact of private training on the explanations provided by DL models, applied to six image datasets and five time series datasets across various domains. The analysis comprises three privacy techniques, nine XAI methods, and seven model architectures. The findings suggest non-negligible changes in explanations through the implementation of privacy measures. Apart from reporting individual effects of PPML on XAI, the paper gives clear recommendations for the choice of techniques in real applications. By unveiling the interdependencies of these pivotal technologies, this research marks an initial step toward resolving the challenges that hinder the deployment of AI in safety-critical settings.},
  annotation = {TLDR: This work is the first to thoroughly investigate the effects of privacy-preserving techniques on explanations generated by common XAI methods for DL models, and suggests non-negligible changes in explanations through the implementation of privacy measures.},
  timestamp = {2025-04-04T06:28:09Z}
}

@article{salahuddin2022transparency,
  title = {Transparency of Deep Neural Networks for Medical Image Analysis: {{A}} Review of Interpretability Methods},
  author = {Salahuddin, Zohaib and Woodruff, Henry C and Chatterjee, Avishek and Lambin, Philippe},
  year = {2022},
  journal = {Computers in biology and medicine},
  volume = {140},
  pages = {105111},
  publisher = {Elsevier},
  timestamp = {2025-05-17T11:46:31Z}
}

@article{salzberg1996finding,
  title = {Finding Genes in {{DNA}} Using Decision Trees and Dynamic Programming},
  author = {Salzberg, S. and Chen, X. and Henderson, J. and Fasman, K.},
  year = {1996},
  journal = {Proceedings. International Conference on Intelligent Systems for Molecular Biology},
  volume = {4},
  pages = {201--210},
  issn = {1553-0833},
  abstract = {This study demonstrates the use of decision tree classifiers as the basis for a general gene-finding system. The system uses a dynamic programming algorithm that finds the optimal segmentation of a DNA sequence into coding and non-coding regions (exons and introns). The optimality property is dependent on a separate scoring function that takes a subsequence and assigns to it a score reflecting the probability that the sequence is an exon. In this study, the scoring functions were sets of decision trees and rules that were combined to give the probability estimate. Experimental results on a newly collected database of human DNA sequences are encouraging, and some new observations about the structure of classifiers for the gene-finding problem have emerged from this study. We also provide descriptions of a new probability chain model that produces very accurate filters to find donor and acceptor sites.},
  langid = {english},
  pmid = {8877520},
  keywords = {Algorithms,Chromosomes Human,Decision Trees,Exons,Genomic Library,Humans,Introns,Sequence Analysis DNA,Software},
  timestamp = {2025-06-12T14:08:48Z}
}

@misc{samek2017explainable,
  title = {Explainable {{Artificial Intelligence}}: {{Understanding}}, {{Visualizing}} and {{Interpreting Deep Learning Models}}},
  shorttitle = {Explainable {{Artificial Intelligence}}},
  author = {Samek, Wojciech and Wiegand, Thomas and M{\"u}ller, Klaus-Robert},
  year = {2017},
  month = aug,
  number = {arXiv:1708.08296},
  eprint = {1708.08296},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.08296},
  urldate = {2025-05-17},
  abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  timestamp = {2025-05-17T10:47:14Z}
}

@article{samek2021explaining,
  ids = {samek2021explaininga},
  title = {Explaining Deep Neural Networks and beyond: {{A}} Review of Methods and Applications},
  shorttitle = {Explaining {{Deep Neural Networks}} and {{Beyond}}},
  author = {Samek, Wojciech and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Anders, Christopher J and M{\"u}ller, Klaus-Robert},
  year = {2021},
  month = mar,
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {3},
  pages = {247--278},
  publisher = {IEEE},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2021.3060483},
  urldate = {2025-04-21},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {american},
  annotation = {TLDR: This work aims to provide a timely overview of this active emerging field of XAI, with a focus on ``post hoc'' explanations, and explain its theoretical foundations, and put interpretability algorithms to a test both from a theory and comparative evaluation perspective.},
  timestamp = {2025-05-12T00:31:19Z}
}

@inproceedings{sammak2022classifying,
  title = {Classifying {{Cardiovascular Disease Using Gradient Boosting Method Risk Factor Analysis}} Using {{Shapely Additive exPlanations}}},
  booktitle = {2022 13th {{International Conference}} on {{Computing Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  author = {Sammak, Musabbir Hasan and Arman, Md Shohel and Jahan, Hasnur and Mahmud, Imran and Alam, Md. Rittique and Biplob, Khalid Been Md. Badruzzaman},
  year = {2022},
  month = oct,
  pages = {1--5},
  publisher = {IEEE},
  address = {Kharagpur, India},
  doi = {10.1109/ICCCNT54827.2022.9984277},
  urldate = {2025-06-12},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-6654-5262-5},
  annotation = {TLDR: Shapely Additive exPlanations or shape value is utilized to discover the best features for accurately identifying heart disease and will assist medical professionals in identifying cardiac disease through practical application and help save countless lives.},
  timestamp = {2025-06-12T15:44:39Z}
}

@article{sanchez2022causal,
  ids = {sanchez2022causala},
  title = {Causal Machine Learning for Healthcare and Precision Medicine},
  author = {Sanchez, Pedro and Voisey, Jeremy P and Xia, Tian and Watson, Hannah I and O'Neil, Alison Q and Tsaftaris, Sotirios A},
  year = {2022},
  journal = {Royal Society Open Science},
  volume = {9},
  number = {8},
  pages = {220638},
  publisher = {The Royal Society},
  doi = {10.1098/rsos.220638},
  urldate = {2025-05-04},
  abstract = {Causal machine learning (CML) has experienced increasing popularity in healthcare. Beyond the inherent capabilities of adding domain knowledge into learning systems, CML provides a complete toolset for investigating how a system would react to an ...},
  copyright = {{\copyright} 2022 The Authors.},
  langid = {english},
  annotation = {TLDR: Important challenges present in healthcare applications such as processing high-dimensional and unstructured data, generalization to out-of-distribution samples and temporal relationships, that despite the great effort from the research community remain to be solved are discussed.},
  timestamp = {2025-05-29T11:17:54Z}
}

@article{sanderson2022mendelian,
  title = {Mendelian Randomization},
  author = {Sanderson, Eleanor and Glymour, M. Maria and Holmes, Michael V. and Kang, Hyunseung and Morrison, Jean and Munaf{\`o}, Marcus R. and Palmer, Tom and Schooling, C. Mary and Wallace, Chris and Zhao, Qingyuan and Smith, George Davey},
  year = {2022},
  month = feb,
  journal = {Nature Reviews. Methods Primers},
  volume = {2},
  pages = {6},
  issn = {2662-8449},
  doi = {10.1038/s43586-021-00092-5},
  abstract = {Mendelian randomization (MR) is a term that applies to the use of genetic variation to address causal questions about how modifiable exposures influence different outcomes. The principles of MR are based on Mendel's laws of inheritance and instrumental variable estimation methods, which enable the inference of causal effects in the presence of unobserved confounding. In this Primer, we outline the principles of MR, the instrumental variable conditions underlying MR estimation and some of the methods used for estimation. We go on to discuss how the assumptions underlying an MR study can be assessed and give methods of estimation that are robust to certain violations of these assumptions. We give examples of a range of studies in which MR has been applied, the limitations of current methods of analysis and the outlook for MR in the future. The difference between the assumptions required for MR analysis and other forms of non-interventional epidemiological studies means that MR can be used as part of a triangulation across multiple sources of evidence for causal inference.},
  langid = {english},
  pmcid = {PMC7614635},
  pmid = {37325194},
  annotation = {TLDR: This Primer explains the concepts of and the conditions required for Mendelian randomization analysis, describes key examples of its application and looks towards applying the technique to growing genomic datasets.},
  timestamp = {2025-06-13T03:20:37Z}
}

@article{santorsola2023promise,
  title = {The Promise of Explainable Deep Learning for Omics Data Analysis: {{Adding}} New Discovery Tools to {{AI}}},
  shorttitle = {The Promise of Explainable Deep Learning for Omics Data Analysis},
  author = {Santorsola, Mariangela and Lescai, Francesco},
  year = {2023},
  month = nov,
  journal = {New Biotechnology},
  volume = {77},
  pages = {1--11},
  publisher = {Elsevier BV},
  issn = {1871-6784},
  doi = {10.1016/j.nbt.2023.06.002},
  urldate = {2025-07-24},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  annotation = {TLDR: This review provides an overview of the transformative effects explainable deep learning is having on multiple sectors, ranging from genome engineering and genomics, from radiomics to drug design and clinical trials, by suggesting learning resources they can use to move their first steps in this field.},
  timestamp = {2025-07-24T10:47:36Z}
}

@article{saraswat2022explainable,
  title = {Explainable {{AI}} for {{Healthcare}} 5.0: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Explainable {{AI}} for {{Healthcare}} 5.0},
  author = {Saraswat, Deepti and Bhattacharya, Pronaya and Verma, Ashwin and Prasad, Vivek Kumar and Tanwar, Sudeep and Sharma, Gulshan and Bokoro, Pitshou N. and Sharma, Ravi},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {84486--84517},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3197671},
  urldate = {2025-04-21},
  abstract = {In the healthcare domain, a transformative shift is envisioned towards Healthcare 5.0. It expands the operational boundaries of Healthcare 4.0 and leverages patient-centric digital wellness. Healthcare 5.0 focuses on real-time patient monitoring, ambient control and wellness, and privacy compliance through assisted technologies like artificial intelligence (AI), Internet-of-Things (IoT), big data, and assisted networking channels. However, healthcare operational procedures, verifiability of prediction models, resilience, and lack of ethical and regulatory frameworks are potential hindrances to the realization of Healthcare 5.0. Recently, explainable AI (EXAI) has been a disruptive trend in AI that focuses on the explainability of traditional AI models by leveraging the decision-making of the models and prediction outputs. The explainability factor opens new opportunities to the black-box models and brings confidence in healthcare stakeholders to interpret the machine learning (ML) and deep learning (DL) models. EXAI is focused on improving clinical health practices and brings transparency to the predictive analysis, which is crucial in the healthcare domain. Recent surveys on EXAI in healthcare have not significantly focused on the data analysis and interpretation of models, which lowers its practical deployment opportunities. Owing to the gap, the proposed survey explicitly details the requirements of EXAI in Healthcare 5.0, the operational and data collection process. Based on the review method and presented research questions, systematically, the article unfolds a proposed architecture that presents an EXAI ensemble on the computerized tomography (CT) image classification and segmentation process. A solution taxonomy of EXAI in Healthcare 5.0 is proposed, and operational challenges are presented. A supported case study on electrocardiogram (ECG) monitoring is presented that preserves the privacy of local models via federated learning (FL) and EXAI for metric validation. The case-study is supported through experimental validation. The analysis proves the efficacy of EXAI in health setups that envisions real-life model deployments in a wide range of clinical applications.},
  keywords = {Analytical models,Artificial intelligence,deep learning,Deep learning,Explainable AI,healthcare 50,Medical diagnostic imaging,Medical services,metrics,Prediction algorithms,Predictive models},
  annotation = {TLDR: A proposed architecture that presents an EXAI ensemble on the computerized tomography (CT) image classification and segmentation process is unfolded that proves the efficacy of EXAI in health setups that envisions real-life model deployments in a wide range of clinical applications.},
  timestamp = {2025-04-21T14:07:27Z}
}

@inbook{sarhan2019learning,
  title = {Learning {{Interpretable Disentangled Representations Using Adversarial VAEs}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  year = {2019},
  pages = {37--44},
  publisher = {Springer International Publishing},
  address = {Cham},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-030-33391-1_5},
  urldate = {2025-07-25},
  collaborator = {Sarhan, Mhd Hasan and Eslami, Abouzar and Navab, Nassir and Albarqouni, Shadi},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-030-33390-4 978-3-030-33391-1},
  langid = {english},
  timestamp = {2025-07-25T03:05:04Z}
}

@misc{sarker2017explaining,
  title = {Explaining {{Trained Neural Networks}} with {{Semantic Web Technologies}}: {{First Steps}}},
  shorttitle = {Explaining {{Trained Neural Networks}} with {{Semantic Web Technologies}}},
  author = {Sarker, Md Kamruzzaman and Xie, Ning and Doran, Derek and Raymer, Michael and Hitzler, Pascal},
  year = {2017},
  month = oct,
  number = {arXiv:1710.04324},
  eprint = {1710.04324},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.04324},
  urldate = {2025-03-17},
  abstract = {The ever increasing prevalence of publicly available structured data on the World Wide Web enables new applications in a variety of domains. In this paper, we provide a conceptual approach that leverages such data in order to explain the input-output behavior of trained artificial neural networks. We apply existing Semantic Web technologies in order to provide an experimental proof of concept.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  timestamp = {2025-03-17T09:26:21Z}
}

@article{sayres2019using,
  title = {Using a {{Deep Learning Algorithm}} and {{Integrated Gradients Explanation}} to {{Assist Grading}} for {{Diabetic Retinopathy}}},
  author = {Sayres, Rory and Taly, Ankur and Rahimy, Ehsan and Blumer, Katy and Coz, David and Hammel, Naama and Krause, Jonathan and Narayanaswamy, Arunachalam and Rastegar, Zahra and Wu, Derek and Xu, Shawn and Barb, Scott and Joseph, Anthony and Shumski, Michael and Smith, Jesse and Sood, Arjun B. and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
  year = {2019},
  month = apr,
  journal = {Ophthalmology},
  volume = {126},
  number = {4},
  pages = {552--564},
  issn = {01616420},
  doi = {10.1016/j.ophtha.2018.11.016},
  urldate = {2025-06-12},
  langid = {english},
  annotation = {TLDR: Deep learning algorithms can improve the accuracy of, and confidence in, DR diagnosis in an assisted read setting, and they also may increase grading time, although these effects may be ameliorated with experience.},
  timestamp = {2025-06-12T16:15:24Z}
}

@article{schnake2022higherorder,
  title = {Higher-{{Order Explanations}} of {{Graph Neural Networks}} via {{Relevant Walks}}},
  author = {Schnake, Thomas and Eberle, Oliver and Lederer, Jonas and Nakajima, Shinichi and Sch{\"u}tt, Kristof T. and M{\"u}ller, Klaus-Robert and Montavon, Gr{\'e}goire},
  year = {2022},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {11},
  eprint = {2006.03589},
  primaryclass = {cs},
  pages = {7581--7596},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2021.3115452},
  urldate = {2025-03-28},
  abstract = {Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e. by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {TLDR: It is shown that GNNs can in fact be naturally explained using higher-order expansions, i.e., by identifying groups of edges that jointly contribute to the prediction, and that such explanations can be extracted using a nested attribution scheme.},
  timestamp = {2025-03-28T07:43:39Z}
}

@inproceedings{schneeberger2020european,
  title = {The {{European}} Legal Framework for Medical {{AI}}},
  booktitle = {International Cross-Domain Conference for Machine Learning and Knowledge Extraction},
  author = {Schneeberger, David and St{\"o}ger, Karl and Holzinger, Andreas},
  year = {2020},
  pages = {209--226},
  publisher = {Springer},
  timestamp = {2025-03-28T08:08:05Z}
}

@article{scholkopf2021causal,
  title = {Toward {{Causal Representation Learning}}},
  author = {Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  year = {2021},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {5},
  pages = {612--634},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2021.3058954},
  urldate = {2025-05-17},
  abstract = {The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {american},
  keywords = {Adaptation models,Artificial intelligence,causality,Data models,deep learning,Differential equations,Inference algorithms,Machine learning,Mathematical model,representation learning,Representation learning,Training data},
  annotation = {TLDR: Fundamental concepts of causal inference are reviewed and related to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research.},
  timestamp = {2025-05-17T09:02:29Z}
}

@misc{scholkopf2021causala,
  title = {Towards {{Causal Representation Learning}}},
  author = {Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  year = {2021},
  month = feb,
  number = {arXiv:2102.11107},
  eprint = {2102.11107},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.11107},
  urldate = {2025-03-24},
  abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  timestamp = {2025-03-24T07:18:14Z}
}

@article{schubert2021high,
  title = {High/{{Low}} Frequency Detectors},
  author = {Schubert, Ludwig and Voss, Chelsea and Olah, Chris},
  year = {2021},
  month = jan,
  journal = {Distill},
  volume = {6},
  number = {1},
  pages = {10.23915/distill.00024.005},
  issn = {2476-0757},
  doi = {10.23915/distill.00024.005},
  urldate = {2025-09-02},
  timestamp = {2025-09-02T02:40:28Z}
}

@article{schubert2025ai,
  title = {{{AI}} Education for Clinicians},
  author = {Schubert, Tim and Oosterlinck, Tim and Stevens, Robert D. and Maxwell, Patrick H. and {van der Schaar}, Mihaela},
  year = {2025},
  month = jan,
  journal = {EClinicalMedicine},
  volume = {79},
  pages = {102968},
  issn = {2589-5370},
  doi = {10.1016/j.eclinm.2024.102968},
  abstract = {Rapid advancements in medical AI necessitate targeted educational initiatives for clinicians to ensure AI tools are safe and used effectively to improve patient outcomes. To support decision-making among stakeholders in medical education, we propose three tiers of medical AI expertise and outline the challenges for medical education at different educational stages. Additionally, we offer recommendations and examples, encouraging stakeholders to adapt and shape curricula for their specific healthcare setting using this framework.},
  langid = {english},
  pmcid = {PMC11667627},
  pmid = {39720600},
  keywords = {Artificial intelligence,Clinicians,Framework,Machine learning,Medical education},
  annotation = {TLDR: This work proposes three tiers of medical AI expertise and outlines the challenges for medical education at different educational stages, encouraging stakeholders to adapt and shape curricula for their specific healthcare setting using this framework.},
  timestamp = {2025-04-16T02:50:26Z}
}

@article{schuhmacher2022framework,
  title = {A Framework for Falsifiable Explanations of Machine Learning Models with an Application in Computational Pathology},
  author = {Schuhmacher, David and Sch{\"o}rner, Stephanie and K{\"u}pper, Claus and Gro{\ss}erueschkamp, Frederik and Sternemann, Carlo and Lugnier, Celine and Kraeft, Anna-Lena and J{\"u}tte, Hendrik and Tannapfel, Andrea and {Reinacher-Schick}, Anke and Gerwert, Klaus and Mosig, Axel},
  year = {2022},
  month = nov,
  journal = {Medical Image Analysis},
  volume = {82},
  pages = {102594},
  issn = {1361-8423},
  doi = {10.1016/j.media.2022.102594},
  abstract = {In recent years, deep learning has been the key driver of breakthrough developments in computational pathology and other image based approaches that support medical diagnosis and treatment. The underlying neural networks as inherent black boxes lack transparency and are often accompanied by approaches to explain their output. However, formally defining explainability has been a notorious unsolved riddle. Here, we introduce a hypothesis-based framework for falsifiable explanations of machine learning models. A falsifiable explanation is a hypothesis that connects an intermediate space induced by the model with the sample from which the data originate. We instantiate this framework in a computational pathology setting using hyperspectral infrared microscopy. The intermediate space is an activation map, which is trained with an inductive bias to localize tumor. An explanation is constituted by hypothesizing that activation corresponds to tumor and associated structures, which we validate by histological staining as an independent secondary experiment.},
  langid = {english},
  pmid = {36058053},
  keywords = {Explainable artificial intelligence,Falsifiability,Humans,Machine Learning,Microscopy,Neoplasms,Neural Networks Computer,Tumor segmentation,U-Net},
  timestamp = {2025-09-04T07:51:52Z}
}

@article{schulte-sasse2021integration,
  title = {Integration of Multiomics Data with Graph Convolutional Networks to Identify New Cancer Genes and Their Associated Molecular Mechanisms},
  author = {{Schulte-Sasse}, Roman and Budach, Stefan and Hnisz, Denes and Marsico, Annalisa},
  year = {2021},
  month = jun,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {6},
  pages = {513--526},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00325-y},
  urldate = {2025-05-16},
  abstract = {The increase in available high-throughput molecular data creates computational challenges for the identification of cancer genes. Genetic as well as non-genetic causes contribute to tumorigenesis, and this necessitates the development of predictive models to effectively integrate different data modalities while being interpretable. We introduce EMOGI, an explainable machine learning method based on graph convolutional networks to predict cancer genes by combining multiomics pan-cancer data---such as mutations, copy number changes, DNA methylation and gene expression---together with protein--protein interaction (PPI) networks. EMOGI was on average more accurate than other methods across different PPI networks and datasets. We used layer-wise relevance propagation to stratify genes according to whether their classification was driven by the interactome or any of the omics levels, and to identify important modules in the PPI network. We propose 165 novel cancer genes that do not necessarily harbour recurrent alterations but interact with known cancer genes, and we show that they correspond to essential genes from loss-of-function screens. We believe that our method can open new avenues in precision oncology and be applied to predict biomarkers for other complex diseases.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Cancer genomics,Cellular signalling networks,Data integration,Machine learning,Tumour biomarkers},
  annotation = {TLDR: EMOGI, an explainable machine learning method based on graph convolutional networks to predict cancer genes by combining multiomics pan-cancer data---such as mutations, copy number changes, DNA methylation and gene expression---together with protein--protein interaction (PPI) networks is introduced.},
  timestamp = {2025-05-16T08:44:50Z}
}

@misc{schutte2021using,
  title = {Using {{StyleGAN}} for {{Visual Interpretability}} of {{Deep Learning Models}} on {{Medical Images}}},
  author = {Schutte, Kathryn and Moindrot, Olivier and H{\'e}rent, Paul and Schiratti, Jean-Baptiste and J{\'e}gou, Simon},
  year = {2021},
  month = jan,
  number = {arXiv:2101.07563},
  eprint = {2101.07563},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.07563},
  urldate = {2025-05-18},
  abstract = {As AI-based medical devices are becoming more common in imaging fields like radiology and histology, interpretability of the underlying predictive models is crucial to expand their use in clinical practice. Existing heatmap-based interpretability methods such as GradCAM only highlight the location of predictive features but do not explain how they contribute to the prediction. In this paper, we propose a new interpretability method that can be used to understand the predictions of any black-box model on images, by showing how the input image would be modified in order to produce different predictions. A StyleGAN is trained on medical images to provide a mapping between latent vectors and images. Our method identifies the optimal direction in the latent space to create a change in the model prediction. By shifting the latent representation of an input image along this direction, we can produce a series of new synthetic images with changed predictions. We validate our approach on histology and radiology images, and demonstrate its ability to provide meaningful explanations that are more informative than GradCAM heatmaps. Our method reveals the patterns learned by the model, which allows clinicians to build trust in the model's predictions, discover new biomarkers and eventually reveal potential biases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  timestamp = {2025-05-18T13:05:20Z}
}

@inproceedings{schwab2019cxplain,
  title = {{{CXPlain}}: {{Causal Explanations}} for {{Model Interpretation}} under {{Uncertainty}}},
  shorttitle = {{{CXPlain}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schwab, Patrick and Karlen, Walter},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-12-03},
  abstract = {Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.},
  langid = {american},
  timestamp = {2024-12-03T12:41:03Z}
}

@article{schwab2019grangercausal,
  title = {Granger-{{Causal Attentive Mixtures}} of {{Experts}}: {{Learning Important Features}} with {{Neural Networks}}},
  shorttitle = {Granger-{{Causal Attentive Mixtures}} of {{Experts}}},
  author = {Schwab, Patrick and Miladinovic, Djordje and Karlen, Walter},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {4846--4853},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33014846},
  urldate = {2024-12-24},
  abstract = {Knowledge of the importance of input features towards decisions made by machine-learning models is essential to increase our understanding of both the models and the underlying data. Here, we present a new approach to estimating feature importance with neural networks based on the idea of distributing the features of interest among experts in an attentive mixture of experts (AME). AMEs use attentive gating networks trained with a Granger-causal objective to learn to jointly produce accurate predictions as well as estimates of feature importance in a single model. Our experiments show (i) that the feature importance estimates provided by AMEs compare favourably to those provided by state-of-theart methods, (ii) that AMEs are significantly faster at estimating feature importance than existing methods, and (iii) that the associations discovered by AMEs are consistent with those reported by domain experts.},
  copyright = {https://www.aaai.org},
  langid = {american},
  annotation = {TLDR: The experiments show that the feature importance estimates provided by AMEs compare favourably to those provided by state-of-theart methods, that AMEs are significantly faster at estimating feature importance than existing methods, and that the associations discovered are consistent with those reported by domain experts.},
  timestamp = {2024-12-24T08:55:13Z}
}

@inproceedings{schwab2020learning,
  title = {Learning Counterfactual Representations for Estimating Individual Dose-Response Curves},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Schwab, Patrick and Linhardt, Lorenz and Bauer, Stefan and Buhmann, Joachim M and Karlen, Walter},
  year = {2020},
  volume = {34},
  pages = {5612--5619},
  timestamp = {2025-03-19T12:15:18Z}
}

@article{schwab2021artificial,
  title = {Artificial Intelligence {{MacHIne}} Learning for the Detection and Treatment of Atrial Fibrillation Guidelines in the Emergency Department Setting ({{AIM HIGHER}}): {{Assessing}} a Machine Learning Clinical Decision Support Tool to Detect and Treat Non-Valvular Atrial Fibrillation in the Emergency Department},
  shorttitle = {Artificial Intelligence {{MacHIne}} Learning for the Detection and Treatment of Atrial Fibrillation Guidelines in the Emergency Department Setting ({{AIM HIGHER}})},
  author = {Schwab, Kim and Nguyen, Dacloc and Ungab, GilAnthony and Feld, Gregory and Maisel, Alan S. and Than, Martin and Joyce, Laura and Peacock, W. Frank},
  year = {2021},
  month = aug,
  journal = {Journal of the American College of Emergency Physicians Open},
  volume = {2},
  number = {4},
  pages = {e12534},
  issn = {2688-1152},
  doi = {10.1002/emp2.12534},
  abstract = {OBJECTIVE: Advanced machine learning technology provides an opportunity to improve clinical electrocardiogram (ECG) interpretation, allowing non-cardiology clinicians to initiate care for atrial fibrillation (AF). The Lucia Atrial Fibrillation Application (Lucia App) photographs the ECG to determine rhythm detection, calculates CHA2DS2-VASc and HAS-BLED scores, and then provides guideline-recommended anticoagulation. Our purpose was to determine the rate of accurate AF identification and appropriate anticoagulation recommendations in emergency department (ED) patients ultimately diagnosed with AF. METHODS: We performed a single-center, observational retrospective chart review in an urban California ED, with an annual census of 70,000 patients. A convenience sample of hospitalized patients with AF as a primary or secondary discharge diagnosis were evaluated for accurate ED AF diagnosis and ED anticoagulation rates. This was done by comparing the Lucia App against a gold standard board-certified cardiologist diagnosis and using the American College of Emergency Physicians AF anticoagulation guidelines. RESULTS: Two hundred and ninety seven patients were enrolled from January 2016 until December 2019. The median age was 79 years and 44.1\% were female. Compared to the gold standard diagnosis, the Lucia App detected AF in 98.3\% of the cases. Physicians recommended guideline-consistent anticoagulation therapy in 78.5\%~versus 98.3\% for the Lucia App. Of the patients with indications for anticoagulation and discharged from the ED, only 25.0\% were started at discharge. CONCLUSION: Use of a cloud-based ECG identification tool can allow non-cardiologists to achieve similar rates of AF identification as board-certified cardiologists and achieve higher rates of guideline-recommended anticoagulation therapy in the ED.},
  langid = {english},
  pmcid = {PMC8353018},
  pmid = {34401870},
  keywords = {artificial intelligence,atrial fibrillation,clinical decision support,emergency department,guidelines,machine learning,oral anticoagulation},
  annotation = {TLDR: The purpose was to determine the rate of accurate AF identification and appropriate anticoagulation recommendations in emergency department (ED) patients ultimately diagnosed with AF.},
  timestamp = {2025-05-02T14:06:29Z}
}

@article{selvaraju2016gradcam,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1610.02391},
  urldate = {2025-06-12},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  timestamp = {2025-06-12T15:47:33Z}
}

@article{selvaraju2020gradcam,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  primaryclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2025-05-03},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {TLDR: The proposed Grad-CAM technique uses the gradients of any target concept flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept, and shows that even non-attention based models learn to localize discriminative regions of input image.},
  timestamp = {2025-05-03T06:45:24Z}
}

@article{seo2020regional,
  title = {Regional {{Multi-Scale Approach}} for {{Visually Pleasing Explanations}} of {{Deep Neural Networks}}},
  author = {Seo, Dasom and Oh, Kanghan and Oh, Il-Seok},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {8572--8582},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2963055},
  urldate = {2025-07-24},
  abstract = {Recently, many methods to interpret and visualize deep neural network predictions have been proposed, and significant progress has been made. However, a more class-discriminative and visually pleasing explanation is required. Thus, this paper proposes a region-based approach that estimates feature importance in terms of appropriately segmented regions. By fusing the saliency maps generated from multi-scale segmentations, a more class-discriminative and visually pleasing map is obtained. This paper incorporates this regional multi-scale concept into a prediction difference method that is model-agnostic. An input image is segmented in several scales using the superpixel method, and exclusion of a region is simulated by sampling a normal distribution constructed via the boundary prior. The experimental results demonstrate that the regional multi-scale method produces much more class-discriminative and visually pleasing saliency maps.},
  keywords = {Computer vision,explainable artificial intelligence,Image segmentation,machine learning,neural networks,Neural networks,Neurons,Prediction algorithms,Predictive models,Visualization},
  annotation = {TLDR: This paper proposes a region-based approach that estimates feature importance in terms of appropriately segmented regions by fusing the saliency maps generated from multi-scale segmentations by incorporating this regional multi- scale concept into a prediction difference method that is model-agnostic.},
  timestamp = {2025-07-24T14:22:40Z}
}

@misc{serrano2019attention,
  title = {Is {{Attention Interpretable}}?},
  author = {Serrano, Sofia and Smith, Noah A.},
  year = {2019},
  month = jun,
  number = {arXiv:1906.03731},
  eprint = {1906.03731},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.03731},
  urldate = {2025-07-05},
  abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  timestamp = {2025-07-05T06:17:29Z}
}

@misc{shaik2024qxai,
  title = {{{QXAI}}: {{Explainable AI Framework}} for {{Quantitative Analysis}} in {{Patient Monitoring Systems}}},
  shorttitle = {{{QXAI}}},
  author = {Shaik, Thanveer and Tao, Xiaohui and Xie, Haoran and Li, Lin and Velasquez, Juan D. and Higgins, Niall},
  year = {2024},
  month = feb,
  number = {arXiv:2309.10293},
  eprint = {2309.10293},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.10293},
  urldate = {2025-04-08},
  abstract = {Artificial Intelligence techniques can be used to classify a patient's physical activities and predict vital signs for remote patient monitoring. Regression analysis based on non-linear models like deep learning models has limited explainability due to its black-box nature. This can require decision-makers to make blind leaps of faith based on non-linear model results, especially in healthcare applications. In non-invasive monitoring, patient data from tracking sensors and their predisposing clinical attributes act as input features for predicting future vital signs. Explaining the contributions of various features to the overall output of the monitoring application is critical for a clinician's decision-making. In this study, an Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach. This was achieved by utilizing the Shapley values concept and incorporating attention mechanisms in deep learning models. We adopted the artificial neural networks (ANN) and attention-based Bidirectional LSTM (BiLSTM) models for the prediction of heart rate and classification of physical activities based on sensor data. The deep learning models achieved state-of-the-art results in both prediction and classification tasks. Global explanation and local explanation were conducted on input data to understand the feature contribution of various patient data. The proposed QXAI framework was evaluated using PPG-DaLiA data to predict heart rate and mobile health (MHEALTH) data to classify physical activities based on sensor data. Monte Carlo approximation was applied to the framework to overcome the time complexity and high computation power requirements required for Shapley value calculations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {TLDR: An Explainable AI for Quantitative analysis (QXAI) framework is proposed with post-hoc model explainability and intrinsic explainability for regression and classification tasks in a supervised learning approach by utilizing the Shapley values concept and incorporating attention mechanisms in deep learning models.},
  timestamp = {2025-04-08T13:46:32Z}
}

@inproceedings{shalit2017estimating,
  title = {Estimating Individual Treatment Effect: Generalization Bounds and Algorithms},
  booktitle = {International Conference on Machine Learning},
  author = {Shalit, Uri and Johansson, Fredrik D and Sontag, David},
  year = {2017},
  pages = {3076--3085},
  publisher = {PMLR},
  timestamp = {2025-03-20T07:49:18Z}
}

@misc{sharma2019dowhy,
  title = {{{DoWhy}}: A Python Package for Causal Inference},
  author = {Sharma, Amit and Kiciman, Emre},
  year = {2019},
  howpublished = {GitHub repository},
  version = {0.x},
  timestamp = {2025-03-20T10:20:17Z}
}

@article{sharma2020reasonable,
  title = {Reasonable Explainability for Regulating {{AI}} in Health},
  author = {Sharma, Yukuti and Verma, Abhinav and Rao, Krisstina and Eluri, Vivek},
  year = {2020},
  journal = {ORF Issue Brief},
  volume = {401},
  pages = {1--6},
  timestamp = {2025-04-16T03:06:44Z}
}

@misc{sheetrit2017temporal,
  title = {Temporal {{Pattern Discovery}} for {{Accurate Sepsis Diagnosis}} in {{ICU Patients}}},
  author = {Sheetrit, Eitam and Nissim, Nir and Klimov, Denis and Fuchs, Lior and Elovici, Yuval and Shahar, Yuval},
  year = {2017},
  month = sep,
  number = {arXiv:1709.01720},
  eprint = {1709.01720},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.01720},
  urldate = {2025-04-06},
  abstract = {Sepsis is a condition caused by the body's overwhelming and life-threatening response to infection, which can lead to tissue damage, organ failure, and finally death. Common signs and symptoms include fever, increased heart rate, increased breathing rate, and confusion. Sepsis is difficult to predict, diagnose, and treat. Patients who develop sepsis have an increased risk of complications and death and face higher health care costs and longer hospitalization. Today, sepsis is one of the leading causes of mortality among populations in intensive care units (ICUs). In this paper, we look at the problem of early detection of sepsis by using temporal data mining. We focus on the use of knowledge-based temporal abstraction to create meaningful interval-based abstractions, and on time-interval mining to discover frequent interval-based patterns. We used 2,560 cases derived from the MIMIC-III database. We found that the distribution of the temporal patterns whose frequency is above 10\% discovered in the records of septic patients during the last 6 and 12 hours before onset of sepsis is significantly different from that distribution within a similar period, during an equivalent time window during hospitalization, in the records of non-septic patients. This discovery is encouraging for the purpose of performing an early diagnosis of sepsis using the discovered patterns as constructed features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications},
  timestamp = {2025-04-06T14:12:40Z}
}

@article{shen2022charid,
  title = {{{CharID}}: A Two-Step Model for Universal Prediction of Interactions between Chromatin Accessible Regions},
  shorttitle = {{{CharID}}},
  author = {Shen, Yin and Zhong, Quan and Liu, Tian and Wen, Zi and Shen, Wei and Li, Li},
  year = {2022},
  month = mar,
  journal = {Briefings in Bioinformatics},
  volume = {23},
  number = {2},
  pages = {bbab602},
  issn = {1477-4054},
  doi = {10.1093/bib/bbab602},
  abstract = {Open chromatin regions (OCRs) allow direct interaction between cis-regulatory elements and trans-acting factors. Therefore, predicting all potential OCR-mediated loops is essential for deciphering the regulation mechanism of gene expression. However, existing loop prediction tools are restricted to specific anchor types. Here, we present CharID (Chromatin Accessible Region Interaction Detector), a two-step model that combines neural network and ensemble learning to predict OCR-mediated loops. In the first step, CharID-Anchor, an attention-based hybrid CNN-BiGRU network is constructed to discriminate between the anchor and nonanchor OCRs. In the second step, CharID-Loop uses gradient boosting decision tree with chromosome-split strategy to predict the interactions between anchor OCRs. The performance was assessed in three human cell lines, and CharID showed superior prediction performance compared with other algorithms. In contrast to the methods designed to predict a particular type of loops, CharID can detect varieties of chromatin loops not limited to enhancer-promoter loops or architectural protein-mediated loops. We constructed the OCR-mediated interaction network using the predicted loops and identified hub anchors, which are highlighted by their proximity to housekeeping genes. By analyzing loops containing SNPs associated with cardiovascular disease, we identified an SNP-gene loop indicating the regulation mechanism of the GFOD1. Taken together, CharID universally predicts diverse chromatin loops beyond other state-of-the-art methods, which are limited by anchor types, and experimental techniques, which are limited by sensitivities drastically decaying with the genomic distance of anchors. Finally, we hosted Peaksniffer, a user-friendly web server that provides online prediction, query and visualization of OCRs and associated loops.},
  langid = {english},
  pmid = {35077535},
  keywords = {3D genome,Algorithms,Chromatin,chromatin loop,Chromosomes,convolutional neural network,Humans,Neural Networks Computer,open chromatin region,Promoter Regions Genetic},
  annotation = {TLDR: CharID (Chromatin Accessible Region Interaction Detector), a two-step model that combines neural network and ensemble learning to predict OCR-mediated loops, universally predicts diverse chromatin loops beyond other state-of-the-art methods, which is limited by anchor types, and experimental techniques, which are limited by sensitivities drastically decaying with the genomic distance of anchors.},
  timestamp = {2025-07-05T06:08:43Z}
}

@article{sheu2022survey,
  title = {A Survey on Medical Explainable {{AI}} ({{XAI}}): Recent Progress, Explainability Approach, Human Interaction and Scoring System},
  author = {Sheu, Ruey-Kai and Pardeshi, Mayuresh Sunil},
  year = {2022},
  journal = {Sensors},
  volume = {22},
  number = {20},
  pages = {8068},
  publisher = {MDPI},
  timestamp = {2025-04-15T14:16:58Z}
}

@misc{ShiBuWeiLianHeFaBuShiXingBanFa,
  title = {  - },
  urldate = {2025-05-05},
  howpublished = {http://www.moe.gov.cn/jyb\_xwfb/s5147/202310/t20231011\_1085003.html},
  timestamp = {2025-05-05T02:19:41Z}
}

@inproceedings{shirley2024shapley,
  title = {Shapley {{Additive Explanations}} ({{SHAP}}) for {{Cardiovascular Diseases Prediction}}},
  booktitle = {2024 2nd {{International Conference}} on {{Sustainable Computing}} and {{Smart Systems}} ({{ICSCSS}})},
  author = {Shirley, Mbabazi Elizabeth and Kasujja, Namatovu Hasifah and Marvin, Ggaliwango},
  year = {2024},
  month = jul,
  pages = {1429--1437},
  publisher = {IEEE},
  address = {Coimbatore, India},
  doi = {10.1109/ICSCSS60660.2024.10625027},
  urldate = {2025-06-12},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-7999-0},
  annotation = {TLDR: Developing an explainable AI model to predict CVDs over a period of ten years and it is observed that the XGBoost model performs better than the other models with an accuracy of 89\%.},
  timestamp = {2025-06-12T15:43:46Z}
}

@misc{shrikumar2019learning,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  year = {2019},
  month = oct,
  number = {arXiv:1704.02685},
  eprint = {1704.02685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.02685},
  urldate = {2025-07-05},
  abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  timestamp = {2025-07-05T11:42:48Z}
}

@misc{shrikumar2020technical,
  title = {Technical {{Note}} on {{Transcription Factor Motif Discovery}} from {{Importance Scores}} ({{TF-MoDISco}}) Version 0.5.6.5},
  author = {Shrikumar, Avanti and Tian, Katherine and Avsec, {\v Z}iga and Shcherbina, Anna and Banerjee, Abhimanyu and Sharmin, Mahfuza and Nair, Surag and Kundaje, Anshul},
  year = {2020},
  month = apr,
  number = {arXiv:1811.00416},
  eprint = {1811.00416},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.00416},
  urldate = {2025-07-05},
  abstract = {TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) is an algorithm for identifying motifs from basepair-level importance scores computed on genomic sequence data. This technical note focuses on version v0.5.6.5. The implementation is available at https://github.com/kundajelab/tfmodisco/tree/v0.5.6.5},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Genomics,Statistics - Machine Learning},
  timestamp = {2025-07-05T12:02:45Z}
}

@article{shuang2024explainable,
  title = {Explainable {{Machine Learning Predictions}} for the {{Benefit From Chemotherapy}} in {{Advanced Non}}-{{Small Cell Lung Cancer Without Available Targeted Mutations}}},
  author = {Shuang, Zhao and Xingyu, Xiong and Yue, Cheng and Mingjing, Yu},
  year = {2024},
  month = dec,
  journal = {The Clinical Respiratory Journal},
  volume = {18},
  number = {12},
  pages = {e70044},
  issn = {1752-6981, 1752-699X},
  doi = {10.1111/crj.70044},
  urldate = {2025-08-09},
  abstract = {ABSTRACT                            Background               Non-small cell lung cancer (NSCLC) is a global health challenge. Chemotherapy remains the standard therapy for advanced NSCLC without mutations, but drug resistance often reduces effectiveness. Developing more effective methods to predict and monitor chemotherapy benefits early is crucial.                                         Methods               We carried out a retrospective cohort study of NSCLC patients without targeted mutations who received chemotherapy at West China Hospital from 2009 to 2013. We identified variables associated with chemotherapy outcomes and built four predictive models by machine learning. Shapley additive explanations (SHAP) interpreted the best model's predictions. The Kaplan--Meier method assessed key variables' impact on 5-year overall survival.                                         Results               The study enrolled 461 NSCLC patients. Eight variables were selected for the model: differentiation, surgery history, neutrophil-to-lymphocyte ratio (NLR), platelet-to-lymphocyte ratio (PLR), total bilirubin (TBIL), total protein (TP), alanine aminotransferase (ALT), and lactate dehydrogenase (LDH). The extreme gradient boosting (Xgboost) model exhibited superior discriminatory ability in predicting complete response (CR) probabilities to chemotherapy, with an AUC of 0.78. SHAP plots showed surgery history and high differentiation were related to CR benefits from chemotherapy. Absence of surgery, higher NLR, higher PLR, and higher LDH were all independent prognostic factors for poor survivals in NSCLC patients without mutations receiving chemotherapy.                                         Conclusions               By machine learning, we developed a predictive model to assess chemotherapy benefits in NSCLC patients without targeted mutations, utilizing eight readily available and non-invasive clinical indicators. Demonstrating satisfactory predictive performance and clinical practicability, this model may help clinicians identify patients' tendency to benefit from chemotherapy, potentially improving their prognosis.},
  langid = {english},
  annotation = {TLDR: Non-small cell lung cancer (NSCLC) is a global health challenge and drug resistance often reduces effectiveness, so developing more effective methods to predict and monitor chemotherapy benefits early is crucial.},
  timestamp = {2025-08-09T15:11:31Z}
}

@article{siddiqui2021tsinsight,
  title = {{{TSInsight}}: {{A Local-Global Attribution Framework}} for {{Interpretability}} in {{Time Series Data}}},
  shorttitle = {{{TSInsight}}},
  author = {Siddiqui, Shoaib Ahmed and Mercier, Dominique and Dengel, Andreas and Ahmed, Sheraz},
  year = {2021},
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {21},
  pages = {7373},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s21217373},
  urldate = {2025-04-09},
  abstract = {With the rise in the employment of deep learning methods in safety-critical scenarios, interpretability is more essential than ever before. Although many different directions regarding interpretability have been explored for visual modalities, time series data has been neglected, with only a handful of methods tested due to their poor intelligibility. We approach the problem of interpretability in a novel way by proposing TSInsight, where we attach an auto-encoder to the classifier with a sparsity-inducing norm on its output and fine-tune it based on the gradients from the classifier and a reconstruction penalty. TSInsight learns to preserve features that are important for prediction by the classifier and suppresses those that are irrelevant, i.e., serves as a feature attribution method to boost the interpretability. In contrast to most other attribution frameworks, TSInsight is capable of generating both instance-based and model-based explanations. We evaluated TSInsight along with nine other commonly used attribution methods on eight different time series datasets to validate its efficacy. The evaluation results show that TSInsight naturally achieves output space contraction; therefore, it is an effective tool for the interpretability of deep time series models.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {auto-encoder,deep learning,demystification,feature attribution,feature importance,interpretability,time series analysis},
  annotation = {TLDR: TSInsight, where an auto-encoder is attached to the classifier with a sparsity-inducing norm on its output and fine-tune it based on the gradients from the classifiers, is an effective tool for the interpretability of deep time series models.},
  timestamp = {2025-04-09T03:54:27Z}
}

@inproceedings{Simons1998RNASA,
  title = {{{RNA}} Structure and Function},
  author = {Simons, Robert W. and Grunberg-Manago, Marianne},
  year = {1998},
  timestamp = {2025-05-27T15:48:15Z}
}

@article{singh2021interpretable,
  title = {An {{Interpretable Deep Learning Model}} for {{Covid-19 Detection With Chest X-Ray Images}}},
  author = {Singh, Gurmail and Yow, Kin-Choong},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {85198--85208},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3087583},
  urldate = {2025-05-17},
  abstract = {Timely and accurate detection of an epidemic/pandemic is always desired to prevent its spread. For the detection of any disease, there can be more than one approach including deep learning models. However, transparency/interpretability of the reasoning process of a deep learning model related to health science is a necessity. Thus, we introduce an interpretable deep learning model: Gen-ProtoPNet. Gen-ProtoPNet is closely related to two interpretable deep learning models: ProtoPNet and NP-ProtoPNet The latter two models use prototypes of spacial dimension 1{\textbackslash}times 1 and the distance function L2 . In our model, we use a generalized version of the distance function L2 that enables us to use prototypes of any type of spacial dimensions, that is, square spacial dimensions and rectangular spacial dimensions to classify an input image. The accuracy and precision that our model receives is on par with the best performing non-interpretable deep learning models when we tested the models on the dataset of X -ray images. Our model attains the highest accuracy of 87.27\% on classification of three classes of images, that is close to the accuracy of 88.42\% attained by a non-interpretable model on the classification of the given dataset.},
  keywords = {Birds,Convolution,Covid-19,COVID-19,deep learning,Deep learning,image recognition,pneumonia,Prototypes,prototypical part,Pulmonary diseases,X-ray,X-ray imaging},
  annotation = {TLDR: An interpretable deep learning model: Gen-ProtoPNet is introduced that attains the highest accuracy and precision and is on par with the best performing non-interpretable deeplearning models when tested the models on the dataset of {$<$}inline-formula{$>$} {$<$}tex-math notation="LaTeX"{$>\$$}X\$ {$<$}/tex- math{$><$}/inline- formula{$>$}-ray images.},
  timestamp = {2025-05-17T10:40:29Z}
}

@article{singhal2023large,
  title = {Large Language Models Encode Clinical Knowledge},
  author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and {Cole-Lewis}, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Babiker, Abubakr and Sch{\"a}rli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and {Demner-Fushman}, Dina and {Ag{\"u}era y Arcas}, Blaise and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
  year = {2023},
  month = aug,
  journal = {Nature},
  volume = {620},
  number = {7972},
  pages = {172--180},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06291-2},
  urldate = {2025-03-03},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and~a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension,~reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model1 (PaLM,~a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics6), including 67.6\% accuracy on MedQA~(US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Health care,Medical research},
  annotation = {TLDR: MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and~a new dataset of medical questions searched online, is presented and a human evaluation framework for model answers is proposed, suggesting the potential utility of LLMs in medicine.},
  timestamp = {2025-03-03T11:47:10Z}
}

@misc{singlecell,
  title = {Single-Cell Gene Regulatory Network Prediction by Explainable {{AI}} {\textbar} {{Nucleic Acids Research}} {\textbar} {{Oxford Academic}}},
  urldate = {2025-04-18},
  howpublished = {https://academic.oup.com/nar/article/51/4/e20/6984592?login=false},
  timestamp = {2025-04-18T03:25:20Z}
}

@article{sitapati2017integrated,
  title = {Integrated Precision Medicine: The Role of Electronic Health Records in Delivering Personalized Treatment},
  shorttitle = {Integrated Precision Medicine},
  author = {Sitapati, Amy and Kim, Hyeoneui and Berkovich, Barbara and Marmor, Rebecca and Singh, Siddharth and {El-Kareh}, Robert and Clay, Brian and {Ohno-Machado}, Lucila},
  year = {2017},
  month = may,
  journal = {Wiley Interdisciplinary Reviews. Systems Biology and Medicine},
  volume = {9},
  number = {3},
  issn = {1939-005X},
  doi = {10.1002/wsbm.1378},
  abstract = {Precision Medicine involves the delivery of a targeted, personalized treatment for a given patient. By harnessing the power of electronic health records (EHRs), we are increasingly able to practice precision medicine to improve patient outcomes. In this article, we introduce the scientific community at large to important building blocks for personalized treatment, such as terminology standards that are the foundation of the EHR and allow for exchange of health information across systems. We briefly review different types of clinical decision support (CDS) and present the current state of CDS, which is already improving the care patients receive with genetic profile-based tailored recommendations regarding diagnostic and treatment plans. We also report on limitations of current systems, which are slowly beginning to integrate new genomic data into patient records but still present many challenges. Finally, we discuss future directions and how the EHR can evolve to increase the capacity of the healthcare system in delivering Precision Medicine at the point of care. WIREs Syst Biol Med 2017, 9:e1378. doi: 10.1002/wsbm.1378 For further resources related to this article, please visit the WIREs website.},
  langid = {english},
  pmcid = {PMC5400726},
  pmid = {28207198},
  keywords = {Decision Support Systems Clinical,Delivery of Health Care Integrated,Electronic Health Records,Health Information Exchange,Humans,Precision Medicine},
  annotation = {TLDR: The scientific community at large is introduced to important building blocks for personalized treatment, such as terminology standards that are the foundation of the EHR and allow for exchange of health information across systems.},
  timestamp = {2025-05-28T00:18:25Z}
}

@misc{sivaraman2023ignore,
  title = {Ignore, {{Trust}}, or {{Negotiate}}: {{Understanding Clinician Acceptance}} of {{AI-Based Treatment Recommendations}} in {{Health Care}}},
  shorttitle = {Ignore, {{Trust}}, or {{Negotiate}}},
  author = {Sivaraman, Venkatesh and Bukowski, Leigh A. and Levin, Joel and Kahn, Jeremy M. and Perer, Adam},
  year = {2023},
  month = jan,
  number = {arXiv:2302.00096},
  eprint = {2302.00096},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00096},
  urldate = {2025-05-28},
  abstract = {Artificial intelligence (AI) in healthcare has the potential to improve patient outcomes, but clinician acceptance remains a critical barrier. We developed a novel decision support interface that provides interpretable treatment recommendations for sepsis, a life-threatening condition in which decisional uncertainty is common, treatment practices vary widely, and poor outcomes can occur even with optimal decisions. This system formed the basis of a mixed-methods study in which 24 intensive care clinicians made AI-assisted decisions on real patient cases. We found that explanations generally increased confidence in the AI, but concordance with specific recommendations varied beyond the binary acceptance or rejection described in prior work. Although clinicians sometimes ignored or trusted the AI, they also often prioritized aspects of the recommendations to follow, reject, or delay in a process we term "negotiation." These results reveal novel barriers to adoption of treatment-focused AI tools and suggest ways to better support differing clinician perspectives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  timestamp = {2025-05-28T01:01:56Z}
}

@article{sloss2024patient,
  title = {Patient and {{Caregiver Perceptions}} of an {{Interface Design}} to {{Communicate Artificial Intelligence}}--{{Based Prognosis}} for {{Patients With Advanced Solid Tumors}}},
  author = {Sloss, Elizabeth A. and McPherson, Jordan P. and Beck, Anna C. and Guo, Jia-Wen and Scheese, Carolyn H. and Flake, Naomi R. and Chalkidis, George and Staes, Catherine J.},
  year = {2024},
  month = apr,
  journal = {JCO Clinical Cancer Informatics},
  number = {8},
  pages = {e2300187},
  publisher = {Wolters Kluwer},
  doi = {10.1200/CCI.23.00187},
  urldate = {2025-04-12},
  abstract = {PurposeUse of artificial intelligence (AI) in cancer care is increasing. What remains unclear is how best to design patient-facing systems that communicate AI output. With oncologist input, we designed an interface that presents patient-specific, machine learning--based 6-month survival prognosis information designed to aid oncology providers in preparing for and discussing prognosis with patients with advanced solid tumors and their caregivers. The primary purpose of this study was to assess patient and caregiver perceptions and identify enhancements of the interface for communicating 6-month survival and other prognosis information when making treatment decisions concerning anticancer and supportive therapy.MethodsThis qualitative study included interviews and focus groups conducted between November and December 2022. Purposive sampling was used to recruit former patients with cancer and/or former caregivers of patients with cancer who had participated in cancer treatment decisions from Utah or elsewhere in the United States. Categories and themes related to perceptions of the interface were identified.ResultsWe received feedback from 20 participants during eight individual interviews and two focus groups, including four cancer survivors, 13 caregivers, and three representing both. Overall, most participants expressed positive perceptions about the tool and identified its value for supporting decision making, feeling less alone, and supporting communication among oncologists, patients, and their caregivers. Participants identified areas for improvement and implementation considerations, particularly that oncologists should share the tool and guide discussions about prognosis with patients who want to receive the information.ConclusionThis study revealed important patient and caregiver perceptions of and enhancements for the proposed interface. Originally designed with input from oncology providers, patient and caregiver participants identified additional interface design recommendations and implementation considerations to support communication about prognosis.},
  annotation = {TLDR: An interface that presents patient-specific, machine learning--based 6-month survival prognosis information designed to aid oncology providers in preparing for and discussing prognosis with patients with advanced solid tumors and their caregivers is designed.},
  timestamp = {2025-04-12T04:15:45Z}
}

@misc{snell2017prototypical,
  title = {Prototypical {{Networks}} for {{Few-shot Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  year = {2017},
  month = jun,
  number = {arXiv:1703.05175},
  eprint = {1703.05175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1703.05175},
  urldate = {2025-09-06},
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-09-06T07:16:21Z}
}

@incollection{sovrano2020modelling,
  title = {Modelling {{GDPR-Compliant Explanations}} for {{Trustworthy AI}}},
  author = {Sovrano, Francesco and Vitali, Fabio and Palmirani, Monica},
  year = {2020},
  volume = {12394},
  eprint = {2109.04165},
  primaryclass = {cs},
  pages = {219--233},
  doi = {10.1007/978-3-030-58957-8_16},
  urldate = {2025-05-28},
  abstract = {Through the General Data Protection Regulation (GDPR), the European Union has set out its vision for Automated Decision- Making (ADM) and AI, which must be reliable and human-centred. In particular we are interested on the Right to Explanation, that requires industry to produce explanations of ADM. The High-Level Expert Group on Artificial Intelligence (AI-HLEG), set up to support the implementation of this vision, has produced guidelines discussing the types of explanations that are appropriate for user-centred (interactive) Explanatory Tools. In this paper we propose our version of Explanatory Narratives (EN), based on user-centred concepts drawn from ISO 9241, as a model for user-centred explanations aligned with the GDPR and the AI-HLEG guidelines. Through the use of ENs we convert the problem of generating explanations for ADM into the identification of an appropriate path over an Explanatory Space, allowing explainees to interactively explore it and produce the explanation best suited to their needs. To this end we list suitable exploration heuristics, we study the properties and structure of explanations, and discuss the proposed model identifying its weaknesses and strengths.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  timestamp = {2025-05-28T02:11:41Z}
}

@book{spirtes2001causation,
  title = {Causation, {{Prediction}}, and {{Search}}},
  author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
  year = {2001},
  month = jan,
  edition = {2},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/1754.001.0001},
  urldate = {2025-04-13},
  abstract = {The authors address the assumptions and methods that allow us to turn observations into causal knowledge, and use even incomplete causal knowledge in planning and prediction to influence and control our environment.             What assumptions and methods allow us to turn observations into causal knowledge, and how can even incomplete causal knowledge be used in planning and prediction to influence and control our environment? In this book Peter Spirtes, Clark Glymour, and Richard Scheines address these questions using the formalism of Bayes networks, with results that have been applied in diverse areas of research in the social, behavioral, and physical sciences.             The authors show that although experimental and observational study designs may not always permit the same inferences, they are subject to uniform principles. They axiomatize the connection between causal structure and probabilistic independence, explore several varieties of causal indistinguishability, formulate a theory of manipulation, and develop asymptotically reliable procedures for searching over equivalence classes of causal models, including models of categorical data and structural equation models with and without latent variables.             The authors show that the relationship between causality and probability can also help to clarify such diverse topics in statistics as the comparative power of experimentation versus observation, Simpson's paradox, errors in regression models, retrospective versus prospective sampling, and variable selection.             The second edition contains a new introduction and an extensive survey of advances and applications that have appeared since the first edition was published in 1993.             Bradford Books imprint},
  isbn = {978-0-262-28415-8},
  langid = {english},
  timestamp = {2025-04-13T07:22:31Z}
}

@article{splawa1990application,
  title = {On the Application of Probability Theory to Agricultural Experiments. {{Essay}} on Principles. {{Section}} 9},
  author = {{Splawa-Neyman}, Jerzy and Dabrowska, Dorota M and Speed, Terrence P},
  year = {1990},
  journal = {Statistical Science},
  pages = {465--472},
  publisher = {JSTOR},
  timestamp = {2025-03-19T12:20:50Z}
}

@article{stepin2021survey,
  title = {A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence},
  author = {Stepin, Ilia and Alonso, Jose M and Catala, Alejandro and {Pereira-Fari{\~n}a}, Mart{\'{\i}}n},
  year = {2021},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {9},
  pages = {11974--12001},
  publisher = {IEEE},
  timestamp = {2025-03-29T14:03:35Z}
}

@article{stiglic2020interpretability,
  title = {Interpretability of Machine Learning Based Prediction Models in Healthcare},
  author = {Stiglic, Gregor and Kocbek, Primoz and Fijacko, Nino and Zitnik, Marinka and Verbert, Katrien and Cilar, Leona},
  year = {2020},
  month = sep,
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {10},
  number = {5},
  eprint = {2002.08596},
  primaryclass = {cs},
  pages = {e1379},
  issn = {1942-4787, 1942-4795},
  doi = {10.1002/widm.1379},
  urldate = {2025-03-25},
  abstract = {There is a need of ensuring machine learning models that are interpretable. Higher interpretability of the model means easier comprehension and explanation of future predictions for end-users. Further, interpretable machine learning models allow healthcare experts to make reasonable and data-driven decisions to provide personalized decisions that can ultimately lead to higher quality of service in healthcare. Generally, we can classify interpretability approaches in two groups where the first focuses on personalized interpretation (local interpretability) while the second summarizes prediction models on a population level (global interpretability). Alternatively, we can group interpretability methods into model-specific techniques, which are designed to interpret predictions generated by a specific model, such as a neural network, and model-agnostic approaches, which provide easy-to-understand explanations of predictions made by any machine learning model. Here, we give an overview of interpretability approaches and provide examples of practical interpretability of machine learning in different areas of healthcare, including prediction of health-related outcomes, optimizing treatments or improving the efficiency of screening for specific conditions. Further, we outline future directions for interpretable machine learning and highlight the importance of developing algorithmic solutions that can enable machine-learning driven decision making in high-stakes healthcare problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {TLDR: An overview of interpretability approaches is given and examples of practical interpretability of machine learning in different areas of healthcare, including prediction of health-related outcomes, optimizing treatments or improving the efficiency of screening for specific conditions are provided.},
  timestamp = {2025-03-25T11:50:48Z}
}

@article{strielkowski2024ai,
  title = {{{AI-driven}} Adaptive Learning for Sustainable Educational Transformation},
  author = {Strielkowski, Wadim and Grebennikova, Veronika and Lisovskiy, Alexander and Rakhimova, Guzalbegim and Vasileva, Tatiana},
  year = {2024},
  journal = {Sustainable Development},
  publisher = {Wiley Online Library},
  timestamp = {2025-04-16T02:24:17Z}
}

@article{sudlow2015ukbiobank,
  title = {{{UK}} Biobank: {{An}} Open Access Resource for Identifying the Causes of a Wide Range of Complex Diseases of Middle and Old Age},
  author = {Sudlow, Cathie and Gallacher, John and Allen, Naomi and Beral, Valerie and Burton, Paul and Danesh, John and {and}, Elina Ehteshamzadeh},
  year = {2015},
  journal = {PLoS Medicine},
  volume = {12},
  number = {3},
  pages = {e1001779},
  doi = {10.1371/journal.pmed.1001779},
  annotation = {TLDR: The UK Biobank is described, a large population-based prospective study, established to allow investigation of the genetic and non-genetic determinants of the diseases of middle and old age.},
  timestamp = {2025-04-12T13:38:50Z}
}

@inproceedings{sui2022causal,
  title = {Causal Attention for Interpretable and Generalizable Graph Classification},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD}} Conference on Knowledge Discovery and Data Mining},
  author = {Sui, Yongduo and Wang, Xiang and Wu, Jiancan and Lin, Min and He, Xiangnan and Chua, Tat-Seng},
  year = {2022},
  pages = {1696--1705},
  timestamp = {2025-03-23T02:59:56Z}
}

@article{sun2023digital,
  title = {Digital Twin in Healthcare: {{Recent}} Updates and Challenges},
  shorttitle = {Digital Twin in Healthcare},
  author = {Sun, Tianze and He, Xiwang and Li, Zhonghai},
  year = {2023},
  month = jan,
  journal = {Digital Health},
  volume = {9},
  pages = {20552076221149651},
  issn = {2055-2076},
  doi = {10.1177/20552076221149651},
  urldate = {2025-04-12},
  abstract = {As simulation is playing an increasingly important role in medicine, providing the individual patient with a customised diagnosis and treatment is envisaged as part of future precision medicine. Such customisation will become possible through the emergence of digital twin (DT) technology. The objective of this article is to review the progress of prominent research on DT technology in medicine and discuss the potential applications and future opportunities as well as several challenges remaining in digital healthcare. A review of the literature was conducted using PubMed, Web of Science, Google Scholar, Scopus and related bibliographic resources, in which the following terms and their derivatives were considered during the search: DT, medicine and digital health virtual healthcare. Finally, analyses of the literature yielded 465 pertinent articles, of which we selected 22 for detailed review. We summarised the application examples of DT in medicine and analysed the applications in many fields of medicine. It revealed encouraging results that DT is being increasing applied in medicine. Results from this literature review indicated that DT healthcare, as a key fusion approach of future medicine, will bring the advantages of precision diagnose and personalised treatment into reality.},
  pmcid = {PMC9830576},
  pmid = {36636729},
  annotation = {TLDR: Results from this literature review indicated that DT healthcare, as a key fusion approach of future medicine, will bring the advantages of precision diagnose and personalised treatment into reality.},
  timestamp = {2025-04-12T08:03:20Z}
}

@misc{sun2024explainable,
  ids = {sun2024explainablea},
  title = {Explainable {{Artificial Intelligence}} for {{Medical Applications}}: {{A Review}}},
  shorttitle = {Explainable {{Artificial Intelligence}} for {{Medical Applications}}},
  author = {Sun, Qiyang and Akman, Alican and Schuller, Bj{\"o}rn W.},
  year = {2024},
  month = nov,
  number = {arXiv:2412.01829},
  eprint = {2412.01829},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.01829},
  urldate = {2025-06-13},
  abstract = {The continuous development of artificial intelligence (AI) theory has propelled this field to unprecedented heights, owing to the relentless efforts of scholars and researchers. In the medical realm, AI takes a pivotal role, leveraging robust machine learning (ML) algorithms. AI technology in medical imaging aids physicians in X-ray, computed tomography (CT) scans, and magnetic resonance imaging (MRI) diagnoses, conducts pattern recognition and disease prediction based on acoustic data, delivers prognoses on disease types and developmental trends for patients, and employs intelligent health management wearable devices with human-computer interaction technology to name but a few. While these well-established applications have significantly assisted in medical field diagnoses, clinical decision-making, and management, collaboration between the medical and AI sectors faces an urgent challenge: How to substantiate the reliability of decision-making? The underlying issue stems from the conflict between the demand for accountability and result transparency in medical scenarios and the black-box model traits of AI. This article reviews recent research grounded in explainable artificial intelligence (XAI), with an emphasis on medical practices within the visual, audio, and multimodal perspectives. We endeavour to categorise and synthesise these practices, aiming to provide support and guidance for future researchers and healthcare professionals.},
  archiveprefix = {arXiv},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  timestamp = {2025-08-15T10:03:39Z}
}

@article{sun2025explainable,
  title = {Explainable {{Artificial Intelligence}} for {{Medical Applications}}: {{A Review}}},
  shorttitle = {Explainable {{Artificial Intelligence}} for {{Medical Applications}}},
  author = {Sun, Qiyang and Akman, Alican and Schuller, Bj{\"o}rn W.},
  year = {2025},
  month = apr,
  journal = {ACM Transactions on Computing for Healthcare},
  volume = {6},
  number = {2},
  pages = {1--31},
  issn = {2637-8051},
  doi = {10.1145/3709367},
  urldate = {2025-04-06},
  abstract = {The continuous development of artificial intelligence (AI) theory has propelled this field to unprecedented heights, owing to the relentless efforts of scholars and researchers. In the medical realm, AI takes a pivotal role, leveraging robust machine learning (ML) algorithms. AI technology in medical imaging aids physicians in X-ray, computed tomography (CT) scans, and magnetic resonance imaging (MRI) diagnoses, conducts pattern recognition and disease prediction based on acoustic data, delivers prognoses on disease types and developmental trends for patients, and employs intelligent health management wearable devices with human-computer interaction technology to name but a few. While these well-established applications have significantly assisted in medical field diagnoses, clinical decision-making, and management, collaboration between the medical and AI sectors faces an urgent challenge: How to substantiate the reliability of decision-making? The underlying issue stems from the conflict between the demand for accountability and result transparency in medical scenarios and the black-box model traits of AI. This article reviews recent research grounded in explainable artificial intelligence (XAI), with an emphasis on medical practices within the visual, audio, and multimodal perspectives. We endeavor to categorize and synthesize these practices, aiming to provide support and guidance for future researchers and healthcare professionals.},
  langid = {english},
  annotation = {TLDR: This article reviews recent research grounded in explainable artificial intelligence (XAI), with an emphasis on medical practices within the visual, audio, and multimodal perspectives, and endeavour to categorise and synthesise these practices, aiming to provide support and guidance for future researchers and healthcare professionals.},
  timestamp = {2025-04-06T13:16:17Z}
}

@article{sutrave2025explainable,
  title = {Explainable {{AI}} Methods in Medical Image Analysis},
  author = {Sutrave, Kruttika and Mannem, Mallikarjuna Rao and Sattu, Mani Sharath Chandra},
  year = {2025},
  timestamp = {2025-08-13T10:55:44Z}
}

@article{szklarczyk2023string,
  title = {The {{STRING}} Database in 2023: Protein--Protein Association Networks and Functional Enrichment Analyses for Any Sequenced Genome of Interest},
  shorttitle = {The {{STRING}} Database in 2023},
  author = {Szklarczyk, Damian and Kirsch, Rebecca and Koutrouli, Mikaela and Nastou, Katerina and Mehryary, Farrokh and Hachilif, Radja and Gable, Annika L and Fang, Tao and Doncheva, Nadezhda~T and Pyysalo, Sampo and Bork, Peer and Jensen, Lars~J and {von~Mering}, Christian},
  year = {2023},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {51},
  number = {D1},
  pages = {D638-D646},
  issn = {0305-1048},
  doi = {10.1093/nar/gkac1000},
  urldate = {2025-05-17},
  abstract = {Much of the complexity within cells arises from functional and regulatory interactions among proteins. The core of these interactions is increasingly known, but novel interactions continue to be discovered, and the information remains scattered across different database resources, experimental modalities and levels of mechanistic detail. The STRING database (https://string-db.org/) systematically collects and integrates protein--protein interactions---both physical interactions as well as functional associations. The data originate from a number of sources: automated text mining of the scientific literature, computational interaction predictions from co-expression, conserved genomic context, databases of interaction experiments and known complexes/pathways from curated sources. All of these interactions are critically assessed, scored, and subsequently automatically transferred to less well-studied organisms using hierarchical orthology information. The data can be accessed via the website, but also programmatically and via bulk downloads. The most recent developments in STRING (version 12.0) are: (i) it is now possible to create, browse and analyze a full interaction network for any novel genome of interest, by submitting its complement of encoded proteins, (ii) the co-expression channel now uses variational auto-encoders to predict interactions, and it covers two new sources, single-cell RNA-seq and experimental proteomics data and (iii) the confidence in each experimentally derived interaction is now estimated based on the detection method used, and communicated to the user in the web-interface. Furthermore, STRING continues to enhance its facilities for functional enrichment analysis, which are now fully available also for user-submitted genomes.},
  langid = {american},
  annotation = {TLDR: The most recent developments in STRING are: it is now possible to create, browse and analyze a full interaction network for any novel genome of interest, by submitting its complement of encoded proteins, and the co-expression channel now uses variational auto-encoders to predict interactions.},
  timestamp = {2025-05-17T08:25:50Z}
}

@inproceedings{tal2023target,
  title = {Target Specification Bias, Counterfactual Prediction, and Algorithmic Fairness in Healthcare},
  booktitle = {Proceedings of the 2023 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Tal, Eran},
  year = {2023},
  month = aug,
  eprint = {2308.02081},
  primaryclass = {cs},
  pages = {312--321},
  doi = {10.1145/3600211.3604678},
  urldate = {2025-05-05},
  abstract = {Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology - the science of measurement - suggests ways of counteracting target specification bias and avoiding its harmful consequences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Methodology},
  annotation = {TLDR: A more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias, which gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients.},
  timestamp = {2025-05-05T14:12:33Z}
}

@article{talukder2021interpretation,
  title = {Interpretation of Deep Learning in Genomics and Epigenomics},
  author = {Talukder, Amlan and Barham, Clayton and Li, Xiaoman and Hu, Haiyan},
  year = {2021},
  month = may,
  journal = {Briefings in Bioinformatics},
  volume = {22},
  number = {3},
  pages = {bbaa177},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbaa177},
  urldate = {2025-04-18},
  abstract = {Abstract             Machine learning methods have been widely applied to big data analysis in genomics and epigenomics research. Although accuracy and efficiency are common goals in many modeling tasks, model interpretability is especially important to these studies towards understanding the underlying molecular and cellular mechanisms. Deep neural networks (DNNs) have recently gained popularity in various types of genomic and epigenomic studies due to their capabilities in utilizing large-scale high-throughput bioinformatics data and achieving high accuracy in predictions and classifications. However, DNNs are often challenged by their potential to explain the predictions due to their black-box nature. In this review, we present current development in the model interpretation of DNNs, focusing on their applications in genomics and epigenomics. We first describe state-of-the-art DNN interpretation methods in representative machine learning fields. We then summarize the DNN interpretation methods in recent studies on genomics and epigenomics, focusing on current data- and computing-intensive topics such as sequence motif identification, genetic variations, gene expression, chromatin interactions and non-coding RNAs. We also present the biological discoveries that resulted from these interpretation methods. We finally discuss the advantages and limitations of current interpretation approaches in the context of genomic and epigenomic studies. Contact:xiaoman@mail.ucf.edu, haihu@cs.ucf.edu},
  copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  annotation = {TLDR: This review presents current development in the model interpretation of DNNs, focusing on their applications in genomics and epigenomics, and describes state-of-the-art DNN interpretation methods in representative machine learning fields.},
  timestamp = {2025-04-18T03:09:29Z}
}

@misc{tamoyan2025factual,
  title = {Factual {{Self-Awareness}} in {{Language Models}}: {{Representation}}, {{Robustness}}, and {{Scaling}}},
  shorttitle = {Factual {{Self-Awareness}} in {{Language Models}}},
  author = {Tamoyan, Hovhannes and Dutta, Subhabrata and Gurevych, Iryna},
  year = {2025},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2505.21399},
  urldate = {2025-09-01},
  abstract = {Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). In this work, we provide evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation. We demonstrate that for a given subject entity and a relation, LLMs internally encode linear features in the Transformer's residual stream that dictate whether it will be able to recall the correct attribute (that forms a valid entity-relation-attribute triplet). This self-awareness signal is robust to minor formatting variations. We investigate the effects of context perturbation via different example selection strategies. Scaling experiments across model sizes and training dynamics highlight that self-awareness emerges rapidly during training and peaks in intermediate layers. These findings uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  annotation = {TLDR: This work provides evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation, and uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.},
  timestamp = {2025-09-01T12:29:01Z}
}

@article{tan2022multimodal,
  title = {A Multi-Modal Fusion Framework Based on Multi-Task Correlation Learning for Cancer Prognosis Prediction},
  author = {Tan, Kaiwen and Huang, Weixian and Liu, Xiaofeng and Hu, Jinlong and Dong, Shoubin},
  year = {2022},
  month = apr,
  journal = {Artificial Intelligence in Medicine},
  volume = {126},
  pages = {102260},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2022.102260},
  urldate = {2025-04-20},
  abstract = {Morphological attributes from histopathological images and molecular profiles from genomic data are important information to drive diagnosis, prognosis, and therapy of cancers. By integrating these heterogeneous but complementary data, many multi-modal methods are proposed to study the complex mechanisms of cancers, and most of them achieve comparable or better results from previous single-modal methods. However, these multi-modal methods are restricted to a single task (e.g., survival analysis or grade classification), and thus neglect the correlation between different tasks. In this study, we present a multi-modal fusion framework based on multi-task correlation learning (MultiCoFusion) for survival analysis and cancer grade classification, which combines the power of multiple modalities and multiple tasks. Specifically, a pre-trained ResNet-152 and a sparse graph convolutional network (SGCN) are used to learn the representations of histopathological images and mRNA expression data respectively. Then these representations are fused by a fully connected neural network (FCNN), which is also a multi-task shared network. Finally, the results of survival analysis and cancer grade classification output simultaneously. The framework is trained by an alternate scheme. We systematically evaluate our framework using glioma datasets from The Cancer Genome Atlas (TCGA). Results demonstrate that MultiCoFusion learns better representations than traditional feature extraction methods. With the help of multi-task alternating learning, even simple multi-modal concatenation can achieve better performance than other deep learning and traditional methods. Multi-task learning can improve the performance of multiple tasks not just one of them, and it is effective in both single-modal and multi-modal data.},
  keywords = {Cancer grade,Multi-modal fusion,Multi-task learning,Survival analysis},
  timestamp = {2025-04-20T11:26:19Z}
}

@article{tan2023transfer,
  title = {A {{Transfer Learning Approach}} to {{Breast Cancer Classification}} in a {{Federated Learning Framework}}},
  author = {Tan, Y. Nguyen and Tinh, Vo Phuc and Lam, Pham Duc and Nam, Nguyen Hoang and Khoa, Tran Anh},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {27462--27476},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3257562},
  urldate = {2025-04-04},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  annotation = {TLDR: A federated learning (FL) facility that extracts features from participating environments rather than a CL facility is developed that leads to much higher classification performance than other approaches and is viable for use in AI healthcare applications.},
  timestamp = {2025-04-04T06:36:00Z}
}

@article{tanevski2022explainable,
  title = {Explainable Multiview Framework for Dissecting Spatial Relationships from Highly Multiplexed Data},
  author = {Tanevski, Jovan and Flores, Ricardo Omar Ramirez and Gabor, Attila and Schapiro, Denis and {Saez-Rodriguez}, Julio},
  year = {2022},
  month = apr,
  journal = {Genome Biology},
  volume = {23},
  number = {1},
  pages = {97},
  issn = {1474-760X},
  doi = {10.1186/s13059-022-02663-5},
  urldate = {2025-04-19},
  abstract = {The advancement of highly multiplexed spatial technologies requires scalable methods that can leverage spatial information. We present MISTy, a flexible, scalable, and explainable machine learning framework for extracting relationships from any spatial omics data, from dozens to thousands of measured markers. MISTy builds multiple views focusing on different spatial or functional contexts to dissect different effects. We evaluated MISTy on in silico and breast cancer datasets measured by imaging mass cytometry and spatial transcriptomics. We estimated structural and functional interactions coming from different spatial contexts in breast cancer and demonstrated how to relate MISTy's results to clinical features.},
  keywords = {Intercellular signaling,Machine learning,Multiplexed data,Spatial omics},
  annotation = {TLDR: This work presents MISTy, a flexible, scalable, and explainable machine learning framework for extracting relationships from any spatial omics data, from dozens to thousands of measured markers.},
  timestamp = {2025-04-19T04:03:48Z}
}

@inproceedings{tang2020clinicianintheloop,
  title = {Clinician-in-the-Loop Decision Making: Reinforcement Learning with near-Optimal Set-Valued Policies},
  shorttitle = {Clinician-in-the-Loop Decision Making},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Tang, Shengpu and Modi, Aditya and Sjoding, Michael W. and Wiens, Jenna},
  year = {2020},
  month = jul,
  series = {{{ICML}}'20},
  volume = {119},
  pages = {9387--9396},
  publisher = {JMLR.org},
  urldate = {2025-05-03},
  abstract = {Standard reinforcement learning (RL) aims to find an optimal policy that identifies the best action for each state. However, in healthcare settings, many actions may be near-equivalent with respect to the reward (e.g., survival). We consider an alternative objective - learning set-valued policies to capture near-equivalent actions that lead to similar cumulative rewards. We propose a model-free algorithm based on temporal difference learning and a near-greedy heuristic for action selection. We analyze the theoretical properties of the proposed algorithm, providing optimality guarantees and demonstrate our approach on simulated environments and a real clinical task. Empirically, the proposed algorithm exhibits good convergence properties and discovers meaning-ful near-equivalent actions. Our work provides theoretical, as well as practical, foundations for clinician/human-in-the-loop decision making, in which humans (e.g., clinicians, patients) can in-corporate additional knowledge (e.g., side effects, patient preference) when selecting among near-equivalent actions.},
  timestamp = {2025-05-03T15:50:14Z}
}

@misc{tang2024interpretable,
  title = {Interpretable High-Resolution Dimension Reduction of Spatial Transcriptomics Data by {{SpaHDmap}}},
  author = {Tang, Junjie and Chen, Zihao and Qian, Kun and Huang, Siyuan and He, Yang and Yin, Shenyi and He, Xinyu and Ye, Buqing and Zhuang, Yan and Meng, Hongxue and Xi, Jianzhong Jeff and Xi, Ruibin},
  year = {2024},
  month = sep,
  publisher = {Bioinformatics},
  doi = {10.1101/2024.09.12.612666},
  urldate = {2025-04-21},
  abstract = {Abstract           Spatial transcriptomics (ST) technologies have revolutionized tissue architecture studies by capturing gene expression with spatial context. However, high-dimensional ST data often have limited spatial resolution and exhibit considerable noise and sparsity, posing significant challenges in deciphering subtle spatial structures and underlying biological activities. Here, we introduce SpaHDmap, an interpretable dimension reduction framework that enhances spatial resolution by integrating ST gene expression with high-resolution histology images. SpaHDmap incorporates non-negative matrix factorization into a multimodal fusion encoder-decoder architecture, enabling the identification of interpretable, high-resolution embeddings. Furthermore, SpaHDmap can simultaneously analyze multiple samples and is compatible with various types of histology images. Extensive evaluations on synthetic and real ST datasets from various technologies and tissue types demonstrate that SpaHDmap can effectively produce highly interpretable, high-resolution embeddings, and detects refined spatial structures. SpaHDmap represents a powerful approach for integrating ST data and histology images, offering deeper insights into complex tissue structures and functions.},
  archiveprefix = {Bioinformatics},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  timestamp = {2025-04-21T13:06:06Z}
}

@article{tanno2025collaboration,
  title = {Collaboration between Clinicians and Vision--Language Models in Radiology Report Generation},
  author = {Tanno, Ryutaro and Barrett, David G. T. and Sellergren, Andrew and Ghaisas, Sumedh and Dathathri, Sumanth and See, Abigail and Welbl, Johannes and Lau, Charles and Tu, Tao and Azizi, Shekoofeh and Singhal, Karan and Schaekermann, Mike and May, Rhys and Lee, Roy and Man, SiWai and Mahdavi, Sara and Ahmed, Zahra and Matias, Yossi and Barral, Joelle and Eslami, S. M. Ali and Belgrave, Danielle and Liu, Yun and Kalidindi, Sreenivasa Raju and Shetty, Shravya and Natarajan, Vivek and Kohli, Pushmeet and Huang, Po-Sen and Karthikesalingam, Alan and Ktena, Ira},
  year = {2025},
  month = feb,
  journal = {Nature Medicine},
  volume = {31},
  number = {2},
  pages = {599--608},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-024-03302-1},
  urldate = {2025-03-03},
  abstract = {Automated radiology report generation has the potential to improve patient care and reduce the workload of radiologists. However, the path toward real-world adoption has been stymied by the challenge of evaluating the clinical quality of artificial intelligence (AI)-generated reports. We build a state-of-the-art report generation system for chest radiographs, called Flamingo-CXR, and perform an expert evaluation of AI-generated reports by engaging a panel of board-certified radiologists. We observe a wide distribution of preferences across the panel and across clinical settings, with 56.1\% of Flamingo-CXR intensive care reports evaluated to be preferable or equivalent to clinician reports, by half or more of the panel, rising to 77.7\% for in/outpatient X-rays overall and to 94\% for the subset of cases with no pertinent abnormal findings. Errors were observed in human-written reports and Flamingo-CXR reports, with 24.8\% of in/outpatient cases containing clinically significant errors in both report types, 22.8\% in Flamingo-CXR reports only and 14.0\% in human reports only. For reports that contain errors we develop an assistive setting, a demonstration of clinician--AI collaboration for radiology report composition, indicating new possibilities for potential clinical utility.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Medical imaging,Medical research},
  annotation = {TLDR: A state-of-the-art report generation system for chest radiographs, called Flamingo-CXR, is built, and an expert evaluation of AI-generated reports is performed by engaging a panel of board-certified radiologists, indicating new possibilities for potential clinical utility.},
  timestamp = {2025-03-03T07:12:00Z}
}

@article{tanzi2021realtime,
  title = {Real-Time Deep Learning Semantic Segmentation during Intra-Operative Surgery for {{3D}} Augmented Reality Assistance},
  author = {Tanzi, Leonardo and Piazzolla, Pietro and Porpiglia, Francesco and Vezzetti, Enrico},
  year = {2021},
  month = sep,
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  volume = {16},
  number = {9},
  pages = {1435--1445},
  issn = {1861-6429},
  doi = {10.1007/s11548-021-02432-y},
  urldate = {2025-05-04},
  abstract = {The current study aimed to propose a Deep Learning (DL) and Augmented Reality (AR) based solution for a in-vivo robot-assisted radical prostatectomy (RARP), to improve the precision of a published work from our group. We implemented a two-steps automatic system to align a 3D virtual ad-hoc model of a patient's organ with its 2D endoscopic image, to assist surgeons during the procedure.},
  langid = {english},
  keywords = {Deep learning,Intra-operative,Neural network,Semantic segmentation},
  annotation = {TLDR: A two-steps automatic system to align a 3D virtual ad-hoc model of a patient's organ with its 2D endoscopic image, to assist surgeons during the procedure, to improve the precision of a published work.},
  timestamp = {2025-05-04T01:54:41Z}
}

@article{tao2022interpretable,
  title = {Interpretable Deep Learning for Chromatin-Informed Inference of Transcriptional Programs Driven by Somatic Alterations across Cancers},
  author = {Tao, Yifeng and Ma, Xiaojun and Palmer, Drake and Schwartz, Russell and Lu, Xinghua and Osmanbeyoglu, Hatice~Ulku},
  year = {2022},
  month = oct,
  journal = {Nucleic Acids Research},
  volume = {50},
  number = {19},
  pages = {10869--10881},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkac881},
  urldate = {2025-07-05},
  abstract = {Abstract             Cancer is a disease of gene dysregulation, where cells acquire somatic and epigenetic alterations that drive aberrant cellular signaling. These alterations adversely impact transcriptional programs and cause profound changes in gene expression. Interpreting somatic alterations within context-specific transcriptional programs will facilitate personalized therapeutic decisions but is a monumental task. Toward this goal, we develop a partially interpretable neural network model called Chromatin-informed Inference of Transcriptional Regulators Using Self-attention mechanism (CITRUS). CITRUS models the impact of somatic alterations on transcription factors and downstream transcriptional programs. Our approach employs a self-attention mechanism to model the contextual impact of somatic alterations. Furthermore, CITRUS uses a layer of hidden nodes to explicitly represent the state of transcription factors (TFs) to learn the relationships between TFs and their target genes based on TF binding motifs in the open chromatin regions of tumor samples. We apply CITRUS to genomic, transcriptomic, and epigenomic data from 17 cancer types profiled by The Cancer Genome Atlas. CITRUS predicts patient-specific TF activities and reveals transcriptional program variations between and within tumor types. We show that CITRUS yields biological insights into delineating TFs associated with somatic alterations in individual tumors. Thus, CITRUS is a promising tool for precision oncology.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: A partially interpretable neural network model with encoder-decoder architecture, called Chromatin-informed Inference of Transcriptional Regulators Using Self-attention mechanism (CITRUS), to model the impact of somatic alterations on cellular states and further onto downstream gene expression programs.},
  timestamp = {2025-07-05T13:05:38Z}
}

@article{tarabanis2023explainable,
  title = {Explainable {{SHAP-XGBoost}} Models for in-Hospital Mortality after Myocardial Infarction},
  author = {Tarabanis, Constantine and Kalampokis, Evangelos and Khalil, Mahmoud and Alviar, Carlos L. and Chinitz, Larry A. and Jankelson, Lior},
  year = {2023},
  month = aug,
  journal = {Cardiovascular Digital Health Journal},
  volume = {4},
  number = {4},
  pages = {126--132},
  issn = {2666-6936},
  doi = {10.1016/j.cvdhj.2023.06.001},
  abstract = {BACKGROUND: A lack of explainability in published machine learning (ML) models limits clinicians' understanding of how predictions are made, in turn undermining uptake of the models into clinical practice. OBJECTIVE: The purpose of this study was to develop explainable ML models to predict in-hospital mortality in patients hospitalized for myocardial infarction (MI). METHODS: Adult patients hospitalized for an MI were identified in the National Inpatient Sample between January 1, 2012, and September 30, 2015. The resulting cohort comprised 457,096 patients described by 64 predictor variables relating to demographic/comorbidity characteristics and in-hospital complications. The gradient boosting algorithm eXtreme Gradient Boosting (XGBoost) was used to develop explainable models for in-hospital mortality prediction in the overall cohort and patient subgroups based on MI type and/or sex. RESULTS: The resulting models exhibited an area under the receiver operating characteristic curve (AUC) ranging from 0.876 to 0.942, specificity 82\% to 87\%, and sensitivity 75\% to 87\%. All models exhibited high negative predictive value {$\geq$}0.974. The SHapley Additive exPlanation (SHAP) framework was applied to explain the models. The top predictor variables of increasing and decreasing mortality were age and undergoing percutaneous coronary intervention, respectively. Other notable findings included a decreased mortality risk associated with certain patient subpopulations with hyperlipidemia and a comparatively greater risk of death among women below age 55 years. CONCLUSION: The literature lacks explainable ML models predicting in-hospital mortality after an MI. In a national registry, explainable ML models performed best in ruling out in-hospital death post-MI, and their explanation illustrated their potential for guiding hypothesis generation and future study design.},
  langid = {english},
  pmcid = {PMC10435947},
  pmid = {37600443},
  keywords = {Acute coronary syndrome,Explainable machine learning,In-hospital mortality SHAP,Myocardial infarction},
  annotation = {TLDR: In a national registry, explainable ML models performed best in ruling out in-hospital death post-MI, and their explanation illustrated their potential for guiding hypothesis generation and future study design.},
  timestamp = {2025-06-12T15:50:35Z}
}

@article{tariq2025multimodal,
  title = {Multimodal Artificial Intelligence Models for Radiology},
  author = {Tariq, Amara and Banerjee, Imon and Trivedi, Hari and Gichoya, Judy},
  year = {2025},
  month = jan,
  journal = {BJR{\textbar}Artificial Intelligence},
  volume = {2},
  number = {1},
  pages = {ubae017},
  issn = {2976-8705},
  doi = {10.1093/bjrai/ubae017},
  urldate = {2025-04-12},
  abstract = {Artificial intelligence (AI) models in medicine often fall short in real-world deployment due to inability to incorporate multiple data modalities in their decision-making process as clinicians do. Clinicians integrate evidence and signals from multiple data sources like radiology images, patient clinical status as recorded in electronic health records, consultations from fellow providers, and even subtle clues using the appearance of a patient, when making decisions about diagnosis or treatment. To bridge this gap, significant research effort has focused on building fusion models capable of harnessing multi-modal data for advanced decision making. We present a broad overview of the landscape of research in multimodal AI for radiology covering a wide variety of approaches from traditional fusion modelling to modern vision-language models. We provide analysis of comparative merits and drawbacks of each approach to assist future research and highlight ethical consideration in developing multimodal AI. In practice, the quality and quantity of available training data, availability of computational resources, and clinical application dictates which fusion method may be most suitable.},
  annotation = {TLDR: A broad overview of the landscape of research in multimodal AI for radiology covering a wide variety of approaches from traditional fusion modeling to modern vision-language models is presented and analysis of comparative merits and drawbacks of each approach is provided.},
  timestamp = {2025-04-12T07:25:23Z}
}

@article{tarski1944,
  title = {The Semantic Conception of Truth and the Foundations of Semantics},
  author = {Tarski, Alfred},
  year = {1944},
  journal = {Philosophy and Phenomenological Research},
  volume = {4},
  number = {3},
  pages = {341--376},
  timestamp = {2025-09-06T00:42:24Z}
}

@article{tegaw2025machine,
  title = {Machine Learning Analysis of Survival Outcomes in Breast Cancer Patients Treated with Chemotherapy, Hormone Therapy, Surgery, and Radiotherapy},
  author = {Tegaw, Eyachew Misganew and Asfaw, Betelhem Bizuneh},
  year = {2025},
  month = jul,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {24981},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-97763-0},
  urldate = {2025-08-11},
  abstract = {Breast cancer continues to be a leading cause of death among women in the world. The prediction of survival outcomes based on treatment modalities, i.e., chemotherapy, hormone therapy, surgery, and radiation therapy is an essential step towards personalization in treatment planning. However, Machine Learning (ML) models may improve these predictions by investigating intricate relationships between clinical variables and survival. This study investigates the performance of several ML models to predict survival rate in patients undergoing diverse breast cancer treatments i.e., chemotherapy, hormone therapy, surgery and radiation using multiple clinical parameters. The dataset consisted of 5000 samples and turned into downloaded from Kaggle. The models assessed blanketed Support Vector Machines (SVM), K-Nearest Neighbor (KNN), AdaBoost, Gradient Boosting, Random Forest, Gaussian Naive Bayes, Logistic Regression, Extreme Gradient Boosting (XG boost), and Decision tree. Performance of the models was assessed using parameters such as Accuracy, Precision, Recall, F1-Score and Area under the Receiver Operating Characteristic Curve (AUC-ROC). SHAP (SHapley Additive exPlanations) values analysis was done to provide an explanation for the impact of a feature on model predictions using Waterfall and Beeswarm plots. Anticipated baseline (E(f(x))) were in comparison to the predictions (f(x)) for each therapy group. Performance of Gradient Boosting was shown to be the best with an Accuracy: 0.972, Precision: 0.973, Recall: 0.972, F1-Score: 0.973, and AUC-ROC Score: 0.997. Chemotherapy had a notably bad impact on survival, with an f(x) of -0.274 and an E(f(x)) of -0.025. Hormone therapy showed the maximum detrimental effect on survival, with an f(x) of -0.408. Surgical operation had an extraordinarily impartial impact (f(x) = -0.041), even as radiation therapy positively impacted survival results with an f(x) of 0.22. Gradient Boosting was the most predictive algorithm for breast cancer survival outcomes. This SHAP-primarily based analysis provides a complete knowledge of ways one-of-a-kind treatments have an effect on survival predictions in breast cancer patients. Radiation therapy indicates the most tremendous effect on survival, whilst hormone therapy reveals the maximum poor effect. Future studies need to explore personalized treatment strategies that comprise these insights to enhance patient effects.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Cancer,Oncology},
  annotation = {TLDR: Gradient Boosting was the most predictive algorithm for breast cancer survival outcomes, whilst radiation therapy indicates the most tremendous effect on survival, whilst hormone therapy reveals the maximum poor effect.},
  timestamp = {2025-08-11T03:32:59Z}
}

@article{tejada-lapuerta2025causal,
  ids = {tejada2025causal},
  title = {Causal Machine Learning for Single-Cell Genomics},
  author = {{Tejada-Lapuerta}, Alejandro and Bertin, Paul and Bauer, Stefan and Aliee, Hananeh and Bengio, Yoshua and Theis, Fabian J.},
  year = {2025},
  month = apr,
  journal = {Nature Genetics},
  volume = {57},
  number = {4},
  pages = {797--808},
  publisher = {Nature Publishing Group},
  issn = {1546-1718},
  doi = {10.1038/s41588-025-02124-2},
  urldate = {2025-05-12},
  abstract = {Advances in single-cell '-omics' allow unprecedented insights into the transcriptional profiles of individual cells and, when combined with large-scale perturbation screens, enable measuring of the effect of targeted perturbations on the whole transcriptome. These advances provide an opportunity to better understand the causative role of genes in complex biological processes. In this Perspective, we delineate the application of causal machine learning to single-cell genomics and its associated challenges. We first present the causal model that is most commonly applied to single-cell biology and then identify and discuss potential approaches to three open problems: the lack of generalization of models to novel experimental conditions, the complexity of interpreting learned models, and the difficulty of learning cell dynamics.},
  copyright = {2025 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cell biology,Computational science},
  annotation = {TLDR: The causal model that is most commonly applied to single-cell biology is presented and potential approaches to three open problems are identified: the lack of generalization of models to novel experimental conditions, the complexity of interpreting learned models, and the difficulty of learning cell dynamics are discussed.},
  timestamp = {2025-05-12T00:33:19Z}
}

@article{teng2024personalized,
  title = {Personalized Three-Year Survival Prediction and Prognosis Forecast by Interpretable Machine Learning for Pancreatic Cancer Patients: A Population-Based Study and an External Validation},
  shorttitle = {Personalized Three-Year Survival Prediction and Prognosis Forecast by Interpretable Machine Learning for Pancreatic Cancer Patients},
  author = {Teng, Buwei and Zhang, Xiaofeng and Ge, Mingshu and Miao, Miao and Li, Wei and Ma, Jun},
  year = {2024},
  month = oct,
  journal = {Frontiers in Oncology},
  volume = {14},
  pages = {1488118},
  issn = {2234-943X},
  doi = {10.3389/fonc.2024.1488118},
  urldate = {2025-08-11},
  abstract = {Purpose               The overall survival of patients with pancreatic cancer is extremely low. We aimed to establish machine learning (ML) based model to accurately predict three-year survival and prognosis of pancreatic cancer patients.                                         Methods               We analyzed pancreatic cancer patients from the Surveillance, Epidemiology, and End Results (SEER) database between 2000 and 2021. Univariate and multivariate logistic analysis were employed to select variables. Recursive Feature Elimination (RFE) method based on 6 ML algorithms was utilized in feature selection. To construct predictive model, 13 ML algorithms were evaluated by area under the curve (AUC), area under precision-recall curve (PRAUC), accuracy, sensitivity, specificity, precision, cross-entropy, Brier scores and Balanced Accuracy (bacc) and F Beta Score (fbeta). An optimal ML model was constructed to predict three-year survival, and the predictive results were explained by SHapley Additive exPlanations (SHAP) framework. Meanwhile, 101 ML algorithm combinations were developed to select the best model with highest C-index to predict prognosis of pancreatic cancer patients.                                         Results               A total of 20,064 pancreatic cancer patients from SEER database was consecutively enrolled. We utilized eight clinical variables to establish prediction model for three-year survival. CatBoost model was selected as the best prediction model, and AUC was 0.932 [0.924, 0.939], 0.899 [0.873, 0.934] and 0.826 [0.735, 0.919] in training, internal test and external test sets, with 0.839 [0.831, 0.847] accuracy, 0.872 [0.858, 0.887] sensitivity, 0.803 [0.784, 0.825] specificity and 0.832 [0.821, 0.853] precision. Surgery type had the greatest effects on three-year survival according to SHAP results. For prognosis prediction, ``RSF+GBM'' algorithm was the best prognostic model with C-index of 0.774, 0.722 and 0.674 in training, internal test and external test sets.                                         Conclusions               Our ML models demonstrate excellent accuracy and reliability, offering more precise personalized prognostic prediction to pancreatic cancer patients.},
  langid = {american},
  annotation = {TLDR: CatBoost model was selected as the best prediction model, and AUC was 0.932, CatBoost model was selected as the best prediction model, for three-year survival according to SHAP results.},
  timestamp = {2025-08-11T03:23:43Z}
}

@misc{tian2024answers,
  title = {Beyond {{Answers}}: {{Transferring Reasoning Capabilities}} to {{Smaller LLMs Using Multi-Teacher Knowledge Distillation}}},
  shorttitle = {Beyond {{Answers}}},
  author = {Tian, Yijun and Han, Yikun and Chen, Xiusi and Wang, Wei and Chawla, Nitesh V.},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.04616},
  urldate = {2025-06-13},
  abstract = {Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a new knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that TinyLLM can outperform large teacher LLMs significantly, despite a considerably smaller model size. The source code is available at: https://github.com/YikunHan42/TinyLLM.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  timestamp = {2025-06-13T07:37:15Z}
}

@article{tiddi2022knowledge,
  title = {Knowledge Graphs as Tools for Explainable Machine Learning: {{A}} Survey},
  shorttitle = {Knowledge Graphs as Tools for Explainable Machine Learning},
  author = {Tiddi, Ilaria and Schlobach, Stefan},
  year = {2022},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {302},
  pages = {103627},
  issn = {00043702},
  doi = {10.1016/j.artint.2021.103627},
  urldate = {2024-11-11},
  langid = {english},
  timestamp = {2025-02-02T09:39:21Z}
}

@article{tjoa2021survey,
  title = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}}): {{Toward Medical XAI}}},
  shorttitle = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Tjoa, Erico and Guan, Cuntai},
  year = {2021},
  month = nov,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {11},
  pages = {4793--4813},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3027314},
  urldate = {2025-05-28},
  abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide ``obviously'' interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
  langid = {american},
  keywords = {Artificial intelligence,Explainable artificial intelligence (XAI),interpretability,Machine learning,machine learning (ML),Machine learning algorithms,medical information system,Medical information systems,survey},
  annotation = {TLDR: A review on interpretabilities suggested by different research works and categorize them is provided, hoping that insight into interpretability will be born with more considerations for medical practices and initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.},
  timestamp = {2025-05-28T10:36:15Z}
}

@article{tobias2023second,
  title = {Second International Consensus Report on Gaps and Opportunities for the Clinical Translation of Precision Diabetes Medicine},
  author = {Tobias, Deirdre K. and Merino, Jordi and Ahmad, Abrar and Aiken, Catherine and Benham, Jamie L. and Bodhini, Dhanasekaran and Clark, Amy L. and Colclough, Kevin and Corcoy, Rosa and Cromer, Sara J. and Duan, Daisy and Felton, Jamie L. and Francis, Ellen C. and Gillard, Pieter and Gingras, V{\'e}ronique and Gaillard, Romy and Haider, Eram and Hughes, Alice and Ikle, Jennifer M. and Jacobsen, Laura M. and Kahkoska, Anna R. and Kettunen, Jarno L. T. and Kreienkamp, Raymond J. and Lim, Lee-Ling and M{\"a}nnist{\"o}, Jonna M. E. and Massey, Robert and Mclennan, Niamh-Maire and Miller, Rachel G. and Morieri, Mario Luca and Most, Jasper and Naylor, Rochelle N. and Ozkan, Bige and Patel, Kashyap Amratlal and Pilla, Scott J. and Prystupa, Katsiaryna and Raghavan, Sridharan and Rooney, Mary R. and Sch{\"o}n, Martin and {Semnani-Azad}, Zhila and {Sevilla-Gonzalez}, Magdalena and Svalastoga, Pernille and Takele, Wubet Worku and Tam, Claudia Ha-Ting and Thuesen, Anne Cathrine B. and Tosur, Mustafa and Wallace, Amelia S. and Wang, Caroline C. and Wong, Jessie J. and Yamamoto, Jennifer M. and Young, Katherine and Amouyal, Chlo{\'e} and Andersen, Mette K. and Bonham, Maxine P. and Chen, Mingling and Cheng, Feifei and Chikowore, Tinashe and Chivers, Sian C. and Clemmensen, Christoffer and Dabelea, Dana and Dawed, Adem Y. and Deutsch, Aaron J. and Dickens, Laura T. and DiMeglio, Linda A. and {Dudenh{\"o}ffer-Pfeifer}, Monika and {Evans-Molina}, Carmella and {Fern{\'a}ndez-Balsells}, Mar{\'i}a Merc{\`e} and Fitipaldi, Hugo and Fitzpatrick, Stephanie L. and Gitelman, Stephen E. and Goodarzi, Mark O. and Grieger, Jessica A. and {Guasch-Ferr{\'e}}, Marta and Habibi, Nahal and Hansen, Torben and Huang, Chuiguo and {Harris-Kawano}, Arianna and Ismail, Heba M. and Hoag, Benjamin and Johnson, Randi K. and Jones, Angus G. and Koivula, Robert W. and Leong, Aaron and Leung, Gloria K. W. and Libman, Ingrid M. and Liu, Kai and Long, S. Alice and Lowe, William L. and Morton, Robert W. and Motala, Ayesha A. and {Onengut-Gumuscu}, Suna and Pankow, James S. and Pathirana, Maleesa and Pazmino, Sofia and Perez, Dianna and Petrie, John R. and Powe, Camille E. and Quinteros, Alejandra and Jain, Rashmi and Ray, Debashree and {Ried-Larsen}, Mathias and Saeed, Zeb and Santhakumar, Vanessa and Kanbour, Sarah and Sarkar, Sudipa and Monaco, Gabriela S. F. and Scholtens, Denise M. and Selvin, Elizabeth and Sheu, Wayne Huey-Herng and Speake, Cate and Stanislawski, Maggie A. and Steenackers, Nele and Steck, Andrea K. and Stefan, Norbert and St{\o}y, Julie and Taylor, Rachael and Tye, Sok Cin and Ukke, Gebresilasea Gendisha and Urazbayeva, Marzhan and {Van der Schueren}, Bart and Vatier, Camille and Wentworth, John M. and Hannah, Wesley and White, Sara L. and Yu, Gechang and Zhang, Yingchai and Zhou, Shao J. and Beltrand, Jacques and Polak, Michel and Aukrust, Ingvild and {de Franco}, Elisa and Flanagan, Sarah E. and Maloney, Kristin A. and McGovern, Andrew and Molnes, Janne and Nakabuye, Mariam and Nj{\o}lstad, P{\aa}l Rasmus and {Pomares-Millan}, Hugo and Provenzano, Michele and {Saint-Martin}, C{\'e}cile and Zhang, Cuilin and Zhu, Yeyi and Auh, Sungyoung and {de Souza}, Russell and Fawcett, Andrea J. and Gruber, Chandra and Mekonnen, Eskedar Getie and Mixter, Emily and Sherifali, Diana and Eckel, Robert H. and Nolan, John J. and Philipson, Louis H. and Brown, Rebecca J. and Billings, Liana K. and Boyle, Kristen and Costacou, Tina and Dennis, John M. and Florez, Jose C. and Gloyn, Anna L. and Gomez, Maria F. and Gottlieb, Peter A. and Greeley, Siri Atma W. and Griffin, Kurt and Hattersley, Andrew T. and Hirsch, Irl B. and Hivert, Marie-France and Hood, Korey K. and Josefson, Jami L. and Kwak, Soo Heon and Laffel, Lori M. and Lim, Siew S. and Loos, Ruth J. F. and Ma, Ronald C. W. and Mathieu, Chantal and Mathioudakis, Nestoras and Meigs, James B. and Misra, Shivani and Mohan, Viswanathan and Murphy, Rinki and Oram, Richard and Owen, Katharine R. and Ozanne, Susan E. and Pearson, Ewan R. and Perng, Wei and Pollin, Toni I. and {Pop-Busui}, Rodica and Pratley, Richard E. and Redman, Leanne M. and Redondo, Maria J. and Reynolds, Rebecca M. and Semple, Robert K. and Sherr, Jennifer L. and Sims, Emily K. and Sweeting, Arianne and Tuomi, Tiinamaija and Udler, Miriam S. and Vesco, Kimberly K. and Vilsb{\o}ll, Tina and Wagner, Robert and Rich, Stephen S. and Franks, Paul W.},
  year = {2023},
  month = oct,
  journal = {Nature Medicine},
  volume = {29},
  number = {10},
  pages = {2438--2457},
  issn = {1546-170X},
  doi = {10.1038/s41591-023-02502-5},
  abstract = {Precision medicine is part of the logical evolution of contemporary evidence-based medicine that seeks to reduce errors and optimize outcomes when making medical decisions and health recommendations. Diabetes affects hundreds of millions of people worldwide, many of whom will develop life-threatening complications and die prematurely. Precision medicine can potentially address this enormous problem by accounting for heterogeneity in the etiology, clinical presentation and pathogenesis of common forms of diabetes and risks of complications. This second international consensus report on precision diabetes medicine summarizes the findings from a systematic evidence review across the key pillars of precision medicine (prevention, diagnosis, treatment, prognosis) in four recognized forms of diabetes (monogenic, gestational, type 1, type 2). These reviews address key questions about the translation of precision medicine research into practice. Although not complete, owing to the vast literature on this topic, they revealed opportunities for the immediate or near-term clinical implementation of precision diabetes medicine; furthermore, we expose important gaps in knowledge, focusing on the need to obtain new clinically relevant evidence. Gaps include the need for common standards for clinical readiness, including consideration of cost-effectiveness, health equity, predictive accuracy, liability and accessibility. Key milestones are outlined for the broad clinical implementation of precision diabetes medicine.},
  langid = {american},
  pmcid = {PMC10735053},
  pmid = {37794253},
  keywords = {Consensus,Diabetes Mellitus,Evidence-Based Medicine,Humans,Precision Medicine},
  annotation = {TLDR: This second international consensus report on precision diabetes medicine summarizes the findings from a systematic evidence review across the key pillars of precision medicine (prevention, diagnosis, treatment, prognosis) in four recognized forms of diabetes.},
  timestamp = {2025-08-12T01:35:51Z}
}

@article{tong2024integrating,
  title = {Integrating {{Multi-Omics Data With EHR}} for {{Precision Medicine Using Advanced Artificial Intelligence}}},
  author = {Tong, Li and Shi, Wenqi and Isgut, Monica and Zhong, Yishan and Lais, Peter and Gloster, Logan and Sun, Jimin and Swain, Aniketh and Giuste, Felipe and Wang, May D.},
  year = {2024},
  journal = {IEEE Reviews in Biomedical Engineering},
  volume = {17},
  pages = {80--97},
  issn = {1937-3333, 1941-1189},
  doi = {10.1109/RBME.2023.3324264},
  urldate = {2025-05-27},
  copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  annotation = {TLDR: This review paper summarizes the state-of-the-art AI models for integrating multi-omics data and electronic health records (EHRs) for precision medicine and discusses the challenges and opportunities in integrating multi-omics data with EHRs and future directions.},
  timestamp = {2025-05-27T23:54:38Z}
}

@article{tononi2016integrated,
  title = {Integrated Information Theory: From Consciousness to Its Physical Substrate},
  shorttitle = {Integrated Information Theory},
  author = {Tononi, Giulio and Boly, Melanie and Massimini, Marcello and Koch, Christof},
  year = {2016},
  month = jul,
  journal = {Nature Reviews. Neuroscience},
  volume = {17},
  number = {7},
  pages = {450--461},
  issn = {1471-0048},
  doi = {10.1038/nrn.2016.44},
  abstract = {In this Opinion article, we discuss how integrated information theory accounts for several aspects of the relationship between consciousness and the brain. Integrated information theory starts from the essential properties of phenomenal experience, from which it derives the requirements for the physical substrate of consciousness. It argues that the physical substrate of consciousness must be a maximum of intrinsic cause-effect power and provides a means to determine, in principle, the quality and quantity of experience. The theory leads to some counterintuitive predictions and can be used to develop new tools for assessing consciousness in non-communicative patients.},
  langid = {english},
  pmid = {27225071},
  keywords = {Animals,Brain,Consciousness,Humans,Information Theory,Models Neurological,Nerve Net,Task Performance and Analysis},
  annotation = {TLDR: How integrated information theory accounts for several aspects of the relationship between consciousness and the brain is discussed and can be used to develop new tools for assessing consciousness in non-communicative patients.},
  timestamp = {2025-09-01T03:09:09Z}
}

@article{toonsi2024causal,
  title = {Causal Relationships between Diseases Mined from the Literature Improve the Use of Polygenic Risk Scores},
  author = {Toonsi, Sumyyah and Gauran, Iris Ivy and Ombao, Hernando and Schofield, Paul N and Hoehndorf, Robert},
  year = {2024},
  month = nov,
  journal = {Bioinformatics},
  volume = {40},
  number = {11},
  pages = {btae639},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btae639},
  urldate = {2025-04-17},
  abstract = {Identifying causal relations between diseases allows for the study of shared pathways, biological mechanisms, and inter-disease risks. Such causal relations can facilitate the identification of potential disease precursors and candidates for drug re-purposing. However, computational methods often lack access to these causal relations. Few approaches have been developed to automatically extract causal relationships between diseases from unstructured text, but they are often only focused on a small number of diseases, lack validation of the extracted causal relations, or do not make their data available.We automatically mined statements asserting a causal relation between diseases from the scientific literature by leveraging lexical patterns. Following automated mining of causal relations, we mapped the diseases to the International Classification of Diseases (ICD) identifiers to allow the direct application to clinical data. We provide quantitative and qualitative measures to evaluate the mined causal relations and compare to UK Biobank diagnosis data as a completely independent data source. The validated causal associations were used to create a directed acyclic graph that can be used by causal inference frameworks. We demonstrate the utility of our causal network by performing causal inference using the do-calculus, using relations within the graph to construct and improve polygenic risk scores, and disentangle the pleiotropic effects of variants.The data are available through https://github.com/bio-ontology-research-group/causal-relations-between-diseases.},
  annotation = {TLDR: The utility of the causal network is demonstrated by performing causal inference using the do-calculus, using relations within the graph to construct and improve polygenic risk scores, and disentangle the pleiotropic effects of variants.},
  timestamp = {2025-04-17T15:23:44Z}
}

@article{topol2019highperformance,
  title = {High-Performance Medicine: The Convergence of Human and Artificial Intelligence},
  shorttitle = {High-Performance Medicine},
  author = {Topol, Eric J.},
  year = {2019},
  month = jan,
  journal = {Nature Medicine},
  volume = {25},
  number = {1},
  pages = {44--56},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-018-0300-7},
  urldate = {2025-05-13},
  abstract = {The use of artificial intelligence, and the deep-learning subtype in particular, has been enabled by the use of labeled big data, along with markedly enhanced computing power and cloud storage, across all sectors. In medicine, this is beginning to have an impact at three levels: for clinicians, predominantly via rapid, accurate image interpretation; for health systems, by improving workflow and the potential for reducing medical errors; and for patients, by enabling them to process their own data to promote health. The current limitations, including bias, privacy and security, and lack of transparency, along with the future directions of these applications will be discussed in this article. Over time, marked improvements in accuracy, productivity, and workflow will likely be actualized, but whether that will be used to improve the patient--doctor relationship or facilitate its erosion remains to be seen.},
  copyright = {2019 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Health care,Machine learning},
  annotation = {TLDR: Over time, marked improvements in accuracy, productivity, and workflow will likely be actualized, but whether that will be used to improve the patient--doctor relationship or facilitate its erosion remains to be seen.},
  timestamp = {2025-05-13T11:23:42Z}
}

@article{tosh2025bayesian,
  title = {A {{Bayesian}} Active Learning Platform for Scalable Combination Drug Screens},
  author = {Tosh, Christopher and Tec, Mauricio and White, Jessica B. and Quinn, Jeffrey F. and Ibanez Sanchez, Glorymar and Calder, Paul and Kung, Andrew L. and Dela Cruz, Filemon S. and Tansey, Wesley},
  year = {2025},
  month = jan,
  journal = {Nature Communications},
  volume = {16},
  number = {1},
  pages = {156},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-55287-7},
  urldate = {2025-05-17},
  abstract = {Large-scale combination drug screens are generally considered intractable due to the immense number of possible combinations. Existing approaches use ad hoc fixed experimental designs then train machine learning models to impute unobserved combinations. Here we propose BATCHIE, an orthogonal approach that conducts experiments dynamically in batches. BATCHIE uses information theory and probabilistic modeling to design each batch to be maximally informative based on the results of previous experiments. On retrospective experiments from previous large-scale screens, BATCHIE designs rapidly discover highly effective and synergistic combinations. In a prospective combination screen of a library of 206 drugs on a collection of pediatric cancer cell lines, the BATCHIE model accurately predicts unseen combinations and detects synergies after exploring only 4\% of the 1.4M possible experiments. Further, the model identifies a panel of top combinations for Ewing sarcomas, which follow-up validation experiments confirm to be effective, including the rational and translatable top hit of PARP plus topoisomerase I inhibition. These results demonstrate that adaptive experiments can enable large-scale unbiased combination drug screens with a relatively small number of experiments. BATCHIE is open source and publicly available (https://github.com/tansey-lab/batchie).},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Cancer therapy,High-throughput screening,Machine learning,Statistical methods},
  annotation = {TLDR: Adaptive experiments can enable large-scale unbiased combination drug screens with a relatively small number of experiments, thereby powering a new wave of combination drug discoveries.},
  timestamp = {2025-05-17T08:11:46Z}
}

@article{toussaint2024explainable,
  ids = {toussaint2024explainablea},
  title = {Explainable Artificial Intelligence for Omics Data: A Systematic Mapping Study},
  shorttitle = {Explainable Artificial Intelligence for Omics Data},
  author = {Toussaint, Philipp A and Leiser, Florian and Thiebes, Scott and Schlesner, Matthias and Brors, Benedikt and Sunyaev, Ali},
  year = {2024},
  month = jan,
  journal = {Briefings in Bioinformatics},
  volume = {25},
  number = {1},
  pages = {bbad453},
  issn = {1477-4054},
  doi = {10.1093/bib/bbad453},
  urldate = {2025-04-17},
  abstract = {Researchers increasingly turn to explainable artificial intelligence (XAI) to analyze omics data and gain insights into the underlying biological processes. Yet, given the interdisciplinary nature of the field, many findings have only been shared in their respective research community. An overview of XAI for omics data is needed to highlight promising approaches and help detect common issues. Toward this end, we conducted a systematic mapping study. To identify relevant literature, we queried Scopus, PubMed, Web of Science, BioRxiv, MedRxiv and arXiv. Based on keywording, we developed a coding scheme with 10 facets regarding the studies' AI methods, explainability methods and omics data. Our mapping study resulted in 405 included papers published between 2010 and 2023. The inspected papers analyze DNA-based (mostly genomic), transcriptomic, proteomic or metabolomic data by means of neural networks, tree-based methods, statistical methods and further AI methods. The preferred post-hoc explainability methods are feature relevance (n~=\,166) and visual explanation (n~=\,52), while papers using interpretable approaches often resort to the use of transparent models (n~=\,83) or architecture modifications (n~=\,72). With many research gaps still apparent for XAI for omics data, we deduced eight research directions and discuss their potential for the field. We also provide exemplary research questions for each direction. Many problems with the adoption of XAI for omics data in clinical practice are yet to be resolved. This systematic mapping study outlines extant research on the topic and provides research directions for researchers and practitioners.},
  annotation = {TLDR: A systematic mapping study outlines extant research on the topic and provides research directions for researchers and practitioners and deduced eight research directions for XAI for omics data.},
  timestamp = {2025-04-21T02:51:38Z}
}

@article{tran2020benchmark,
  title = {A Benchmark of Batch-Effect Correction Methods for Single-Cell {{RNA}} Sequencing Data},
  author = {Tran, Hoa Thi Nhu and Ang, Kok Siong and Chevrier, Marion and Zhang, Xiaomeng and Lee, Nicole Yee Shin and Goh, Michelle and Chen, Jinmiao},
  year = {2020},
  journal = {Genome biology},
  volume = {21},
  pages = {1--32},
  publisher = {Springer},
  timestamp = {2025-05-17T07:41:35Z}
}

@article{tran2021artificial,
  title = {Artificial Intelligence in Healthcare---the Road to Precision Medicine},
  author = {Tran, Tran Quoc Bao and Du Toit, Clea and Padmanabhan, Sandosh},
  year = {2021},
  month = sep,
  journal = {Journal of Hospital Management and Health Policy},
  volume = {5},
  pages = {29--29},
  issn = {25232533},
  doi = {10.21037/jhmhp-20-132},
  urldate = {2025-05-13},
  langid = {american},
  annotation = {TLDR: A broad overview of the prospects and potential for AI in precision medicine is provided and some of the challenges and evolving solutions that are revolutionising healthcare are discussed.},
  timestamp = {2025-05-13T10:48:57Z}
}

@article{treppner2022interpretable,
  title = {Interpretable Generative Deep Learning: An Illustration with Single Cell Gene Expression Data},
  shorttitle = {Interpretable Generative Deep Learning},
  author = {Treppner, Martin and Binder, Harald and Hess, Moritz},
  year = {2022},
  month = sep,
  journal = {Human Genetics},
  volume = {141},
  number = {9},
  pages = {1481--1498},
  issn = {1432-1203},
  doi = {10.1007/s00439-021-02417-6},
  urldate = {2025-04-18},
  abstract = {Deep generative models can learn the underlying structure, such as pathways or gene programs, from omics data. We provide an introduction as well as an overview of such techniques, specifically illustrating their use with single-cell gene expression data. For example, the low dimensional latent representations offered by various approaches, such as variational auto-encoders, are useful to get a better understanding of the relations between observed gene expressions and experimental factors or phenotypes. Furthermore, by providing a generative model for the latent and observed variables, deep generative models can generate synthetic observations, which allow us to assess the uncertainty in the learned representations. While deep generative models are useful to learn the structure of high-dimensional omics data by efficiently capturing non-linear dependencies between genes, they are sometimes difficult to interpret due to their neural network building blocks. More precisely, to understand the relationship between learned latent variables and observed variables, e.g., gene transcript abundances and external phenotypes, is difficult. Therefore, we also illustrate current approaches that allow us to infer the relationship between learned latent variables and observed variables as well as external phenotypes. Thereby, we render deep learning approaches more interpretable. In an application with single-cell gene expression data, we demonstrate the utility of the discussed methods.},
  langid = {english},
  keywords = {Deep learning,Dimension reduction,Explainable AI,Generative model},
  annotation = {TLDR: This work illustrates current approaches that allow us to infer the relationship between learned latent variables and observed variables as well as external phenotypes, and renders deep learning approaches more interpretable.},
  timestamp = {2025-04-18T03:20:22Z}
}

@article{tsimenidis2022omics,
  title = {Omics {{Data}} and {{Data Representations}} for {{Deep Learning-Based Predictive Modeling}}},
  author = {Tsimenidis, Stefanos and Vrochidou, Eleni and Papakostas, George A.},
  year = {2022},
  month = jan,
  journal = {International Journal of Molecular Sciences},
  volume = {23},
  number = {20},
  pages = {12272},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1422-0067},
  doi = {10.3390/ijms232012272},
  urldate = {2025-04-18},
  abstract = {Medical discoveries mainly depend on the capability to process and analyze biological datasets, which inundate the scientific community and are still expanding as the cost of next-generation sequencing technologies is decreasing. Deep learning (DL) is a viable method to exploit this massive data stream since it has advanced quickly with there being successive innovations. However, an obstacle to scientific progress emerges: the difficulty of applying DL to biology, and this because both fields are evolving at a breakneck pace, thus making it hard for an individual to occupy the front lines of both of them. This paper aims to bridge the gap and help computer scientists bring their valuable expertise into the life sciences. This work provides an overview of the most common types of biological data and data representations that are used to train DL models, with additional information on the models themselves and the various tasks that are being tackled. This is the essential information a DL expert with no background in biology needs in order to participate in DL-based research projects in biomedicine, biotechnology, and drug discovery. Alternatively, this study could be also useful to researchers in biology to understand and utilize the power of DL to gain better insights into and extract important information from the omics data.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,biological data,complex systems,deep learning,drug discovery,omics,review,system biology},
  annotation = {TLDR: This work provides an overview of the most common types of biological data and data representations that are used to train DL models, with additional information on the models themselves and the various tasks that are being tackled.},
  timestamp = {2025-04-18T03:20:30Z}
}

@article{tsopra2021framework,
  title = {A Framework for Validating {{AI}} in Precision Medicine: Considerations from the {{European ITFoC}} Consortium},
  shorttitle = {A Framework for Validating {{AI}} in Precision Medicine},
  author = {Tsopra, Rosy and Fernandez, Xose and Luchinat, Claudio and Alberghina, Lilia and Lehrach, Hans and Vanoni, Marco and Dreher, Felix and Sezerman, O.Ugur and Cuggia, Marc and De Tayrac, Marie and Miklasevics, Edvins and Itu, Lucian Mihai and Geanta, Marius and Ogilvie, Lesley and Godey, Florence and Boldisor, Cristian Nicolae and {Campillo-Gimenez}, Boris and Cioroboiu, Cosmina and Ciusdel, Costin Florian and Coman, Simona and Hijano Cubelos, Oliver and Itu, Alina and Lange, Bodo and Le Gallo, Matthieu and Lespagnol, Alexandra and Mauri, Giancarlo and Soykam, H.Okan and Rance, Bastien and Turano, Paola and Tenori, Leonardo and Vignoli, Alessia and Wierling, Christoph and Benhabiles, Nora and Burgun, Anita},
  year = {2021},
  month = dec,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {21},
  number = {1},
  pages = {274},
  issn = {1472-6947},
  doi = {10.1186/s12911-021-01634-3},
  urldate = {2025-05-02},
  abstract = {Abstract                            Background               Artificial intelligence (AI) has the potential to transform our healthcare systems significantly. New AI technologies based on machine learning approaches should play a key role in clinical decision-making in the future. However, their implementation in health care settings remains limited, mostly due to a lack of robust validation procedures. There is a need to develop reliable assessment frameworks for the clinical validation of AI. We present here an approach for assessing AI for predicting treatment response in triple-negative breast cancer (TNBC), using real-world data and molecular -omics data from clinical data warehouses and biobanks.                                         Methods               The European ``ITFoC (Information Technology for the Future Of Cancer)'' consortium designed a framework for the clinical validation of AI technologies for predicting treatment response in oncology.                                         Results               This framework is based on seven key steps specifying: (1) the intended use of AI, (2) the target population, (3) the timing of AI evaluation, (4) the datasets used for evaluation, (5) the procedures used for ensuring data safety (including data quality, privacy and security), (6) the metrics used for measuring performance, and (7) the procedures used to ensure that the AI is explainable. This framework forms the basis of a validation platform that we are building for the ``ITFoC Challenge''. This community-wide competition will make it possible to assess and compare AI algorithms for predicting the response to TNBC treatments with external real-world datasets.                                         Conclusions               The predictive performance and safety of AI technologies must be assessed in a robust, unbiased and transparent manner before their implementation in healthcare settings. We believe that the consideration of the ITFoC consortium will contribute to the safe transfer and implementation of AI in clinical settings, in the context of precision oncology and personalized care.},
  langid = {english},
  annotation = {TLDR: An approach for assessing AI for predicting treatment response in triple-negative breast cancer (TNBC), using real-world data and molecular -omics data from clinical data warehouses and biobanks and the basis of a validation platform for the ``ITFoC Challenge''.},
  timestamp = {2025-05-02T12:59:13Z}
}

@article{tu2024generalist,
  title = {Towards {{Generalist Biomedical AI}}},
  author = {Tu, Tao and Azizi, Shekoofeh and Driess, Danny and Schaekermann, Mike and Amin, Mohamed and Chang, Pi-Chuan and Carroll, Andrew and Lau, Charles and Tanno, Ryutaro and Ktena, Ira and Palepu, Anil and Mustafa, Basil and Chowdhery, Aakanksha and Liu, Yun and Kornblith, Simon and Fleet, David and Mansfield, Philip and Prakash, Sushant and Wong, Renee and Virmani, Sunny and Semturs, Christopher and Mahdavi, S. Sara and Green, Bradley and Dominowska, Ewa and y Arcas, Blaise Aguera and Barral, Joelle and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Singhal, Karan and Florence, Pete and Karthikesalingam, Alan and Natarajan, Vivek},
  year = {2024},
  month = feb,
  journal = {NEJM AI},
  volume = {1},
  number = {3},
  pages = {AIoa2300138},
  publisher = {Massachusetts Medical Society},
  doi = {10.1056/AIoa2300138},
  urldate = {2025-04-03},
  abstract = {Medicine is inherently multimodal, requiring the simultaneous interpretation and integration of insights between many data modalities spanning text, imaging, genomics, and more. Generalist biomedic...},
  copyright = {Copyright {\copyright} 2024 Massachusetts Medical Society.},
  langid = {english},
  timestamp = {2025-04-03T12:23:31Z}
}

@article{tuan2024bridging,
  title = {Bridging the Gap between Black Box {{AI}} and Clinical Practice: {{Advancing}} Explainable {{AI}} for Trust, Ethics, and Personalized Healthcare Diagnostics},
  author = {Tuan, Dang Anh},
  year = {2024},
  timestamp = {2025-04-13T16:11:51Z}
}

@article{tung2017batch,
  title = {Batch Effects and the Effective Design of Single-Cell Gene Expression Studies},
  author = {Tung, Po-Yuan and Blischak, John D and Hsiao, Chiaowen Joyce and Knowles, David A and Burnett, Jonathan E and Pritchard, Jonathan K and Gilad, Yoav},
  year = {2017},
  journal = {Scientific reports},
  volume = {7},
  number = {1},
  pages = {39921},
  publisher = {Nature Publishing Group UK London},
  timestamp = {2025-05-17T07:42:45Z}
}

@article{turbe2023evaluation,
  title = {Evaluation of Post-Hoc Interpretability Methods in Time-Series Classification},
  author = {Turb{\'e}, Hugues and Bjelogrlic, Mina and Lovis, Christian and Mengaldo, Gianmarco},
  year = {2023},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {3},
  pages = {250--260},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00620-w},
  urldate = {2025-04-09},
  abstract = {Post-hoc interpretability methods are critical tools to explain neural-network results. Several post-hoc methods have emerged in recent years but they produce different results when applied to a given task, raising the question of which method is the most suitable to provide accurate post-hoc interpretability. To understand the performance of each method, quantitative evaluation of interpretability methods is essential; however, currently available frameworks have several drawbacks that hinder the adoption of post-hoc interpretability methods, especially in high-risk sectors. In this work we propose a framework with quantitative metrics to assess the performance of existing post-hoc interpretability methods, particularly in time-series classification. We show that several drawbacks identified in the literature are addressed, namely, the dependence on human judgement, retraining and the shift in the data distribution when occluding samples. We also design a synthetic dataset with known discriminative features and tunable complexity. The proposed methodology and quantitative metrics can be used to understand the reliability of interpretability methods results obtained in practical applications. In turn, they can be embedded within operational workflows in critical fields that require accurate interpretability results for, example, regulatory policies.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science,Information technology},
  annotation = {TLDR: A framework for quantitative comparison of post-hoc interpretability approaches in time-series classification and the dependence on human judgement, retraining and the shift in the data distribution when occluding samples is developed.},
  timestamp = {2025-04-09T03:46:50Z}
}

@inproceedings{uehara2019prototypebased,
  title = {Prototype-{{Based Interpretation}} of {{Pathological Image Analysis}} by {{Convolutional Neural Networks}}},
  booktitle = {Pattern {{Recognition}}: 5th {{Asian Conference}}, {{ACPR}} 2019, {{Auckland}}, {{New Zealand}}, {{November}} 26--29, 2019, {{Revised Selected Papers}}, {{Part II}}},
  author = {Uehara, Kazuki and Murakawa, Masahiro and Nosato, Hirokazu and Sakanashi, Hidenori},
  year = {2019},
  month = nov,
  pages = {640--652},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-030-41299-9_50},
  urldate = {2025-05-17},
  abstract = {The recent success of convolutional neural networks (CNNs) attracts much attention to applying a computer-aided diagnosis system for digital pathology. However, the basis of CNN's decision is incomprehensible for humans due to its complexity, and this will reduce the reliability of its decision. We improve the interpretability of the decision made using the CNN by presenting them as co-occurrences of interpretable components which typically appeared in parts of images. To this end, we propose a prototype-based interpretation method and define prototypes as the components. The method comprises the following three approaches: (1) presenting typical parts of images as multiple components, (2) allowing humans to interpret the components visually, and (3) making decisions based on the co-occurrence relation of the multiple components. Concretely, we first encode image patches using the encoder of a variational auto-encoder (VAE) and construct clusters for the encoded image patches to obtain prototypes. We then decode prototypes into images using the VAE's decoder to make the prototypes visually interpretable. Finally, we calculate the weighted combinations of the prototype occurrences for image-level classification. The weights enable us to ascertain the prototypes that contributed to decision-making. We verified both the interpretability and classification performance of our method through experiments using two types of datasets. The proposed method showed a significant advantage for interpretation by displaying the association between class discriminative components in an image and the prototypes.},
  isbn = {978-3-030-41298-2},
  annotation = {TLDR: A prototype-based interpretation method that improves the interpretability of the decision made using the CNN by presenting them as co-occurrences of interpretable components which typically appeared in parts of images.},
  timestamp = {2025-05-17T11:37:28Z}
}

@article{ukil2024explainability,
  title = {Explainability by {{Shapley}} Attribution for Electrocardiogram-Based Algorithmic Diagnosis under Subtractive Counterfactual Reasoning Setup},
  author = {Ukil, Arijit and Jara, Antonio J and Marin, Leandro},
  year = {2024},
  timestamp = {2025-04-08T13:48:50Z}
}

@article{umebayashi2025efficacy,
  title = {Efficacy of {{Navigation Systems With Smart Delivery Tools}} in {{Enhancing}} the {{Accuracy}} of {{Percutaneous Pedicle Screw Insertion}}},
  author = {Umebayashi, Takeshi and Hijikata, Yasukazu and Kimura, Takaoki and Kikuchi, Nahoko and Hara, Takeshi and Tsuda, Keiichi and Kumamoto, Shinji and Kawamura, Daichi},
  year = {2025},
  month = feb,
  journal = {Cureus},
  volume = {17},
  number = {2},
  pages = {e78656},
  issn = {2168-8184},
  doi = {10.7759/cureus.78656},
  abstract = {Purpose Minimally invasive surgical techniques are advancing in spinal surgery, creating a need for the development of surgical support systems. This study~evaluates the efficacy of a compact navigation system with smart delivery tools in percutaneous pedicle screw insertion. Methods This retrospective observational study included consecutive thoracic or lumbar spinal fusion patients with percutaneous pedicle screw placement treated from November 2022 to July 2023 in a Japanese private hospital. Primary outcomes were screw deviations (classified as any deviation, deviations of more than 2 mm, deviations from the medial to the caudal side). Pedicle screws were divided into two groups: those placed with the navigation system and those placed with traditional fluoroscopic guidance (non-navigation). Fisher's exact test and a generalized estimation equation for the prevalence ratio were conducted, adjusting for potential confounders. Results This study evaluated 492 pedicle screws (190 in the navigation group) in 78 patients. The median age was 70.5 years, and the most common condition was foraminal stenosis (26 patients, 33\%). Of the study participants, 40 (51\%) were male. Any deviation, deviations of more than 2 mm, and deviations from the medial to the caudal side were observed in 16 screws (8.4\%) vs. 54 screws (21\%) (p{$<$}0.001), six screws (3.2\%) vs. 31 screws (12\%) (p{$<$}0.001), and five screws (2.6\%) vs. 10 screws (3.8\%) (p=0.5) in the navigation and non-navigation groups, respectively. The adjusted prevalence ratios of the navigation group for any deviation, deviations of more than 2 mm, and deviations from the medial to the caudal side were 0.51 (95\%CI 0.27, 0.98), 0.33 (95\%CI 0.14, 0.78), and 0.88 (95\%CI 0.26, 3.05), respectively. Conclusion This study suggests that compact navigation systems with smart delivery tools may improve screw placement accuracy in spinal surgeries.},
  langid = {english},
  pmcid = {PMC11890593},
  pmid = {40062185},
  keywords = {computer-assisted surgery,fluoroscopy,minimally invasive surgical procedures,spinal fusion,surgical instruments},
  annotation = {TLDR: It is suggested that compact navigation systems with smart delivery tools may improve screw placement accuracy in spinal surgeries.},
  timestamp = {2025-04-12T07:59:08Z}
}

@article{universityinstituteoftechnologytheuniversityofburdwanburdwanwbindia2017identifying,
  ids = {sarkar2017identifying},
  title = {{{IDENTIFYING PATIENTS AT RISK OF BREAST CANCER THROUGH DECISION TREES}}},
  author = {{University Institute of Technology, The University of Burdwan Burdwan, WB, India} and Sarkar, Soumya Kanta},
  year = {2017},
  month = aug,
  journal = {International Journal of Advanced Research in Computer Science},
  volume = {8},
  number = {8},
  pages = {88--91},
  issn = {09765697},
  doi = {10.26483/ijarcs.v8i8.4602},
  urldate = {2025-06-12},
  annotation = {TLDR: This paper explores how the C4.5 algorithm can be applied to breast cancer datasets in order to extract and formulate rules for identifying risk factors and creates a decision tree for identifying patients at risk.},
  timestamp = {2025-07-21T07:41:57Z}
}

@article{upadhyaya2021scalable,
  title = {Scalable {{Causal Structure Learning}}: {{New Opportunities}} in {{Biomedicine}}},
  shorttitle = {Scalable {{Causal Structure Learning}}},
  author = {Upadhyaya, Pulakesh and Zhang, Kai and Li, Can and Jiang, Xiaoqian and Kim, Yejin},
  year = {2021},
  journal = {ArXiv},
  urldate = {2025-05-03},
  abstract = {This paper gives a practical tutorial on popular causal structure learning models with examples of real-world data to help healthcare audiences understand and apply them. We review prominent traditional, score-based and machine-learning based schemes for causal structure discovery, study some of their performance over some benchmark datasets, and discuss some of the applications to biomedicine. In the case of sufficient data, machine learning-based approaches can be scalable, can include a greater number of variables than traditional approaches, and can potentially be applied in many biomedical applications},
  timestamp = {2025-05-03T12:11:47Z}
}

@misc{usman2025explainable,
  title = {Explainable {{AI}} Model Reveals Disease-Related Mechanisms in Single-Cell {{RNA-seq}} Data},
  author = {Usman, Mohammad and Varea, Olga and Radeva, Petia and Canals, Josep and Abante, Jordi and Ortiz, Daniel},
  year = {2025},
  month = jan,
  number = {arXiv:2501.03923},
  eprint = {2501.03923},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.03923},
  urldate = {2025-07-24},
  abstract = {Neurodegenerative diseases (NDDs) are complex and lack effective treatment due to their poorly understood mechanism. The increasingly used data analysis from Single nucleus RNA Sequencing (snRNA-seq) allows to explore transcriptomic events at a single cell level, yet face challenges in interpreting the mechanisms underlying a disease. On the other hand, Neural Network (NN) models can handle complex data to offer insights but can be seen as black boxes with poor interpretability. In this context, explainable AI (XAI) emerges as a solution that could help to understand disease-associated mechanisms when combined with efficient NN models. However, limited research explores XAI in single-cell data. In this work, we implement a method for identifying disease-related genes and the mechanistic explanation of disease progression based on NN model combined with SHAP. We analyze available Huntington's disease (HD) data to identify both HD-altered genes and mechanisms by adding Gene Set Enrichment Analysis (GSEA) comparing two methods, differential gene expression analysis (DGE) and NN combined with SHAP approach. Our results show that DGE and SHAP approaches offer both common and differential sets of altered genes and pathways, reinforcing the usefulness of XAI methods for a broader perspective of disease.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Genomics},
  annotation = {TLDR: A method for identifying disease-related genes and the mechanistic explanation of disease progression based on NN model combined with SHAP is implemented and results show that DGE and SHAP approaches offer both common and differential sets of altered genes and pathways, reinforcing the usefulness of XAI methods for a broader perspective of disease.},
  timestamp = {2025-07-24T10:37:51Z}
}

@article{vandervelden2022explainable,
  title = {Explainable Artificial Intelligence ({{XAI}}) in Deep Learning-Based Medical Image Analysis},
  author = {Van Der Velden, Bas H.M. and Kuijf, Hugo J. and Gilhuijs, Kenneth G.A. and Viergever, Max A.},
  year = {2022},
  month = jul,
  journal = {Medical Image Analysis},
  volume = {79},
  pages = {102470},
  publisher = {Elsevier BV},
  issn = {1361-8415},
  doi = {10.1016/j.media.2022.102470},
  urldate = {2025-07-24},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  annotation = {TLDR: An overview of explainable artificial intelligence (XAI) used in deep learning-based medical image analysis methods and an outlook of future opportunities for XAI inmedical image analysis are presented.},
  timestamp = {2025-07-24T12:52:14Z}
}

@article{vandervelden2024explainable,
  title = {Explainable {{AI}}: Current Status and Future Potential},
  shorttitle = {Explainable {{AI}}},
  author = {{van der Velden}, Bas H. M.},
  year = {2024},
  month = feb,
  journal = {European Radiology},
  volume = {34},
  number = {2},
  pages = {1187--1189},
  issn = {1432-1084},
  doi = {10.1007/s00330-023-10121-4},
  langid = {english},
  pmcid = {PMC10853303},
  pmid = {37589904},
  timestamp = {2025-04-12T06:40:41Z}
}

@article{vanhilten2021gennet,
  title = {{{GenNet}} Framework: Interpretable Deep Learning for Predicting Phenotypes from Genetic Data},
  shorttitle = {{{GenNet}} Framework},
  author = {{van Hilten}, Arno and Kushner, Steven A. and Kayser, Manfred and Ikram, M. Arfan and Adams, Hieab H. H. and Klaver, Caroline C. W. and Niessen, Wiro J. and Roshchupkin, Gennady V.},
  year = {2021},
  month = sep,
  journal = {Communications Biology},
  volume = {4},
  number = {1},
  pages = {1094},
  issn = {2399-3642},
  doi = {10.1038/s42003-021-02622-z},
  abstract = {Applying deep learning in population genomics is challenging because of computational issues and lack of interpretable models. Here, we propose GenNet, a novel open-source deep learning framework for predicting phenotypes from genetic variants. In this framework, interpretable and memory-efficient neural network architectures are constructed by embedding biologically knowledge from public databases, resulting in neural networks that contain only biologically plausible connections. We applied the framework to seventeen phenotypes and found well-replicated genes such as HERC2 and OCA2 for hair and eye color, and novel genes such as ZNF773 and PCNT for schizophrenia. Additionally, the framework identified ubiquitin mediated proteolysis, endocrine system and viral infectious diseases as most predictive biological pathways for schizophrenia. GenNet is a freely available, end-to-end deep learning framework that allows researchers to develop and use interpretable neural networks to obtain novel insights into the genetic architecture of complex traits and diseases.},
  langid = {english},
  pmcid = {PMC8448759},
  pmid = {34535759},
  keywords = {Deep Learning,Humans,Neural Networks Computer,Phenotype},
  annotation = {TLDR: GenNet is a freely available, end-to-end deep learning framework that allows researchers to develop and use interpretable neural networks to obtain novel insights into the genetic architecture of complex traits and diseases.},
  timestamp = {2025-07-05T13:27:49Z}
}

@article{vanhilten2024designing,
  title = {Designing Interpretable Deep Learning Applications for Functional Genomics: A Quantitative Analysis},
  shorttitle = {Designing Interpretable Deep Learning Applications for Functional Genomics},
  author = {Van Hilten, Arno and Katz, Sonja and Saccenti, Edoardo and Niessen, Wiro J and Roshchupkin, Gennady V},
  year = {2024},
  month = jul,
  journal = {Briefings in Bioinformatics},
  volume = {25},
  number = {5},
  publisher = {Oxford University Press (OUP)},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbae449},
  urldate = {2025-07-25},
  abstract = {Abstract               Deep learning applications have had a profound impact on many scientific fields, including functional genomics. Deep learning models can learn complex interactions between and within omics data; however, interpreting and explaining these models can be challenging. Interpretability is essential not only to help progress our understanding of the biological mechanisms underlying traits and diseases but also for establishing trust in these model's efficacy for healthcare applications. Recognizing this importance, recent years have seen the development of numerous diverse interpretability strategies, making it increasingly difficult to navigate the field. In this review, we present a quantitative analysis of the challenges arising when designing interpretable deep learning solutions in functional genomics. We explore design choices related to the characteristics of genomics data, the neural network architectures applied, and strategies for interpretation. By quantifying the current state of the field with a predefined set of criteria, we find the most frequent solutions, highlight exceptional examples, and identify unexplored opportunities for developing interpretable deep learning models in genomics.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  annotation = {TLDR: This review presents a quantitative analysis of the challenges arising when designing interpretable deep learning solutions in functional genomics and explores design choices related to the characteristics of genomics data, the neural network architectures applied, and strategies for interpretation.},
  timestamp = {2025-07-25T12:05:16Z}
}

@article{vanleeuwen2024comparison,
  title = {Comparison of {{Commercial AI Software Performance}} for {{Radiograph Lung}}                     {{Nodule Detection}} and {{Bone Age Prediction}}},
  author = {{van Leeuwen}, Kicky G. and Schalekamp, Steven and Rutten, Matthieu J. C. M. and Huisman, Merel and {Schaefer-Prokop}, Cornelia M. and {de Rooij}, Maarten and {van Ginneken}, Bram and Maresch, Bas and Geurts, Bram H. J. and {van Dijke}, Cornelius F. and {Laupman-Koedam}, Emmeline and Hulleman, Enzo V. and Verhoeff, Eric L. and Meys, Evelyne M. J. and Mohamed Hoesein, Firdaus A. A. and {ter Brugge}, Floor M. and {van Hoorn}, Francois and {van der Wel}, Frank and {van den Berk}, Inge A. H. and Luyendijk, Jacqueline M. and Meakin, James and Habets, Jesse and Verbeke, Jonathan I. M. L. and Nederend, Joost and Meys, Karlijn M. E. and Deden, Laura N. and Langezaal, Lucianne C. M. and Nasrollah, Mahtab and Meij, Marleen and Boomsma, Martijn F. and Vermeulen, Matthijs and Vestering, Myrthe M. and Vijlbrief, Onno and Algra, Paul and Algra, Selma and Bollen, Stijn M. and Samson, Tijs and {von Brucken Fock}, Yntor H. G.},
  year = {2024},
  month = jan,
  journal = {Radiology},
  volume = {310},
  number = {1},
  pages = {e230981},
  publisher = {Radiological Society of North America},
  issn = {0033-8419},
  doi = {10.1148/radiol.230981},
  urldate = {2025-05-01},
  abstract = {Background Multiple commercial artificial intelligence (AI) products exist for                             assessing radiographs; however, comparable performance data for these                             algorithms are limited.Purpose To perform an independent, stand-alone validation of commercially                             available AI products for bone age prediction based on hand radiographs                             and lung nodule detection on chest radiographs.Materials and Methods This retrospective study was carried out as part of Project AIR. Nine of                             17 eligible AI products were validated on data from seven Dutch                             hospitals. For bone age prediction, the root mean square error (RMSE)                             and Pearson correlation coefficient were computed. The reference                             standard was set by three to five expert readers. For lung nodule                             detection, the area under the receiver operating characteristic curve                             (AUC) was computed. The reference standard was set by a chest                             radiologist based on CT. Randomized subsets of hand (n                             = 95) and chest (n = 140) radiographs were read by 14                             and 17 human readers, respectively, with varying experience.Results Two bone age prediction algorithms were tested on hand radiographs (from                             January 2017 to January 2022) in 326 patients (mean age, 10 years                             {\textpm} 4 [SD]; 173 female patients) and correlated strongly with the                             reference standard (r = 0.99; P                             {$<$} .001 for both). No difference in RMSE was observed between                             algorithms (0.63 years [95\% CI: 0.58, 0.69] and 0.57 years [95\% CI:                             0.52, 0.61]) and readers (0.68 years [95\% CI: 0.64, 0.73]). Seven lung                             nodule detection algorithms were validated on chest radiographs (from                             January 2012 to May 2022) in 386 patients (mean age, 64 years {\textpm}                             11; 223 male patients). Compared with readers (mean AUC, 0.81 [95\% CI:                             0.77, 0.85]), four algorithms performed better (AUC range,                             0.86--0.93; P value range, {$<$}.001 to                             .04).Conclusions Compared with human readers, four AI algorithms for detecting lung                             nodules on chest radiographs showed improved performance, whereas the                             remaining algorithms tested showed no evidence of a difference in                             performance. {\copyright} RSNA, 2024 Supplemental material is available for this                                     article. See also the editorial by Omoumi and Richiardi in this issue.},
  annotation = {TLDR: Compared with human readers, four AI algorithms for detecting lung nodules on chest radiographs showed improved performance, whereas the remaining algorithms tested showed no evidence of a difference in performance.},
  timestamp = {2025-05-01T03:50:41Z}
}

@misc{vashishtha2023causal,
  title = {Causal {{Inference Using LLM-Guided Discovery}}},
  author = {Vashishtha, Aniket and Reddy, Abbavaram Gowtham and Kumar, Abhinav and Bachu, Saketh and Balasubramanian, Vineeth N. and Sharma, Amit},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15117},
  eprint = {2310.15117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.15117},
  urldate = {2025-02-23},
  abstract = {At the core of causal inference lies the challenge of determining reliable causal graphs solely based on observational data. Since the well-known backdoor criterion depends on the graph, any errors in the graph can propagate downstream to effect inference. In this work, we initially show that complete graph information is not necessary for causal effect inference; the topological order over graph variables (causal order) alone suffices. Further, given a node pair, causal order is easier to elicit from domain experts compared to graph edges since determining the existence of an edge can depend extensively on other variables. Interestingly, we find that the same principle holds for Large Language Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated method to obtain causal order (and hence causal effect) with LLMs acting as virtual domain experts. To this end, we employ different prompting strategies and contextual cues to propose a robust technique of obtaining causal order from LLMs. Acknowledging LLMs' limitations, we also study possible techniques to integrate LLMs with established causal discovery algorithms, including constraint-based and score-based methods, to enhance their performance. Extensive experiments demonstrate that our approach significantly improves causal ordering accuracy as compared to discovery algorithms, highlighting the potential of LLMs to enhance causal inference across diverse fields.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {TLDR: This work initially shows that complete graph information is not necessary for causal effect inference; the topological order over graph variables (causal order) alone suffices; given a node pair, causal order is easier to elicit from domain experts compared to graph edges since determining the existence of an edge can depend extensively on other variables.},
  timestamp = {2025-02-23T02:25:59Z}
}

@inproceedings{vasquez2022interactive,
  title = {Interactive Deep Learning for Explainable Retinal Disease Classification},
  booktitle = {Medical {{Imaging}} 2022: {{Image Processing}}},
  author = {Vasquez, Mariana and Shakya, Suhev and Wang, Ian and Furst, Jacob and Tchoua, Roselyne and Raicu, Daniela},
  editor = {I{\v s}gum, Ivana and Colliot, Olivier},
  year = {2022},
  month = apr,
  pages = {19},
  publisher = {SPIE},
  address = {San Diego, United States},
  doi = {10.1117/12.2611822},
  urldate = {2025-07-25},
  annotation = {TLDR: This proposed deep learning model allows domain experts to correct model behavior during the training process through direct annotations of the regions of interest (ROIs) and integrates these annotations into the learning model.},
  timestamp = {2025-07-25T14:28:01Z}
}

@article{verma2020counterfactual,
  title = {Counterfactual Explanations for Machine Learning: {{A}} Review},
  author = {Verma, Sahil and Dickerson, John and Hines, Keegan},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.10596},
  volume = {2},
  number = {1},
  eprint = {2010.10596},
  pages = {1},
  archiveprefix = {arXiv},
  timestamp = {2025-05-15T11:56:18Z}
}

@misc{verma2022counterfactual,
  title = {Counterfactual {{Explanations}} and {{Algorithmic Recourses}} for {{Machine Learning}}: {{A Review}}},
  shorttitle = {Counterfactual {{Explanations}} and {{Algorithmic Recourses}} for {{Machine Learning}}},
  author = {Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan E. and Dickerson, John P. and Shah, Chirag},
  year = {2022},
  month = nov,
  number = {arXiv:2010.10596},
  eprint = {2010.10596},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.10596},
  urldate = {2025-08-20},
  abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-08-20T04:06:25Z}
}

@article{verma2024counterfactual,
  title = {Counterfactual {{Explanations}} and {{Algorithmic Recourses}} for {{Machine Learning}}: {{A Review}}},
  shorttitle = {Counterfactual {{Explanations}} and {{Algorithmic Recourses}} for {{Machine Learning}}},
  author = {Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan and Dickerson, John and Shah, Chirag},
  year = {2024},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {12},
  pages = {1--42},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3677119},
  urldate = {2025-05-18},
  abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of               explainability               in machine learning. In this article, we seek to review and categorize research on               counterfactual explanations               , a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
  langid = {english},
  annotation = {TLDR: A rubric is designed with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric, serving as an introduction to major research themes in this field.},
  timestamp = {2025-05-18T12:45:47Z}
}

@article{vilela2023biomedical,
  title = {Biomedical Knowledge Graph Embeddings for Personalized Medicine: {{Predicting}} Disease-Gene Associations},
  shorttitle = {Biomedical Knowledge Graph Embeddings for Personalized Medicine},
  author = {Vilela, Joana and Asif, Muhammad and Marques, Ana Rita and Santos, Jo{\~a}o Xavier and Rasga, C{\'e}lia and Vicente, Astrid and Martiniano, Hugo},
  year = {2023},
  journal = {Expert Systems},
  volume = {40},
  number = {5},
  pages = {e13181},
  issn = {1468-0394},
  doi = {10.1111/exsy.13181},
  urldate = {2025-04-17},
  abstract = {Personalized medicine is a concept that has been subject of increasing interest in medical research and practice in the last few years. However, significant challenges stand in the way of practical implementations, namely in regard to extracting clinically valuable insights from the vast amount of biomedical knowledge generated in the last few years. Here, we describe an approach that uses Knowledge Graph Embedding (KGE) methods on a biomedical Knowledge Graph (KG) as a path to reasoning over the wealth of information stored in publicly accessible databases. We built a Knowledge Graph using data from DisGeNET and GO, containing relationships between genes, diseases and other biological entities. The KG contains 93,657 nodes of 5 types and 1,705,585 relationships of 59 types. We applied KGE methods to this KG, obtaining an excellent performance in predicting gene-disease associations (MR 0.13, MRR 0.96, HITS@1 0.93, HITS@3 0.99, and HITS@10 0.99). The optimal hyperparameter set was used to predict all possible novel gene-disease associations. An in-depth analysis of novel gene-disease predictions for disease terms related to Autism Spectrum Disorder (ASD) shows that this approach produces predictions consistent with known candidate genes and biological pathways and yields relevant insights into the biology of this paradigmatic complex disorder.},
  copyright = {{\copyright} 2022 The Authors. Expert Systems published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {Autism Spectrum Disorder,gene-disease associations,Knowledge Graph Embedding,personalized medicine},
  annotation = {TLDR: An in-depth analysis of novel gene-disease predictions for disease terms related to Autism Spectrum Disorder (ASD) shows that this approach produces predictions consistent with known candidate genes and biological pathways and yields relevant insights into the biology of this paradigmatic complex disorder.},
  timestamp = {2025-04-17T15:19:18Z}
}

@article{villalon-garcia2020precision,
  title = {Precision {{Medicine}} in {{Rare Diseases}}},
  author = {{Villal{\'o}n-Garc{\'i}a}, Irene and {\'A}lvarez-C{\'o}rdoba, M{\'o}nica and {Su{\'a}rez-Rivero}, Juan Miguel and {Povea-Cabello}, Suleva and {Talaver{\'o}n-Rey}, Marta and {Su{\'a}rez-Carrillo}, Alejandra and {Munuera-Cabeza}, Manuel and {S{\'a}nchez-Alc{\'a}zar}, Jos{\'e} Antonio},
  year = {2020},
  month = nov,
  journal = {Diseases},
  volume = {8},
  number = {4},
  pages = {42},
  issn = {2079-9721},
  doi = {10.3390/diseases8040042},
  urldate = {2025-05-28},
  abstract = {Rare diseases are those that have a low prevalence in the population (less than 5 individuals per 10,000 inhabitants). However, infrequent pathologies affect a large number of people, since according to the World Health Organization (WHO), there are about 7000 rare diseases that affect 7\% of the world's population. Many patients with rare diseases have suffered the consequences of what is called the diagnostic odyssey, that is, extensive and prolonged serial tests and clinical visits, sometimes for many years, all with the hope of identifying the etiology of their disease. For patients with rare diseases, obtaining the genetic diagnosis can mean the end of the diagnostic odyssey, and the beginning of another, the therapeutic odyssey. This scenario is especially challenging for the scientific community, since more than 90\% of rare diseases do not currently have an effective treatment. This therapeutic failure in rare diseases means that new approaches are necessary. Our research group proposes that the use of precision or personalized medicine techniques can be an alternative to find potential therapies in these diseases. To this end, we propose that patients' own cells can be used to carry out personalized pharmacological screening for the identification of potential treatments.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: It is proposed that patients' own cells can be used to carry out personalized pharmacological screening for the identification of potential treatments in rare diseases.},
  timestamp = {2025-05-28T00:06:35Z}
}

@article{vlontzos2023estimating,
  title = {Estimating Categorical Counterfactuals via Deep Twin Networks},
  author = {Vlontzos, Athanasios and Kainz, Bernhard and {Gilligan-Lee}, Ciar{\'a}n M.},
  year = {2023},
  month = feb,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {2},
  pages = {159--168},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00611-x},
  urldate = {2025-05-03},
  abstract = {Counterfactual inference is a powerful tool, capable of solving challenging problems in high-profile sectors. To perform counterfactual inference, we require knowledge of the underlying causal mechanisms. However, causal mechanisms cannot be uniquely determined from observations and interventions alone. This raises the question of how to choose the causal mechanisms so that the resulting counterfactual inference is trustworthy in a given domain. This question has been addressed in causal models with binary variables, but for the case of categorical variables, it remains unanswered. We address this challenge by introducing for causal models with categorical variables the notion of counterfactual ordering, a principle positing desirable properties that causal mechanisms should possess and prove that it is equivalent to specific functional constraints on the causal mechanisms. To learn causal mechanisms satisfying these constraints, and perform counterfactual inference with them, we introduce deep twin networks. These are deep neural networks that, when trained, are capable of twin network counterfactual inference---an alternative to the abduction--action--prediction method. We empirically test our approach on diverse real-world and semisynthetic data from medicine, epidemiology and finance, reporting accurate estimation of counterfactual probabilities while demonstrating the issues that arise with counterfactual reasoning when counterfactual ordering is not enforced},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Experimental models of disease,Technology},
  annotation = {TLDR: A deep learning-based method for answering counterfactual queries that can deal with categorical variables, rather than only binary ones, using the notion of `counterfactual ordering', a principle positing desirable properties that causal mechanisms should possess and proving that it is equivalent to specific functional constraints on the causal mechanisms.},
  timestamp = {2025-05-03T12:37:13Z}
}

@article{wachter2017counterfactual,
  title = {Counterfactual Explanations without Opening the Black Box: {{Automated}} Decisions and the {{GDPR}}},
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year = {2017},
  journal = {Harv. JL \& Tech.},
  volume = {31},
  pages = {841},
  publisher = {HeinOnline},
  timestamp = {2025-04-13T07:24:04Z}
}

@article{wager2018estimation,
  title = {Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests},
  author = {Wager, Stefan and Athey, Susan},
  year = {2018},
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {523},
  pages = {1228--1242},
  publisher = {Taylor \& Francis},
  timestamp = {2025-04-13T07:25:48Z}
}

@article{wagle2024interpretable,
  title = {Interpretable Deep Learning in Single-Cell Omics},
  author = {Wagle, Manoj M and Long, Siqu and Chen, Carissa and Liu, Chunlei and Yang, Pengyi},
  year = {2024},
  journal = {Bioinformatics (Oxford, England)},
  volume = {40},
  number = {6},
  pages = {btae374},
  publisher = {Oxford University Press},
  timestamp = {2025-04-18T02:37:27Z}
}

@article{wahid2024artificial,
  title = {Artificial {{Intelligence Uncertainty Quantification}} in {{Radiotherapy Applications}} - {{A Scoping Review}}},
  author = {Wahid, Kareem A. and Kaffey, Zaphanlene Y. and Farris, David P. and {Humbert-Vidan}, Laia and Moreno, Amy C. and Rasmussen, Mathis and Ren, Jintao and Naser, Mohamed A. and Netherton, Tucker J. and Korreman, Stine and Balakrishnan, Guha and Fuller, Clifton D. and Fuentes, David and Dohopolski, Michael J.},
  year = {2024},
  month = may,
  journal = {medRxiv: The Preprint Server for Health Sciences},
  pages = {2024.05.13.24307226},
  doi = {10.1101/2024.05.13.24307226},
  abstract = {BACKGROUND/PURPOSE: The use of artificial intelligence (AI) in radiotherapy (RT) is expanding rapidly. However, there exists a notable lack of clinician trust in AI models, underscoring the need for effective uncertainty quantification (UQ) methods. The purpose of this study was to scope existing literature related to UQ in RT, identify areas of improvement, and determine future directions. METHODS: We followed the PRISMA-ScR scoping review reporting guidelines. We utilized the population (human cancer patients), concept (utilization of AI UQ), context (radiotherapy applications) framework to structure our search and screening process. We conducted a systematic search spanning seven databases, supplemented by manual curation, up to January 2024. Our search yielded a total of 8980 articles for initial review. Manuscript screening and data extraction was performed in Covidence. Data extraction categories included general study characteristics, RT characteristics, AI characteristics, and UQ characteristics. RESULTS: We identified 56 articles published from 2015-2024. 10 domains of RT applications were represented; most studies evaluated auto-contouring (50\%), followed by image-synthesis (13\%), and multiple applications simultaneously (11\%). 12 disease sites were represented, with head and neck cancer being the most common disease site independent of application space (32\%). Imaging data was used in 91\% of studies, while only 13\% incorporated RT dose information. Most studies focused on failure detection as the main application of UQ (60\%), with Monte Carlo dropout being the most commonly implemented UQ method (32\%) followed by ensembling (16\%). 55\% of studies did not share code or datasets. CONCLUSION: Our review revealed a lack of diversity in UQ for RT applications beyond auto-contouring. Moreover, there was a clear need to study additional UQ methods, such as conformal prediction. Our results may incentivize the development of guidelines for reporting and implementation of UQ in RT.},
  langid = {english},
  pmcid = {PMC11118597},
  pmid = {38798581},
  annotation = {TLDR: A review of existing literature related to UQ in RT revealed a lack of diversity in UQ for RT applications beyond auto-contouring, suggesting a clear need to study additional UQ methods, such as conformal prediction.},
  timestamp = {2025-04-06T14:15:31Z}
}

@inproceedings{Wang_2020_CVPR_Workshops,
  title = {Visual Commonsense Representation Learning via Causal Inference},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}}) Workshops},
  author = {Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},
  year = {2020},
  month = jun,
  timestamp = {2025-03-20T11:46:00Z}
}

@misc{wang2018tienet,
  title = {{{TieNet}}: {{Text-Image Embedding Network}} for {{Common Thorax Disease Classification}} and {{Reporting}} in {{Chest X-rays}}},
  shorttitle = {{{TieNet}}},
  author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Summers, Ronald M.},
  year = {2018},
  month = jan,
  number = {arXiv:1801.04334},
  eprint = {1801.04334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.04334},
  urldate = {2025-07-25},
  abstract = {Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reports can be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6\% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  timestamp = {2025-07-25T02:55:36Z}
}

@article{wang2021bilateral,
  title = {Bilateral {{Asymmetry Guided Counterfactual Generating Network}} for {{Mammogram Classification}}},
  author = {Wang, Churan and Li, Jing and Zhang, Fandong and Sun, Xinwei and Dong, Hao and Yu, Yizhou and Wang, Yizhou},
  year = {2021},
  journal = {IEEE Transactions on Image Processing},
  volume = {30},
  pages = {7980--7994},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2021.3112053},
  urldate = {2025-04-06},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {TLDR: A new theoretical result for counterfactual generation based on the symmetric prior for bilateral images is derived and the utility of the method and the effectiveness of each module in the model can be verified by state-of-the-art performance on INBreast and an in-house dataset and ablation studies.},
  timestamp = {2025-04-06T12:28:04Z}
}

@article{wang2021current,
  title = {Current Trends in Three-Dimensional Visualization and Real-Time Navigation as Well as Robot-Assisted Technologies in Hepatobiliary Surgery},
  author = {Wang, Yun and Cao, Di and Chen, Si-Lin and Li, Yu-Mei and Zheng, Yun-Wen and Ohkohchi, Nobuhiro},
  year = {2021},
  month = sep,
  journal = {World Journal of Gastrointestinal Surgery},
  volume = {13},
  number = {9},
  pages = {904--922},
  issn = {1948-9366},
  doi = {10.4240/wjgs.v13.i9.904},
  abstract = {With the continuous development of digital medicine, minimally invasive precision and safety have become the primary development trends in hepatobiliary surgery. Due to the specificity and complexity of hepatobiliary surgery, traditional preoperative imaging techniques such as computed tomography and magnetic resonance imaging cannot meet the need for identification of fine anatomical regions. Imaging-based three-dimensional (3D) reconstruction, virtual simulation of surgery and 3D printing optimize the surgical plan through preoperative assessment, improving the controllability and safety of intraoperative operations, and in difficult-to-reach areas of the posterior and superior liver, assistive robots reproduce the surgeon's natural movements with stable cameras, reducing natural vibrations. Electromagnetic navigation in abdominal surgery solves the problem of conventional surgery still relying on direct visual observation or preoperative image assessment. We summarize and compare these recent trends in digital medical solutions for the future development and refinement of digital medicine in hepatobiliary surgery.},
  langid = {english},
  pmcid = {PMC8462083},
  pmid = {34621469},
  keywords = {Electromagnetic tracking,Hepatobiliary surgery,Real-time navigation,Robot-assisted surgery,Three-dimensional printing,Three-dimensional visualization},
  annotation = {TLDR: Electromagnetic navigation in abdominal surgery solves the problem of conventional surgery still relying on direct visual observation or preoperative image assessment, and Imaging-based three-dimensional reconstruction, virtual simulation of surgery and 3D printing optimize the surgical plan through preoperative assessment.},
  timestamp = {2025-05-04T05:45:04Z}
}

@article{wang2022deepcausality,
  title = {{{DeepCausality}}: {{A}} General {{AI-powered}} Causal Inference Framework for Free Text: {{A}} Case Study of {{LiverTox}}},
  shorttitle = {{{DeepCausality}}},
  author = {Wang, Xingqiao and Xu, Xiaowei and Tong, Weida and Liu, Qi and Liu, Zhichao},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  pages = {999289},
  issn = {2624-8212},
  doi = {10.3389/frai.2022.999289},
  abstract = {Causality plays an essential role in multiple scientific disciplines, including the social, behavioral, and biological sciences and portions of statistics and artificial intelligence. Manual-based causality assessment from a large number of free text-based documents is very time-consuming, labor-intensive, and sometimes even impractical. Herein, we proposed a general causal inference framework named DeepCausality to empirically estimate the causal factors for suspected endpoints embedded in the free text. The proposed DeepCausality seamlessly incorporates AI-powered language models, named entity recognition and Judea Pearl's Do-calculus, into a general framework for causal inference to fulfill different domain-specific applications. We exemplified the utility of the proposed DeepCausality framework by employing the LiverTox database to estimate idiosyncratic drug-induced liver injury (DILI)-related causal terms and generate a knowledge-based causal tree for idiosyncratic DILI patient stratification. Consequently, the DeepCausality yielded a prediction performance with an accuracy of 0.92 and an F-score of 0.84 for the DILI prediction. Notably, 90\% of causal terms enriched by the DeepCausality were consistent with the clinical causal terms defined by the American College of Gastroenterology (ACG) clinical guideline for evaluating suspected idiosyncratic DILI (iDILI). Furthermore, we observed a high concordance of 0.91 between the iDILI severity scores generated by DeepCausality and domain experts. Altogether, the proposed DeepCausality framework could be a promising solution for causality assessment from free text and is publicly available through https://github.com/XingqiaoWang/https-github.com-XingqiaoWang-DeepCausality-LiverTox.},
  langid = {english},
  pmcid = {PMC9763446},
  pmid = {36561659},
  keywords = {AI,causal inference analysis,DILI,NLP,transformer},
  annotation = {TLDR: The proposed DeepCausality framework seamlessly incorporates AI-powered language models, named entity recognition and Judea Pearl's Do-calculus, into a general framework for causal inference to fulfill different domain-specific applications.},
  timestamp = {2025-04-18T03:07:00Z}
}

@article{wang2022interpretabilitybased,
  title = {Interpretability-{{Based Multimodal Convolutional Neural Networks}} for {{Skin Lesion Diagnosis}}},
  author = {Wang, Sutong and Yin, Yunqiang and Wang, Dujuan and Wang, Yanzhang and Jin, Yaochu},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Cybernetics},
  volume = {52},
  number = {12},
  pages = {12623--12637},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2021.3069920},
  urldate = {2025-05-17},
  abstract = {Skin lesion diagnosis is a key step for skin cancer screening, which requires high accuracy and interpretability. Though many computer-aided methods, especially deep learning methods, have made remarkable achievements in skin lesion diagnosis, their generalization and interpretability are still a challenge. To solve this issue, we propose an interpretability-based multimodal convolutional neural network (IM-CNN), which is a multiclass classification model with skin lesion images and metadata of patients as input for skin lesion diagnosis. The structure of IM-CNN consists of three main paths to deal with metadata, features extracted from segmented skin lesion with domain knowledge, and skin lesion images, respectively. We add interpretable visual modules to provide explanations for both images and metadata. In addition to area under the ROC curve (AUC), sensitivity, and specificity, we introduce a new indicator, an AUC curve with a sensitivity larger than 80\% (AUC\_SEN\_80) for performance evaluation. Extensive experimental studies are conducted on the popular HAM10000 dataset, and the results indicate that the proposed model has overwhelming advantages compared with popular deep learning models, such as DenseNet, ResNet, and other state-of-the-art models for melanoma diagnosis. The proposed multimodal model also achieves on average 72\% and 21\% improvement in terms of sensitivity and AUC\_SEN\_80, respectively, compared with the single-modal model. The visual explanations can also help gain trust from dermatologists and realize man--machine collaborations, effectively reducing the limitation of black-box models in supporting medical decision making.},
  langid = {american},
  keywords = {Cancer,Convolutional neural networks,Deep learning,Feature extraction,interpretability,Lesions,Medical diagnostic imaging,multimodal convolutional neural network,Skin,skin lesion diagnosis},
  annotation = {TLDR: The proposed IM-CNN model has overwhelming advantages compared with popular deep learning models, such as DenseNet, ResNet, and other state-of-the-art models for melanoma diagnosis, and achieves on average 72\% and 21\% improvement in terms of sensitivity and AUC\_SEN\_80, respectively, compared with the single-modal model.},
  timestamp = {2025-05-17T11:53:01Z}
}

@article{wang2023precision,
  title = {Precision {{Medicine}}: {{Disease Subtyping}} and {{Tailored Treatment}}},
  shorttitle = {Precision {{Medicine}}},
  author = {Wang, Richard C. and Wang, Zhixiang},
  year = {2023},
  month = jul,
  journal = {Cancers},
  volume = {15},
  number = {15},
  pages = {3837},
  issn = {2072-6694},
  doi = {10.3390/cancers15153837},
  urldate = {2025-05-27},
  abstract = {The genomics-based concept of precision medicine began to emerge following the completion of the Human Genome Project. In contrast to evidence-based medicine, precision medicine will allow doctors and scientists to tailor the treatment of different subpopulations of patients who differ in their susceptibility to specific diseases or responsiveness to specific therapies. The current precision medicine model was proposed to precisely classify patients into subgroups sharing a common biological basis of diseases for more effective tailored treatment to achieve improved outcomes. Precision medicine has become a term that symbolizes the new age of medicine. In this review, we examine the history, development, and future perspective of precision medicine. We also discuss the concepts, principles, tools, and applications of precision medicine and related fields. In our view, for precision medicine to work, two essential objectives need to be achieved. First, diseases need to be classified into various subtypes. Second, targeted therapies must be available for each specific disease subtype. Therefore, we focused this review on the progress in meeting these two objectives.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: This review focuses on the progress in meeting two essential objectives of precision medicine, which are to precisely classify patients into subgroups sharing a common biological basis of diseases for more effective tailored treatment to achieve improved outcomes.},
  timestamp = {2025-05-27T14:25:38Z}
}

@article{wang2023retrosynthesis,
  title = {Retrosynthesis Prediction with an Interpretable Deep-Learning Framework Based on Molecular Assembly Tasks},
  author = {Wang, Yu and Pang, Chao and Wang, Yuzhe and Jin, Junru and Zhang, Jingjie and Zeng, Xiangxiang and Su, Ran and Zou, Quan and Wei, Leyi},
  year = {2023},
  month = oct,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {6155},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-41698-5},
  urldate = {2025-05-04},
  abstract = {Automating retrosynthesis with artificial intelligence expedites organic chemistry research in digital laboratories. However, most existing deep-learning approaches are hard to explain, like a ``black box'' with few insights. Here, we propose RetroExplainer, formulizing the retrosynthesis task into a molecular assembly process, containing several retrosynthetic actions guided by deep learning. To guarantee a robust performance of our model, we propose three units: a multi-sense and multi-scale Graph Transformer, structure-aware contrastive learning, and dynamic adaptive multi-task learning. The results on 12 large-scale benchmark datasets demonstrate the effectiveness of RetroExplainer, which outperforms the state-of-the-art single-step retrosynthesis approaches. In addition, the molecular assembly process renders our model with good interpretability, allowing for transparent decision-making and quantitative attribution. When extended to multi-step retrosynthesis planning, RetroExplainer has identified 101 pathways, in which 86.9\% of the single reactions correspond to those already reported in the literature. As a result, RetroExplainer is expected to offer valuable insights for reliable, high-throughput, and high-quality organic synthesis in drug development.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Cheminformatics,Drug discovery and development,Synthetic chemistry methodology},
  annotation = {TLDR: The results on 12 large-scale benchmark datasets demonstrate the effectiveness of RetroExplainer, which outperforms the state-of-the-art single-step retrosynthesis approaches and is expected to offer valuable insights for reliable, high-throughput, and high-quality organic synthesis in drug development.},
  timestamp = {2025-05-04T14:08:00Z}
}

@misc{wang2023selfconsistency,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2023},
  month = mar,
  number = {arXiv:2203.11171},
  eprint = {2203.11171},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.11171},
  urldate = {2025-03-19},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {TLDR: This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths.},
  timestamp = {2025-03-19T09:21:29Z}
}

@misc{wang2024cier,
  title = {{{CIER}}: {{A Novel Experience Replay Approach}} with {{Causal Inference}} in {{Deep Reinforcement Learning}}},
  shorttitle = {{{CIER}}},
  author = {Wang, Jingwen and Du, Dehui and Li, Yida and Li, Yiyang and Chen, Yikang},
  year = {2024},
  month = may,
  number = {arXiv:2405.08380},
  eprint = {2405.08380},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.08380},
  urldate = {2025-03-22},
  abstract = {In the training process of Deep Reinforcement Learning (DRL), agents require repetitive interactions with the environment. With an increase in training volume and model complexity, it is still a challenging problem to enhance data utilization and explainability of DRL training. This paper addresses these challenges by focusing on the temporal correlations within the time dimension of time series. We propose a novel approach to segment multivariate time series into meaningful subsequences and represent the time series based on these subsequences. Furthermore, the subsequences are employed for causal inference to identify fundamental causal factors that significantly impact training outcomes. We design a module to provide feedback on the causality during DRL training. Several experiments demonstrate the feasibility of our approach in common environments, confirming its ability to enhance the effectiveness of DRL training and impart a certain level of explainability to the training process. Additionally, we extended our approach with priority experience replay algorithm, and experimental results demonstrate the continued effectiveness of our approach.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {TLDR: A novel approach to segment multivariate time series into meaningful subsequences and represent the time series based on these subsequences is proposed and employed for causal inference to identify fundamental causal factors that significantly impact training outcomes.},
  timestamp = {2025-04-12T07:51:45Z}
}

@article{wang2024sfinn,
  title = {{{SFINN}}: Inferring Gene Regulatory Network from Single-Cell and Spatial Transcriptomic Data with Shared Factor Neighborhood and Integrated Neural Network},
  shorttitle = {{{SFINN}}},
  author = {Wang, Yongjie and Zhou, Fengfan and Guan, Jinting},
  year = {2024},
  month = jul,
  journal = {Bioinformatics},
  volume = {40},
  number = {7},
  pages = {btae433},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btae433},
  urldate = {2025-04-17},
  abstract = {The rise of single-cell RNA sequencing (scRNA-seq) technology presents new opportunities for constructing detailed cell type-specific gene regulatory networks (GRNs) to study cell heterogeneity. However, challenges caused by noises, technical errors, and dropout phenomena in scRNA-seq data pose significant obstacles to GRN inference, making the design of accurate GRN inference algorithms still essential. The recent growth of both single-cell and spatial transcriptomic sequencing data enables the development of supervised deep learning methods to infer GRNs on these diverse single-cell datasets.In this study, we introduce a novel deep learning framework based on shared factor neighborhood and integrated neural network (SFINN) for inferring potential interactions and causalities between transcription factors and target genes from single-cell and spatial transcriptomic data. SFINN utilizes shared factor neighborhood to construct cellular neighborhood network based on gene expression data and additionally integrates cellular network generated from spatial location information. Subsequently, the cell adjacency matrix and gene pair expression are fed into an integrated neural network framework consisting of a graph convolutional neural network and a fully-connected neural network to determine whether the genes interact. Performance evaluation in the tasks of gene interaction and causality prediction against the existing GRN reconstruction algorithms demonstrates the usability and competitiveness of SFINN across different kinds of data. SFINN can be applied to infer GRNs from conventional single-cell sequencing data and spatial transcriptomic data.SFINN can be accessed at GitHub: https://github.com/JGuan-lab/SFINN.},
  annotation = {TLDR: A novel deep learning framework based on shared factor neighborhood and integrated neural network (SFINN) for inferring potential interactions and causalities between transcription factors and target genes from single-cell and spatial transcriptomic data.},
  timestamp = {2025-04-17T11:51:05Z}
}

@article{wang2024staa,
  title = {{{STAA}}: {{Spatio-temporal}} Attention Attribution for Real-Time Interpreting Transformer-Based Video Models},
  author = {Wang, Zerui and Liu, Yan},
  year = {2024},
  journal = {arXiv preprint arXiv:2411.00630},
  eprint = {2411.00630},
  archiveprefix = {arXiv},
  timestamp = {2025-04-16T01:56:44Z}
}

@article{wang2024tmonet,
  title = {{{TMO-Net}}: An Explainable Pretrained Multi-Omics Model for Multi-Task Learning in Oncology},
  shorttitle = {{{TMO-Net}}},
  author = {Wang, Feng-ao and Zhuang, Zhenfeng and Gao, Feng and He, Ruikun and Zhang, Shaoting and Wang, Liansheng and Liu, Junwei and Li, Yixue},
  year = {2024},
  month = jun,
  journal = {Genome Biology},
  volume = {25},
  number = {1},
  pages = {149},
  issn = {1474-760X},
  doi = {10.1186/s13059-024-03293-9},
  urldate = {2025-05-03},
  abstract = {Cancer is a complex disease composing systemic alterations in multiple scales. In this study, we develop the Tumor Multi-Omics pre-trained Network (TMO-Net) that integrates multi-omics pan-cancer datasets for model pre-training, facilitating cross-omics interactions and enabling joint representation learning and incomplete omics inference. This model enhances multi-omics sample representation and empowers various downstream oncology tasks with incomplete multi-omics datasets. By employing interpretable learning, we characterize the contributions of distinct omics features to clinical outcomes. The TMO-Net model serves as a versatile framework for cross-modal multi-omics learning in oncology, paving the way for tumor omics-specific foundation models.},
  langid = {english},
  keywords = {Cancers,Model pre-training,Multi-omics,Prognosis prediction,Transfer learning},
  annotation = {TLDR: The TMO-Net model serves as a versatile framework for cross-modal multi-omics learning in oncology, paving the way for tumor omics-specific foundation models.},
  timestamp = {2025-05-03T09:40:53Z}
}

@article{wang2024xgraphcds,
  title = {{{XGraphCDS}}: {{An}} Explainable Deep Learning Model for Predicting Drug Sensitivity from Gene Pathways and Chemical Structures},
  shorttitle = {{{XGraphCDS}}},
  author = {Wang, Yimeng and Yu, Xinxin and Gu, Yaxin and Li, Weihua and Zhu, Keyun and Chen, Long and Tang, Yun and Liu, Guixia},
  year = {2024},
  month = jan,
  journal = {Computers in Biology and Medicine},
  volume = {168},
  pages = {107746},
  issn = {00104825},
  doi = {10.1016/j.compbiomed.2023.107746},
  urldate = {2025-08-09},
  langid = {english},
  annotation = {TLDR: An explainable graph neural network framework for predicting cancer drug sensitivity (XGraphCDS) based on comparative learning by integrating cancer gene expression information and drug chemical structure knowledge is developed, providing insights into resistance mechanisms alongside accurate predictions.},
  timestamp = {2025-08-09T14:48:41Z}
}

@misc{wang2025balancing,
  title = {Balancing {{Fairness}} and {{Performance}} in {{Healthcare AI}}: {{A Gradient Reconciliation Approach}}},
  shorttitle = {Balancing {{Fairness}} and {{Performance}} in {{Healthcare AI}}},
  author = {Wang, Xiaoyang and Yang, Christopher C.},
  year = {2025},
  month = apr,
  number = {arXiv:2504.14388},
  eprint = {2504.14388},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.14388},
  urldate = {2025-05-02},
  abstract = {The rapid growth of healthcare data and advances in computational power have accelerated the adoption of artificial intelligence (AI) in medicine. However, AI systems deployed without explicit fairness considerations risk exacerbating existing healthcare disparities, potentially leading to inequitable resource allocation and diagnostic disparities across demographic subgroups. To address this challenge, we propose FairGrad, a novel gradient reconciliation framework that automatically balances predictive performance and multi-attribute fairness optimization in healthcare AI models. Our method resolves conflicting optimization objectives by projecting each gradient vector onto the orthogonal plane of the others, thereby regularizing the optimization trajectory to ensure equitable consideration of all objectives. Evaluated on diverse real-world healthcare datasets and predictive tasks - including Substance Use Disorder (SUD) treatment and sepsis mortality - FairGrad achieved statistically significant improvements in multi-attribute fairness metrics (e.g., equalized odds) while maintaining competitive predictive accuracy. These results demonstrate the viability of harmonizing fairness and utility in mission-critical medical AI applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  timestamp = {2025-05-02T11:46:28Z}
}

@article{wang2025causal,
  title = {Causal {{ECGNet}}: Leveraging Causal Inference for Robust {{ECG}} Classification in Cardiac Disorders},
  shorttitle = {Causal {{ECGNet}}},
  author = {Wang, Mei and You, Cong and Zhang, Wei and Xu, Zibo and Liang, Qi and Li, Qiang},
  year = {2025},
  month = may,
  journal = {Frontiers in Physiology},
  volume = {16},
  pages = {1543417},
  issn = {1664-042X},
  doi = {10.3389/fphys.2025.1543417},
  urldate = {2025-05-20},
  abstract = {Electrocardiogram (ECG) is a graphical representation of the electrical activity of the heart and plays a crucial role in diagnosing heart disease and assessing cardiac function. In the context of infectious diseases, ECG classification is particularly critical, as many infections, such as viral myocarditis and sepsis, can cause significant cardiac complications. Early detection of infection-induced cardiac abnormalities through ECG can provide timely intervention and improve patient outcomes. However, current ECG processing methods often overlook the impact of confounding factors caused by statistical associations, which can compromise classification accuracy, especially in infection-related cardiac conditions. To address this, we propose an innovative approach to causal reasoning based on attention mechanisms. By employing backdoor adjustment for each cardiac lead, our method effectively eliminates confounding factors and models the true causal relationship between ECG patterns and underlying cardiac abnormalities caused by infectious diseases. Furthermore, our approach integrates the concept of entropy with causal inference to enhance ECG classification. By quantifying the information content and variability in ECG signals, we can better identify patterns and anomalies associated with infection-induced cardiac conditions. Experimental results demonstrate that our method achieves significant improvements in classification accuracy and robustness across four benchmark ECG datasets, outperforming existing methods. This work provides a novel perspective on the interplay between infection and cardiac function, offering valuable insights into the detection and understanding of infection-related cardiac complications.},
  annotation = {TLDR: A novel perspective on the interplay between infection and cardiac function is provided, offering valuable insights into the detection and understanding of infection-related cardiac complications.},
  timestamp = {2025-05-22T23:52:20Z}
}

@article{watson1993molecular,
  title = {{{MOLECULAR STRUCTURE OF NUCLEIC ACIDS}}: {{A Structure}} for {{Deoxyribose Nucleic Acid}}},
  shorttitle = {{{MOLECULAR STRUCTURE OF NUCLEIC ACIDS}}},
  author = {Watson, J. D.},
  year = {1993},
  month = apr,
  journal = {JAMA},
  volume = {269},
  number = {15},
  pages = {1966},
  issn = {0098-7484},
  doi = {10.1001/jama.1993.03500150078030},
  urldate = {2025-05-27},
  langid = {english},
  annotation = {TLDR: A structure for the salt of deoxyribose nucleic acid, which consists of three intertwined chains, with the phosphates near the fibre axis, and the bases on the outside, is suggested.},
  timestamp = {2025-05-27T15:26:23Z}
}

@article{watson2022interpretable,
  title = {Interpretable Machine Learning for Genomics},
  author = {Watson, David S.},
  year = {2022},
  month = sep,
  journal = {Human Genetics},
  volume = {141},
  number = {9},
  pages = {1499--1513},
  issn = {1432-1203},
  doi = {10.1007/s00439-021-02387-9},
  abstract = {High-throughput technologies such as next-generation sequencing allow biologists to observe cell function with unprecedented resolution, but the resulting datasets are too large and complicated for humans to understand without the aid of advanced statistical methods. Machine learning (ML) algorithms, which are designed to automatically find patterns in data, are well suited to this task. Yet these models are often so complex as to be opaque, leaving researchers with few clues about underlying mechanisms. Interpretable machine learning (iML) is a burgeoning subdiscipline of computational statistics devoted to making the predictions of ML models more intelligible to end users. This article is a gentle and critical introduction to iML, with an emphasis on genomic applications. I define relevant concepts, motivate leading methodologies, and provide a simple typology of existing approaches. I survey recent examples of iML in genomics, demonstrating how such techniques are increasingly integrated into research workflows. I argue that iML solutions are required to realize the promise of precision medicine. However, several open challenges remain. I examine the limitations of current state-of-the-art tools and propose a number of directions for future research. While the horizon for iML in genomics is wide and bright, continued progress requires close collaboration across disciplines.},
  langid = {english},
  pmcid = {PMC8527313},
  pmid = {34669035},
  keywords = {Algorithms,Genome,Genomics,Humans,Machine Learning,Precision Medicine},
  annotation = {TLDR: This article defines relevant concepts, motivate leading methodologies, and provides a simple typology of existing approaches for Interpretable machine learning (iML), arguing that iML solutions are required to realize the promise of precision medicine.},
  timestamp = {2025-04-17T15:28:39Z}
}

@misc{wei2019eda,
  title = {{{EDA}}: {{Easy Data Augmentation Techniques}} for {{Boosting Performance}} on {{Text Classification Tasks}}},
  shorttitle = {{{EDA}}},
  author = {Wei, Jason and Zou, Kai},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1901.11196},
  urldate = {2025-07-07},
  abstract = {We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50\% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  timestamp = {2025-07-07T04:29:05Z}
}

@misc{wei2022chainofthought,
  ids = {wei2023chainofthought},
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2022},
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2201.11903},
  urldate = {2025-09-01},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),Computer Science - Artificial Intelligence,Computer Science - Computation and Language,FOS: Computer and information sciences},
  timestamp = {2025-09-01T12:22:20Z}
}

@article{wei2022world,
  title = {{World Health Organization guidance Ethical and Governance of Artificial Intelligence for health and implications for China}},
  author = {Wei, B. R. and Xue, P. and Jiang, Y. and Zhai, X. M. and Qiao, Y. L.},
  year = {2022},
  month = mar,
  journal = {Zhonghua Yi Xue Za Zhi},
  volume = {102},
  number = {12},
  pages = {833--837},
  issn = {0376-2491},
  doi = {10.3760/cma.j.cn112137-20211223-02875},
  abstract = {With the explosive growth of deep learning and big data technology, artificial intelligence has penetrated into various fields of medical and health care, bringing efficient and high-quality health services to patients, but also a series of ethical and social governance issues have emerged. In order to avoid and eliminate the foreseeable ethical risks and governance challenges in the development of medical artificial intelligence, the World Health Organization (WHO) first released the Ethical and Governance of Artificial Intelligence for Health guidance on June 28, 2021, aimed to provide a framework for ethical guidelines on the deployment of artificial intelligence in clinical practice. At present, there are still shortcomings and this paper takes Healthy China 2030 agenda and the WHO guidelines as strategic ideas, and proposes to shape a consensus on the ethics of medical artificial intelligence, establish rules for human subjects and ownership of responsibilities, improve the legal and regulatory system, and determine human decision-making and moral subject status, taking into account the cultivation of interdisciplinary talents' ethical literacy and other Chinese inspirations are expected to promote the development of medical artificial intelligence ethics governance.},
  langid = {chi},
  pmid = {35330575},
  keywords = {Artificial Intelligence,China,Humans,Morals,World Health Organization},
  annotation = {TLDR: To shape a consensus on the ethics of medical artificial intelligence, establish rules for human subjects and ownership of responsibilities, improve the legal and regulatory system, and determine human decision-making and moral subject status, taking into account the cultivation of interdisciplinary talents' ethical literacy and other Chinese inspirations are expected to promote the development of medical Artificial Intelligence ethics governance.},
  timestamp = {2025-03-03T07:18:11Z}
}

@article{weiss2025deep,
  title = {A Deep Learning Framework for Causal Inference in Clinical Trial Design: {{The CURE AI}} Large Clinicogenomic Foundation Model},
  author = {Weiss, Amit D and Fomin, Vitalay and Feng, Di and Tang, Zuojian and Cai, James and John, Bino and Pfister, Neil T},
  year = {2025},
  journal = {medRxiv : the preprint server for health sciences},
  pages = {2025--03},
  publisher = {Cold Spring Harbor Laboratory Press},
  timestamp = {2025-04-15T16:27:20Z}
}

@article{wen2023applying,
  title = {Applying Causal Discovery to Single-Cell Analyses Using {{CausalCell}}},
  author = {Wen, Yujian and Huang, Jielong and Guo, Shuhui and Elyahu, Yehezqel and Monsonego, Alon and Zhang, Hai and Ding, Yanqing and Zhu, Hao},
  editor = {Momeni, Babak and Akhmanova, Anna and Momeni, Babak},
  year = {2023},
  month = may,
  journal = {eLife},
  volume = {12},
  pages = {e81464},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.81464},
  urldate = {2025-05-17},
  abstract = {Correlation between objects is prone to occur coincidentally, and exploring correlation or association in most situations does not answer scientific questions rich in causality. Causal discovery (also called causal inference) infers causal interactions between objects from observational data. Reported causal discovery methods and single-cell datasets make applying causal discovery to single cells a promising direction. However, evaluating and choosing causal discovery methods and developing and performing proper workflow remain challenges. We report the workflow and platform CausalCell (http://www.gaemons.net/causalcell/causalDiscovery/) for performing single-cell causal discovery. The workflow/platform is developed upon benchmarking four kinds of causal discovery methods and is examined by analyzing multiple single-cell RNA-sequencing (scRNA-seq) datasets. Our results suggest that different situations need different methods and the constraint-based PC algorithm with kernel-based conditional independence tests work best in most situations. Related issues are discussed and tips for best practices are given. Inferred causal interactions in single cells provide valuable clues for investigating molecular interactions and gene regulations, identifying critical diagnostic and therapeutic targets, and designing experimental and clinical interventions.},
  keywords = {causal analysis,causal relationship,feature selection,network inference,scRNA-seq,single-cell analysis},
  annotation = {TLDR: The results suggest that different situations need different methods and the constraint-based PC algorithm with kernel-based conditional independence tests work best in most situations.},
  timestamp = {2025-05-17T03:01:24Z}
}

@article{wollek2023attention,
  title = {Attention-Based Saliency Maps Improve Interpretability of Pneumothorax Classification},
  author = {Wollek, Alessandro and Graf, Robert and {\v C}e{\v c}atka, Sa{\v s}a and Fink, Nicola and Willem, Theresa and Sabel, Bastian O and Lasser, Tobias},
  year = {2023},
  journal = {Radiology: Artificial Intelligence},
  volume = {5},
  number = {2},
  pages = {e220187},
  publisher = {Radiological Society of North America},
  timestamp = {2025-05-02T11:19:04Z}
}

@article{wu2017omic,
  title = {-{{Omic}} and {{Electronic Health Record Big Data Analytics}} for {{Precision Medicine}}},
  author = {Wu, Po-Yen and Cheng, Chih-Wen and Kaddi, Chanchala D. and Venugopalan, Janani and Hoffman, Ryan and Wang, May D.},
  year = {2017},
  month = feb,
  journal = {IEEE transactions on bio-medical engineering},
  volume = {64},
  number = {2},
  pages = {263--273},
  issn = {1558-2531},
  doi = {10.1109/TBME.2016.2573285},
  abstract = {OBJECTIVE: Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of -omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. METHODS: In this paper, we present -omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. RESULTS: To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating -omic information into EHR. CONCLUSION: Big data analytics is able to address -omic and EHR data challenges for paradigm shift toward precision medicine. SIGNIFICANCE: Big data analytics makes sense of -omic and EHR data to improve healthcare outcome. It has long lasting societal impact.},
  langid = {english},
  pmcid = {PMC5859562},
  pmid = {27740470},
  keywords = {Databases Factual,Electronic Health Records,Genomics,Humans,Medical Informatics,Precision Medicine},
  annotation = {TLDR: This work provides two case studies, including identifying disease biomarkers from multi-omic data and incorporating --omic information into EHR, to demonstrate how big data analytics enables precision medicine.},
  timestamp = {2025-05-28T00:16:44Z}
}

@article{wu2021how,
  title = {How Medical {{AI}} Devices Are Evaluated: Limitations and Recommendations from an Analysis of {{FDA}} Approvals},
  shorttitle = {How Medical {{AI}} Devices Are Evaluated},
  author = {Wu, Eric and Wu, Kevin and Daneshjou, Roxana and Ouyang, David and Ho, Daniel E. and Zou, James},
  year = {2021},
  month = apr,
  journal = {Nature Medicine},
  volume = {27},
  number = {4},
  pages = {582--584},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-021-01312-x},
  urldate = {2025-05-15},
  abstract = {A comprehensive overview of medical AI devices approved by the US Food and Drug Administration sheds new light on limitations of the evaluation process that can mask vulnerabilities of devices when they are deployed on patients.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Business and industry,Medical research,Predictive medicine},
  timestamp = {2025-05-15T02:33:43Z}
}

@article{wu2022automated,
  title = {Automated Causal Inference in Application to Randomized Controlled Clinical Trials},
  author = {Wu, Ji Q. and Horeweg, Nanda and {de Bruyn}, Marco and Nout, Remi A. and {J{\"u}rgenliemk-Schulz}, Ina M. and Lutgens, Ludy C. H. W. and Jobsen, Jan J. and {van der Steen-Banasik}, Elzbieta M. and Nijman, Hans W. and Smit, Vincent T. H. B. M. and Bosse, Tjalling and Creutzberg, Carien L. and Koelzer, Viktor H.},
  year = {2022},
  month = may,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {5},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00470-y},
  urldate = {2025-04-17},
  abstract = {Randomized controlled trials (RCTs) are considered the gold standard for testing causal hypotheses in the clinical domain; however, the investigation of prognostic variables of patient outcome in a hypothesized cause--effect route is not feasible using standard statistical methods. Here we propose a new automated causal inference method (AutoCI) built on the invariant causal prediction (ICP) framework for the causal reinterpretation of clinical trial data. Compared with existing methods, we show that the proposed AutoCI allows one to clearly determine the causal variables of two real-world RCTs of patients with endometrial cancer with mature outcome and extensive clinicopathological and molecular data. This is achieved via suppressing the causal probability of non-causal variables by a wide margin. In ablation studies, we further demonstrate that the assignment of causal probabilities by AutoCI remains consistent in the presence of confounders. In conclusion, these results confirm the robustness and feasibility of AutoCI for future applications in real-world clinical analysis.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Cancer,Endometrial cancer},
  annotation = {TLDR: It is shown that the proposed AutoCI allows one to clearly determine the causal variables of two real-world RCTs of patients with endometrial cancer with mature outcome and extensive clinicopathological and molecular data via suppressing the causal probability of non-causal variables by a wide margin.},
  timestamp = {2025-04-17T15:10:09Z}
}

@article{wu2023interpretable,
  title = {Interpretable Weather Forecasting for Worldwide Stations with a Unified Deep Model},
  author = {Wu, Haixu and Zhou, Hang and Long, Mingsheng and Wang, Jianmin},
  year = {2023},
  month = jun,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {6},
  pages = {602--611},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00667-9},
  urldate = {2025-04-28},
  abstract = {Automatic weather stations are essential for fine-grained weather forecasting; they can be built almost anywhere around the world and are much cheaper than radars and satellites. However, these scattered stations only provide partial observations governed by the continuous space--time global weather system, thus introducing thorny challenges to worldwide forecasting. Here we present the Corrformer model with a novel multi-correlation mechanism, which unifies spatial cross-correlation and temporal auto-correlation into a learned multi-scale tree structure to capture worldwide spatiotemporal correlations. Corrformer reduces the canonical double quadratic complexity of spatiotemporal modelling to linear in spatial modelling and log-linear in temporal modelling, achieving collaborative forecasts for tens of thousands of stations within a unified deep model. Our model can generate interpretable predictions based on inferred propagation directions of weather processes, facilitating a fully data-driven artificial intelligence paradigm for discovering insights for meteorological science. Corrformer yields state-of-the-art forecasts on global, regional and citywide datasets with high confidence and provided skilful weather services for the 2022 Winter Olympics.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Atmospheric science,Computer science},
  annotation = {TLDR: The Corrformer model is presented with a novel multi-correlation mechanism, which unifies spatial cross-correlations and temporal auto-cor correlation into a learned multi-scale tree structure to capture worldwide spatiotemporal correlations.},
  timestamp = {2025-04-28T04:26:03Z}
}

@article{wu2024causal,
  title = {Causal Inference in the Medical Domain: A Survey},
  shorttitle = {Causal Inference in the Medical Domain},
  author = {Wu, Xing and Peng, Shaoqi and Li, Jingwen and Zhang, Jian and Sun, Qun and Li, Weimin and Qian, Quan and Liu, Yue and Guo, Yike},
  year = {2024},
  month = mar,
  journal = {Applied Intelligence},
  volume = {54},
  number = {6},
  pages = {4911--4934},
  issn = {1573-7497},
  doi = {10.1007/s10489-024-05338-9},
  urldate = {2025-03-24},
  abstract = {Causal inference is considered a crucial topic in the medical field, as it enables the determination of causal effects for medical treatments through data analysis. However, the vast volume and complexity of medical data present significant challenges for traditional machine learning methods in accurately assessing treatment effects. Issues such as noise in the data, unstructured information, and label sparsity can lead to unstable causal identification and erroneous correlation inference. To address these challenges, we propose a systematic survey of causal inference in the medical field, which encompasses studies utilizing observational data, aimed at organizing and summarizing the key concepts, methods, and applications of causal inference. Moreover, the causal inference applications are presented across various types of medical data, including medical images and Electronic Medical Records (EMR), using specific medical cases as examples. The thorough review not only emphasizes the theoretical and practical significance of causal inference methods but also highlights potential research directions in the medical domain.},
  langid = {english},
  keywords = {Artificial Intelligence,Causal effect estimation,Causal inference,Causality,Confounder balancing},
  timestamp = {2025-03-24T12:47:04Z}
}

@misc{wu2024regulatinga,
  title = {Regulating {{AI Adaptation}}: {{An Analysis}} of {{AI Medical Device Updates}}},
  shorttitle = {Regulating {{AI Adaptation}}},
  author = {Wu, Kevin and Wu, Eric and Rodolfa, Kit and Ho, Daniel E. and Zou, James},
  year = {2024},
  month = jun,
  pages = {2024.06.26.24309506},
  publisher = {medRxiv},
  doi = {10.1101/2024.06.26.24309506},
  urldate = {2025-04-12},
  abstract = {While the pace of development of AI has rapidly progressed in recent years, the implementation of safe and effective regulatory frameworks has lagged behind. In particular, the adaptive nature of AI models presents unique challenges to regulators as updating a model can improve its performance but also introduce safety risks. In the US, the Food and Drug Administration (FDA) has been a forerunner in regulating and approving hundreds of AI medical devices. To better understand how AI is updated and its regulatory considerations, we systematically analyze the frequency and nature of updates in FDA-approved AI medical devices. We find that less than 2\% of all devices report having been updated by being re-trained on new data. Meanwhile, nearly a quarter of devices report updates in the form of new functionality and marketing claims. As an illustrative case study, we analyze pneumothorax detection models and find that while model performance can degrade by as much as 0.18 AUC when evaluated on new sites, re-training on site-specific data can mitigate this performance drop, recovering up to 0.23 AUC. However, we also observed significant degradation on the original site after retraining using data from new sites, providing insight from one example that challenges the current one-model-fits-all approach to regulatory approvals. Our analysis provides an in-depth look at the current state of FDA-approved AI device updates and insights for future regulatory policies toward model updating and adaptive AI. Data and Code Availability The primary data used in this study are publicly available through the FDA website. Our analysis of the data and code used is available in the supplementary material and will be made publicly available on GitHub at https://github.com/kevinwu23/AIUpdating. Institutional Review Board (IRB) Our research does not require IRB approval.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  annotation = {TLDR: This analysis systematically analyze the frequency and nature of updates in FDA-approved AI medical devices to provide an in-depth look at the current state of FDA-approved AI device updates and insights for future regulatory policies toward model updating and adaptive AI.},
  timestamp = {2025-04-12T08:18:47Z}
}

@inproceedings{xu2019explainable,
  title = {Explainable {{AI}}: {{A}} Brief Survey on History, Research Areas, Approaches and Challenges},
  booktitle = {{{CCF}} International Conference on Natural Language Processing and {{Chinese}} Computing},
  author = {Xu, Feiyu and Uszkoreit, Hans and Du, Yangzhou and Fan, Wei and Zhao, Dongyan and Zhu, Jun},
  year = {2019},
  pages = {563--574},
  publisher = {Springer},
  timestamp = {2025-08-31T08:59:10Z}
}

@incollection{Xu2023,
  title = {Causal Explainable {{AI}}},
  booktitle = {Machine Learning for Causal Inference},
  author = {Xu, Shuyuan and Ge, Yingqiang and Zhang, Yongfeng},
  editor = {Li, Sheng and Chu, Zhixuan},
  year = {2023},
  pages = {137--159},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-35051-1_7},
  abstract = {Machine learning has achieved significant success in many AI applications that have been deployed. Most early approaches focused on optimizing performance measurements such as accuracy. However, as machine learning techniques have been applied to fields that are highly sensitive to risk, such as healthcare, law enforcement, and finance, the trustworthiness of models, especially their explainability, has become an increasingly important concern. Fortunately, explainability has become a crucial aspect of various research directions. These research directions include explainable recommender systems, explainable natural language processing, explainable computer vision, explainable graph neural networks, and explainable fairness. In order to further improve the interpretability of machine learning models, some recent works in explainability have attempted to use causal reasoning techniques. In this chapter, we aim to provide an overview of causal explanation and discuss the design of Causal eXplainable Artificial Intelligence (CXAI).},
  isbn = {978-3-031-35051-1},
  timestamp = {2025-04-17T11:58:01Z}
}

@article{xu2023ai,
  title = {{{AI}}/{{ML}} in {{Precision Medicine}}: {{A Look Beyond}} the {{Hype}}},
  shorttitle = {{{AI}}/{{ML}} in {{Precision Medicine}}},
  author = {Xu, Zhiheng and Biswas, Bipasa and Li, Lin and Amzal, Billy},
  year = {2023},
  month = sep,
  journal = {Therapeutic Innovation \& Regulatory Science},
  volume = {57},
  number = {5},
  pages = {957--962},
  issn = {2168-4790, 2168-4804},
  doi = {10.1007/s43441-023-00541-1},
  urldate = {2025-05-13},
  langid = {english},
  annotation = {TLDR: This paper provides a summary and expansion on the topics discussed in the panel: the application of AI/ML, bias, and data quality.},
  timestamp = {2025-05-13T10:53:25Z}
}

@article{xu2023interpretability,
  title = {Interpretability of {{Clinical Decision Support Systems Based}} on {{Artificial Intelligence}} from {{Technological}} and {{Medical Perspective}}: {{A Systematic Review}}},
  shorttitle = {Interpretability of {{Clinical Decision Support Systems Based}} on {{Artificial Intelligence}} from {{Technological}} and {{Medical Perspective}}},
  author = {Xu, Qian and Xie, Wenzhao and Liao, Bolin and Hu, Chao and Qin, Lu and Yang, Zhengzijin and Xiong, Huan and Lyu, Yi and Zhou, Yue and Luo, Aijing},
  year = {2023},
  journal = {Journal of Healthcare Engineering},
  volume = {2023},
  pages = {9919269},
  issn = {2040-2309},
  doi = {10.1155/2023/9919269},
  abstract = {BACKGROUND: Artificial intelligence (AI) has developed rapidly, and its application extends to clinical decision support system (CDSS) for improving healthcare quality. However, the interpretability of AI-driven CDSS poses significant challenges to widespread application. OBJECTIVE: This study is a review of the knowledge-based and data-based CDSS literature regarding interpretability in health care. It highlights the relevance of interpretability for CDSS and the area for improvement from technological and medical perspectives. METHODS: A systematic search was conducted on the interpretability-related literature published from 2011 to 2020 and indexed in the five databases: Web of Science, PubMed, ScienceDirect, Cochrane, and Scopus. Journal articles that focus on the interpretability of CDSS were included for analysis. Experienced researchers also participated in manually reviewing the selected articles for inclusion/exclusion and categorization. RESULTS: Based on the inclusion and exclusion criteria, 20 articles from 16 journals were finally selected for this review. Interpretability, which means a transparent structure of the model, a clear relationship between input and output, and explainability of artificial intelligence algorithms, is essential for CDSS application in the healthcare setting. Methods for improving the interpretability of CDSS include ante-hoc methods such as fuzzy logic, decision rules, logistic regression, decision trees for knowledge-based AI, and white box models, post hoc methods such as feature importance, sensitivity analysis, visualization, and activation maximization for black box models. A number of factors, such as data type, biomarkers, human-AI interaction, needs of clinicians, and patients, can affect the interpretability of CDSS. CONCLUSIONS: The review explores the meaning of the interpretability of CDSS and summarizes the current methods for improving interpretability from technological and medical perspectives. The results contribute to the understanding of the interpretability of CDSS based on AI in health care. Future studies should focus on establishing formalism for defining interpretability, identifying the properties of interpretability, and developing an appropriate and objective metric for interpretability; in addition, the user's demand for interpretability and how to express and provide explanations are also the directions for future research.},
  langid = {english},
  pmcid = {PMC9918364},
  pmid = {36776958},
  keywords = {Algorithms,Artificial Intelligence,Decision Support Systems Clinical,Delivery of Health Care,Humans,Knowledge Bases},
  annotation = {TLDR: The meaning of the interpretability of CDSS is explored, current methods for improving interpretability from technological and medical perspectives are summarized, and future studies should focus on establishing formalism for defining interpretability.},
  timestamp = {2025-04-02T07:55:05Z}
}

@misc{xu2024survey,
  title = {A {{Survey}} on {{Knowledge Distillation}} of {{Large Language Models}}},
  author = {Xu, Xiaohan and Li, Ming and Tao, Chongyang and Shen, Tao and Cheng, Reynold and Li, Jinyang and Xu, Can and Tao, Dacheng and Zhou, Tianyi},
  year = {2024},
  month = oct,
  number = {arXiv:2402.13116},
  eprint = {2402.13116},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.13116},
  urldate = {2025-05-03},
  abstract = {In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral. Additionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of KD's role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement. Our survey is meticulously structured around three foundational pillars: {\textbackslash}textit\{algorithm\}, {\textbackslash}textit\{skill\}, and {\textbackslash}textit\{verticalization\} -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in KD and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {TLDR: A comprehensive survey of KD's role within the realm of LLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement and proposing future research directions.},
  timestamp = {2025-05-03T14:44:21Z}
}

@misc{xu2025abstract,
  title = {From {{Abstract}} to {{Actionable}}: {{Pairwise Shapley Values}} for {{Explainable AI}}},
  shorttitle = {From {{Abstract}} to {{Actionable}}},
  author = {Xu, Jiaxin and Chau, Hung and Burden, Angela},
  year = {2025},
  month = feb,
  number = {arXiv:2502.12525},
  eprint = {2502.12525},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.12525},
  urldate = {2025-04-08},
  abstract = {Explainable AI (XAI) is critical for ensuring transparency, accountability, and trust in machine learning systems as black-box models are increasingly deployed within high-stakes domains. Among XAI methods, Shapley values are widely used for their fairness and consistency axioms. However, prevalent Shapley value approximation methods commonly rely on abstract baselines or computationally intensive calculations, which can limit their interpretability and scalability. To address such challenges, we propose Pairwise Shapley Values, a novel framework that grounds feature attributions in explicit, human-relatable comparisons between pairs of data instances proximal in feature space. Our method introduces pairwise reference selection combined with single-value imputation to deliver intuitive, model-agnostic explanations while significantly reducing computational overhead. Here, we demonstrate that Pairwise Shapley Values enhance interpretability across diverse regression and classification scenarios--including real estate pricing, polymer property prediction, and drug discovery datasets. We conclude that the proposed methods enable more transparent AI systems and advance the real-world applicability of XAI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {TLDR: Pairwise Shapley Values is proposed, a novel framework that grounds feature attributions in explicit, human-relatable comparisons between pairs of data instances proximal in feature space to deliver intuitive, model-agnostic explanations while significantly reducing computational overhead.},
  timestamp = {2025-04-08T13:47:04Z}
}

@article{xu2025mragent,
  title = {{{MRAgent}}: An {{LLM-based}} Automated Agent for Causal Knowledge Discovery in Disease via {{Mendelian}} Randomization},
  shorttitle = {{{MRAgent}}},
  author = {Xu, Wei and Luo, Gang and Meng, Weiyu and Zhai, Xiaobing and Zheng, Keli and Wu, Ji and Li, Yanrong and Xing, Abao and Li, Junrong and Li, Zhifan and Zheng, Ke and Li, Kefeng},
  year = {2025},
  month = mar,
  journal = {Briefings in Bioinformatics},
  volume = {26},
  number = {2},
  pages = {bbaf140},
  issn = {1477-4054},
  doi = {10.1093/bib/bbaf140},
  urldate = {2025-04-17},
  abstract = {Understanding causality in medical research is essential for developing effective interventions and diagnostic tools. Mendelian Randomization (MR) is a pivotal method for inferring causality through genetic data. However, MR analysis often requires pre-identification of exposure-outcome pairs from clinical experience or literature, which can be challenging to obtain. This poses difficulties for clinicians investigating causal factors of specific diseases. To address this, we introduce MRAgent, an innovative automated agent leveraging Large Language Models (LLMs) to enhance causal knowledge discovery in disease research. MRAgent autonomously scans scientific literature, discovers potential exposure-outcome pairs, and performs MR causal inference using extensive Genome-Wide Association Study data. We conducted both automated and human evaluations to compare different LLMs in operating MRAgent and provided a proof-of-concept case to demonstrate the complete workflow. MRAgent's capability to conduct large-scale causal analyses represents a significant advancement, equipping researchers and clinicians with a robust tool for exploring and validating causal relationships in complex diseases. Our code is public at https://github.com/xuwei1997/MRAgent.},
  annotation = {TLDR: MRAgent's capability to conduct large-scale causal analyses represents a significant advancement, equipping researchers and clinicians with a robust tool for exploring and validating causal relationships in complex diseases.},
  timestamp = {2025-04-17T15:08:54Z}
}

@article{xu2025towards,
  title = {Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models},
  author = {Xu, Fengli and Hao, Qianyue and Zong, Zefang and Wang, Jingwei and Zhang, Yunke and Wang, Jingyi and Lan, Xiaochong and Gong, Jiahui and Ouyang, Tianjian and Meng, Fanjin and others},
  year = {2025},
  journal = {arXiv preprint arXiv:2501.09686},
  eprint = {2501.09686},
  archiveprefix = {arXiv},
  timestamp = {2025-03-03T13:08:16Z}
}

@misc{yan2025pathorchestra,
  title = {{{PathOrchestra}}: {{A Comprehensive Foundation Model}} for {{Computational Pathology}} with {{Over}} 100 {{Diverse Clinical-Grade Tasks}}},
  shorttitle = {{{PathOrchestra}}},
  author = {Yan, Fang and Wu, Jianfeng and Li, Jiawen and Wang, Wei and Lu, Jiaxuan and Chen, Wen and Gao, Zizhao and Li, Jianan and Yan, Hong and Ma, Jiabo and Chen, Minda and Lu, Yang and Chen, Qing and Wang, Yizhi and Ling, Xitong and Wang, Xuenian and Wang, Zihan and Huang, Qiang and Hua, Shengyi and Liu, Mianxin and Ma, Lei and Shen, Tian and Zhang, Xiaofan and He, Yonghong and Chen, Hao and Zhang, Shaoting and Wang, Zhe},
  year = {2025},
  month = mar,
  journal = {arXiv.org},
  urldate = {2025-05-04},
  abstract = {The complexity and variability inherent in high-resolution pathological images present significant challenges in computational pathology. While pathology foundation models leveraging AI have catalyzed transformative advancements, their development demands large-scale datasets, considerable storage capacity, and substantial computational resources. Furthermore, ensuring their clinical applicability and generalizability requires rigorous validation across a broad spectrum of clinical tasks. Here, we present PathOrchestra, a versatile pathology foundation model trained via self-supervised learning on a dataset comprising 300K pathological slides from 20 tissue and organ types across multiple centers. The model was rigorously evaluated on 112 clinical tasks using a combination of 61 private and 51 public datasets. These tasks encompass digital slide preprocessing, pan-cancer classification, lesion identification, multi-cancer subtype classification, biomarker assessment, gene expression prediction, and the generation of structured reports. PathOrchestra demonstrated exceptional performance across 27,755 WSIs and 9,415,729 ROIs, achieving over 0.950 accuracy in 47 tasks, including pan-cancer classification across various organs, lymphoma subtype diagnosis, and bladder cancer screening. Notably, it is the first model to generate structured reports for high-incidence colorectal cancer and diagnostically complex lymphoma-areas that are infrequently addressed by foundational models but hold immense clinical potential. Overall, PathOrchestra exemplifies the feasibility and efficacy of a large-scale, self-supervised pathology foundation model, validated across a broad range of clinical-grade tasks. Its high accuracy and reduced reliance on extensive data annotation underline its potential for clinical integration, offering a pathway toward more efficient and high-quality medical services.},
  howpublished = {https://arxiv.org/abs/2503.24345v1},
  langid = {english},
  timestamp = {2025-05-04T05:30:14Z}
}

@article{yang2019concepts,
  title = {Concepts of {{Artificial Intelligence}} for {{Computer-Assisted Drug Discovery}}},
  author = {Yang, Xin and Wang, Yifei and Byrne, Ryan and Schneider, Gisbert and Yang, Shengyong},
  year = {2019},
  month = sep,
  journal = {Chemical Reviews},
  volume = {119},
  number = {18},
  pages = {10520--10594},
  issn = {1520-6890},
  doi = {10.1021/acs.chemrev.8b00728},
  abstract = {Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.},
  langid = {english},
  pmid = {31294972},
  keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Drug Design,Drug Discovery,Machine Learning,Neural Networks Computer,Small Molecule Libraries},
  annotation = {TLDR: The current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects.},
  timestamp = {2025-05-18T02:54:15Z}
}

@article{yang2022large,
  title = {A Large Language Model for Electronic Health Records},
  author = {Yang, Xi and Chen, Aokun and PourNejatian, Nima and Shin, Hoo Chang and Smith, Kaleb E. and Parisien, Christopher and Compas, Colin and Martin, Cheryl and Costa, Anthony B. and Flores, Mona G. and Zhang, Ying and Magoc, Tanja and Harle, Christopher A. and Lipori, Gloria and Mitchell, Duane A. and Hogan, William R. and Shenkman, Elizabeth A. and Bian, Jiang and Wu, Yonghui},
  year = {2022},
  month = dec,
  journal = {npj Digital Medicine},
  volume = {5},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00742-2},
  urldate = {2025-04-03},
  abstract = {There is an increasing interest in developing artificial intelligence (AI) systems to process and interpret electronic health records (EHRs). Natural language processing (NLP) powered by pretrained language models is the key technology for medical AI systems utilizing clinical narratives. However, there are few clinical language models, the largest of which trained in the clinical domain is comparatively small at 110 million parameters (compared with billions of parameters in the general domain). It is not clear how large clinical language models with billions of parameters can help medical AI systems utilize unstructured EHRs. In this study, we develop from scratch a large clinical language model---GatorTron---using {$>$}90 billion words of text (including {$>$}82 billion words of de-identified clinical text) and systematically evaluate it on five clinical NLP tasks including clinical concept extraction, medical relation extraction, semantic textual similarity, natural language inference (NLI), and medical question answering (MQA). We examine how (1) scaling up the number of parameters and (2) scaling up the size of the training data could benefit these NLP tasks. GatorTron models scale up the clinical language model from 110 million to 8.9 billion parameters and improve five clinical NLP tasks (e.g., 9.6\% and 9.5\% improvement in accuracy for NLI and MQA), which can be applied to medical AI systems to improve healthcare delivery. The GatorTron models are publicly available at: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron\_og.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Health care,Medical research},
  annotation = {TLDR: A large clinical language model---GatorTron---is developed from scratch and systematically evaluate it on five clinical NLP tasks including clinical concept extraction, medical relation extraction, semantic textual similarity, natural language inference (NLI), and medical question answering (MQA).},
  timestamp = {2025-04-03T12:30:31Z}
}

@article{yang2022unbox,
  ids = {yang2022unboxa},
  title = {Unbox the Black-Box for the Medical Explainable {{AI}} via Multi-Modal and Multi-Centre Data Fusion: {{A}} Mini-Review, Two Showcases and Beyond},
  shorttitle = {Unbox the Black-Box for the Medical Explainable {{AI}} via Multi-Modal and Multi-Centre Data Fusion},
  author = {Yang, Guang and Ye, Qinghao and Xia, Jun},
  year = {2022},
  journal = {Information Fusion},
  volume = {77},
  pages = {29--52},
  publisher = {Elsevier},
  issn = {15662535},
  doi = {10.1016/j.inffus.2021.07.016},
  urldate = {2025-03-25},
  abstract = {Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems' black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms cannot manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions.},
  langid = {english},
  annotation = {TLDR: This study surveyed the current progress of XAI and in particular its advances in healthcare applications, and introduced the solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios.},
  timestamp = {2025-03-25T11:30:00Z}
}

@article{yang2023alphafold2,
  title = {{{AlphaFold2}} and Its Applications in the Fields of Biology and Medicine},
  author = {Yang, Zhenyu and Zeng, Xiaoxi and Zhao, Yi and Chen, Runsheng},
  year = {2023},
  month = mar,
  journal = {Signal Transduction and Targeted Therapy},
  volume = {8},
  number = {1},
  pages = {1--14},
  publisher = {Nature Publishing Group},
  issn = {2059-3635},
  doi = {10.1038/s41392-023-01381-z},
  urldate = {2025-05-04},
  abstract = {AlphaFold2 (AF2) is an artificial intelligence (AI) system developed by DeepMind that can predict three-dimensional (3D) structures of proteins from amino acid sequences with atomic-level accuracy. Protein structure prediction is one of the most challenging problems in computational biology and chemistry, and has puzzled scientists for 50 years. The advent of AF2 presents an unprecedented progress in protein structure prediction and has attracted much attention. Subsequent release of structures of more than 200 million proteins predicted by AF2 further aroused great enthusiasm in the science community, especially in the fields of biology and medicine. AF2 is thought to have a significant impact on structural biology and research areas that need protein structure information, such as drug discovery, protein design, prediction of protein function, et al. Though the time is not long since AF2 was developed, there are already quite a few application studies of AF2 in the fields of biology and medicine, with many of them having preliminarily proved the potential of AF2. To better understand AF2 and promote its applications, we will in this article summarize the principle and system architecture of AF2 as well as the recipe of its success, and particularly focus on reviewing its applications in the fields of biology and medicine. Limitations of current AF2 prediction will also be discussed.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Structural biology},
  annotation = {TLDR: The principle and system architecture of AF2 as well as the recipe of its success are summarized, and its applications in the fields of biology and medicine are reviewed.},
  timestamp = {2025-05-04T13:49:12Z}
}

@misc{yang2023distilling,
  title = {Distilling {{Rule-based Knowledge}} into {{Large Language Models}}},
  author = {Yang, Wenkai and Lin, Yankai and Zhou, Jie and Wen, Ji-Rong},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2311.08883},
  urldate = {2025-06-13},
  abstract = {Large language models (LLMs) have shown incredible performance in completing various real-world tasks. The current paradigm of knowledge learning for LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples. However, this learning paradigm may not well learn those complicated rules, especially when the training examples are limited. We are inspired that humans can learn the new tasks or knowledge in another way by learning from rules. That is, humans can learn new tasks or grasp new knowledge quickly and generalize well given only a detailed rule and a few optional examples. Therefore, in this paper, we aim to explore the feasibility of this new learning paradigm, which targets on encoding rule-based knowledge into LLMs. We further propose rule distillation, which first uses the strong in-context abilities of LLMs to extract the knowledge from the textual rules, and then explicitly encode the knowledge into the parameters of LLMs by learning from the above in-context signals produced inside the model. Our experiments show that making LLMs learn from rules by our method is much more efficient than example-based learning in both the sample size and generalization ability. Warning: This paper may contain examples with offensive content.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  timestamp = {2025-06-13T07:38:47Z}
}

@misc{yang2023integrating,
  title = {Integrating {{UMLS Knowledge}} into {{Large Language Models}} for {{Medical Question Answering}}},
  author = {Yang, Rui and {Marrese-Taylor}, Edison and Ke, Yuhe and Cheng, Lechao and Chen, Qingyu and Li, Irene},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02778},
  eprint = {2310.02778},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.02778},
  urldate = {2025-05-03},
  abstract = {Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conducted blind reviews to evaluate the generated content, and the results indicate that this framework effectively enhances the factuality, completeness, and relevance of generated content. Our research demonstrates the effectiveness of using UMLS-augmented LLMs and highlights the potential application value of LLMs in in medical question-answering.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {TLDR: This research demonstrates the effectiveness of using UMLS-augmented LLMs and highlights the potential application value of LLMs in in medical question-answering.},
  timestamp = {2025-05-03T10:27:43Z}
}

@article{yang2024cpmkga,
  title = {{{CPMKG}}: A Condition-Based Knowledge Graph for Precision Medicine},
  shorttitle = {{{CPMKG}}},
  author = {Yang, Jiaxin and Zhuang, Xinhao and Li, Zhenqi and Xiong, Gang and Xu, Ping and Ling, Yunchao and Zhang, Guoqing},
  year = {2024},
  month = sep,
  journal = {Database},
  volume = {2024},
  pages = {baae102},
  issn = {1758-0463},
  doi = {10.1093/database/baae102},
  urldate = {2025-06-12},
  abstract = {Abstract             Personalized medicine tailors treatments and dosages based on a patient's unique characteristics, particularly its genetic profile. Over the decades, stratified research and clinical trials have uncovered crucial drug-related information---such as dosage, effectiveness, and side effects---affecting specific individuals with particular genetic backgrounds. This genetic-specific knowledge, characterized by complex multirelationships and conditions, cannot be adequately represented or stored in conventional knowledge systems. To address these challenges, we developed CPMKG, a condition-based platform that enables comprehensive knowledge representation. Through information extraction and meticulous curation, we compiled 307\,614 knowledge entries, encompassing thousands of drugs, diseases, phenotypes (complications/side effects), genes, and genomic variations across four key categories: drug side effects, drug sensitivity, drug mechanisms, and drug indications. CPMKG facilitates drug-centric exploration and enables condition-based multiknowledge inference, accelerating knowledge discovery through three pivotal applications. To enhance user experience, we seamlessly integrated a sophisticated large language model that provides textual interpretations for each subgraph, bridging the gap between structured graphs and language expressions. With its comprehensive knowledge graph and user-centric applications, CPMKG serves as a valuable resource for clinical research, offering drug information tailored to personalized genetic profiles, syndromes, and phenotypes.             Database URL: https://www.biosino.org/cpmkg/},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: CPMKG facilitates drug-centric exploration and enables condition-based multiknowledge inference, accelerating knowledge discovery through three pivotal applications and serves as a valuable resource for clinical research, offering drug information tailored to personalized genetic profiles, syndromes, and phenotypes.},
  timestamp = {2025-06-12T13:41:12Z}
}

@article{yang2024graphpca,
  title = {{{GraphPCA}}: A Fast and Interpretable Dimension Reduction Algorithm for Spatial Transcriptomics Data},
  shorttitle = {{{GraphPCA}}},
  author = {Yang, Jiyuan and Wang, Lu and Liu, Lin and Zheng, Xiaoqi},
  year = {2024},
  month = nov,
  journal = {Genome Biology},
  volume = {25},
  number = {1},
  pages = {287},
  issn = {1474-760X},
  doi = {10.1186/s13059-024-03429-x},
  urldate = {2025-04-21},
  abstract = {The rapid advancement of spatial transcriptomics technologies has revolutionized our understanding of cell heterogeneity and intricate spatial structures within tissues and organs. However, the high dimensionality and noise in spatial transcriptomic data present significant challenges for downstream data analyses. Here, we develop GraphPCA, an interpretable and quasi-linear dimension reduction algorithm that leverages the strengths of graphical regularization and principal component analysis. Comprehensive evaluations on simulated and multi-resolution spatial transcriptomic datasets generated from various platforms demonstrate the capacity of GraphPCA to enhance downstream analysis tasks including spatial domain detection, denoising, and trajectory inference compared to other state-of-the-art methods.},
  langid = {english},
  keywords = {Dimension reduction,PCA,Spatial domain detection,Spatial transcriptomics},
  annotation = {TLDR: Comprehensive evaluations on simulated and multi-resolution spatial transcriptomic datasets generated from various platforms demonstrate the capacity of GraphPCA to enhance downstream analysis tasks including spatial domain detection, denoising, and trajectory inference compared to other state-of-the-art methods.},
  timestamp = {2025-04-21T13:04:41Z}
}

@misc{yang2024interpretable,
  title = {Interpretable {{Machine Learning}} for {{Weather}} and {{Climate Prediction}}: {{A Survey}}},
  shorttitle = {Interpretable {{Machine Learning}} for {{Weather}} and {{Climate Prediction}}},
  author = {Yang, Ruyi and Hu, Jingyu and Li, Zihao and Mu, Jianli and Yu, Tingzhao and Xia, Jiangjiang and Li, Xuhong and Dasgupta, Aritra and Xiong, Haoyi},
  year = {2024},
  month = mar,
  number = {arXiv:2403.18864},
  eprint = {2403.18864},
  primaryclass = {physics},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.18864},
  urldate = {2025-04-28},
  abstract = {Advanced machine learning models have recently achieved high predictive accuracy for weather and climate prediction. However, these complex models often lack inherent transparency and interpretability, acting as "black boxes" that impede user trust and hinder further model improvements. As such, interpretable machine learning techniques have become crucial in enhancing the credibility and utility of weather and climate modeling. In this survey, we review current interpretable machine learning approaches applied to meteorological predictions. We categorize methods into two major paradigms: 1) Post-hoc interpretability techniques that explain pre-trained models, such as perturbation-based, game theory based, and gradient-based attribution methods. 2) Designing inherently interpretable models from scratch using architectures like tree ensembles and explainable neural networks. We summarize how each technique provides insights into the predictions, uncovering novel meteorological relationships captured by machine learning. Lastly, we discuss research challenges around achieving deeper mechanistic interpretations aligned with physical principles, developing standardized evaluation benchmarks, integrating interpretability into iterative model development workflows, and providing explainability for large foundation models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Physics - Atmospheric and Oceanic Physics},
  annotation = {TLDR: This survey reviews current interpretable machine learning approaches applied to meteorological predictions, and categorizes methods into two major paradigms: post-hoc interpretability techniques that explain pre-trained models, and Designing inherently interpretable models from scratch using architectures like tree ensembles and explainable neural networks.},
  timestamp = {2025-04-28T07:06:49Z}
}

@article{yao2021survey,
  title = {A {{Survey}} on {{Causal Inference}}},
  author = {Yao, Liuyi and Chu, Zhixuan and Li, Sheng and Li, Yaliang and Gao, Jing and Zhang, Aidong},
  year = {2021},
  month = oct,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {15},
  number = {5},
  pages = {1--46},
  issn = {1556-4681, 1556-472X},
  doi = {10.1145/3444944},
  urldate = {2025-02-24},
  abstract = {Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.},
  langid = {english},
  annotation = {TLDR: This survey provides a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks, and presents the plausible applications of these methods, including the applications in advertising, recommendation, medicine, and so on.},
  timestamp = {2025-02-24T03:56:25Z}
}

@article{yazdani2023gene,
  title = {Gene Signatures Derived from Transcriptomic-Causal Networks Stratified Colorectal Cancer Patients for Effective Targeted Therapy},
  author = {Yazdani, Akram and Lenz, Heinz-Josef and Pillonetto, Gianluigi and {Mendez-Giraldez}, Raul and Yazdani, Azam and Sanof, Hanna and Hadi, Reza and Samiei, Esmat and Venook, Alan P and Ratain, Mark J and Rashid, Naim and Vincent, Benjamin G and Qu, Xueping and Wen, Yujia and Kosorok, Michael and Symmans, William F and Shen, John Paul Y.C. and Lee, Michael S and Kopetz, Scott and Nixon, Andrew B and Bertagnolli, Monica M and Perou, Charles M and Innocenti, Federico},
  year = {2023},
  month = dec,
  publisher = {In Review},
  doi = {10.21203/rs.3.rs-3673588/v1},
  urldate = {2025-05-04},
  abstract = {Abstract           Predictive and prognostic gene signatures derived from interconnectivity among genes can tailor clinical care to patients in cancer treatment. We identified gene interconnectivity as the transcriptomic-causal network by integrating germline genotyping and tumor RNA-seq data from 1,165 patients with metastatic colorectal cancer (CRC). The patients were enrolled in a clinical trial with randomized treatment, either cetuximab or bevacizumab in combination with chemotherapy. We linked the network to overall survival (OS) and detected novel biomarkers by controlling for confounding genes. Our data-driven approach discerned sets of genes, each set collectively stratify patients based on OS. Two signatures under the cetuximab treatment were related to wound healing and macrophages. The signature under the bevacizumab treatment was related to cytotoxicity and we replicated its effect on OS using an external cohort. We also showed that the genes influencing OS within the signatures are downregulated in CRC tumor vs. normal tissue using another external cohort. Furthermore, the corresponding proteins encoded by the genes within the signatures interact each other and are functionally related. In conclusion, this study identified a group of genes that collectively stratified patients based on OS and uncovered promising novel prognostic biomarkers for personalized treatment of CRC using transcriptomic causal networks.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  annotation = {TLDR: This study identified a group of genes that collectively stratified patients based on OS and uncovered promising novel prognostic biomarkers for personalized treatment of CRC using transcriptomic causal networks.},
  timestamp = {2025-05-04T07:08:58Z}
}

@inproceedings{ye2020discovering,
  title = {Discovering Biomedical Causality by a Generative {{Bayesian}} Causal Network under Uncertainty},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Ye, Ting and Liao, Jun and Yan, Xuewen and Luo, Hao and Zhang, Wenbing and Liu, Li},
  year = {2020},
  month = jul,
  pages = {1--8},
  publisher = {IEEE},
  address = {Glasgow, United Kingdom},
  doi = {10.1109/IJCNN48605.2020.9207057},
  urldate = {2025-04-03},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-6926-2},
  annotation = {TLDR: A generative Bayesian causal network is introduced that combines neural network to explicitly characterize these unique causal-effect relationships as a variable number of nodes and links among biomedical variables.},
  timestamp = {2025-04-03T02:48:49Z}
}

@misc{ye2023causal,
  title = {Causal {{Intervention}} for {{Measuring Confidence}} in {{Drug-Target Interaction Prediction}}},
  author = {Ye, Wenting and Li, Chen and Xie, Yang and Zhang, Wen and Zhang, Hong-Yu and Wang, Bowen and Cheng, Debo and Feng, Zaiwen},
  year = {2023},
  month = nov,
  number = {arXiv:2306.00041},
  eprint = {2306.00041},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.00041},
  urldate = {2025-05-04},
  abstract = {Identifying and discovering drug-target interactions(DTIs) are vital steps in drug discovery and development. They play a crucial role in assisting scientists in finding new drugs and accelerating the drug development process. Recently, knowledge graph and knowledge graph embedding (KGE) models have made rapid advancements and demonstrated impressive performance in drug discovery. However, such models lack authenticity and accuracy in drug target identification, leading to an increased misjudgment rate and reduced drug development efficiency. To address these issues, we focus on the problem of drug-target interactions, with knowledge mapping as the core technology. Specifically, a causal intervention-based confidence measure is employed to assess the triplet score to improve the accuracy of the drug-target interaction prediction model. Experimental results demonstrate that the developed confidence measurement method based on causal intervention can significantly enhance the accuracy of DTI link prediction, particularly for high-precision models. The predicted results are more valuable in guiding the design and development of subsequent drug development experiments, thereby significantly improving the efficiency of drug development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  timestamp = {2025-05-04T13:18:09Z}
}

@misc{yeh2019infidelity,
  title = {On the ({{In}})Fidelity and {{Sensitivity}} for {{Explanations}}},
  author = {Yeh, Chih-Kuan and Hsieh, Cheng-Yu and Suggala, Arun Sai and Inouye, David I. and Ravikumar, Pradeep},
  year = {2019},
  month = nov,
  number = {arXiv:1901.09392},
  eprint = {1901.09392},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.09392},
  urldate = {2025-04-06},
  abstract = {We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature: (in)fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-04-06T12:48:05Z}
}

@misc{ying2019gnnexplainera,
  title = {{{GNNExplainer}}: {{Generating Explanations}} for {{Graph Neural Networks}}},
  shorttitle = {{{GNNExplainer}}},
  author = {Ying, Rex and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  year = {2019},
  month = nov,
  number = {arXiv:1903.03894},
  eprint = {1903.03894},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.03894},
  urldate = {2025-03-28},
  abstract = {Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs.GNNs combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by GNNs remains unsolved. Here we propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1\% on average. GNNExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  timestamp = {2025-03-28T09:47:27Z}
}

@article{yoo2019relevance,
  title = {Relevance Regularization of Convolutional Neural Network for Interpretable Classification},
  author = {Yoo, Chae Hwa and Kim, Nayoung and Kang, Je-Won},
  year = {2019},
  journal = {Network (Bristol, England)},
  volume = {50},
  number = {5},
  timestamp = {2025-05-17T13:29:25Z}
}

@article{yoo2025prediction,
  title = {Prediction of Checkpoint Inhibitor Immunotherapy Efficacy for Cancer Using Routine Blood Tests and Clinical Data},
  author = {Yoo, Seong-Keun and Fitzgerald, Conall W. and Cho, Byuri Angela and Fitzgerald, Bailey G. and Han, Catherine and Koh, Elizabeth S. and Pandey, Abhinav and Sfreddo, Hannah and Crowley, Fionnuala and Korostin, Michelle Rudshteyn and Debnath, Neha and Leyfman, Yan and Valero, Cristina and Lee, Mark and Vos, Joris L. and Lee, Andrew Sangho and Zhao, Karena and Lam, Stanley and Olumuyide, Ezekiel and Kuo, Fengshen and Wilson, Eric A. and Hamon, Pauline and Hennequin, Clotilde and Saffern, Miriam and Vuong, Lynda and Hakimi, A. Ari and Brown, Brian and Merad, Miriam and Gnjatic, Sacha and Bhardwaj, Nina and Galsky, Matthew D. and Schadt, Eric E. and Samstein, Robert M. and Marron, Thomas U. and G{\"o}nen, Mithat and Morris, Luc G. T. and Chowell, Diego},
  year = {2025},
  month = mar,
  journal = {Nature Medicine},
  volume = {31},
  number = {3},
  pages = {869--880},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-024-03398-5},
  urldate = {2025-05-04},
  abstract = {Predicting whether a patient with cancer will benefit from immune checkpoint inhibitors (ICIs) without resorting to advanced genomic or immunologic assays is an important clinical need. To address this, we developed and evaluated SCORPIO, a machine learning system that utilizes routine blood tests (complete blood count and comprehensive metabolic profile) alongside clinical characteristics from 9,745 ICI-treated patients across 21 cancer types. SCORPIO was trained on data from 1,628 patients across 17 cancer types from Memorial Sloan Kettering Cancer Center. In two internal test sets comprising 2,511 patients across 19 cancer types, SCORPIO achieved median time-dependent area under the receiver operating characteristic curve (AUC(t)) values of 0.763 and 0.759 for predicting overall survival at 6, 12, 18, 24 and 30 months, outperforming tumor mutational burden (TMB), which showed median AUC(t) values of 0.503 and 0.543. Additionally, SCORPIO demonstrated superior predictive performance for predicting clinical benefit (tumor response or prolonged stability), with AUC values of 0.714 and 0.641, compared to TMB (AUC\,=\,0.546 and 0.573). External validation was performed using 10 global phase 3 trials (4,447 patients across 6 cancer types) and a real-world cohort from the Mount Sinai Health System (1,159 patients across 18 cancer types). In these external cohorts, SCORPIO maintained robust performance in predicting ICI outcomes, surpassing programmed death-ligand 1 immunostaining. These findings underscore SCORPIO's reliability and adaptability, highlighting its potential to predict patient outcomes with ICI therapy across diverse cancer types and healthcare settings.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Cancer immunotherapy,Machine learning,Outcomes research,Predictive markers,Prognostic markers},
  annotation = {TLDR: SCORPIO demonstrated superior predictive performance for predicting clinical benefit (tumor response or prolonged stability), highlighting its potential to predict patient outcomes with ICI therapy across diverse cancer types and healthcare settings.},
  timestamp = {2025-05-04T07:19:02Z}
}

@inproceedings{yoon2018ganite,
  title = {{{GANITE}}: {{Estimation}} of {{Individualized Treatment Effects}} Using {{Generative Adversarial Nets}}},
  shorttitle = {{{GANITE}}},
  booktitle = {International Conference on Learning Representations},
  author = {Yoon, Jinsung and Jordon, James and van der Schaar, Mihaela},
  year = {2018},
  month = feb,
  urldate = {2025-03-02},
  abstract = {Estimating individualized treatment effects (ITE) is a challenging task due to the need for an individual's potential outcomes to be learned from biased data and without having access to the counterfactuals. We propose a novel method for inferring ITE based on the Generative Adversarial Nets (GANs) framework. Our method, termed Generative Adversarial Nets for inference of Individualized Treatment Effects (GANITE), is motivated by the possibility that we can capture the uncertainty in the counterfactual distributions by attempting to learn them using a GAN. We generate proxies of the counterfactual outcomes using a counterfactual generator, G, and then pass these proxies to an ITE generator, I, in order to train it. By modeling both of these using the GAN framework, we are able to infer based on the factual data, while still accounting for the unseen counterfactuals. We test our method on three real-world datasets (with both binary and multiple treatments) and show that GANITE outperforms state-of-the-art methods.},
  langid = {english},
  timestamp = {2025-03-05T02:14:34Z}
}

@article{yordanov2023integrated,
  title = {An Integrated Approach to Geographic Validation Helped Scrutinize Prediction Model Performance and Its Variability},
  author = {Yordanov, Tsvetan R. and Lopes, Ricardo R. and Ravelli, Anita C.J. and Vis, Marije and Houterman, Saskia and Marquering, Henk and {Abu-Hanna}, Ameen},
  year = {2023},
  month = may,
  journal = {Journal of Clinical Epidemiology},
  volume = {157},
  pages = {13--21},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2023.02.021},
  urldate = {2025-04-04},
  langid = {english},
  annotation = {TLDR: The illustrated combination of approaches provides useful insights to inspect multicenter-based prediction models, and exposes their limitations in transportability and performance variability when applied to different populations.},
  timestamp = {2025-04-16T08:19:47Z}
}

@article{young2008relation,
  title = {Relation of Low Hemoglobin and Anemia to Morbidity and Mortality in Patients Hospitalized with Heart Failure (Insight from the {{OPTIMIZE-HF}} Registry)},
  author = {Young, James B. and Abraham, William T. and Albert, Nancy M. and Gattis Stough, Wendy and Gheorghiade, Mihai and Greenberg, Barry H. and O'Connor, Christopher M. and She, Lilin and Sun, Jie Lena and Yancy, Clyde W. and Fonarow, Gregg C. and {OPTIMIZE-HF Investigators and Coordinators}},
  year = {2008},
  month = jan,
  journal = {The American Journal of Cardiology},
  volume = {101},
  number = {2},
  pages = {223--230},
  issn = {0002-9149},
  doi = {10.1016/j.amjcard.2007.07.067},
  abstract = {Anemia in heart failure (HF) is increasingly recognized and treated, but little is known about the prevalence and its relation to outcomes in patients hospitalized for decompensated HF in a situation of both reduced and preserved systolic function. We hypothesized that lower hemoglobin is correlated with death during hospitalization and 60 to 90 days postdischarge in patients with HF. The Organized Program to Initiate Lifesaving Treatment in Patients with Heart Failure is a registry and performance improvement program for hospitalized patients with HF. Study cohorts were defined by admission hemoglobin quartile. Data from 48,612 patients at 259 hospitals showed that half of the total cohort had low hemoglobin ({$<$}12.1 g/dl) and that 25\% were moderately to severely anemic (lowest hemoglobin quartile, 5 to 10.7 g/dl). Patients with low hemoglobin were older, were more often women and Caucasian, and had preserved systolic function and elevated creatinine. They were also less likely to receive angiotensin-converting enzyme inhibitors and beta blockers at discharge. Anemic patients had higher in-hospital mortality (4.8\% vs 3.0\%, lowest vs highest quartile), longer hospital length of stay (6.5 vs 5.3 days), and more readmissions by 90 days (33.1\% vs 24.2\%) (all p {$<$}0.0001). In conclusion, these data reveal a higher prevalence of low hemoglobin in hospitalized patients than noted in randomized HF trials and outpatient registries. Lower hemoglobin is associated with higher morbidity and mortality in hospitalized patients with HF.},
  langid = {english},
  pmid = {18178411},
  keywords = {Anemia,Female,Heart Failure,Hemoglobins,Hospital Mortality,Humans,Length of Stay,Male,Middle Aged,Prevalence,Prospective Studies,Quality Assurance Health Care,Registries,Survival Analysis,United States},
  annotation = {TLDR: Data reveal a higher prevalence of low hemoglobin in hospitalized patients than noted in randomized HF trials and outpatient registries, and lower hemoglobin is associated with higher morbidity and mortality in hospital patients with HF.},
  timestamp = {2025-05-06T06:53:51Z}
}

@misc{yu2021explainable,
  title = {Explainable Autoencoder-Based Representation Learning for Gene Expression Data},
  author = {Yu, Yang and Kossinna, Pathum and Li, Qing and Liao, Wenyuan and Zhang, Qingrun},
  year = {2021},
  month = dec,
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2021.12.21.473742},
  urldate = {2025-07-25},
  abstract = {AbstractModern machine learning methods have been extensively utilized in gene expression data analysis. In particular, autoencoders (AE) have been employed in processing noisy and heterogenous RNA-Seq data. However, AEs usually lead to ``black-box'' hidden variables difficult to interpret, hindering downstream experimental validation and clinical translation. To bridge the gap between complicated models and biological interpretations, we developed a tool, XAE4Exp (eXplainable AutoEncoder for Expression data), which integrates AE and SHapley Additive exPlanations (SHAP), a flagship technique in the field of eXplainable AI (XAI). It quantitatively evaluates the contributions of each gene to the hidden structure learned by an AE, substantially improving the expandability of AE outcomes. By applying XAE4Exp to The Cancer Genome Atlas (TCGA) breast cancer gene expression data, we identified genes that are not differentially expressed, and pathways in various cancer-related classes. This tool will enable researchers and practitioners to analyze high-dimensional expression data intuitively, paving the way towards broader uses of deep learning.AvailabilityOpen source at https://github.com/QingrunZhangLab/Explainable-Deep-Autoencoder.Contactsqingrun.zhang@ucalgary.ca and wliao@ucalgary.ca.},
  archiveprefix = {Cold Spring Harbor Laboratory},
  annotation = {TLDR: This tool is developed, which integrates AE and SHapley Additive exPlanations (SHAP), a flagship technique in the field of eXplainable AI (XAI), and quantitatively evaluates the contributions of each gene to the hidden structure learned by an AE, substantially improving the expandability of AE outcomes.},
  timestamp = {2025-07-25T13:42:22Z}
}

@article{yu2022spatial,
  title = {Spatial Transcriptomics Technology in Cancer Research},
  author = {Yu, Qichao and Jiang, Miaomiao and Wu, Liang},
  year = {2022},
  month = oct,
  journal = {Frontiers in Oncology},
  volume = {12},
  pages = {1019111},
  issn = {2234-943X},
  doi = {10.3389/fonc.2022.1019111},
  urldate = {2025-06-13},
  abstract = {In recent years, spatial transcriptomics (ST) technologies have developed rapidly and have been widely used in constructing spatial tissue atlases and characterizing spatiotemporal heterogeneity of cancers. Currently, ST has been used to profile spatial heterogeneity in multiple cancer types. Besides, ST is a benefit for identifying and comprehensively understanding special spatial areas such as tumor interface and tertiary lymphoid structures (TLSs), which exhibit unique tumor microenvironments (TMEs). Therefore, ST has also shown great potential to improve pathological diagnosis and identify novel prognostic factors in cancer. This review presents recent advances and prospects of applications on cancer research based on ST technologies as well as the challenges.},
  annotation = {TLDR: Recent advances and prospects of applications on cancer research based on ST technologies as well as the challenges are presented.},
  timestamp = {2025-06-13T08:51:55Z}
}

@article{yu2024deep,
  title = {Deep Learning Large-Scale Drug Discovery and Repurposing},
  author = {Yu, Min and Li, Weiming and Yu, Yunru and Zhao, Yu and Xiao, Lizhi and Lauschke, Volker M. and Cheng, Yiyu and Zhang, Xingcai and Wang, Yi},
  year = {2024},
  month = aug,
  journal = {Nature Computational Science},
  volume = {4},
  number = {8},
  pages = {600--614},
  publisher = {Nature Publishing Group},
  issn = {2662-8457},
  doi = {10.1038/s43588-024-00679-4},
  urldate = {2025-05-04},
  abstract = {Large-scale drug discovery and repurposing is challenging. Identifying the mechanism of action (MOA) is crucial, yet current approaches are costly and low-throughput. Here we present an approach for MOA identification by profiling changes in mitochondrial phenotypes. By temporally imaging mitochondrial morphology and membrane potential, we established a pipeline for monitoring time-resolved mitochondrial images, resulting in a dataset comprising 570,096 single-cell images of cells exposed to 1,068 United States Food and Drug Administration-approved drugs. A deep learning model named MitoReID, using a re-identification (ReID) framework and an Inflated 3D ResNet backbone, was developed. It achieved 76.32\% Rank-1 and 65.92\% mean average precision on the testing set and successfully identified the MOAs for six untrained drugs on the basis of mitochondrial phenotype. Furthermore, MitoReID identified cyclooxygenase-2 inhibition as the MOA of the natural compound epicatechin in tea, which was successfully validated in vitro. Our approach thus provides an automated and cost-effective alternative for target identification that could accelerate large-scale drug discovery and repurposing.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Cell biology,Computational models,Computational science,Drug discovery},
  annotation = {TLDR: The MitoReID approach provides an automated and cost-effective alternative for target identification that could accelerate large-scale drug discovery and repurposing.},
  timestamp = {2025-05-04T14:06:18Z}
}

@article{yuan2025inferring,
  title = {Inferring Gene Regulatory Networks from Single-Cell Multiome Data Using Atlas-Scale External Data},
  author = {Yuan, Qiuyue and Duren, Zhana},
  year = {2025},
  month = feb,
  journal = {Nature Biotechnology},
  volume = {43},
  number = {2},
  pages = {247--257},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-024-02182-7},
  urldate = {2025-04-18},
  abstract = {Existing methods for gene regulatory network (GRN) inference rely on gene expression data alone or on lower resolution bulk data. Despite the recent integration of chromatin accessibility and RNA sequencing data, learning complex mechanisms from limited independent data points still presents a daunting challenge. Here we present LINGER (Lifelong neural network for gene regulation), a machine-learning method to infer GRNs from single-cell paired gene expression and chromatin accessibility data. LINGER incorporates atlas-scale external bulk data across diverse cellular contexts and prior knowledge of transcription factor motifs as a manifold regularization. LINGER achieves a fourfold to sevenfold relative increase in accuracy over existing methods and reveals a complex regulatory landscape of genome-wide association studies, enabling enhanced interpretation of disease-associated variants and genes. Following the GRN inference from reference single-cell multiome data, LINGER enables the estimation of transcription factor activity solely from bulk or single-cell gene expression data, leveraging the abundance of available gene expression data to identify driver regulators from case-control studies.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Dynamic networks,Gene regulatory networks},
  annotation = {TLDR: LINGER (Lifelong neural network for gene regulation), a machine-learning method to infer GRNs from single-cell paired gene expression and chromatin accessibility data, achieves a fourfold to sevenfold relative increase in accuracy over existing methods and reveals a complex regulatory landscape of genome-wide association studies, enabling enhanced interpretation of disease-associated variants and genes.},
  timestamp = {2025-04-18T03:22:12Z}
}

@article{zahedi2024deep,
  title = {Deep Learning in Spatially Resolved Transcriptomics: A Comprehensive Technical View},
  shorttitle = {Deep Learning in Spatially Resolved Transcriptomics},
  author = {Zahedi, Roxana and Ghamsari, Reza and Argha, Ahmadreza and Macphillamy, Callum and Beheshti, Amin and Alizadehsani, Roohallah and Lovell, Nigel H and Lotfollahi, Mohammad and {Alinejad-Rokny}, Hamid},
  year = {2024},
  month = jan,
  journal = {Briefings in Bioinformatics},
  volume = {25},
  number = {2},
  pages = {bbae082},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbae082},
  urldate = {2025-04-17},
  abstract = {Abstract             Spatially resolved transcriptomics (SRT) is a pioneering method for simultaneously studying morphological contexts and gene expression at single-cell precision. Data emerging from SRT are multifaceted, presenting researchers with intricate gene expression matrices, precise spatial details and comprehensive histology visuals. Such rich and intricate datasets, unfortunately, render many conventional methods like traditional machine learning and statistical models ineffective. The unique challenges posed by the specialized nature of SRT data have led the scientific community to explore more sophisticated analytical avenues. Recent trends indicate an increasing reliance on deep learning algorithms, especially in areas such as spatial clustering, identification of spatially variable genes and data alignment tasks. In this manuscript, we provide a rigorous critique of these advanced deep learning methodologies, probing into their merits, limitations and avenues for further refinement. Our in-depth analysis underscores that while the recent innovations in deep learning tailored for SRT have been promising, there remains a substantial potential for enhancement. A crucial area that demands attention is the development of models that can incorporate intricate biological nuances, such as phylogeny-aware processing or in-depth analysis of minuscule histology image segments. Furthermore, addressing challenges like the elimination of batch effects, perfecting data normalization techniques and countering the overdispersion and zero inflation patterns seen in gene expression is pivotal. To support the broader scientific community in their SRT endeavors, we have meticulously assembled a comprehensive directory of readily accessible SRT databases, hoping to serve as a foundation for future research initiatives.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {TLDR: To support the broader scientific community in their SRT endeavors, a comprehensive directory of readily accessible SRT databases is assembled, hoping to serve as a foundation for future research initiatives.},
  timestamp = {2025-04-17T11:55:50Z}
}

@article{zakka2020mammoganesis,
  title = {{{MammoGANesis}}: {{Controlled Generation}} of {{High-Resolution Mammograms}} for {{Radiology Education}}},
  shorttitle = {{{MammoGANesis}}},
  author = {Zakka, Cyril and Saheb, Ghida and Najem, Elie and Berjawi, G.},
  year = {2020},
  month = oct,
  journal = {ArXiv},
  urldate = {2025-04-06},
  abstract = {During their formative years, radiology trainees are required to interpret hundreds of mammograms per month, with the objective of becoming apt at discerning the subtle patterns differentiating benign from malignant lesions. Unfortunately, medico-legal and technical hurdles make it difficult to access and query medical images for training.  In this paper we train a generative adversarial network (GAN) to synthesize 512 x 512 high-resolution mammograms. The resulting model leads to the unsupervised separation of high-level features (e.g. the standard mammography views and the nature of the breast lesions), with stochastic variation in the generated images (e.g. breast adipose tissue, calcification), enabling user-controlled global and local attribute-editing of the synthesized images.  We demonstrate the model's ability to generate anatomically and medically relevant mammograms by achieving an average AUC of 0.54 in a double-blind study on four expert mammography radiologists to distinguish between generated and real images, ascribing to the high visual quality of the synthesized and edited mammograms, and to their potential use in advancing and facilitating medical education.},
  timestamp = {2025-04-06T12:26:39Z}
}

@inproceedings{zang2023discovering,
  title = {Discovering the {{Real Association}}: {{Multimodal Causal Reasoning}} in {{Video Question Answering}}},
  shorttitle = {Discovering the {{Real Association}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zang, Chuanqi and Wang, Hanqing and Pei, Mingtao and Liang, Wei},
  year = {2023},
  pages = {19027--19036},
  urldate = {2025-03-23},
  langid = {english},
  timestamp = {2025-03-23T07:06:09Z}
}

@misc{zanga2023survey,
  title = {A {{Survey}} on {{Causal Discovery}}: {{Theory}} and {{Practice}}},
  shorttitle = {A {{Survey}} on {{Causal Discovery}}},
  author = {Zanga, Alessio and Stella, Fabio},
  year = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.10032},
  urldate = {2025-06-13},
  abstract = {Understanding the laws that govern a phenomenon is the core of scientific progress. This is especially true when the goal is to model the interplay between different aspects in a causal fashion. Indeed, causal inference itself is specifically designed to quantify the underlying relationships that connect a cause to its effect. Causal discovery is a branch of the broader field of causality in which causal graphs is recovered from data (whenever possible), enabling the identification and estimation of causal effects. In this paper, we explore recent advancements in a unified manner, provide a consistent overview of existing algorithms developed under different settings, report useful tools and data, present real-world applications to understand why and how these methods can be fruitfully exploited.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences},
  annotation = {TLDR: Recent advancements in a unified manner are explored, a consistent overview of existing algorithms developed under different settings are provided, useful tools and data are reported, and real-world applications are presented to understand why and how these methods can be fruitfully exploited.},
  timestamp = {2025-06-13T02:58:54Z}
}

@inproceedings{zeiler2010deconvolutional,
  title = {Deconvolutional Networks},
  booktitle = {2010 {{IEEE Computer Society Conference}} on Computer Vision and Pattern Recognition},
  author = {Zeiler, Matthew D and Krishnan, Dilip and Taylor, Graham W and Fergus, Rob},
  year = {2010},
  pages = {2528--2535},
  publisher = {IEEE},
  timestamp = {2025-03-18T06:33:07Z}
}

@article{zeineldin2022explainability,
  title = {Explainability of Deep Neural Networks for {{MRI}} Analysis of Brain Tumors},
  author = {Zeineldin, Ramy A. and Karar, Mohamed E. and Elshaer, Ziad and Coburger, {$\cdot$}Jan and Wirtz, Christian R. and Burgert, Oliver and {Mathis-Ullrich}, Franziska},
  year = {2022},
  month = sep,
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  volume = {17},
  number = {9},
  pages = {1673--1683},
  issn = {1861-6429},
  doi = {10.1007/s11548-022-02619-x},
  urldate = {2025-03-26},
  abstract = {Artificial intelligence (AI), in particular deep neural networks, has achieved remarkable results for medical image analysis in several applications. Yet the lack of explainability of deep neural models is considered the principal restriction before applying these methods in clinical practice.},
  langid = {english},
  keywords = {Artificial Intelligence,Brain glioma,Computer-aided diagnosis,Convolutional neural networks,Explainable AI},
  annotation = {TLDR: A NeuroXAI framework for explainable AI of deep learning networks to increase the trust of medical experts is proposed and implemented, which implements seven state-of-the-art explanation methods providing visualization maps to help make deep learning models transparent.},
  timestamp = {2025-03-26T15:00:19Z}
}

@misc{zhang2016topdown,
  title = {Top-down {{Neural Attention}} by {{Excitation Backprop}}},
  author = {Zhang, Jianming and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
  year = {2016},
  month = aug,
  number = {arXiv:1608.00507},
  eprint = {1608.00507},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1608.00507},
  urldate = {2025-03-28},
  abstract = {We aim to model the top-down attention of a Convolutional Neural Network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. In experiments, we demonstrate the accuracy and generalizability of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images.},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  timestamp = {2025-03-28T12:08:15Z}
}

@article{zhang2018interpretable,
  title = {An Interpretable Framework for Clustering Single-Cell {{RNA-Seq}} Datasets},
  author = {Zhang, Jesse M. and Fan, Jue and Fan, H. Christina and Rosenfeld, David and Tse, David N.},
  year = {2018},
  month = mar,
  journal = {BMC Bioinformatics},
  volume = {19},
  number = {1},
  pages = {93},
  issn = {1471-2105},
  doi = {10.1186/s12859-018-2092-7},
  urldate = {2025-04-18},
  abstract = {With the recent proliferation of single-cell RNA-Seq experiments, several methods have been developed for unsupervised analysis of the resulting datasets. These methods often rely on unintuitive hyperparameters and do not explicitly address the subjectivity associated with clustering.},
  keywords = {Clustering,Feature selection,Interpretability,Single-cell RNA-seq},
  annotation = {TLDR: DendroSplit is presented, an interpretable framework for analyzing single-cell RNA-Seq datasets that addresses both the clustering interpretability and clustering subjectivity issues and offers a novel perspective on the single-cell RNA-Seq clustering problem motivated by the definition of ``cell type''.},
  timestamp = {2025-04-18T03:17:24Z}
}

@article{zhang2018topdown,
  title = {Top-{{Down Neural Attention}} by {{Excitation Backprop}}},
  author = {Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
  year = {2018},
  month = oct,
  journal = {International Journal of Computer Vision},
  volume = {126},
  number = {10},
  pages = {1084--1102},
  issn = {1573-1405},
  doi = {10.1007/s11263-017-1059-x},
  urldate = {2025-03-28},
  abstract = {We aim to model the top-down attention of a convolutional neural network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. We show a theoretic connection between the proposed contrastive attention formulation and the Class Activation Map computation. Efficient implementation of Excitation Backprop for common neural network layers is also presented. In experiments, we visualize the evidence of a model's classification decision by computing the proposed top-down attention maps. For quantitative evaluation, we report the accuracy of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images. Finally, we demonstrate applications of our method in model interpretation and data annotation assistance for facial expression analysis and medical imaging tasks.},
  langid = {english},
  keywords = {Artificial Intelligence,Convolutional neural network,Selective tuning,Top-down attention},
  annotation = {TLDR: A new backpropagation scheme, called Excitation Backprop, is proposed to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process, and the concept of contrastive attention is introduced to make the top- down attention maps more discriminative.},
  timestamp = {2025-03-28T12:07:02Z}
}

@misc{zhang2021causal,
  title = {Causal {{Inference}} in Medicine and in Health Policy, a Summary},
  author = {Zhang, Wenhao and Ramezani, Ramin and Naeim, Arash},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2105.04655},
  urldate = {2025-05-05},
  abstract = {A data science task can be deemed as making sense of the data or testing a hypothesis about it. The conclusions inferred from data can greatly guide us to make informative decisions. Big data has enabled us to carry out countless prediction tasks in conjunction with machine learning, such as identifying high risk patients suffering from a certain disease and taking preventable measures. However, healthcare practitioners are not content with mere predictions - they are also interested in the cause-effect relation between input features and clinical outcomes. Understanding such relations will help doctors treat patients and reduce the risk effectively. Causality is typically identified by randomized controlled trials. Often such trials are not feasible when scientists and researchers turn to observational studies and attempt to draw inferences. However, observational studies may also be affected by selection and/or confounding biases that can result in wrong causal conclusions. In this chapter, we will try to highlight some of the drawbacks that may arise in traditional machine learning and statistical approaches to analyze the observational data, particularly in the healthcare data analytics domain. We will discuss causal inference and ways to discover the cause-effect from observational studies in healthcare domain. Moreover, we will demonstrate the applications of causal inference in tackling some common machine learning issues such as missing data and model transportability. Finally, we will discuss the possibility of integrating reinforcement learning with causality as a way to counter confounding bias.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)},
  timestamp = {2025-05-05T14:03:22Z}
}

@article{zhang2022applications,
  title = {Applications of Explainable Artificial Intelligence in Diagnosis and Surgery},
  author = {Zhang, Yiming and Weng, Ying and Lund, Jonathan},
  year = {2022},
  journal = {Diagnostics},
  volume = {12},
  number = {2},
  pages = {237},
  publisher = {MDPI},
  timestamp = {2025-04-19T13:26:05Z}
}

@misc{zhang2024multimodal,
  title = {Multimodal {{Chain-of-Thought Reasoning}} in {{Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  year = {2024},
  month = may,
  number = {arXiv:2302.00923},
  eprint = {2302.00923},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00923},
  urldate = {2025-03-19},
  abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {TLDR: This work proposes Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference, and indicates that it offers the advantages of mitigating hallucination and enhancing convergence speed.},
  timestamp = {2025-03-19T08:56:36Z}
}

@misc{zhang2024robust,
  title = {Towards {{Robust Multimodal Sentiment Analysis}} with {{Incomplete Data}}},
  author = {Zhang, Haoyu and Wang, Wenbin and Yu, Tianshu},
  year = {2024},
  month = nov,
  number = {arXiv:2409.20012},
  eprint = {2409.20012},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-11},
  abstract = {The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an emerging direction seeking to tackle the issue of data incompleteness. Recognizing that the language modality typically contains dense sentiment information, we consider it as the dominant modality and present an innovative Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust MSA. The proposed LNLN features a dominant modality correction (DMC) module and dominant modality based multimodal learning (DMML) module, which enhances the model's robustness across various noise scenarios by ensuring the quality of dominant modality representations. Aside from the methodical design, we perform comprehensive experiments under random data missing scenarios, utilizing diverse and meaningful settings on several popular datasets ({\textbackslash}textit\{e.g.,\} MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and fairness compared to existing evaluations in the literature. Empirically, LNLN consistently outperforms existing baselines, demonstrating superior performance across these challenging and extensive evaluation metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Multimedia},
  timestamp = {2024-11-11T16:11:36Z}
}

@article{zhang2025artificial,
  title = {Artificial Intelligence in Drug Development},
  author = {Zhang, Kang and Yang, Xin and Wang, Yifei and Yu, Yunfang and Huang, Niu and Li, Gen and Li, Xiaokun and Wu, Joseph C. and Yang, Shengyong},
  year = {2025},
  month = jan,
  journal = {Nature Medicine},
  volume = {31},
  number = {1},
  pages = {45--59},
  publisher = {Nature Publishing Group},
  issn = {1546-170X},
  doi = {10.1038/s41591-024-03434-4},
  urldate = {2025-05-04},
  abstract = {Drug development is a complex and time-consuming endeavor that traditionally relies on the experience of drug developers and trial-and-error experimentation. The advent of artificial intelligence (AI) technologies, particularly emerging large language models and generative AI, is poised to redefine this paradigm. The integration of AI-driven methodologies into the drug development pipeline has already heralded subtle yet meaningful enhancements in both the efficiency and effectiveness of this process. Here we present an overview of recent advancements in AI applications across the entire drug development workflow, encompassing the identification of disease targets, drug discovery, preclinical and clinical studies, and post-market surveillance. Lastly, we critically examine the prevailing challenges to highlight promising future research directions in AI-augmented drug development.},
  copyright = {2025 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational models,Drug discovery},
  annotation = {TLDR: An overview of recent advancements in AI applications across the entire drug development workflow, encompassing the identification of disease targets, drug discovery, preclinical and clinical studies, and post-market surveillance is presented.},
  timestamp = {2025-05-04T13:40:20Z}
}

@article{zhang2025relationshipa,
  title = {The Relationship between Epigenetic Biomarkers and the Risk of Diabetes and Cancer: A Machine Learning Modeling Approach},
  shorttitle = {The Relationship between Epigenetic Biomarkers and the Risk of Diabetes and Cancer},
  author = {Zhang, Shiqi and Jin, Jianan and Xu, Benfeng and Zheng, Qi and Mou, Haibo},
  year = {2025},
  journal = {Frontiers in Public Health},
  volume = {13},
  pages = {1509458},
  issn = {2296-2565},
  doi = {10.3389/fpubh.2025.1509458},
  abstract = {INTRODUCTION: Epigenetic biomarkers are molecular indicators of epigenetic changes, and some studies have suggested that these biomarkers have predictive power for disease risk. This study aims to analyze the relationship between 30 epigenetic biomarkers and the risk of diabetes and cancer using machine learning modeling. METHODS: The data for this study were sourced from the NHANES database, which includes DNA methylation arrays and epigenetic biomarker datasets. Nine machine learning algorithms were used to build models: AdaBoost, GBM, KNN, lightGBM, MLP, RF, SVM, XGBoost, and logistics. Model stability was evaluated using metrics such as Accuracy, MCC, and Sensitivity. The performance and decision-making ability of the models were displayed using ROC curves and DCA curves, while SHAP values were used to visualize the importance of each epigenetic biomarker. RESULTS: Epigenetic age acceleration was strongly associated with cancer risk but had a weaker relationship with diabetes. In the diabetes model, the top three contributing features were logA1Mort, family income-to-poverty ratio, and marital status. In the cancer model, the top three contributing features were gender, non-Hispanic White ethnicity, and PACKYRSMort. CONCLUSION: Our study identified the relationship between epigenetic biomarkers and the risk of diabetes and cancer, and used machine learning techniques to analyze the contributions of various epigenetic biomarkers to disease risk.},
  langid = {english},
  pmcid = {PMC11968389},
  pmid = {40190762},
  keywords = {Adult,Biomarkers,cancer,diabetes,Diabetes Mellitus,DNA Methylation,Epigenesis Genetic,epigenetic age acceleration,epigenetic biomarkers,epigenetic clocks,Female,Humans,machine learning,Machine Learning,Male,Middle Aged,Neoplasms,Nutrition Surveys,Risk Factors},
  annotation = {TLDR: The relationship between epigenetic biomarkers and the risk of diabetes and cancer, and used machine learning techniques to analyze the contributions of various epigenetic biomarkers to disease risk is identified.},
  timestamp = {2025-07-23T14:14:47Z}
}

@article{zhao2023causal,
  title = {Causal {{ML}}: {{Python}} Package for Causal Inference Machine Learning},
  shorttitle = {Causal {{ML}}},
  author = {Zhao, Yang and Liu, Qing},
  year = {2023},
  month = feb,
  journal = {SoftwareX},
  volume = {21},
  pages = {101294},
  issn = {23527110},
  doi = {10.1016/j.softx.2022.101294},
  urldate = {2024-12-28},
  langid = {english},
  timestamp = {2025-02-02T09:38:42Z}
}

@article{zhao2024inferring,
  title = {Inferring Single-Cell Spatial Gene Expression with Tissue Morphology via Explainable Deep Learning},
  author = {Zhao, Yue and Alizadeh, Elaheh and Liu, Yang and Xu, Ming and Mahoney, J Matthew and Li, Sheng},
  year = {2024},
  journal = {bioRxiv : the preprint server for biology},
  pages = {2024--06},
  publisher = {Cold Spring Harbor Laboratory},
  timestamp = {2025-04-18T02:55:20Z}
}

@article{zheng2022datadriven,
  title = {Data-Driven Causal Model Discovery and Personalized Prediction in {{Alzheimer}}'s Disease},
  author = {Zheng, Haoyang and Petrella, Jeffrey R. and Doraiswamy, P. Murali and Lin, Guang and Hao, Wenrui},
  year = {2022},
  month = sep,
  journal = {npj Digital Medicine},
  volume = {5},
  number = {1},
  pages = {1--12},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00632-7},
  urldate = {2025-04-17},
  abstract = {With the explosive growth of biomarker data in Alzheimer's disease (AD) clinical trials, numerous mathematical models have been developed to characterize disease-relevant biomarker trajectories over time. While some of these models are purely empiric, others are causal, built upon various hypotheses of AD pathophysiology, a complex and incompletely understood area of research. One of the most challenging problems in computational causal modeling is using a purely data-driven approach to derive the model's parameters and the mathematical model itself, without any prior hypothesis bias. In this paper, we develop an innovative data-driven modeling approach to build and parameterize a causal model to characterize the trajectories of AD biomarkers. This approach integrates causal model learning, population parameterization, parameter sensitivity analysis, and personalized prediction. By applying this integrated approach to a large multicenter database of AD biomarkers, the Alzheimer's Disease Neuroimaging Initiative, several causal models for different AD stages are revealed. In addition, personalized models for each subject are calibrated and provide accurate predictions of future cognitive status.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational models,Data processing},
  annotation = {TLDR: This paper develops an innovative data-driven modeling approach to build and parameterize a causal model to characterize the trajectories of AD biomarkers, which integrates causal model learning, population parameterization, parameter sensitivity analysis, and personalized prediction.},
  timestamp = {2025-04-17T15:31:17Z}
}

@article{zheng2023shocksurv,
  title = {{{ShockSurv}}: {{A}} Machine Learning Model to Accurately Predict 28-Day Mortality for Septic Shock Patients in the Intensive Care Unit},
  author = {Zheng, Fudan and Wang, Luhao and Pang, Yuxian and Chen, Zhiguang and Lu, Yutong and Yang, Yuedong and Wu, Jianfeng},
  year = {2023},
  journal = {Biomedical Signal Processing and Control},
  volume = {86},
  pages = {105146},
  publisher = {Elsevier},
  timestamp = {2025-05-01T08:42:27Z}
}

@article{zhong2024interpretable,
  title = {Interpretable Spatially Aware Dimension Reduction of Spatial Transcriptomics with {{STAMP}}},
  author = {Zhong, Chengwei and Ang, Kok Siong and Chen, Jinmiao},
  year = {2024},
  month = nov,
  journal = {Nature Methods},
  volume = {21},
  number = {11},
  pages = {2072--2083},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02463-8},
  urldate = {2025-04-14},
  abstract = {Spatial transcriptomics produces high-dimensional gene expression measurements with spatial context. Obtaining a biologically meaningful low-dimensional representation of such data is crucial for effective interpretation and downstream analysis. Here, we present Spatial Transcriptomics Analysis with topic Modeling to uncover spatial Patterns (STAMP), an interpretable spatially aware dimension reduction method built on a deep generative model that returns biologically relevant, low-dimensional spatial topics and associated gene modules. STAMP can analyze data ranging from a single section to multiple sections and from different technologies to time-series data, returning topics matching known biological domains and associated gene modules containing established markers highly ranked within. In a lung cancer sample, STAMP delineated cell states with supporting markers at a higher resolution than the original annotation and uncovered cancer-associated fibroblasts concentrated on the tumor edge's exterior. In time-series data of mouse embryonic development, STAMP disentangled the erythro-myeloid hematopoiesis and hepatocytes developmental trajectories within the liver. STAMP is highly scalable and can handle more than 500,000 cells.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational models,Machine learning},
  annotation = {TLDR: STAMP is an interpretable spatially aware dimension reduction method built on a deep generative model that returns biologically relevant, low-dimensional spatial topics and associated gene modules containing established markers highly ranked within.},
  timestamp = {2025-04-14T01:50:07Z}
}

@article{zhong2025enterprise,
  title = {Enterprise Violation Risk Deduction Combining Generative {{AI}} and Event Evolution Graph},
  author = {Zhong, Chao and Li, Pengjun and Wang, Jinlong and Xiong, Xiaoyun and Lv, Zhihan and Zhou, Xiaochen and Zhao, Qixin},
  year = {2025},
  journal = {Expert Systems},
  volume = {42},
  number = {1},
  pages = {e13622},
  issn = {1468-0394},
  doi = {10.1111/exsy.13622},
  urldate = {2025-04-12},
  abstract = {In the current realms of scientific research and commercial applications, the risk inference of regulatory violations by publicly listed enterprises has attracted considerable attention. However, there are some problems in the existing research on the deduction and prediction of violation risk of listed enterprises, such as the lack of analysis of the causal logic association between violation events, the low interpretability and effectiveness of the deduction and the lack of training data. To solve these problems, we propose a framework for enterprise violation risk deduction based on generative AI and event evolution graphs. First, the generative AI technology was used to generate a new text summary of the lengthy and complex enterprise violation announcement to realize a concise overview of the violation matters. Second, by fine-tuning the generative AI model, an event entity and causality extraction framework based on automated data augmentation are proposed, and the UIE (Unified Structure Generation for Universal Information Extraction) event entity extraction model is used to create the event entity extraction for listed enterprises `violations. Then, a causality extraction model CDDP-GAT (Event Causality Extraction Based on Chinese Dictionary and Dependency Parsing of GAT) is proposed. This model aims to identify and analyse the causal links between corporate breaches, thereby deepening the understanding of the event logic. Then, the merger of similar events was realized, and the causal correlation weights between enterprise violation-related events were evaluated. Finally, the listed enterprise's violation risk event evolution graph was constructed, and the enterprise violation risk deduction was carried out to form an expert system of financial violations. The deduction results show that the method can effectively reveal signs of enterprise violations and adverse consequences.},
  copyright = {{\copyright} 2024 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {causal extraction,enterprise violation,event evolution graph,generative AI,risk deduction},
  annotation = {TLDR: A framework for enterprise violation risk deduction based on generative AI and event evolution graphs is proposed and results show that the method can effectively reveal signs of enterprise violations and adverse consequences.},
  timestamp = {2025-04-12T07:44:31Z}
}

@inproceedings{Zhou_2016_CVPR,
  title = {Learning Deep Features for Discriminative Localization},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2016},
  month = jun,
  timestamp = {2025-04-21T15:43:27Z}
}

@article{zhou2007temporal,
  title = {Temporal Reasoning with Medical Data---a Review with Emphasis on Medical Natural Language Processing},
  author = {Zhou, Li and Hripcsak, George},
  year = {2007},
  journal = {Journal of biomedical informatics},
  volume = {40},
  number = {2},
  pages = {183--202},
  publisher = {Elsevier},
  timestamp = {2025-04-16T01:58:40Z}
}

@article{zhou2022generating,
  title = {Generating {{Counterfactual Explanations For Causal Inference}} in {{Breast Cancer Treatment Response}}},
  author = {Zhou, Siqiong and Pfeiffer, Nicholaus and Islam, Upala J. and Banerjee, Imon and Patel, Bhavika K. and Iquebal, Ashif S.},
  year = {2022},
  month = aug,
  journal = {IEEE International Conference on Automation Science and Engineering (CASE): [proceedings]. IEEE Conference on Automation Science and Engineering},
  volume = {2022},
  pages = {955--960},
  issn = {2161-8070},
  doi = {10.1109/case49997.2022.9926519},
  abstract = {Imaging phenotypes extracted via radiomics of magnetic resonance imaging has shown great potential at predicting the treatment response in breast cancer patients after administering neoadjuvant systemic therapy (NST). Existing machine learning models are, however, limited in providing an expert-level interpretation of these models, particularly interpretability towards generating causal inference. Causal relationships between imaging phenotypes, clinical information, molecular features, and the treatment response may be useful in guiding the treatment strategies, management plans, and gaining acceptance in medical communities. In this work, we leverage the concept of counterfactual explanations to extract causal relationships between various imaging phenotypes, clinical information, molecular features, and the treatment response after NST. We implement the methodology on a publicly available breast cancer dataset and demonstrate the causal relationships generated from counterfactual explanations. We also compare and contrast our results with traditional explanations, such as LIME and Shapley.},
  langid = {english},
  pmcid = {PMC11513173},
  pmid = {39463881},
  keywords = {counterfactual explanations,machine learning,magnetic resonance imaging,radiomics},
  annotation = {TLDR: This work uses the concept of counterfactual explanations to extract causal relationships between various imaging phenotypes, clinical information, molecular features, and the treatment response after NST, and implements the methodology on a publicly available breast cancer dataset and demonstrates the causal relationships generated from counterfactually explanations.},
  timestamp = {2025-05-04T09:06:30Z}
}

@misc{zhou2023xai,
  title = {{{XAI}} Meets {{Biology}}: {{A Comprehensive Review}} of {{Explainable AI}} in {{Bioinformatics Applications}}},
  shorttitle = {{{XAI}} Meets {{Biology}}},
  author = {Zhou, Zhongliang and Hu, Mengxuan and Salcedo, Mariah and Gravel, Nathan and Yeung, Wayland and Venkat, Aarya and Guo, Dongliang and Zhang, Jielu and Kannan, Natarajan and Li, Sheng},
  year = {2023},
  month = dec,
  number = {arXiv:2312.06082},
  eprint = {2312.06082},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.06082},
  urldate = {2025-07-25},
  abstract = {Artificial intelligence (AI), particularly machine learning and deep learning models, has significantly impacted bioinformatics research by offering powerful tools for analyzing complex biological data. However, the lack of interpretability and transparency of these models presents challenges in leveraging these models for deeper biological insights and for generating testable hypotheses. Explainable AI (XAI) has emerged as a promising solution to enhance the transparency and interpretability of AI models in bioinformatics. This review provides a comprehensive analysis of various XAI techniques and their applications across various bioinformatics domains including DNA, RNA, and protein sequence analysis, structural analysis, gene expression and genome analysis, and bioimaging analysis. We introduce the most pertinent machine learning and XAI methods, then discuss their diverse applications and address the current limitations of available XAI tools. By offering insights into XAI's potential and challenges, this review aims to facilitate its practical implementation in bioinformatics research and help researchers navigate the landscape of XAI tools.},
  archiveprefix = {arXiv},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  langid = {american},
  keywords = {Artificial Intelligence (cs.AI),Computer Science - Artificial Intelligence,FOS: Biological sciences,FOS: Computer and information sciences,Quantitative Biology - Quantitative Methods,Quantitative Methods (q-bio.QM)},
  annotation = {TLDR: This review provides a comprehensive analysis of various XAI techniques and their applications across various bioinformatics domains including DNA, RNA, and protein sequence analysis, structural analysis, gene expression and genome analysis, and bioimaging analysis.},
  timestamp = {2025-07-25T12:34:32Z}
}

@article{zhou2024scgan,
  title = {{{SCGAN}}: {{Sparse CounterGAN}} for {{Counterfactual Explanations}} in {{Breast Cancer Prediction}}},
  shorttitle = {{{SCGAN}}},
  author = {Zhou, Siqiong and Islam, Upala J. and Pfeiffer, Nicholaus and Banerjee, Imon and Patel, Bhavika K. and Iquebal, Ashif S.},
  year = {2024},
  month = jul,
  journal = {IEEE Transactions on Automation Science and Engineering},
  volume = {21},
  number = {3},
  pages = {2264--2275},
  issn = {1558-3783},
  doi = {10.1109/TASE.2023.3333788},
  urldate = {2025-04-17},
  abstract = {Imaging phenotypes extracted via radiomics of magnetic resonance imaging have shown great potential in predicting the treatment response in breast cancer patients after administering neoadjuvant systemic therapy (NST). Understanding the causal relationships between the treatment response and Imaging phenotypes, Clinical information, and Molecular (ICM) features are critical in guiding treatment strategies and management plans. Counterfactual explanations provide an interpretable approach to generating causal inference. However, existing approaches are either computationally prohibitive for high dimensional problems, generate unrealistic counterfactuals, or confound the effects of causal features by changing multiple features simultaneously. This paper proposes a new method called Sparse CounteRGAN (SCGAN) for generating counterfactual instances to reveal causal relationships between ICM features and the treatment response after NST. The generative approach learns the distribution of the original instances and, therefore, ensures that the new instances are realistic. We propose dropout training of the discriminator to promote sparsity and introduce a diversity term in the loss function to maximize the distances among generated counterfactuals. We evaluate the proposed method on two publicly available datasets, followed by the breast cancer dataset, and compare their performance with existing methods in the literature. Results show that SCGAN generates sparse and diverse counterfactual instances that also achieve plausibility and feasibility, making it a valuable tool for understanding the causal relationships between ICM features and treatment response. Note to Practitioners--- Determining the suitability of NST for a breast cancer patient before surgery is complex and depends on factors such as patient demographics, tumor characteristics, clinical history, and molecular subtypes. Understanding the causal relationships between different features and pathologic responses to NST may lead to opportunities for targeted therapies and help oncologists make informed decisions about continuing or limiting systemic therapy after the initial consultation. The lack of causal explanations in traditional machine learning models for predicting NST has limited their applicability in clinical decision-making. SCGAN proposed in this paper overcomes the limitations of existing methods in generating causal inference via counterfactual explanations. This approach helps identify causal relationships between imaging phenotypes, clinical history, molecular features, and pathologic response to NST. The resulting information can aid in developing personalized treatment plans for patients and ultimately improve patient outcomes. The proposed approach may be extended to broader applications beyond healthcare, such as manufacturing, where it could improve quality control processes and enhance production efficiency.},
  keywords = {Breast cancer,Correlation,Counterfactual explanations,Decision making,generative adversarial networks,Imaging,Machine learning,magnetic resonance imaging,Magnetic resonance imaging,radiomics,Surgery},
  annotation = {TLDR: This paper proposes a new method called Sparse CounteRGAN (SCGAN) for generating counterfactual instances to reveal causal relationships between ICM features and the treatment response after NST, and shows that SCGAN generates sparse and diverse counterfactual instances that also achieve plausibility and feasibility.},
  timestamp = {2025-04-17T15:22:03Z}
}

@misc{zhou2024survey,
  title = {A {{Survey}} on {{Efficient Inference}} for {{Large Language Models}}},
  author = {Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and Yan, Shengen and Dai, Guohao and Zhang, Xiao-Ping and Dong, Yuhan and Wang, Yu},
  year = {2024},
  month = jul,
  number = {arXiv:2404.14294},
  eprint = {2404.14294},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.14294},
  urldate = {2025-03-23},
  abstract = {Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {TLDR: A comprehensive survey of the existing literature on efficient LLM inference is presented, analyzing the primary causes of the inefficient LLM inference and introducing a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization.},
  timestamp = {2025-03-23T10:21:51Z}
}

@misc{zhou2025emerging,
  title = {Emerging {{Synergies}} in {{Causality}} and {{Deep Generative Models}}: {{A Survey}}},
  shorttitle = {Emerging {{Synergies}} in {{Causality}} and {{Deep Generative Models}}},
  author = {Zhou, Guanglin and Xie, Shaoan and Hao, Guang-Yuan and Chen, Shiming and Huang, Biwei and Xu, Xiwei and Wang, Chen and Zhu, Liming and Yao, Lina and Zhang, Kun},
  year = {2025},
  month = mar,
  number = {arXiv:2301.12351},
  eprint = {2301.12351},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.12351},
  urldate = {2025-03-23},
  abstract = {In the field of artificial intelligence (AI), the quest to understand and model data-generating processes (DGPs) is of paramount importance. Deep generative models (DGMs) have proven adept in capturing complex data distributions but often fall short in generalization and interpretability. On the other hand, causality offers a structured lens to comprehend the mechanisms driving data generation and highlights the causal-effect dynamics inherent in these processes. While causality excels in interpretability and the ability to extrapolate, it grapples with intricacies of high-dimensional spaces. Recognizing the synergistic potential, we delve into the confluence of causality and DGMs. We elucidate the integration of causal principles within DGMs, investigate causal identification using DGMs, and navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs). We offer insights into methodologies, highlight open challenges, and suggest future directions, positioning our comprehensive review as an essential guide in this swiftly emerging and evolving area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {TLDR: A comprehensive survey of Causal deep Generative Models (CGMs), which combine SCMs and deep generative models in a way that boosts several trustworthy properties such as robustness, fairness, and interpretability.},
  timestamp = {2025-03-23T10:20:27Z}
}

@inproceedings{zhu2017unpaired,
  title = {Unpaired {{Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = {2017},
  month = oct,
  pages = {2242--2251},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.244},
  urldate = {2025-05-03},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X {$\rightarrow$} Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y {$\rightarrow$} X and introduce a cycle consistency loss to push F(G(X)) {$\approx$} X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  keywords = {Extraterrestrial measurements,Graphics,Painting,Semantics,Training,Training data},
  annotation = {TLDR: This work presents an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples, and introduces a cycle consistency loss to push F(G(X)) {$\approx$} X (and vice versa).},
  timestamp = {2025-05-03T13:04:43Z}
}

@article{zhu2024computational,
  title = {Computational Intelligence-Based Classification System for the Diagnosis of Memory Impairment in Psychoactive Substance Users},
  author = {Zhu, Chaoyang},
  year = {2024},
  journal = {Journal of Cloud Computing},
  volume = {13},
  number = {1},
  pages = {119},
  publisher = {Springer},
  timestamp = {2025-03-29T14:21:09Z}
}

@article{ziegelstein2017personomics,
  title = {Personomics and Precision Medicine},
  author = {Ziegelstein, Roy C},
  year = {2017},
  journal = {Transactions of the American Clinical and Climatological Association},
  volume = {128},
  pages = {160},
  timestamp = {2025-05-13T13:09:34Z}
}

@misc{zintgraf2017visualizing,
  title = {Visualizing {{Deep Neural Network Decisions}}: {{Prediction Difference Analysis}}},
  shorttitle = {Visualizing {{Deep Neural Network Decisions}}},
  author = {Zintgraf, Luisa M. and Cohen, Taco S. and Adel, Tameem and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1702.04595},
  eprint = {1702.04595},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.04595},
  urldate = {2025-07-24},
  abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  timestamp = {2025-07-24T14:24:06Z}
}

@misc{zong2022const,
  title = {{{conST}}: An Interpretable Multi-Modal Contrastive Learning Framework for Spatial Transcriptomics},
  shorttitle = {{{conST}}},
  author = {Zong, Yongshuo and Yu, Tingyang and Wang, Xuesong and Wang, Yixuan and Hu, Zhihang and Li, Yu},
  year = {2022},
  month = jan,
  publisher = {Bioinformatics},
  doi = {10.1101/2022.01.14.476408},
  urldate = {2025-02-23},
  abstract = {Abstract                        Motivation             Spatially resolved transcriptomics (SRT) shows its impressive power in yielding biological insights into neuroscience, disease study, and even plant biology. However, current methods do not sufficiently explore the expressiveness of the multi-modal SRT data, leaving a large room for improvement of performance. Moreover, the current deep learning based methods lack interpretability due to the ``black box'' nature, impeding its further applications in the areas that require explanation.                                   Results                            We propose conST, a powerful and flexible SRT data analysis framework utilizing contrastive learning techniques. conST can learn low-dimensional embeddings by effectively integrating multi-modal SRT data,               i               .               e               . gene expression, spatial information, and morphology (if applicable). The learned embeddings can be then used for various downstream tasks, including clustering, trajectory and pseudotime inference, cell-to-cell interaction,               etc               . Extensive experiments in various datasets have been conducted to demonstrate the effectiveness and robustness of the proposed conST, achieving up to 10\% improvement in clustering ARI in the commonly used benchmark dataset. We also show that the learned embedding can be used in complicated scenarios, such as predicting cancer progression by analyzing the tumour microenvironment and cell-to-cell interaction (CCI) of breast cancer. Our framework is interpretable in that it is able to find the correlated spots that support the clustering, which matches the CCI interaction pairs as well, providing more confidence to clinicians when making clinical decisions.},
  archiveprefix = {Bioinformatics},
  langid = {english},
  timestamp = {2025-02-23T07:06:01Z}
}

@misc{zucker2021leveraging,
  title = {Leveraging {{Structured Biological Knowledge}} for {{Counterfactual Inference}}: A {{Case Study}} of {{Viral Pathogenesis}}},
  shorttitle = {Leveraging {{Structured Biological Knowledge}} for {{Counterfactual Inference}}},
  author = {Zucker, Jeremy and Paneri, Kaushal and {Mohammad-Taheri}, Sara and Bhargava, Somya and Kolambkar, Pallavi and Bakker, Craig and Teuton, Jeremy and Hoyt, Charles Tapley and Oxford, Kristie and Ness, Robert and Vitek, Olga},
  year = {2021},
  month = jan,
  number = {arXiv:2101.05136},
  eprint = {2101.05136},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2101.05136},
  urldate = {2025-04-12},
  abstract = {Counterfactual inference is a useful tool for comparing outcomes of interventions on complex systems. It requires us to represent the system in form of a structural causal model, complete with a causal diagram, probabilistic assumptions on exogenous variables, and functional assignments. Specifying such models can be extremely difficult in practice. The process requires substantial domain expertise, and does not scale easily to large systems, multiple systems, or novel system modifications. At the same time, many application domains, such as molecular biology, are rich in structured causal knowledge that is qualitative in nature. This manuscript proposes a general approach for querying a causal biological knowledge graph, and converting the qualitative result into a quantitative structural causal model that can learn from data to answer the question. We demonstrate the feasibility, accuracy and versatility of this approach using two case studies in systems biology. The first demonstrates the appropriateness of the underlying assumptions and the accuracy of the results. The second demonstrates the versatility of the approach by querying a knowledge base for the molecular determinants of a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-induced cytokine storm, and performing counterfactual inference to estimate the causal effect of medical countermeasures for severely ill patients.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  timestamp = {2025-04-12T07:33:30Z}
}

@comment{jabref-meta: groupsversion:3;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:diagnosis\;0\;;
1 ExplicitGroup:epigenetics\;0\;;
1 ExplicitGroup:omics\;0\;;
1 ExplicitGroup:prognostic\;0\;;
1 ExplicitGroup:Reasoning\;0\;;
1 ExplicitGroup:review\;0\;;
1 ExplicitGroup:treatment\;0\;;
}
