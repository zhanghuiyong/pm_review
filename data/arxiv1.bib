@article{2409.15698v2,
 abstract = {Explainability is crucial for the application of black-box Graph Neural
Networks (GNNs) in critical fields such as healthcare, finance, cybersecurity,
and more. Various feature attribution methods, especially the
perturbation-based methods, have been proposed to indicate how much each
node/edge contributes to the model predictions. However, these methods fail to
generate connected explanatory subgraphs that consider the causal interaction
between edges within different coalition scales, which will result in
unfaithful explanations. In our study, we propose GISExplainer, a novel
game-theoretic interaction based explanation method that uncovers what the
underlying GNNs have learned for node classification by discovering
human-interpretable causal explanatory subgraphs. First, GISExplainer defines a
causal attribution mechanism that considers the game-theoretic interaction of
multi-granularity coalitions in candidate explanatory subgraph to quantify the
causal effect of an edge on the prediction. Second, GISExplainer assumes that
the coalitions with negative effects on the predictions are also significant
for model interpretation, and the contribution of the computation graph stems
from the combined influence of both positive and negative interactions within
the coalitions. Then, GISExplainer regards the explanation task as a sequential
decision process, in which a salient edges is successively selected and
connected to the previously selected subgraph based on its causal effect to
form an explanatory subgraph, ultimately striving for better explanations.
Additionally, an efficiency optimization scheme is proposed for the causal
attribution mechanism through coalition sampling. Extensive experiments
demonstrate that GISExplainer achieves better performance than state-of-the-art
approaches w.r.t. two quantitative metrics: Fidelity and Sparsity.},
 author = {Xingping Xian and Jianlu Liu and Chao Wang and Tao Wu and Shaojie Qiao and Xiaochuan Tang and Qun Liu},
 citations = {0},
 comment = {13 pages, 7 figures},
 doi = {https://arxiv.org/abs/2409.15698},
 eprint = {2409.15698v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {GISExplainer: On Explainability of Graph Neural Networks via Game-theoretic Interaction Subgraphs},
 url = {http://arxiv.org/abs/2409.15698v2},
 year = {2024}
}

@article{2409.16395v2,
 abstract = {Medication errors significantly threaten patient safety, leading to adverse
drug events and substantial economic burdens on healthcare systems. Clinical
Decision Support Systems (CDSSs) aimed at mitigating these errors often face
limitations when processing unstructured clinical data, including reliance on
static databases and rule-based algorithms, frequently generating excessive
alerts that lead to alert fatigue among healthcare providers. This paper
introduces HELIOT, an innovative CDSS for adverse drug reaction management that
processes free-text clinical information using Large Language Models (LLMs)
integrated with a comprehensive pharmaceutical data repository. HELIOT
leverages advanced natural language processing capabilities to interpret
medical narratives, extract relevant drug reaction information from
unstructured clinical notes, and learn from past patient-specific medication
tolerances to reduce false alerts, enabling more nuanced and contextual adverse
drug event warnings across primary care, specialist consultations, and hospital
settings. An initial evaluation using a synthetic dataset of clinical
narratives and expert-verified ground truth shows promising results. HELIOT
achieves high accuracy in a controlled setting. In addition, by intelligently
analyzing previous medication tolerance documented in clinical notes and
distinguishing between cases requiring different alert types, HELIOT can
potentially reduce interruptive alerts by over 50% compared to traditional
CDSSs. While these preliminary findings are encouraging, real-world validation
will be essential to confirm these benefits in clinical practice.},
 author = {Gabriele De Vito and Filomena Ferrucci and Athanasios Angelakis},
 citations = {1},
 comment = {},
 doi = {10.1016/j.knosys.2025.114184},
 eprint = {2409.16395v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {HELIOT: LLM-Based CDSS for Adverse Drug Reaction Management},
 url = {http://arxiv.org/abs/2409.16395v2},
 year = {2024}
}

@article{2409.16872v2,
 abstract = {The popularisation of applying AI in businesses poses significant challenges
relating to ethical principles, governance, and legal compliance. Although
businesses have embedded AI into their day-to-day processes, they lack a
unified approach for mitigating its potential risks. This paper introduces a
framework ensuring that AI must be ethical, controllable, viable, and
desirable. Balancing these factors ensures the design of a framework that
addresses its trade-offs, such as balancing performance against explainability.
A successful framework provides practical advice for businesses to meet
regulatory requirements in sectors such as finance and healthcare, where it is
critical to comply with standards like GPDR and the EU AI Act. Different case
studies validate this framework by integrating AI in both academic and
practical environments. For instance, large language models are cost-effective
alternatives for generating synthetic opinions that emulate attitudes to
environmental issues. These case studies demonstrate how having a structured
framework could enhance transparency and maintain performance levels as shown
from the alignment between synthetic and expected distributions. This alignment
is quantified using metrics like Chi-test scores, normalized mutual
information, and Jaccard indexes. Future research should explore the
framework's empirical validation in diverse industrial settings further,
ensuring the model's scalability and adaptability.},
 author = {Haocheng Lin},
 citations = {4},
 comment = {The current version improves significantly by integrating ethical
  frameworks, expanding methodology and case studies, enhancing scalability and
  ethical-legal alignment, acknowledging prior work, and offering clearer
  structure and practical relevance},
 doi = {doi.org/10.48550/arXiv.2409.16872},
 eprint = {2409.16872v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Ethical and Scalable Automation: A Governance and Compliance Framework for Business Applications},
 url = {http://arxiv.org/abs/2409.16872v2},
 year = {2024}
}

@article{2409.17815v2,
 abstract = {Electroencephalography (EEG) provides a non-invasive way to observe brain
activity in real time. Deep learning has enhanced EEG analysis, enabling
meaningful pattern detection for clinical and research purposes. However, most
existing frameworks for EEG data analysis are either focused on preprocessing
techniques or deep learning model development, often overlooking the crucial
need for structured documentation and model interpretability. In this paper, we
introduce DREAMS (Deep REport for AI ModelS), a Python-based framework designed
to generate automated model cards for deep learning models applied to EEG data.
Unlike generic model reporting tools, DREAMS is specifically tailored for
EEG-based deep learning applications, incorporating domain-specific metadata,
preprocessing details, performance metrics, and uncertainty quantification. The
framework seamlessly integrates with deep learning pipelines, providing
structured YAML-based documentation. We evaluate DREAMS through two case
studies: an EEG emotion classification task using the FACED dataset and a
abnormal EEG classification task using the Temple Univeristy Hospital (TUH)
Abnormal dataset. These evaluations demonstrate how the generated model card
enhances transparency by documenting model performance, dataset biases, and
interpretability limitations. Unlike existing model documentation approaches,
DREAMS provides visualized performance metrics, dataset alignment details, and
model uncertainty estimations, making it a valuable tool for researchers and
clinicians working with EEG-based AI. The source code for DREAMS is
open-source, facilitating broad adoption in healthcare AI, research, and
ethical AI development.},
 author = {Rabindra Khadka and Pedro G Lind and Anis Yazidi and Asma Belhadi},
 citations = {0},
 comment = {},
 doi = {10.1016/j.softx.2025.102140},
 eprint = {2409.17815v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DREAMS: A python framework for Training Deep Learning Models on EEG Data with Model Card Reporting for Medical Applications},
 url = {http://arxiv.org/abs/2409.17815v2},
 year = {2024}
}

@article{2410.00366v1,
 abstract = {The rapid advancements in artificial intelligence (AI) have revolutionized
smart healthcare, driving innovations in wearable technologies, continuous
monitoring devices, and intelligent diagnostic systems. However, security,
explainability, robustness, and performance optimization challenges remain
critical barriers to widespread adoption in clinical environments. This
research presents an innovative algorithmic method using the Adaptive Feature
Evaluator (AFE) algorithm to improve feature selection in healthcare datasets
and overcome problems. AFE integrating Genetic Algorithms (GA), Explainable
Artificial Intelligence (XAI), and Permutation Combination Techniques (PCT),
the algorithm optimizes Clinical Decision Support Systems (CDSS), thereby
enhancing predictive accuracy and interpretability. The proposed method is
validated across three diverse healthcare datasets using six distinct machine
learning algorithms, demonstrating its robustness and superiority over
conventional feature selection techniques. The results underscore the
transformative potential of AFE in smart healthcare, enabling personalized and
transparent patient care. Notably, the AFE algorithm, when combined with a
Multi-layer Perceptron (MLP), achieved an accuracy of up to 98.5%, highlighting
its capability to improve clinical decision-making processes in real-world
healthcare applications.},
 author = {Prasenjit Maji and Amit Kumar Mondal and Hemanta Kumar Mondal and Saraju P. Mohanty},
 citations = {0},
 comment = {},
 doi = {10.1007/s42979-025-04260-2},
 eprint = {2410.00366v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Easydiagnos: a framework for accurate feature selection for automatic diagnosis in smart healthcare},
 url = {http://arxiv.org/abs/2410.00366v1},
 year = {2024}
}

@article{2410.01268v2,
 abstract = {Artificial intelligence (AI), machine learning, and deep learning have become
transformative forces in big data analytics and management, enabling
groundbreaking advancements across diverse industries. This article delves into
the foundational concepts and cutting-edge developments in these fields, with a
particular focus on large language models (LLMs) and their role in natural
language processing, multimodal reasoning, and autonomous decision-making.
Highlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores
their applications in data analysis, model design, and optimization.
  The integration of advanced algorithms like neural networks, reinforcement
learning, and generative models has enhanced the capabilities of AI systems to
process, visualize, and interpret complex datasets. Additionally, the emergence
of technologies like edge computing and automated machine learning (AutoML)
democratizes access to AI, empowering users across skill levels to engage with
intelligent systems. This work also underscores the importance of ethical
considerations, transparency, and fairness in the deployment of AI
technologies, paving the way for responsible innovation.
  Through practical insights into hardware configurations, software
environments, and real-world applications, this article serves as a
comprehensive resource for researchers and practitioners. By bridging
theoretical underpinnings with actionable strategies, it showcases the
potential of AI and LLMs to revolutionize big data management and drive
meaningful advancements across domains such as healthcare, finance, and
autonomous systems.},
 author = {Pohsun Feng and Ziqian Bi and Yizhu Wen and Xuanhe Pan and Benji Peng and Ming Liu and Jiawei Xu and Keyu Chen and Junyu Liu and Caitlyn Heqi Yin and Sen Zhang and Jinlang Wang and Qian Niu and Ming Li and Tianyang Wang},
 citations = {0},
 comment = {This book contains 155 pages and 9 figures},
 doi = {doi.org/10.48550/arXiv.2410.01268},
 eprint = {2410.01268v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Unveiling AI's Potential Through Tools, Techniques, and Applications},
 url = {http://arxiv.org/abs/2410.01268v2},
 year = {2024}
}

@article{2410.01855v2,
 abstract = {Diagnosis prediction is a critical task in healthcare, where timely and
accurate identification of medical conditions can significantly impact patient
outcomes. Traditional machine learning and deep learning models have achieved
notable success in this domain but often lack interpretability which is a
crucial requirement in clinical settings. In this study, we explore the use of
neuro-symbolic methods, specifically Logical Neural Networks (LNNs), to develop
explainable models for diagnosis prediction. Essentially, we design and
implement LNN-based models that integrate domain-specific knowledge through
logical rules with learnable thresholds. Our models, particularly
$M_{\text{multi-pathway}}$ and $M_{\text{comprehensive}}$, demonstrate superior
performance over traditional models such as Logistic Regression, SVM, and
Random Forest, achieving higher accuracy (up to 80.52\%) and AUROC scores (up
to 0.8457) in the case study of diabetes prediction. The learned weights and
thresholds within the LNN models provide direct insights into feature
contributions, enhancing interpretability without compromising predictive
power. These findings highlight the potential of neuro-symbolic approaches in
bridging the gap between accuracy and explainability in healthcare AI
applications. By offering transparent and adaptable diagnostic models, our work
contributes to the advancement of precision medicine and supports the
development of equitable healthcare solutions. Future research will focus on
extending these methods to larger and more diverse datasets to further validate
their applicability across different medical conditions and populations.},
 author = {Qiuhao Lu and Rui Li and Elham Sagheb and Andrew Wen and Jinlian Wang and Liwei Wang and Jungwei W. Fan and Hongfang Liu},
 citations = {},
 comment = {Proceedings of AMIA Informatics Summit 2025},
 doi = {https://doi.org/10.48550/arXiv.2410.01855},
 eprint = {2410.01855v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable Diagnosis Prediction through Neuro-Symbolic Integration},
 url = {http://arxiv.org/abs/2410.01855v2},
 year = {2024}
}

@article{2410.01859v4,
 abstract = {Objective: To improve prediction of Chronic Kidney Disease (CKD) progression
to End Stage Renal Disease (ESRD) using machine learning (ML) and deep learning
(DL) models applied to an integrated clinical and claims dataset of varying
observation windows, supported by explainable AI (XAI) to enhance
interpretability and reduce bias.
  Materials and Methods: We utilized data about 10,326 CKD patients, combining
their clinical and claims information from 2009 to 2018. Following data
preprocessing, cohort identification, and feature engineering, we evaluated
multiple statistical, ML and DL models using data extracted from five distinct
observation windows. Feature importance and Shapley value analysis were
employed to understand key predictors. Models were tested for robustness,
clinical relevance, misclassification errors and bias issues.
  Results: Integrated data models outperformed those using single data sources,
with the Long Short-Term Memory (LSTM) model achieving the highest AUC (0.93)
and F1 score (0.65). A 24-month observation window was identified as optimal
for balancing early detection and prediction accuracy. The 2021 eGFR equation
improved prediction accuracy and reduced racial bias, notably for African
American patients. Discussion: Improved ESRD prediction accuracy, results
interpretability and bias mitigation strategies presented in this study have
the potential to significantly enhance CKD and ESRD management, support
targeted early interventions and reduce healthcare disparities.
  Conclusion: This study presents a robust framework for predicting ESRD
outcomes in CKD patients, improving clinical decision-making and patient care
through multi-sourced, integrated data and AI/ML methods. Future research will
expand data integration and explore the application of this framework to other
chronic diseases.},
 author = {Yubo Li and Rema Padman},
 citations = {},
 comment = {},
 doi = {10.1093/jamia/ocaf118},
 eprint = {2410.01859v4},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Enhancing End Stage Renal Disease Outcome Prediction: A Multi-Sourced Data-Driven Approach},
 url = {http://arxiv.org/abs/2410.01859v4},
 year = {2024}
}

@article{2410.02133v1,
 abstract = {In many domains, such as healthcare, time-series data is often irregularly
sampled with varying intervals between observations. This poses challenges for
classical time-series models that require equally spaced data. To address this,
we propose a novel time-series Transformer called Trajectory Generative
Pre-trained Transformer (TrajGPT). TrajGPT employs a novel Selective Recurrent
Attention (SRA) mechanism, which utilizes a data-dependent decay to adaptively
filter out irrelevant past information based on contexts. By interpreting
TrajGPT as discretized ordinary differential equations (ODEs), it effectively
captures the underlying continuous dynamics and enables time-specific inference
for forecasting arbitrary target timesteps. Experimental results demonstrate
that TrajGPT excels in trajectory forecasting, drug usage prediction, and
phenotype classification without requiring task-specific fine-tuning. By
evolving the learned continuous dynamics, TrajGPT can interpolate and
extrapolate disease risk trajectories from partially-observed time series. The
visualization of predicted health trajectories shows that TrajGPT forecasts
unseen diseases based on the history of clinically relevant phenotypes (i.e.,
contexts).},
 author = {Ziyang Song and Qingcheng Lu and He Zhu and David Buckeridge and Yue Li},
 citations = {},
 comment = {9 pages},
 doi = {https://doi.org/10.48550/arXiv.2410.02133},
 eprint = {2410.02133v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {TrajGPT: Irregular Time-Series Representation Learning for Health Trajectory Analysis},
 url = {http://arxiv.org/abs/2410.02133v1},
 year = {2024}
}

@article{2410.03334v1,
 abstract = {Radiological services are experiencing unprecedented demand, leading to
increased interest in automating radiology report generation. Existing
Vision-Language Models (VLMs) suffer from hallucinations, lack
interpretability, and require expensive fine-tuning. We introduce SAE-Rad,
which uses sparse autoencoders (SAEs) to decompose latent representations from
a pre-trained vision transformer into human-interpretable features. Our hybrid
architecture combines state-of-the-art SAE advancements, achieving accurate
latent reconstructions while maintaining sparsity. Using an off-the-shelf
language model, we distil ground-truth reports into radiological descriptions
for each SAE feature, which we then compile into a full report for each image,
eliminating the need for fine-tuning large models for this task. To the best of
our knowledge, SAE-Rad represents the first instance of using mechanistic
interpretability techniques explicitly for a downstream multi-modal reasoning
task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific
metrics compared to state-of-the-art models while using significantly fewer
computational resources for training. Qualitative analysis reveals that SAE-Rad
learns meaningful visual concepts and generates reports aligning closely with
expert interpretations. Our results suggest that SAEs can enhance multimodal
reasoning in healthcare, providing a more interpretable alternative to existing
VLMs.},
 author = {Ahmed Abdulaal and Hugo Fry and Nina Montaña-Brown and Ayodeji Ijishakin and Jack Gao and Stephanie Hyland and Daniel C. Alexander and Daniel C. Castro},
 citations = {},
 comment = {},
 doi = {https://doi.org/10.48550/arXiv.2410.03334},
 eprint = {2410.03334v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation},
 url = {http://arxiv.org/abs/2410.03334v1},
 year = {2024}
}

@article{2410.05046v1,
 abstract = {This technical report introduces a Named Clinical Entity Recognition
Benchmark for evaluating language models in healthcare, addressing the crucial
natural language processing (NLP) task of extracting structured information
from clinical narratives to support applications like automated coding,
clinical trial cohort identification, and clinical decision support.
  The leaderboard provides a standardized platform for assessing diverse
language models, including encoder and decoder architectures, on their ability
to identify and classify clinical entities across multiple medical domains. A
curated collection of openly available clinical datasets is utilized,
encompassing entities such as diseases, symptoms, medications, procedures, and
laboratory measurements. Importantly, these entities are standardized according
to the Observational Medical Outcomes Partnership (OMOP) Common Data Model,
ensuring consistency and interoperability across different healthcare systems
and datasets, and a comprehensive evaluation of model performance. Performance
of models is primarily assessed using the F1-score, and it is complemented by
various assessment modes to provide comprehensive insights into model
performance. The report also includes a brief analysis of models evaluated to
date, highlighting observed trends and limitations.
  By establishing this benchmarking framework, the leaderboard aims to promote
transparency, facilitate comparative analyses, and drive innovation in clinical
entity recognition tasks, addressing the need for robust evaluation methods in
healthcare NLP.},
 author = {Wadood M Abdul and Marco AF Pimentel and Muhammad Umar Salman and Tathagata Raha and Clément Christophe and Praveen K Kanithi and Nasir Hayat and Ronnie Rajan and Shadab Khan},
 citations = {},
 comment = {Technical Report},
 doi = {},
 eprint = {2410.05046v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Named Clinical Entity Recognition Benchmark},
 url = {http://arxiv.org/abs/2410.05046v1},
 year = {2024}
}

@article{2410.09674v2,
 abstract = {Neuromorphic computing has emerged as a promising energy-efficient
alternative to traditional artificial intelligence, predominantly utilizing
spiking neural networks (SNNs) implemented on neuromorphic hardware.
Significant advancements have been made in SNN-based convolutional neural
networks (CNNs) and Transformer architectures. However, neuromorphic computing
for the medical imaging domain remains underexplored. In this study, we
introduce EG-SpikeFormer, an SNN architecture tailored for clinical tasks that
incorporates eye-gaze data to guide the model's attention to the diagnostically
relevant regions in medical images. Our developed approach effectively
addresses shortcut learning issues commonly observed in conventional models,
especially in scenarios with limited clinical data and high demands for model
reliability, generalizability, and transparency. Our EG-SpikeFormer not only
demonstrates superior energy efficiency and performance in medical image
prediction tasks but also enhances clinical relevance through multi-modal
information alignment. By incorporating eye-gaze data, the model improves
interpretability and generalization, opening new directions for applying
neuromorphic computing in healthcare.},
 author = {Yi Pan and Hanqi Jiang and Junhao Chen and Yiwei Li and Huaqin Zhao and Yifan Zhou and Peng Shu and Zihao Wu and Zhengliang Liu and Dajiang Zhu and Xiang Li and Yohannes Abate and Tianming Liu},
 citations = {1},
 comment = {},
 doi = {10.1109/isbi60581.2025.10980855},
 eprint = {2410.09674v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {EG-SpikeFormer: Eye-Gaze Guided Transformer on Spiking Neural Networks for Medical Image Analysis},
 url = {http://arxiv.org/abs/2410.09674v2},
 year = {2024}
}

@article{2410.09704v1,
 abstract = {Echocardiography is the most widely used cardiac imaging modality, capturing
ultrasound video data to assess cardiac structure and function. Artificial
intelligence (AI) in echocardiography has the potential to streamline manual
tasks and improve reproducibility and precision. However, most echocardiography
AI models are single-view, single-task systems that do not synthesize
complementary information from multiple views captured during a full exam, and
thus lead to limited performance and scope of applications. To address this
problem, we introduce EchoPrime, a multi-view, view-informed, video-based
vision-language foundation model trained on over 12 million video-report pairs.
EchoPrime uses contrastive learning to train a unified embedding model for all
standard views in a comprehensive echocardiogram study with representation of
both rare and common diseases and diagnoses. EchoPrime then utilizes
view-classification and a view-informed anatomic attention model to weight
video-specific interpretations that accurately maps the relationship between
echocardiographic views and anatomical structures. With retrieval-augmented
interpretation, EchoPrime integrates information from all echocardiogram videos
in a comprehensive study and performs holistic comprehensive clinical
echocardiography interpretation. In datasets from two independent healthcare
systems, EchoPrime achieves state-of-the art performance on 23 diverse
benchmarks of cardiac form and function, surpassing the performance of both
task-specific approaches and prior foundation models. Following rigorous
clinical evaluation, EchoPrime can assist physicians in the automated
preliminary assessment of comprehensive echocardiography.},
 author = {Milos Vukadinovic and Xiu Tang and Neal Yuan and Paul Cheng and Debiao Li and Susan Cheng and Bryan He and David Ouyang},
 citations = {},
 comment = {30 pages, 3 tables, 3 figures},
 doi = {},
 eprint = {2410.09704v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation},
 url = {http://arxiv.org/abs/2410.09704v1},
 year = {2024}
}

@article{2410.10041v2,
 abstract = {Dynamic concepts in time series are crucial for understanding complex systems
such as financial markets, healthcare, and online activity logs. These concepts
help reveal structures and behaviors in sequential data for better
decision-making and forecasting. However, existing models often struggle to
detect and track concept drift due to limitations in interpretability and
adaptability. To address this challenge, inspired by the flexibility of the
recent Kolmogorov-Arnold Network (KAN), we propose WormKAN, a concept-aware
KAN-based model to address concept drift in co-evolving time series. WormKAN
consists of three key components: Patch Normalization, Temporal Representation
Module, and Concept Dynamics. Patch normalization processes co-evolving time
series into patches, treating them as fundamental modeling units to capture
local dependencies while ensuring consistent scaling. The temporal
representation module learns robust latent representations by leveraging a
KAN-based autoencoder, complemented by a smoothness constraint, to uncover
inter-patch correlations. Concept dynamics identifies and tracks dynamic
transitions, revealing structural shifts in the time series through concept
identification and drift detection. These transitions, akin to passing through
a \textit{wormhole}, are identified by abrupt changes in the latent space.
Experiments show that KAN and KAN-based models (WormKAN) effectively segment
time series into meaningful concepts, enhancing the identification and tracking
of concept drift.},
 author = {Kunpeng Xu and Lifei Chen and Shengrui Wang},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2410.10041v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {WormKAN: Are KAN Effective for Identifying and Tracking Concept Drift in Time Series?},
 url = {http://arxiv.org/abs/2410.10041v2},
 year = {2024}
}

@article{2410.10220v1,
 abstract = {Deep learning has made significant strides in medical imaging, leveraging the
use of large datasets to improve diagnostics and prognostics. However, large
datasets often come with inherent errors through subject selection and
acquisition. In this paper, we investigate the use of Diffusion Autoencoder
(DAE) embeddings for uncovering and understanding data characteristics and
biases, including biases for protected variables like sex and data
abnormalities indicative of unwanted protocol variations. We use sagittal
T2-weighted magnetic resonance (MR) images of the neck, chest, and lumbar
region from 11186 German National Cohort (NAKO) participants. We compare DAE
embeddings with existing generative models like StyleGAN and Variational
Autoencoder. Evaluations on a large-scale dataset consisting of sagittal
T2-weighted MR images of three spine regions show that DAE embeddings
effectively separate protected variables such as sex and age. Furthermore, we
used t-SNE visualization to identify unwanted variations in imaging protocols,
revealing differences in head positioning. Our embedding can identify samples
where a sex predictor will have issues learning the correct sex. Our findings
highlight the potential of using advanced embedding techniques like DAEs to
detect data quality issues and biases in medical imaging datasets. Identifying
such hidden relations can enhance the reliability and fairness of deep learning
models in healthcare applications, ultimately improving patient care and
outcomes.},
 author = {Robert Graf and Florian Hunecke and Soeren Pohl and Matan Atad and Hendrik Moeller and Sophie Starck and Thomas Kroencke and Stefanie Bette and Fabian Bamberg and Tobias Pischon and Thoralf Niendorf and Carsten Schmidt and Johannes C. Paetzold and Daniel Rueckert and Jan S Kirschke},
 citations = {},
 comment = {This paper was accepted in the "Workshop on Interpretability of
  Machine Intelligence in Medical Image Computing" (iMIMIC) at MICCAI 2024},
 doi = {10.1007/978-3-031-77610-6_8},
 eprint = {2410.10220v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Detecting Unforeseen Data Properties with Diffusion Autoencoder Embeddings using Spine MRI data},
 url = {http://arxiv.org/abs/2410.10220v1},
 year = {2024}
}

@article{2410.10505v1,
 abstract = {Background: Conventional prediction methods such as logistic regression and
gradient boosting have been widely utilized for disease onset prediction for
their reliability and interpretability. Deep learning methods promise enhanced
prediction performance by extracting complex patterns from clinical data, but
face challenges like data sparsity and high dimensionality.
  Methods: This study compares conventional and deep learning approaches to
predict lung cancer, dementia, and bipolar disorder using observational data
from eleven databases from North America, Europe, and Asia. Models were
developed using logistic regression, gradient boosting, ResNet, and
Transformer, and validated both internally and externally across the data
sources. Discrimination performance was assessed using AUROC, and calibration
was evaluated using Eavg.
  Findings: Across 11 datasets, conventional methods generally outperformed
deep learning methods in terms of discrimination performance, particularly
during external validation, highlighting their better transportability.
Learning curves suggest that deep learning models require substantially larger
datasets to reach the same performance levels as conventional methods.
Calibration performance was also better for conventional methods, with ResNet
showing the poorest calibration.
  Interpretation: Despite the potential of deep learning models to capture
complex patterns in structured observational healthcare data, conventional
models remain highly competitive for disease onset prediction, especially in
scenarios involving smaller datasets and if lengthy training times need to be
avoided. The study underscores the need for future research focused on
optimizing deep learning models to handle the sparsity, high dimensionality,
and heterogeneity inherent in healthcare datasets, and find new strategies to
exploit the full capabilities of deep learning methods.},
 author = {Luis H. John and Chungsoo Kim and Jan A. Kors and Junhyuk Chang and Hannah Morgan-Cooper and Priya Desai and Chao Pang and Peter R. Rijnbeek and Jenna M. Reps and Egill A. Fridgeirsson},
 citations = {0},
 comment = {},
 doi = {10.54254/2755-2721/6/20230910},
 eprint = {2410.10505v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Comparison of deep learning and conventional methods for disease onset prediction},
 url = {http://arxiv.org/abs/2410.10505v1},
 year = {2024}
}

@article{2410.11531v1,
 abstract = {Large Language Models~(LLMs) have demonstrated capabilities across various
applications but face challenges such as hallucination, limited reasoning
abilities, and factual inconsistencies, especially when tackling complex,
domain-specific tasks like question answering~(QA). While Knowledge
Graphs~(KGs) have been shown to help mitigate these issues, research on the
integration of LLMs with background KGs remains limited. In particular, user
accessibility and the flexibility of the underlying KG have not been thoroughly
explored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based
Interaction and Graphical Representation), a platform for knowledge management
through natural language interaction. It integrates knowledge extraction,
integration, and real-time visualization. AGENTiGraph employs a multi-agent
architecture to dynamically interpret user intents, manage tasks, and integrate
new knowledge, ensuring adaptability to evolving user requirements and data
contexts. Our approach demonstrates superior performance in knowledge graph
interactions, particularly for complex domain-specific tasks. Experimental
results on a dataset of 3,500 test cases show AGENTiGraph significantly
outperforms state-of-the-art zero-shot baselines, achieving 95.12\% accuracy in
task classification and 90.45\% success rate in task execution. User studies
corroborate its effectiveness in real-world scenarios. To showcase versatility,
we extended AGENTiGraph to legislation and healthcare domains, constructing
specialized KGs capable of answering complex queries in legal and medical
contexts.},
 author = {Xinjie Zhao and Moritz Blum and Rui Yang and Boming Yang and Luis Márquez Carpintero and Mónica Pina-Navarro and Tony Wang and Xin Li and Huitao Li and Yanran Fu and Rongrong Wang and Juntao Zhang and Irene Li},
 citations = {},
 comment = {30 pages, 7 figures; Submitted to COLING 2025 System Demonstrations
  Track},
 doi = {},
 eprint = {2410.11531v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data},
 url = {http://arxiv.org/abs/2410.11531v1},
 year = {2024}
}

@article{2410.12034v1,
 abstract = {Tabular data, widely used in industries like healthcare, finance, and
transportation, presents unique challenges for deep learning due to its
heterogeneous nature and lack of spatial structure. This survey reviews the
evolution of deep learning models for tabular data, from early fully connected
networks (FCNs) to advanced architectures like TabNet, SAINT, TabTranSELU, and
MambaNet. These models incorporate attention mechanisms, feature embeddings,
and hybrid architectures to address tabular data complexities. TabNet uses
sequential attention for instance-wise feature selection, improving
interpretability, while SAINT combines self-attention and intersample attention
to capture complex interactions across features and data points, both advancing
scalability and reducing computational overhead. Hybrid architectures such as
TabTransformer and FT-Transformer integrate attention mechanisms with
multi-layer perceptrons (MLPs) to handle categorical and numerical data, with
FT-Transformer adapting transformers for tabular datasets. Research continues
to balance performance and efficiency for large datasets. Graph-based models
like GNN4TDL and GANDALF combine neural networks with decision trees or graph
structures, enhancing feature representation and mitigating overfitting in
small datasets through advanced regularization techniques. Diffusion-based
models like the Tabular Denoising Diffusion Probabilistic Model (TabDDPM)
generate synthetic data to address data scarcity, improving model robustness.
Similarly, models like TabPFN and Ptab leverage pre-trained language models,
incorporating transfer learning and self-supervised techniques into tabular
tasks. This survey highlights key advancements and outlines future research
directions on scalability, generalization, and interpretability in diverse
tabular data applications.},
 author = {Shriyank Somvanshi and Subasish Das and Syed Aaqib Javed and Gian Antariksa and Ahmed Hossain},
 citations = {},
 comment = {43 pages, 18 figures, 3 tables},
 doi = {},
 eprint = {2410.12034v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Survey on Deep Tabular Learning},
 url = {http://arxiv.org/abs/2410.12034v1},
 year = {2024}
}

@article{2410.13217v1,
 abstract = {Automatic subphenotyping from electronic health records (EHRs)provides
numerous opportunities to understand diseases with unique subgroups and enhance
personalized medicine for patients. However, existing machine learning
algorithms either focus on specific diseases for better interpretability or
produce coarse-grained phenotype topics without considering nuanced disease
patterns. In this study, we propose a guided topic model, MixEHR-Nest, to infer
sub-phenotype topics from thousands of disease using multi-modal EHR data.
Specifically, MixEHR-Nest detects multiple subtopics from each phenotype topic,
whose prior is guided by the expert-curated phenotype concepts such as
Phenotype Codes (PheCodes) or Clinical Classification Software (CCS) codes. We
evaluated MixEHR-Nest on two EHR datasets: (1) the MIMIC-III dataset consisting
of over 38 thousand patients from intensive care unit (ICU) from Beth Israel
Deaconess Medical Center (BIDMC) in Boston, USA; (2) the healthcare
administrative database PopHR, comprising 1.3 million patients from Montreal,
Canada. Experimental results demonstrate that MixEHR-Nest can identify
subphenotypes with distinct patterns within each phenotype, which are
predictive for disease progression and severity. Consequently, MixEHR-Nest
distinguishes between type 1 and type 2 diabetes by inferring subphenotypes
using CCS codes, which do not differentiate these two subtype concepts.
Additionally, MixEHR-Nest not only improved the prediction accuracy of
short-term mortality of ICU patients and initial insulin treatment in diabetic
patients but also revealed the contributions of subphenotypes. For longitudinal
analysis, MixEHR-Nest identified subphenotypes of distinct age prevalence under
the same phenotypes, such as asthma, leukemia, epilepsy, and depression. The
MixEHR-Nest software is available at GitHub:
https://github.com/li-lab-mcgill/MixEHR-Nest.},
 author = {Ruohan Wang and Zilong Wang and Ziyang Song and David Buckeridge and Yue Li},
 citations = {0},
 comment = {},
 doi = {10.1145/3698587.3701368},
 eprint = {2410.13217v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records through Hierarchical Guided-Topic Modeling},
 url = {http://arxiv.org/abs/2410.13217v1},
 year = {2024}
}

@article{2410.14911v1,
 abstract = {The robustness of Vision-Language Models (VLMs) such as CLIP is critical for
their deployment in safety-critical applications like autonomous driving,
healthcare diagnostics, and security systems, where accurate interpretation of
visual and textual data is essential. However, these models are highly
susceptible to adversarial attacks, which can severely compromise their
performance and reliability in real-world scenarios. Previous methods have
primarily focused on improving robustness through adversarial training and
generating adversarial examples using models like FGSM, AutoAttack, and
DeepFool. However, these approaches often rely on strong assumptions, such as
fixed perturbation norms or predefined attack patterns, and involve high
computational complexity, making them challenging to implement in practical
settings. In this paper, we propose a novel adversarial training framework that
integrates multiple attack strategies and advanced machine learning techniques
to significantly enhance the robustness of VLMs against a broad range of
adversarial attacks. Experiments conducted on real-world datasets, including
CIFAR-10 and CIFAR-100, demonstrate that the proposed method significantly
enhances model robustness. The fine-tuned CLIP model achieved an accuracy of
43.5% on adversarially perturbed images, compared to only 4% for the baseline
model. The neural network model achieved a high accuracy of 98% in these
challenging classification tasks, while the XGBoost model reached a success
rate of 85.26% in prediction tasks.},
 author = {Yuhan Liang and Yijun Li and Yumeng Niu and Qianhe Shen and Hangyu Liu},
 citations = {1},
 comment = {},
 doi = {10.1117/12.3061115},
 eprint = {2410.14911v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Hybrid Defense Strategy for Boosting Adversarial Robustness in Vision-Language Models},
 url = {http://arxiv.org/abs/2410.14911v1},
 year = {2024}
}

@article{2410.15403v2,
 abstract = {We present MMDS, a system capable of recognizing medical images and patient
facial details, and providing professional medical diagnoses. The system
consists of two core components:The first component is the analysis of medical
images and videos. We trained a specialized multimodal medical model capable of
interpreting medical images and accurately analyzing patients' facial emotions
and facial paralysis conditions. The model achieved an accuracy of 72.59% on
the FER2013 facial emotion recognition dataset, with a 91.1% accuracy in
recognizing the "happy" emotion. In facial paralysis recognition, the model
reached an accuracy of 92%, which is 30% higher than that of GPT-4o. Based on
this model, we developed a parser for analyzing facial movement videos of
patients with facial paralysis, achieving precise grading of the paralysis
severity. In tests on 30 videos of facial paralysis patients, the system
demonstrated a grading accuracy of 83.3%.The second component is the generation
of professional medical responses. We employed a large language model,
integrated with a medical knowledge base, to generate professional diagnoses
based on the analysis of medical images or videos. The core innovation lies in
our development of a department-specific knowledge base routing management
mechanism, in which the large language model categorizes data by medical
departments and, during the retrieval process, determines the appropriate
knowledge base to query. This significantly improves retrieval accuracy in the
RAG (retrieval-augmented generation) process.},
 author = {Yi Ren and HanZhi Zhang and Weibin Li and Jun Fu and Diandong Liu and Tianyi Zhang and Jie He and Licheng Jiao},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2410.15403v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis and Knowledge-based Departmental Consultation},
 url = {http://arxiv.org/abs/2410.15403v2},
 year = {2024}
}

@article{2410.16537v1,
 abstract = {The impressive performance of deep learning models, particularly
Convolutional Neural Networks (CNNs), is often hindered by their lack of
interpretability, rendering them "black boxes." This opacity raises concerns in
critical areas like healthcare, finance, and autonomous systems, where trust
and accountability are crucial. This paper introduces the QIXAI Framework
(Quantum-Inspired Explainable AI), a novel approach for enhancing neural
network interpretability through quantum-inspired techniques. By utilizing
principles from quantum mechanics, such as Hilbert spaces, superposition,
entanglement, and eigenvalue decomposition, the QIXAI framework reveals how
different layers of neural networks process and combine features to make
decisions.
  We critically assess model-agnostic methods like SHAP and LIME, as well as
techniques like Layer-wise Relevance Propagation (LRP), highlighting their
limitations in providing a comprehensive view of neural network operations. The
QIXAI framework overcomes these limitations by offering deeper insights into
feature importance, inter-layer dependencies, and information propagation. A
CNN for malaria parasite detection is used as a case study to demonstrate how
quantum-inspired methods like Singular Value Decomposition (SVD), Principal
Component Analysis (PCA), and Mutual Information (MI) provide interpretable
explanations of model behavior. Additionally, we explore the extension of QIXAI
to other architectures, including Recurrent Neural Networks (RNNs), Long
Short-Term Memory (LSTM) networks, Transformers, and Natural Language
Processing (NLP) models, and its application to generative models and
time-series analysis. The framework applies to both quantum and classical
systems, demonstrating its potential to improve interpretability and
transparency across a range of models, advancing the broader goal of developing
trustworthy AI systems.},
 author = {John M. Willis},
 citations = {0},
 comment = {18 pages, 3 figures},
 doi = {},
 eprint = {2410.16537v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {QIXAI: A Quantum-Inspired Framework for Enhancing Classical and Quantum Model Transparency and Understanding},
 url = {http://arxiv.org/abs/2410.16537v1},
 year = {2024}
}

@article{2410.17459v1,
 abstract = {As AI systems increasingly integrate into critical societal sectors, the
demand for robust privacy-preserving methods has escalated. This paper
introduces Data Obfuscation through Latent Space Projection (LSP), a novel
technique aimed at enhancing AI governance and ensuring Responsible AI
compliance. LSP uses machine learning to project sensitive data into a latent
space, effectively obfuscating it while preserving essential features for model
training and inference. Unlike traditional privacy methods like differential
privacy or homomorphic encryption, LSP transforms data into an abstract,
lower-dimensional form, achieving a delicate balance between data utility and
privacy. Leveraging autoencoders and adversarial training, LSP separates
sensitive from non-sensitive information, allowing for precise control over
privacy-utility trade-offs. We validate LSP's effectiveness through experiments
on benchmark datasets and two real-world case studies: healthcare cancer
diagnosis and financial fraud analysis. Our results show LSP achieves high
performance (98.7% accuracy in image classification) while providing strong
privacy (97.3% protection against sensitive attribute inference), outperforming
traditional anonymization and privacy-preserving methods. The paper also
examines LSP's alignment with global AI governance frameworks, such as GDPR,
CCPA, and HIPAA, highlighting its contribution to fairness, transparency, and
accountability. By embedding privacy within the machine learning pipeline, LSP
offers a promising approach to developing AI systems that respect privacy while
delivering valuable insights. We conclude by discussing future research
directions, including theoretical privacy guarantees, integration with
federated learning, and enhancing latent space interpretability, positioning
LSP as a critical tool for ethical AI advancement.},
 author = {Mahesh Vaijainthymala Krishnamoorthy},
 citations = {3},
 comment = {19 pages, 6 figures, submitted to Conference ICADCML2025},
 doi = {10.2196/70100},
 eprint = {2410.17459v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Data Obfuscation through Latent Space Projection (LSP) for Privacy-Preserving AI Governance: Case Studies in Medical Diagnosis and Finance Fraud Detection},
 url = {http://arxiv.org/abs/2410.17459v1},
 year = {2024}
}

@article{2410.17781v1,
 abstract = {As AI becomes fundamental in sectors like healthcare, explainable AI (XAI)
tools are essential for trust and transparency. However, traditional user
studies used to evaluate these tools are often costly, time consuming, and
difficult to scale. In this paper, we explore the use of Large Language Models
(LLMs) to replicate human participants to help streamline XAI evaluation. We
reproduce a user study comparing counterfactual and causal explanations,
replicating human participants with seven LLMs under various settings. Our
results show that (i) LLMs can replicate most conclusions from the original
study, (ii) different LLMs yield varying levels of alignment in the results,
and (iii) experimental factors such as LLM memory and output variability affect
alignment with human responses. These initial findings suggest that LLMs could
provide a scalable and cost-effective way to simplify qualitative XAI
evaluation.},
 author = {Francesco Bombassei De Bona and Gabriele Dominici and Tim Miller and Marc Langheinrich and Martin Gjoreski},
 citations = {6},
 comment = {},
 doi = {},
 eprint = {2410.17781v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Evaluating Explanations Through LLMs: Beyond Traditional User Studies},
 url = {http://arxiv.org/abs/2410.17781v1},
 year = {2024}
}

@article{2410.18725v2,
 abstract = {Artificial Intelligence is rapidly advancing and radically impacting everyday
life, driven by the increasing availability of computing power. Despite this
trend, the adoption of AI in real-world healthcare is still limited. One of the
main reasons is the trustworthiness of AI models and the potential hesitation
of domain experts with model predictions. Explainable Artificial Intelligence
(XAI) techniques aim to address these issues. However, explainability can mean
different things to people with different backgrounds, expertise, and goals. To
address the target audience with diverse needs, we develop storytelling XAI. In
this research, we have developed an approach that combines multi-task
distillation with interpretability techniques to enable audience-centric
explainability. Using multi-task distillation allows the model to exploit the
relationships between tasks, potentially improving interpretability as each
task supports the other leading to an enhanced interpretability from the
perspective of a domain expert. The distillation process allows us to extend
this research to large deep models that are highly complex. We focus on both
model-agnostic and model-specific methods of interpretability, supported by
textual justification of the results in healthcare through our use case. Our
methods increase the trust of both the domain experts and the machine learning
experts to enable a responsible AI.},
 author = {Akshat Dubey and Zewen Yang and Georges Hattab},
 citations = {4},
 comment = {Pre-print of the accepted manuscript in EXPLIMED - First Workshop on
  Explainable Artificial Intelligence for the Medical Domain, European
  Conference on Artificial Intelligence (ECAI) - 2024, Santiago de Compostela,
  Spain},
 doi = {},
 eprint = {2410.18725v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AI Readiness in Healthcare through Storytelling XAI},
 url = {http://arxiv.org/abs/2410.18725v2},
 year = {2024}
}

@article{2410.19081v1,
 abstract = {Survival analysis is an important research topic with applications in
healthcare, business, and manufacturing. One essential tool in this area is the
Cox proportional hazards (CPH) model, which is widely used for its
interpretability, flexibility, and predictive performance. However, for modern
data science challenges such as high dimensionality (both $n$ and $p$) and high
feature correlations, current algorithms to train the CPH model have drawbacks,
preventing us from using the CPH model at its full potential. The root cause is
that the current algorithms, based on the Newton method, have trouble
converging due to vanishing second order derivatives when outside the local
region of the minimizer. To circumvent this problem, we propose new
optimization methods by constructing and minimizing surrogate functions that
exploit hidden mathematical structures of the CPH model. Our new methods are
easy to implement and ensure monotonic loss decrease and global convergence.
Empirically, we verify the computational efficiency of our methods. As a direct
application, we show how our optimization methods can be used to solve the
cardinality-constrained CPH problem, producing very sparse high-quality models
that were not previously practical to construct. We list several extensions
that our breakthrough enables, including optimization opportunities,
theoretical questions on CPH's mathematical structure, as well as other
CPH-related applications.},
 author = {Jiachang Liu and Rui Zhang and Cynthia Rudin},
 citations = {0},
 comment = {Accepted into NeurIPS 2024},
 doi = {},
 eprint = {2410.19081v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {FastSurvival: Hidden Computational Blessings in Training Cox Proportional Hazards Models},
 url = {http://arxiv.org/abs/2410.19081v1},
 year = {2024}
}

@article{2410.19781v1,
 abstract = {Early detection of atrial fibrillation (AFib) is challenging due to its
asymptomatic and paroxysmal nature. However, advances in deep learning
algorithms and the vast collection of electrocardiogram (ECG) data from devices
such as the Internet of Things (IoT) hold great potential for the development
of an effective solution. This study assesses the feasibility of training a
neural network on a Federated Learning (FL) platform to detect AFib using raw
ECG data. The performance of an advanced neural network is evaluated in
centralized, local, and federated settings. The effects of different
aggregation methods on model performance are investigated, and various
normalization strategies are explored to address issues related to neural
network federation. The results demonstrate that federated learning can
significantly improve the accuracy of detection over local training. The best
performing federated model achieved an F1 score of 77\%, improving performance
by 15\% compared to the average performance of individually trained clients.
This study emphasizes the promise of FL in medical diagnostics, offering a
privacy-preserving and interpretable solution for large-scale healthcare
applications.},
 author = {Diogo Reis Santos and Andrea Protani and Lorenzo Giusti and Albert Sund Aillet and Pierpaolo Brutti and Luigi Serio},
 citations = {2},
 comment = {},
 doi = {10.1109/healthcom60970.2024.10880809},
 eprint = {2410.19781v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Feasibility Analysis of Federated Neural Networks for Explainable Detection of Atrial Fibrillation},
 url = {http://arxiv.org/abs/2410.19781v1},
 year = {2024}
}

@article{2410.20873v1,
 abstract = {The integration of artificial intelligence into business processes has
significantly enhanced decision-making capabilities across various industries
such as finance, healthcare, and retail. However, explaining the decisions made
by these AI systems poses a significant challenge due to the opaque nature of
recent deep learning models, which typically function as black boxes. To
address this opacity, a multitude of explainability techniques have emerged.
However, in practical business applications, the challenge lies in selecting an
appropriate explainability method that balances comprehensibility with
accuracy. This paper addresses the practical need of understanding differences
in the output of explainability techniques by proposing a novel method for the
assessment of the agreement of different explainability techniques. Based on
our proposed methods, we provide a comprehensive comparative analysis of six
leading explainability techniques to help guiding the selection of such
techniques in practice. Our proposed general-purpose method is evaluated on top
of one of the most popular deep learning architectures, the Vision Transformer
model, which is frequently employed in business applications. Notably, we
propose a novel metric to measure the agreement of explainability techniques
that can be interpreted visually. By providing a practical framework for
understanding the agreement of diverse explainability techniques, our research
aims to facilitate the broader integration of interpretable AI systems in
business applications.},
 author = {Arne Grobrugge and Nidhi Mishra and Johannes Jakubik and Gerhard Satzger},
 citations = {3},
 comment = {10 pages, 5 figures},
 doi = {10.1109/cbi62504.2024.00018},
 eprint = {2410.20873v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainability in AI Based Applications: A Framework for Comparing Different Techniques},
 url = {http://arxiv.org/abs/2410.20873v1},
 year = {2024}
}

@article{2410.22223v1,
 abstract = {Medical image segmentation is pivotal in healthcare, enhancing diagnostic
accuracy, informing treatment strategies, and tracking disease progression.
This process allows clinicians to extract critical information from visual
data, enabling personalized patient care. However, developing neural networks
for segmentation remains challenging, especially when preserving image
resolution, which is essential in detecting subtle details that influence
diagnoses. Moreover, the lack of transparency in these deep learning models has
slowed their adoption in clinical practice. Efforts in model interpretability
are increasingly focused on making these models' decision-making processes more
transparent. In this paper, we introduce MAPUNetR, a novel architecture that
synergizes the strengths of transformer models with the proven U-Net framework
for medical image segmentation. Our model addresses the resolution preservation
challenge and incorporates attention maps highlighting segmented regions,
increasing accuracy and interpretability. Evaluated on the BraTS 2020 dataset,
MAPUNetR achieved a dice score of 0.88 and a dice coefficient of 0.92 on the
ISIC 2018 dataset. Our experiments show that the model maintains stable
performance and potential as a powerful tool for medical image segmentation in
clinical practice.},
 author = {Ovais Iqbal Shah and Danish Raza Rizvi and Aqib Nazir Mir},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2410.22223v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MAPUNetR: A Hybrid Vision Transformer and U-Net Architecture for Efficient and Interpretable Medical Image Segmentation},
 url = {http://arxiv.org/abs/2410.22223v1},
 year = {2024}
}

@article{2410.23425v1,
 abstract = {Understanding the genetic basis of complex traits is a longstanding challenge
in the field of genomics. Genome-wide association studies (GWAS) have
identified thousands of variant-trait associations, but most of these variants
are located in non-coding regions, making the link to biological function
elusive. While traditional approaches, such as transcriptome-wide association
studies (TWAS), have advanced our understanding by linking genetic variants to
gene expression, they often overlook gene-gene interactions. Here, we review
current approaches to integrate different molecular data, leveraging machine
learning methods to identify gene modules based on co-expression and functional
relationships. These integrative approaches, like PhenoPLIER, combine TWAS and
drug-induced transcriptional profiles to effectively capture biologically
meaningful gene networks. This integration provides a context-specific
understanding of disease processes while highlighting both core and peripheral
genes. These insights pave the way for novel therapeutic targets and enhance
the interpretability of genetic studies in personalized medicine.},
 author = {Marc Subirana-Granés and Jill Hoffman and Haoyu Zhang and Christina Akirtava and Sutanu Nandi and Kevin Fotso and Milton Pividori},
 citations = {1},
 comment = {26 pages, 4 figures GitHub:
  https://github.com/pivlab/annual_review_of_biomedical_data_science HTML:
  https://pivlab.github.io/annual_review_of_biomedical_data_science/},
 doi = {10.1146/annurev-biodatasci-103123-095355},
 eprint = {2410.23425v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Genetic studies through the lens of gene networks},
 url = {http://arxiv.org/abs/2410.23425v1},
 year = {2024}
}

@article{2411.00054v1,
 abstract = {Single-cell RNA-seq (scRNA-seq) technology is a powerful tool for unraveling
the complexity of biological systems. One of essential and fundamental tasks in
scRNA-seq data analysis is Cell Type Annotation (CTA). In spite of tremendous
efforts in developing machine learning methods for this problem, several
challenges remains. They include identifying Out-of-Domain (OOD) cell types,
quantifying the uncertainty of unseen cell type annotations, and determining
interpretable cell type-specific gene drivers for an OOD case. OOD cell types
are often associated with therapeutic responses and disease origins, making
them critical for precision medicine and early disease diagnosis. Additionally,
scRNA-seq data contains tens thousands of gene expressions. Pinpointing gene
drivers underlying CTA can provide deep insight into gene regulatory mechanisms
and serve as disease biomarkers. In this study, we develop a new method, eDOC,
to address aforementioned challenges. eDOC leverages a transformer architecture
with evidential learning to annotate In-Domain (IND) and OOD cell types as well
as to highlight genes that contribute both IND cells and OOD cells in a single
cell resolution. Rigorous experiments demonstrate that eDOC significantly
improves the efficiency and effectiveness of OOD cell type and gene driver
identification compared to other state-of-the-art methods. Our findings suggest
that eDOC may provide new insights into single-cell biology.},
 author = {Chaochen Wu and Meiyun Zuo and Lei Xie},
 citations = {},
 comment = {under review},
 doi = {},
 eprint = {2411.00054v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {eDOC: Explainable Decoding Out-of-domain Cell Types with Evidential Learning},
 url = {http://arxiv.org/abs/2411.00054v1},
 year = {2024}
}

@article{2411.00069v1,
 abstract = {The Artificial intelligence in critical sectors-healthcare, finance, and
public safety-has made system integrity paramount for maintaining societal
trust. Current verification methods for AI systems lack comprehensive lifecycle
assurance, creating significant vulnerabilities in deployment of both powerful
and trustworthy AI. This research introduces Meta-Sealing, a cryptographic
framework that fundamentally changes integrity verification in AI systems
throughout their operational lifetime. Meta-Sealing surpasses traditional
integrity protocols through its implementation of cryptographic seal chains,
establishing verifiable, immutable records for all system decisions and
transformations. The framework combines advanced cryptography with distributed
verification, delivering tamper-evident guarantees that achieve both
mathematical rigor and computational efficiency. Our implementation addresses
urgent regulatory requirements for AI system transparency and auditability. The
framework integrates with current AI governance standards, specifically the
EU's AI Act and FDA's healthcare AI guidelines, enabling organizations to
maintain operational efficiency while meeting compliance requirements. Testing
on financial institution data demonstrated Meta-Sealing's capability to reduce
audit timeframes by 62% while enhancing stakeholder confidence by 47%. Results
can establish a new benchmark for integrity assurance in enterprise AI
deployments. This research presents Meta-Sealing not merely as a technical
solution, but as a foundational framework ensuring AI system integrity aligns
with human values and regulatory requirements. As AI continues to influence
critical decisions, provides the necessary bridge between technological
advancement and verifiable trust. Meta-Sealing serves as a guardian of trust,
ensuring that the AI systems we depend on are as reliable and transparent as
they are powerful.},
 author = {Mahesh Vaijainthymala Krishnamoorthy},
 citations = {},
 comment = {24 pages, 3 figures and 10 Code blocks, to be presented in the
  conference},
 doi = {},
 eprint = {2411.00069v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Meta-Sealing: A Revolutionizing Integrity Assurance Protocol for Transparent, Tamper-Proof, and Trustworthy AI System},
 url = {http://arxiv.org/abs/2411.00069v1},
 year = {2024}
}

@article{2411.00173v2,
 abstract = {Medical coding, the translation of unstructured clinical text into
standardized medical codes, is a crucial but time-consuming healthcare
practice. Though large language models (LLM) could automate the coding process
and improve the efficiency of such tasks, interpretability remains paramount
for maintaining patient trust. Current efforts in interpretability of medical
coding applications rely heavily on label attention mechanisms, which often
leads to the highlighting of extraneous tokens irrelevant to the ICD code. To
facilitate accurate interpretability in medical language models, this paper
leverages dictionary learning that can efficiently extract sparsely activated
representations from dense language model embeddings in superposition. Compared
with common label attention mechanisms, our model goes beyond token-level
representations by building an interpretable dictionary which enhances the
mechanistic-based explanations for each ICD code prediction, even when the
highlighted tokens are medically irrelevant. We show that dictionary features
can steer model behavior, elucidate the hidden meanings of upwards of 90% of
medically irrelevant tokens, and are human interpretable.},
 author = {John Wu and David Wu and Jimeng Sun},
 citations = {1},
 comment = {https://aclanthology.org/2024.emnlp-main.500/},
 doi = {10.18653/v1/2024.emnlp-main.500},
 eprint = {2411.00173v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning},
 url = {http://arxiv.org/abs/2411.00173v2},
 year = {2024}
}

@article{2411.00265v3,
 abstract = {Trustworthiness in neural networks is crucial for their deployment in
critical applications, where reliability, confidence, and uncertainty play
pivotal roles in decision-making. Traditional performance metrics such as
accuracy and precision fail to capture these aspects, particularly in cases
where models exhibit overconfidence. To address these limitations, this paper
introduces a novel framework for quantifying the trustworthiness of neural
networks by incorporating subjective logic into the evaluation of Expected
Calibration Error (ECE). This method provides a comprehensive measure of trust,
disbelief, and uncertainty by clustering predicted probabilities and fusing
opinions using appropriate fusion operators. We demonstrate the effectiveness
of this approach through experiments on MNIST and CIFAR-10 datasets, where
post-calibration results indicate improved trustworthiness. The proposed
framework offers a more interpretable and nuanced assessment of AI models, with
potential applications in sensitive domains such as healthcare and autonomous
systems.},
 author = {Koffi Ismael Ouattara and Ioannis Krontiris and Theo Dimitrakos and Frank Kargl},
 citations = {2},
 comment = {This is the preprint of the paper accepted to Fusion 2025 (28th
  International Conference on Information Fusion, Rio de Janeiro, Brazil, July
  7-10, 2025). The published version is available at
  https://doi.org/10.23919/FUSION65864.2025.11124121},
 doi = {10.23919/fusion65864.2025.11124121},
 eprint = {2411.00265v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Quantifying Calibration Error in Neural Networks Through Evidence-Based Theory},
 url = {http://arxiv.org/abs/2411.00265v3},
 year = {2024}
}

@article{2411.00916v2,
 abstract = {Osteoporosis is a common condition that increases fracture risk, especially
in older adults. Early diagnosis is vital for preventing fractures, reducing
treatment costs, and preserving mobility. However, healthcare providers face
challenges like limited labeled data and difficulties in processing medical
images. This study presents a novel multi-modal learning framework that
integrates clinical and imaging data to improve diagnostic accuracy and model
interpretability. The model utilizes three pre-trained networks-VGG19,
InceptionV3, and ResNet50-to extract deep features from X-ray images. These
features are transformed using PCA to reduce dimensionality and focus on the
most relevant components. A clustering-based selection process identifies the
most representative components, which are then combined with preprocessed
clinical data and processed through a fully connected network (FCN) for final
classification. A feature importance plot highlights key variables, showing
that Medical History, BMI, and Height were the main contributors, emphasizing
the significance of patient-specific data. While imaging features were
valuable, they had lower importance, indicating that clinical data are crucial
for accurate predictions. This framework promotes precise and interpretable
predictions, enhancing transparency and building trust in AI-driven diagnoses
for clinical integration.},
 author = {Mehdi Hosseini Chagahi and Saeed Mohammadi Dashtaki and Niloufar Delfan and Nadia Mohammadi and Alireza Samari and Behzad Moshiri and Md. Jalil Piran and Oliver Faust},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2411.00916v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Enhancing Osteoporosis Detection: An Explainable Multi-Modal Learning Framework with Feature Fusion and Variable Clustering},
 url = {http://arxiv.org/abs/2411.00916v2},
 year = {2024}
}

@article{2411.01070v1,
 abstract = {In this paper, we present XST-GCNN (eXplainable Spatio-Temporal Graph
Convolutional Neural Network), a novel architecture for processing
heterogeneous and irregular Multivariate Time Series (MTS) data. Our approach
captures temporal and feature dependencies within a unified spatio-temporal
pipeline by leveraging a GCNN that uses a spatio-temporal graph aimed at
optimizing predictive accuracy and interoperability. For graph estimation, we
introduce techniques, including one based on the (heterogeneous) Gower
distance. Once estimated, we propose two methods for graph construction: one
based on the Cartesian product, treating temporal instants homogeneously, and
another spatio-temporal approach with distinct graphs per time step. We also
propose two GCNN architectures: a standard GCNN with a normalized adjacency
matrix and a higher-order polynomial GCNN. In addition to accuracy, we
emphasize explainability by designing an inherently interpretable model and
performing a thorough interpretability analysis, identifying key feature-time
combinations that drive predictions. We evaluate XST-GCNN using real-world
Electronic Health Record data from University Hospital of Fuenlabrada to
predict Multidrug Resistance (MDR) in ICU patients, a critical healthcare
challenge linked to high mortality and complex treatments. Our architecture
outperforms traditional models, achieving a mean ROC-AUC score of 81.03 +-
2.43. Furthermore, the interpretability analysis provides actionable insights
into clinical factors driving MDR predictions, enhancing model transparency.
This work sets a benchmark for tackling complex inference tasks with
heterogeneous MTS, offering a versatile, interpretable solution for real-world
applications.},
 author = {Óscar Escudero-Arnanz and Cristina Soguero-Ruiz and Antonio G. Marques},
 citations = {},
 comment = {},
 doi = {10.1109/tsipn.2025.3613951},
 eprint = {2411.01070v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable Spatio-Temporal GCNNs for Irregular Multivariate Time Series: Architecture and Application to ICU Patient Data},
 url = {http://arxiv.org/abs/2411.01070v1},
 year = {2024}
}

@article{2411.02174v1,
 abstract = {We investigate the use of sequence analysis for behavior modeling,
emphasizing that sequential context often outweighs the value of aggregate
features in understanding human behavior. We discuss framing common problems in
fields like healthcare, finance, and e-commerce as sequence modeling tasks, and
address challenges related to constructing coherent sequences from fragmented
data and disentangling complex behavior patterns. We present a framework for
sequence modeling using Ensembles of Hidden Markov Models, which are
lightweight, interpretable, and efficient. Our ensemble-based scoring method
enables robust comparison across sequences of different lengths and enhances
performance in scenarios with imbalanced or scarce data. The framework scales
in real-world scenarios, is compatible with downstream feature-based modeling,
and is applicable in both supervised and unsupervised learning settings. We
demonstrate the effectiveness of our method with results on a longitudinal
human behavior dataset.},
 author = {Maxime Kawawa-Beaudan and Srijan Sood and Soham Palande and Ganapathy Mani and Tucker Balch and Manuela Veloso},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2411.02174v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Behavioral Sequence Modeling with Ensemble Learning},
 url = {http://arxiv.org/abs/2411.02174v1},
 year = {2024}
}

@article{2411.02464v1,
 abstract = {This research proposes a novel drift detection methodology for machine
learning (ML) models based on the concept of ''deformation'' in the vector
space representation of data. Recognizing that new data can act as forces
stretching, compressing, or twisting the geometric relationships learned by a
model, we explore various mathematical frameworks to quantify this deformation.
We investigate measures such as eigenvalue analysis of covariance matrices to
capture global shape changes, local density estimation using kernel density
estimation (KDE), and Kullback-Leibler divergence to identify subtle shifts in
data concentration. Additionally, we draw inspiration from continuum mechanics
by proposing a ''strain tensor'' analogy to capture multi-faceted deformations
across different data types. This requires careful estimation of the
displacement field, and we delve into strategies ranging from density-based
approaches to manifold learning and neural network methods. By continuously
monitoring these deformation metrics and correlating them with model
performance, we aim to provide a sensitive, interpretable, and adaptable drift
detection system capable of distinguishing benign data evolution from true
drift, enabling timely interventions and ensuring the reliability of machine
learning systems in dynamic environments. Addressing the computational
challenges of this methodology, we discuss mitigation strategies like
dimensionality reduction, approximate algorithms, and parallelization for
real-time and large-scale applications. The method's effectiveness is
demonstrated through experiments on real-world text data, focusing on detecting
context shifts in Generative AI. Our results, supported by publicly available
code, highlight the benefits of this deformation-based approach in capturing
subtle drifts that traditional statistical methods often miss. Furthermore, we
present a detailed application example within the healthcare domain, showcasing
the methodology's potential in diverse fields. Future work will focus on
further improving computational efficiency and exploring additional
applications across different ML domains.},
 author = {Giancarlo Cobino and Simone Farci},
 citations = {},
 comment = {},
 doi = {10.1093/oso/9780199332182.003.0002},
 eprint = {2411.02464v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {You are out of context!},
 url = {http://arxiv.org/abs/2411.02464v1},
 year = {2024}
}

@article{2411.02523v1,
 abstract = {Differential diagnosis is crucial for medicine as it helps healthcare
providers systematically distinguish between conditions that share similar
symptoms. This study assesses the impact of lab test results on differential
diagnoses (DDx) made by large language models (LLMs). Clinical vignettes from
50 case reports from PubMed Central were created incorporating patient
demographics, symptoms, and lab results. Five LLMs GPT-4, GPT-3.5, Llama-2-70b,
Claude-2, and Mixtral-8x7B were tested to generate Top 10, Top 5, and Top 1 DDx
with and without lab data. A comprehensive evaluation involving GPT-4, a
knowledge graph, and clinicians was conducted. GPT-4 performed best, achieving
55% accuracy for Top 1 diagnoses and 60% for Top 10 with lab data, with lenient
accuracy up to 80%. Lab results significantly improved accuracy, with GPT-4 and
Mixtral excelling, though exact match rates were low. Lab tests, including
liver function, metabolic/toxicology panels, and serology/immune tests, were
generally interpreted correctly by LLMs for differential diagnosis.},
 author = {Balu Bhasuran and Qiao Jin and Yuzhang Xie and Carl Yang and Karim Hanna and Jennifer Costa and Cindy Shavor and Zhiyong Lu and Zhe He},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2411.02523v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Evaluating the Impact of Lab Test Results on Large Language Models Generated Differential Diagnoses from Clinical Case Vignettes},
 url = {http://arxiv.org/abs/2411.02523v1},
 year = {2024}
}

@article{2411.03224v1,
 abstract = {The healthcare sector has experienced a rapid accumulation of digital data
recently, especially in the form of electronic health records (EHRs). EHRs
constitute a precious resource that IS researchers could utilize for clinical
applications (e.g., morbidity prediction). Deep learning seems like the obvious
choice to exploit this surfeit of data. However, numerous studies have shown
that deep learning does not enjoy the same kind of success on EHR data as it
has in other domains; simple models like logistic regression are frequently as
good as sophisticated deep learning ones. Inspired by this observation, we
develop a novel model called rational logistic regression (RLR) that has
standard logistic regression (LR) as its special case (and thus inherits LR's
inductive bias that aligns with EHR data). RLR has rational series as its
theoretical underpinnings, works on longitudinal time-series data, and learns
interpretable patterns. Empirical comparisons on real-world clinical tasks
demonstrate RLR's efficacy.},
 author = {Thiti Suttaket and L Vivek Harsha Vardhan and Stanley Kok},
 citations = {},
 comment = {ICIS 2021 Proceedings ( see
  https://aisel.aisnet.org/icis2021/is_health/is_health/18 )},
 doi = {},
 eprint = {2411.03224v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable Predictive Models for Healthcare via Rational Logistic Regression},
 url = {http://arxiv.org/abs/2411.03224v1},
 year = {2024}
}

@article{2411.04008v1,
 abstract = {In mission-critical domains such as law enforcement and medical diagnosis,
the ability to explain and interpret the outputs of deep learning models is
crucial for ensuring user trust and supporting informed decision-making.
Despite advancements in explainability, existing methods often fall short in
providing explanations that mirror the depth and clarity of those given by
human experts. Such expert-level explanations are essential for the dependable
application of deep learning models in law enforcement and medical contexts.
Additionally, we recognize that most explanations in real-world scenarios are
communicated primarily through natural language. Addressing these needs, we
propose a novel approach that utilizes characteristic descriptors to explain
model decisions by identifying their presence in images, thereby generating
expert-like explanations. Our method incorporates a concept bottleneck layer
within the model architecture, which calculates the similarity between image
and descriptor encodings to deliver inherent and faithful explanations. Through
experiments in face recognition and chest X-ray diagnosis, we demonstrate that
our approach offers a significant contrast over existing techniques, which are
often limited to the use of saliency maps. We believe our approach represents a
significant step toward making deep learning systems more accountable,
transparent, and trustworthy in the critical domains of face recognition and
medical diagnosis.},
 author = {Bharat Chandra Yalavarthi and Nalini Ratha},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2411.04008v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Aligning Characteristic Descriptors with Images for Human-Expert-like Explainability},
 url = {http://arxiv.org/abs/2411.04008v1},
 year = {2024}
}

@article{2411.04855v3,
 abstract = {Explainable AI (XAI) holds the promise of advancing the implementation and
adoption of AI-based tools in practice, especially in high-stakes environments
like healthcare. However, most of the current research lacks input from end
users, and therefore their practical value is limited. To address this, we
conducted semi-structured interviews with clinicians to discuss their thoughts,
hopes, and concerns. Clinicians from our sample generally think positively
about developing AI-based tools for clinical practice, but they have concerns
about how these will fit into their workflow and how it will impact
clinician-patient relations. We further identify training of clinicians on AI
as a crucial factor for the success of AI in healthcare and highlight aspects
clinicians are looking for in (X)AI-based tools. In contrast to other studies,
we take on a holistic and exploratory perspective to identify general
requirements for (X)AI products for healthcare before moving on to testing
specific tools.},
 author = {T. E. Röber and R. Goedhart and S. İ. Birbil},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2411.04855v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Clinicians' Voice: Fundamental Considerations for XAI in Healthcare},
 url = {http://arxiv.org/abs/2411.04855v3},
 year = {2024}
}

@article{2411.05196v2,
 abstract = {In democratic societies, electoral systems play a crucial role in translating
public preferences into political representation. Among these, the D'Hondt
method is widely used to ensure proportional representation, balancing fair
representation with governmental stability. Recently, there has been a growing
interest in applying similar principles of proportional representation to
enhance interpretability in machine learning, specifically in Explainable AI
(XAI). This study investigates the integration of D'Hondt-based voting
principles in the DhondtXAI method, which leverages resource allocation
concepts to interpret feature importance within AI models. Through a comparison
of SHAP (Shapley Additive Explanations) and DhondtXAI, we evaluate their
effectiveness in feature attribution within CatBoost and XGBoost models for
breast cancer and diabetes prediction, respectively. The DhondtXAI approach
allows for alliance formation and thresholding to enhance interpretability,
representing feature importance as seats in a parliamentary view. Statistical
correlation analyses between SHAP values and DhondtXAI allocations support the
consistency of interpretations, demonstrating DhondtXAI's potential as a
complementary tool for understanding feature importance in AI models. The
results highlight that integrating electoral principles, such as proportional
representation and alliances, into AI explainability can improve user
understanding, especially in high-stakes fields like healthcare.},
 author = {Turker Berk Donmez},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2411.05196v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable AI through a Democratic Lens: DhondtXAI for Proportional Feature Importance Using the D'Hondt Method},
 url = {http://arxiv.org/abs/2411.05196v2},
 year = {2024}
}

@article{2411.05825v1,
 abstract = {Current brain surface-based prediction models often overlook the variability
of regional attributes at the cortical feature level. While graph neural
networks (GNNs) excel at capturing regional differences, they encounter
challenges when dealing with complex, high-density graph structures. In this
work, we consider the cortical surface mesh as a sparse graph and propose an
interpretable prediction model-Surface Graph Neural Network (SurfGNN). SurfGNN
employs topology-sampling learning (TSL) and region-specific learning (RSL)
structures to manage individual cortical features at both lower and higher
scales of the surface mesh, effectively tackling the challenges posed by the
overly abundant mesh nodes and addressing the issue of heterogeneity in
cortical regions. Building on this, a novel score-weighted fusion (SWF) method
is implemented to merge nodal representations associated with each cortical
feature for prediction. We apply our model to a neonatal brain age prediction
task using a dataset of harmonized MR images from 481 subjects (503 scans).
SurfGNN outperforms all existing state-of-the-art methods, demonstrating an
improvement of at least 9.0% and achieving a mean absolute error (MAE) of
0.827+0.056 in postmenstrual weeks. Furthermore, it generates feature-level
activation maps, indicating its capability to identify robust regional
variations in different morphometric contributions for prediction.},
 author = {Zhuoshuo Li and Jiong Zhang and Youbing Zeng and Jiaying Lin and Dan Zhang and Jianjia Zhang and Duan Xu and Hosung Kim and Bingguang Liu and Mengting Liu},
 citations = {},
 comment = {15 pages, 6 figures},
 doi = {10.1016/j.media.2025.103793},
 eprint = {2411.05825v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SurfGNN: A robust surface-based prediction model with interpretability for coactivation maps of spatial and cortical features},
 url = {http://arxiv.org/abs/2411.05825v1},
 year = {2024}
}

@article{2411.05923v1,
 abstract = {Survival analysis is a classic problem in statistics with important
applications in healthcare. Most machine learning models for survival analysis
are black-box models, limiting their use in healthcare settings where
interpretability is paramount. More recently, glass-box machine learning models
have been introduced for survival analysis, with both strong predictive
performance and interpretability. Still, several gaps remain, as no prior
glass-box survival model can produce calibrated shape functions with enough
flexibility to capture the complex patterns often found in real data. To fill
this gap, we introduce a new glass-box machine learning model for survival
analysis called DNAMite. DNAMite uses feature discretization and kernel
smoothing in its embedding module, making it possible to learn shape functions
with a flexible balance of smoothness and jaggedness. Further, DNAMite produces
calibrated shape functions that can be directly interpreted as contributions to
the cumulative incidence function. Our experiments show that DNAMite generates
shape functions closer to true shape functions on synthetic data, while making
predictions with comparable predictive performance and better calibration than
previous glass-box and black-box models.},
 author = {Mike Van Ness and Billy Block and Madeleine Udell},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2411.05923v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DNAMite: Interpretable Calibrated Survival Analysis with Discretized Additive Models},
 url = {http://arxiv.org/abs/2411.05923v1},
 year = {2024}
}

@article{2411.06338v1,
 abstract = {Causal inference and model interpretability are gaining increasing attention,
particularly in the biomedical domain. Despite recent advance, decorrelating
features in nonlinear environments with human-interpretable representations
remains underexplored. In this study, we introduce a novel method called causal
rule generation with target trial emulation framework (CRTRE), which applies
randomize trial design principles to estimate the causal effect of association
rules. We then incorporate such association rules for the downstream
applications such as prediction of disease onsets. Extensive experiments on six
healthcare datasets, including synthetic data, real-world disease collections,
and MIMIC-III/IV, demonstrate the model's superior performance. Specifically,
our method achieved a $\beta$ error of 0.907, outperforming DWR (1.024) and SVM
(1.141). On real-world datasets, our model achieved accuracies of 0.789, 0.920,
and 0.300 for Esophageal Cancer, Heart Disease, and Cauda Equina Syndrome
prediction task, respectively, consistently surpassing baseline models. On the
ICD code prediction tasks, it achieved AUC Macro scores of 92.8 on MIMIC-III
and 96.7 on MIMIC-IV, outperforming the state-of-the-art models KEPT and MSMN.
Expert evaluations further validate the model's effectiveness, causality, and
interpretability.},
 author = {Junda Wang and Weijian Li and Han Wang and Hanjia Lyu and Caroline P. Thirukumaran and Addisu Mesfin and Hong Yu and Jiebo Luo},
 citations = {0},
 comment = {},
 doi = {10.1109/bigdata62323.2024.10825951},
 eprint = {2411.06338v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {CRTRE: Causal Rule Generation with Target Trial Emulation Framework},
 url = {http://arxiv.org/abs/2411.06338v1},
 year = {2024}
}

@article{2411.06428v1,
 abstract = {Machine learning models deployed in sensitive areas such as healthcare must
be interpretable to ensure accountability and fairness. Rule lists (if Age < 35
$\wedge$ Priors > 0 then Recidivism = True, else if Next Condition . . . )
offer full transparency, making them well-suited for high-stakes decisions.
However, learning such rule lists presents significant challenges. Existing
methods based on combinatorial optimization require feature pre-discretization
and impose restrictions on rule size. Neuro-symbolic methods use more scalable
continuous optimization yet place similar pre-discretization constraints and
suffer from unstable optimization. To address the existing limitations, we
introduce NeuRules, an end-to-end trainable model that unifies discretization,
rule learning, and rule order into a single differentiable framework. We
formulate a continuous relaxation of the rule list learning problem that
converges to a strict rule list through temperature annealing. NeuRules learns
both the discretizations of individual features, as well as their combination
into conjunctive rules without any pre-processing or restrictions. Extensive
experiments demonstrate that NeuRules consistently outperforms both
combinatorial and neuro-symbolic methods, effectively learning simple and
complex rules, as well as their order, across a wide range of datasets.},
 author = {Sascha Xu and Nils Philipp Walter and Jilles Vreeken},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2411.06428v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Neuro-Symbolic Rule Lists},
 url = {http://arxiv.org/abs/2411.06428v1},
 year = {2024}
}

@article{2411.08392v1,
 abstract = {Reinforcement Learning (RL) is a rapidly growing area of machine learning
that finds its application in a broad range of domains, from finance and
healthcare to robotics and gaming. Compared to other machine learning
techniques, RL agents learn from their own experiences using trial and error,
and improve their performance over time. However, assessing RL models can be
challenging, which makes it difficult to interpret their behaviour. While
reward is a widely used metric to evaluate RL models, it may not always provide
an accurate measure of training performance. In some cases, the reward may seem
increasing while the model's performance is actually decreasing, leading to
misleading conclusions about the effectiveness of the training. To overcome
this limitation, we have developed RLInspect - an interactive visual analytic
tool, that takes into account different components of the RL model - state,
action, agent architecture and reward, and provides a more comprehensive view
of the RL training. By using RLInspect, users can gain insights into the
model's behaviour, identify issues during training, and potentially correct
them effectively, leading to a more robust and reliable RL system.},
 author = {Geetansh Kalra and Divye Singh and Justin Jose},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2411.08392v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {RLInspect: An Interactive Visual Approach to Assess Reinforcement Learning Algorithm},
 url = {http://arxiv.org/abs/2411.08392v1},
 year = {2024}
}

@article{2411.08469v2,
 abstract = {Growing concerns over the lack of transparency in AI, particularly in
high-stakes fields like healthcare and finance, drive the need for explainable
and trustworthy systems. While Large Language Models (LLMs) perform
exceptionally well in generating accurate outputs, their "black box" nature
poses significant challenges to transparency and trust. To address this, the
paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs.
By leveraging domain expert knowledge, retrieval-augmented generation (RAG),
and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet
enhances LLM outputs with structured reasoning and verification.This approach
strives to help AI systems deliver results that are as accurate, explainable,
and trustworthy as possible, aligning with regulatory expectations for
transparency and accountability. TranspNet provides a solution for developing
AI systems that are reliable and interpretable, making it suitable for
real-world applications where trust is critical.},
 author = {Fadi Al Machot and Martin Thomas Horsch and Habib Ullah},
 citations = {3},
 comment = {},
 doi = {10.1007/978-3-031-89274-5_3},
 eprint = {2411.08469v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Building Trustworthy AI: Transparent AI Systems via Large Language Models, Ontologies, and Logical Reasoning (TranspNet)},
 url = {http://arxiv.org/abs/2411.08469v2},
 year = {2024}
}

@article{2411.09512v2,
 abstract = {Generative Adversarial Networks (GANs) have surfaced as a revolutionary
element within the domain of low-dose computed tomography (LDCT) imaging,
providing an advanced resolution to the enduring issue of reconciling radiation
exposure with image quality. This comprehensive review synthesizes the rapid
advancements in GAN-based LDCT denoising techniques, examining the evolution
from foundational architectures to state-of-the-art models incorporating
advanced features such as anatomical priors, perceptual loss functions, and
innovative regularization strategies. We critically analyze various GAN
architectures, including conditional GANs (cGANs), CycleGANs, and
Super-Resolution GANs (SRGANs), elucidating their unique strengths and
limitations in the context of LDCT denoising. The evaluation provides both
qualitative and quantitative results related to the improvements in performance
in benchmark and clinical datasets with metrics such as PSNR, SSIM, and LPIPS.
After highlighting the positive results, we discuss some of the challenges
preventing a wider clinical use, including the interpretability of the images
generated by GANs, synthetic artifacts, and the need for clinically relevant
metrics. The review concludes by highlighting the essential significance of
GAN-based methodologies in the progression of precision medicine via tailored
LDCT denoising models, underlining the transformative possibilities presented
by artificial intelligence within contemporary radiological practice.},
 author = {Yunuo Wang and Ningning Yang and Jialin Li},
 citations = {},
 comment = {},
 doi = {10.4108/eai.21-11-2024.2354627},
 eprint = {2411.09512v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {GAN-Based Architecture for Low-dose Computed Tomography Imaging Denoising},
 url = {http://arxiv.org/abs/2411.09512v2},
 year = {2024}
}

@article{2411.12707v1,
 abstract = {Imaging-based deep learning has transformed healthcare research, yet its
clinical adoption remains limited due to challenges in comparing imaging models
with traditional non-imaging and tabular data. To bridge this gap, we introduce
Barttender, an interpretable framework that uses deep learning for the direct
comparison of the utility of imaging versus non-imaging tabular data for tasks
like disease prediction.
  Barttender converts non-imaging tabular features, such as scalar data from
electronic health records, into grayscale bars, facilitating an interpretable
and scalable deep learning based modeling of both data modalities. Our
framework allows researchers to evaluate differences in utility through
performance measures, as well as local (sample-level) and global
(population-level) explanations. We introduce a novel measure to define global
feature importances for image-based deep learning models, which we call gIoU.
Experiments on the CheXpert and MIMIC datasets with chest X-rays and scalar
data from electronic health records show that Barttender performs comparably to
traditional methods and offers enhanced explainability using deep learning
models.},
 author = {Ayush Singla and Shakson Isaac and Chirag J. Patel},
 citations = {},
 comment = {Accepted to the Proceedings Track at Machine Learning for Health
  (ML4H 2024) conference, held on December 15-16, 2024 in Vancouver, Canada},
 doi = {},
 eprint = {2411.12707v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Barttender: An approachable & interpretable way to compare medical imaging and non-imaging data},
 url = {http://arxiv.org/abs/2411.12707v1},
 year = {2024}
}

@article{2411.14471v1,
 abstract = {Diabetes, particularly Type 2 diabetes (T2D), poses a substantial global
health burden, compounded by its associated complications such as
cardiovascular diseases, kidney failure, and vision impairment. Early detection
of T2D is critical for improving healthcare outcomes and optimizing resource
allocation. In this study, we address the gap in early T2D detection by
leveraging machine learning (ML) techniques on gene expression data obtained
from T2D patients. Our primary objective was to enhance the accuracy of early
T2D detection through advanced ML methodologies and increase the model's
trustworthiness using the explainable artificial intelligence (XAI) technique.
Analyzing the biological mechanisms underlying T2D through gene expression
datasets represents a novel research frontier, relatively less explored in
previous studies. While numerous investigations have focused on utilizing
clinical and demographic data for T2D prediction, the integration of molecular
insights from gene expression datasets offers a unique and promising avenue for
understanding the pathophysiology of the disease. By employing six ML
classifiers on data sourced from NCBI's Gene Expression Omnibus (GEO), we
observed promising performance across all models. Notably, the XGBoost
classifier exhibited the highest accuracy, achieving 97%. Our study addresses a
notable gap in early T2D detection methodologies, emphasizing the importance of
leveraging gene expression data and advanced ML techniques.},
 author = {Aurora Lithe Roy and Md Kamrul Siam and Nuzhat Noor Islam Prova and Sumaiya Jahan and Abdullah Al Maruf},
 citations = {2},
 comment = {8 pages},
 doi = {10.1145/3723178.3723228},
 eprint = {2411.14471v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Leveraging Gene Expression Data and Explainable Machine Learning for Enhanced Early Detection of Type 2 Diabetes},
 url = {http://arxiv.org/abs/2411.14471v1},
 year = {2024}
}

@article{2411.16084v1,
 abstract = {Objectives: The vast and complex nature of human genomic sequencing data
presents challenges for effective analysis. This review aims to investigate the
application of Natural Language Processing (NLP) techniques, particularly Large
Language Models (LLMs) and transformer architectures, in deciphering genomic
codes, focusing on tokenization, transformer models, and regulatory annotation
prediction. The goal of this review is to assess data and model accessibility
in the most recent literature, gaining a better understanding of the existing
capabilities and constraints of these tools in processing genomic sequencing
data.
  Methods: Following Preferred Reporting Items for Systematic Reviews and
Meta-Analyses (PRISMA) guidelines, our scoping review was conducted across
PubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library.
Studies were included if they focused on NLP methodologies applied to genomic
sequencing data analysis, without restrictions on publication date or article
type.
  Results: A total of 26 studies published between 2021 and April 2024 were
selected for review. The review highlights that tokenization and transformer
models enhance the processing and understanding of genomic data, with
applications in predicting regulatory annotations like transcription-factor
binding sites and chromatin accessibility.
  Discussion: The application of NLP and LLMs to genomic sequencing data
interpretation is a promising field that can help streamline the processing of
large-scale genomic data while also providing a better understanding of its
complex structures. It has the potential to drive advancements in personalized
medicine by offering more efficient and scalable solutions for genomic
analysis. Further research is also needed to discuss and overcome current
limitations, enhancing model transparency and applicability.},
 author = {Shuyan Cheng and Yishu Wei and Yiliang Zhou and Zihan Xu and Drew N Wright and Jinze Liu and Yifan Peng},
 citations = {3},
 comment = {},
 doi = {10.1093/jamia/ocaf029},
 eprint = {2411.16084v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Deciphering genomic codes using advanced NLP techniques: a scoping review},
 url = {http://arxiv.org/abs/2411.16084v1},
 year = {2024}
}

@article{2411.16404v1,
 abstract = {Modern distributed applications in healthcare, supply chain, and the Internet
of Things handle a large amount of data in a diverse application setting with
multiple stakeholders. Such applications leverage advanced artificial
intelligence (AI) and machine learning algorithms to automate business
processes. The proliferation of modern AI technologies increases the data
demand. However, real-world networks often include private and sensitive
information of businesses, users, and other organizations. Emerging
data-protection regulations such as the General Data Protection Regulation
(GDPR) and the California Consumer Privacy Act (CCPA) introduce policies around
collecting, storing, and managing digital data. While Blockchain technology
offers transparency, auditability, and immutability for multi-stakeholder
applications, it lacks inherent support for privacy. Typically, privacy support
is added to a blockchain-based application by incorporating cryptographic
schemes, consent mechanisms, and self-sovereign identity. This article surveys
the literature on blockchain-based privacy-preserving systems and identifies
the tools for protecting privacy. Besides, consent mechanisms and identity
management in the context of blockchain-based systems are also analyzed. The
article concludes by highlighting the list of open challenges and further
research opportunities.},
 author = {Rodrigo Dutra Garcia and Gowri Ramachandran and Kealan Dunnett and Raja Jurdak and Caetano Ranieri and Bhaskar Krishnamachari and Jo Ueyama},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2411.16404v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Survey of Blockchain-Based Privacy Applications: An Analysis of Consent Management and Self-Sovereign Identity Approaches},
 url = {http://arxiv.org/abs/2411.16404v1},
 year = {2024}
}

@article{2411.17284v5,
 abstract = {Large language models (LLMs) acquire a breadth of information across various
domains. However, their computational complexity, cost, and lack of
transparency often hinder their direct application for predictive tasks where
privacy and interpretability are paramount. In fields such as healthcare,
biology, and finance, specialised and interpretable linear models still hold
considerable value. In such domains, labelled data may be scarce or expensive
to obtain. Well-specified prior distributions over model parameters can reduce
the sample complexity of learning through Bayesian inference; however,
eliciting expert priors can be time-consuming. We therefore introduce
AutoElicit to extract knowledge from LLMs and construct priors for predictive
models. We show these priors are informative and can be refined using natural
language. We perform a careful study contrasting AutoElicit with in-context
learning and demonstrate how to perform model selection between the two
methods. We find that AutoElicit yields priors that can substantially reduce
error over uninformative priors, using fewer labels, and consistently
outperform in-context learning. We show that AutoElicit saves over 6 months of
labelling effort when building a new predictive model for urinary tract
infections from sensor recordings of people living with dementia.},
 author = {Alexander Capstick and Rahul G. Krishnan and Payam Barnaghi},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2411.17284v5},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling},
 url = {http://arxiv.org/abs/2411.17284v5},
 year = {2024}
}

@article{2411.19742v1,
 abstract = {Objective: In modern healthcare, accurately predicting diseases is a crucial
matter. This study introduces a novel approach using graph neural networks
(GNNs) and a Graph Transformer (GT) to predict the incidence of heart failure
(HF) on a patient similarity graph at the next hospital visit. Materials and
Methods: We used electronic health records (EHR) from the MIMIC-III dataset and
applied the K-Nearest Neighbors (KNN) algorithm to create a patient similarity
graph using embeddings from diagnoses, procedures, and medications. Three
models - GraphSAGE, Graph Attention Network (GAT), and Graph Transformer (GT) -
were implemented to predict HF incidence. Model performance was evaluated using
F1 score, AUROC, and AUPRC metrics, and results were compared against baseline
algorithms. An interpretability analysis was performed to understand the
model's decision-making process. Results: The GT model demonstrated the best
performance (F1 score: 0.5361, AUROC: 0.7925, AUPRC: 0.5168). Although the
Random Forest (RF) baseline achieved a similar AUPRC value, the GT model
offered enhanced interpretability due to the use of patient relationships in
the graph structure. A joint analysis of attention weights, graph connectivity,
and clinical features provided insight into model predictions across different
classification groups. Discussion and Conclusion: Graph-based approaches such
as GNNs provide an effective framework for predicting HF. By leveraging a
patient similarity graph, GNNs can capture complex relationships in EHR data,
potentially improving prediction accuracy and clinical interpretability.},
 author = {Heloisa Oss Boll and Ali Amirahmadi and Amira Soliman and Stefan Byttner and Mariana Recamonde-Mendoza},
 citations = {4},
 comment = {},
 doi = {10.5753/sbcas_estendido.2025.7013},
 eprint = {2411.19742v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph},
 url = {http://arxiv.org/abs/2411.19742v1},
 year = {2024}
}

@article{2412.00800v2,
 abstract = {Explainable Artificial Intelligence (XAI) addresses the growing need for
transparency and interpretability in AI systems, enabling trust and
accountability in decision-making processes. This book offers a comprehensive
guide to XAI, bridging foundational concepts with advanced methodologies. It
explores interpretability in traditional models such as Decision Trees, Linear
Regression, and Support Vector Machines, alongside the challenges of explaining
deep learning architectures like CNNs, RNNs, and Large Language Models (LLMs),
including BERT, GPT, and T5. The book presents practical techniques such as
SHAP, LIME, Grad-CAM, counterfactual explanations, and causal inference,
supported by Python code examples for real-world applications.
  Case studies illustrate XAI's role in healthcare, finance, and policymaking,
demonstrating its impact on fairness and decision support. The book also covers
evaluation metrics for explanation quality, an overview of cutting-edge XAI
tools and frameworks, and emerging research directions, such as
interpretability in federated learning and ethical AI considerations. Designed
for a broad audience, this resource equips readers with the theoretical
insights and practical skills needed to master XAI. Hands-on examples and
additional resources are available at the companion GitHub repository:
https://github.com/Echoslayer/XAI_From_Classical_Models_to_LLMs.},
 author = {Weiche Hsieh and Ziqian Bi and Chuanqi Jiang and Junyu Liu and Benji Peng and Sen Zhang and Xuanhe Pan and Jiawei Xu and Jinlang Wang and Keyu Chen and Pohsun Feng and Yizhu Wen and Xinyuan Song and Tianyang Wang and Ming Liu and Junjie Yang and Ming Li and Bowen Jing and Jintao Ren and Junhao Song and Hong-Ming Tseng and Yichao Zhang and Lawrence K. Q. Yan and Qian Niu and Silin Chen and Yunze Wang and Chia Xin Liang},
 citations = {},
 comment = {},
 doi = {10.31219/osf.io/wbk36},
 eprint = {2412.00800v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Comprehensive Guide to Explainable AI: From Classical Models to LLMs},
 url = {http://arxiv.org/abs/2412.00800v2},
 year = {2024}
}

@article{2412.01829v1,
 abstract = {The continuous development of artificial intelligence (AI) theory has
propelled this field to unprecedented heights, owing to the relentless efforts
of scholars and researchers. In the medical realm, AI takes a pivotal role,
leveraging robust machine learning (ML) algorithms. AI technology in medical
imaging aids physicians in X-ray, computed tomography (CT) scans, and magnetic
resonance imaging (MRI) diagnoses, conducts pattern recognition and disease
prediction based on acoustic data, delivers prognoses on disease types and
developmental trends for patients, and employs intelligent health management
wearable devices with human-computer interaction technology to name but a few.
While these well-established applications have significantly assisted in
medical field diagnoses, clinical decision-making, and management,
collaboration between the medical and AI sectors faces an urgent challenge: How
to substantiate the reliability of decision-making? The underlying issue stems
from the conflict between the demand for accountability and result transparency
in medical scenarios and the black-box model traits of AI. This article reviews
recent research grounded in explainable artificial intelligence (XAI), with an
emphasis on medical practices within the visual, audio, and multimodal
perspectives. We endeavour to categorise and synthesise these practices, aiming
to provide support and guidance for future researchers and healthcare
professionals.},
 author = {Qiyang Sun and Alican Akman and Björn W. Schuller},
 citations = {1},
 comment = {},
 doi = {10.1201/9781032626345},
 eprint = {2412.01829v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable Artificial Intelligence for Medical Applications: A Review},
 url = {http://arxiv.org/abs/2412.01829v1},
 year = {2024}
}

@article{2412.02173v1,
 abstract = {Since the emergence of Large Language Models (LLMs), the challenge of
effectively leveraging their potential in healthcare has taken center stage. A
critical barrier to using LLMs for extracting insights from unstructured
clinical notes lies in the prompt engineering process. Despite its pivotal role
in determining task performance, a clear framework for prompt optimization
remains absent. Current methods to address this gap take either a manual prompt
refinement approach, where domain experts collaborate with prompt engineers to
create an optimal prompt, which is time-intensive and difficult to scale, or
through employing automatic prompt optimizing approaches, where the value of
the input of domain experts is not fully realized. To address this, we propose
StructEase, a novel framework that bridges the gap between automation and the
input of human expertise in prompt engineering. A core innovation of the
framework is SamplEase, an iterative sampling algorithm that identifies
high-value cases where expert feedback drives significant performance
improvements. This approach minimizes expert intervention, to effectively
enhance classification outcomes. This targeted approach reduces labeling
redundancy, mitigates human error, and enhances classification outcomes. We
evaluated the performance of StructEase using a dataset of de-identified
clinical narratives from the US National Electronic Injury Surveillance System
(NEISS), demonstrating significant gains in classification performance compared
to current methods. Our findings underscore the value of expert integration in
LLM workflows, achieving notable improvements in F1 score while maintaining
minimal expert effort. By combining transparency, flexibility, and scalability,
StructEase sets the foundation for a framework to integrate expert input into
LLM workflows in healthcare and beyond.},
 author = {Nader Karayanni and Aya Awwad and Chein-Lien Hsiao and Surish P Shanmugam},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2412.02173v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Keeping Experts in the Loop: Expert-Guided Optimization for Clinical Data Classification using Large Language Models},
 url = {http://arxiv.org/abs/2412.02173v1},
 year = {2024}
}

@article{2412.02834v1,
 abstract = {Artificial intelligence (AI) has transformed various sectors and
institutions, including education and healthcare. Although AI offers immense
potential for innovation and problem solving, its integration also raises
significant ethical concerns, such as privacy and bias. This paper delves into
key considerations for developing AI policies within institutions. We explore
the importance of interpretability and explainability in AI elements, as well
as the need to mitigate biases and ensure privacy. Additionally, we discuss the
environmental impact of AI and the importance of energy-efficient practices.
The culmination of these important components is centralized in a generalized
framework to be utilized for institutions developing their AI policy. By
addressing these critical factors, institutions can harness the power of AI
while safeguarding ethical principles.},
 author = {William Franz Lamberti},
 citations = {},
 comment = {8 pages, 2 figures},
 doi = {},
 eprint = {2412.02834v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Artificial Intelligence Policy Framework for Institutions},
 url = {http://arxiv.org/abs/2412.02834v1},
 year = {2024}
}

@article{2412.03166v1,
 abstract = {Deep Learning has shown outstanding results in computer vision tasks;
healthcare is no exception. However, there is no straightforward way to expose
the decision-making process of DL models. Good accuracy is not enough for skin
cancer predictions. Understanding the model's behavior is crucial for clinical
application and reliable outcomes. In this work, we identify desiderata for
explanations in skin-lesion models. We analyzed seven methods, four based on
pixel-attribution (Grad-CAM, Score-CAM, LIME, SHAP) and three on high-level
concepts (ACE, ICE, CME), for a deep neural network trained on the
International Skin Imaging Collaboration Archive. Our findings indicate that
while these techniques reveal biases, there is room for improving the
comprehensiveness of explanations to achieve transparency in skin-lesion
models.},
 author = {Rosa Y. G. Paccotacya-Yanque and Alceu Bissoto and Sandra Avila},
 citations = {2},
 comment = {6 pages. Paper accepted at 20th International Symposium on Medical
  Information Processing and Analysis (SIPAIM)},
 doi = {10.1109/sipaim62974.2024.10783606},
 eprint = {2412.03166v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Are Explanations Helpful? A Comparative Analysis of Explainability Methods in Skin Lesion Classifiers},
 url = {http://arxiv.org/abs/2412.03166v1},
 year = {2024}
}

@article{2412.03267v1,
 abstract = {Given the global prevalence of cardiovascular diseases, there is a pressing
need for easily accessible early screening methods. Typically, this requires
medical practitioners to investigate heart auscultations for irregular sounds,
followed by echocardiography and electrocardiography tests. To democratize
early diagnosis, we present a user-friendly solution for abnormal heart sound
detection, utilizing mobile phones and a lightweight neural network optimized
for on-device inference. Unlike previous approaches reliant on specialized
stethoscopes, our method directly analyzes audio recordings, facilitated by a
novel architecture known as IConNet. IConNet, an Interpretable Convolutional
Neural Network, harnesses insights from audio signal processing, enhancing
efficiency and providing transparency in neural pattern extraction from raw
waveform signals. This is a significant step towards trustworthy AI in
healthcare, aiding in remote health monitoring efforts.},
 author = {Linh Vu and Thu Tran},
 citations = {0},
 comment = {N2Women'24 Workshop, MobiSys 2024, Tokyo, Japan},
 doi = {},
 eprint = {2412.03267v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Detecting abnormal heart sound using mobile phones and on-device IConNet},
 url = {http://arxiv.org/abs/2412.03267v1},
 year = {2024}
}

@article{2412.03427v1,
 abstract = {The success of precision medicine requires computational models that can
effectively process and interpret diverse physiological signals across
heterogeneous patient populations. While foundation models have demonstrated
remarkable transfer capabilities across various domains, their effectiveness in
handling individual-specific physiological signals - crucial for precision
medicine - remains largely unexplored. This work introduces a systematic
pipeline for rapidly and efficiently evaluating foundation models' transfer
capabilities in medical contexts. Our pipeline employs a three-stage approach.
First, it leverages physiological simulation software to generate diverse,
clinically relevant scenarios, particularly focusing on data-scarce medical
conditions. This simulation-based approach enables both targeted capability
assessment and subsequent model fine-tuning. Second, the pipeline projects
these simulated signals through the foundation model to obtain embeddings,
which are then evaluated using linear methods. This evaluation quantifies the
model's ability to capture three critical aspects: physiological feature
independence, temporal dynamics preservation, and medical scenario
differentiation. Finally, the pipeline validates these representations through
specific downstream medical tasks. Initial testing of our pipeline on the
Moirai time series foundation model revealed significant limitations in
physiological signal processing, including feature entanglement, temporal
dynamics distortion, and reduced scenario discrimination. These findings
suggest that current foundation models may require substantial architectural
modifications or targeted fine-tuning before deployment in clinical settings.},
 author = {Matthias Christenson and Cove Geary and Brian Locke and Pranav Koirala and Warren Woodrich Pettine},
 citations = {0},
 comment = {Presented at the precision medicine workshop at the AI in Medicine
  conference (2024) in Salt Lake City},
 doi = {},
 eprint = {2412.03427v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Assessing Foundation Models' Transferability to Physiological Signals in Precision Medicine},
 url = {http://arxiv.org/abs/2412.03427v1},
 year = {2024}
}

@article{2412.03576v1,
 abstract = {Artificial intelligence (AI) has rapidly transformed various sectors,
including healthcare, where it holds the potential to revolutionize clinical
practice and improve patient outcomes. However, its integration into medical
settings brings significant ethical challenges that need careful consideration.
This paper examines the current state of AI in healthcare, focusing on five
critical ethical concerns: justice and fairness, transparency, patient consent
and confidentiality, accountability, and patient-centered and equitable care.
These concerns are particularly pressing as AI systems can perpetuate or even
exacerbate existing biases, often resulting from non-representative datasets
and opaque model development processes. The paper explores how bias, lack of
transparency, and challenges in maintaining patient trust can undermine the
effectiveness and fairness of AI applications in healthcare. In addition, we
review existing frameworks for the regulation and deployment of AI, identifying
gaps that limit the widespread adoption of these systems in a just and
equitable manner. Our analysis provides recommendations to address these
ethical challenges, emphasizing the need for fairness in algorithm design,
transparency in model decision-making, and patient-centered approaches to
consent and data privacy. By highlighting the importance of continuous ethical
scrutiny and collaboration between AI developers, clinicians, and ethicists, we
outline pathways for achieving more responsible and inclusive AI implementation
in healthcare. These strategies, if adopted, could enhance both the clinical
value of AI and the trustworthiness of AI systems among patients and healthcare
professionals, ensuring that these technologies serve all populations
equitably.},
 author = {Ellison B. Weiner and Irene Dankwa-Mullan and William A. Nelson and Saeed Hassanpour},
 citations = {23},
 comment = {},
 doi = {10.1371/journal.pdig.0000810},
 eprint = {2412.03576v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Ethical Challenges and Evolving Strategies in the Integration of Artificial Intelligence into Clinical Practice},
 url = {http://arxiv.org/abs/2412.03576v1},
 year = {2024}
}

@article{2412.03884v2,
 abstract = {The fast growth of deep learning has brought great progress in AI-based
applications. However, these models are often seen as "black boxes," which
makes them hard to understand, explain, or trust. Explainable Artificial
Intelligence (XAI) tries to make AI decisions clearer so that people can
understand how and why the model makes certain choices. Even though many
studies have focused on XAI, there is still a lack of standard ways to measure
how well these explanation methods work in real-world situations. This study
introduces a single evaluation framework for XAI. It uses both numbers and user
feedback to check if the explanations are correct, easy to understand, fair,
complete, and reliable. The framework focuses on users' needs and different
application areas, which helps improve the trust and use of AI in important
fields. To fix problems in current evaluation methods, we propose clear steps,
including loading data, creating explanations, and fully testing them. We also
suggest setting common benchmarks. We show the value of this framework through
case studies in healthcare, finance, farming, and self-driving systems. These
examples prove that our method can support fair and trustworthy evaluation of
XAI methods. This work gives a clear and practical way to improve transparency
and trust in AI systems used in the real world.},
 author = {Md. Ariful Islam and Md Abrar Jahin and M. F. Mridha and Nilanjan Dey},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2412.03884v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Unified Framework for Evaluating the Effectiveness and Enhancing the Transparency of Explainable AI Methods in Real-World Applications},
 url = {http://arxiv.org/abs/2412.03884v2},
 year = {2024}
}

@article{2412.04067v1,
 abstract = {Recent advances in deep learning and natural language generation have
significantly improved image captioning, enabling automated, human-like
descriptions for visual content. In this work, we apply these captioning
techniques to generate clinician-like interpretations of ECG data. This study
leverages existing ECG datasets accompanied by free-text reports authored by
healthcare professionals (HCPs) as training data. These reports, while often
inconsistent, provide a valuable foundation for automated learning. We
introduce an encoder-decoder-based method that uses these reports to train
models to generate detailed descriptions of ECG episodes. This represents a
significant advancement in ECG analysis automation, with potential applications
in zero-shot classification and automated clinical decision support.
  The model is tested on various datasets, including both 1- and 12-lead ECGs.
It significantly outperforms the state-of-the-art reference model by Qiu et
al., achieving a METEOR score of 55.53% compared to 24.51% achieved by the
reference model. Furthermore, several key design choices are discussed,
providing a comprehensive overview of current challenges and innovations in
this domain.
  The source codes for this research are publicly available in our Git
repository https://git.zib.de/ableich/ecg-comment-generation-public},
 author = {Amnon Bleich and Antje Linnemann and Bjoern H. Diem and Tim OF Conrad},
 citations = {3},
 comment = {},
 doi = {},
 eprint = {2412.04067v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning},
 url = {http://arxiv.org/abs/2412.04067v1},
 year = {2024}
}

@article{2412.05134v1,
 abstract = {Deep learning models are widely used nowadays for their reliability in
performing various tasks. However, they do not typically provide the reasoning
behind their decision, which is a significant drawback, particularly for more
sensitive areas such as biometrics, security and healthcare. The most commonly
used approaches to provide interpretability create visual attention heatmaps of
regions of interest on an image based on models gradient backpropagation.
Although this is a viable approach, current methods are targeted toward image
settings and default/standard deep learning models, meaning that they require
significant adaptations to work on video/multi-modal settings and custom
architectures. This paper proposes an approach for interpretability that is
model-agnostic, based on a novel use of the Squeeze and Excitation (SE) block
that creates visual attention heatmaps. By including an SE block prior to the
classification layer of any model, we are able to retrieve the most influential
features via SE vector manipulation, one of the key components of the SE block.
Our results show that this new SE-based interpretability can be applied to
various models in image and video/multi-modal settings, namely biometrics of
facial features with CelebA and behavioral biometrics using Active Speaker
Detection datasets. Furthermore, our proposal does not compromise model
performance toward the original task, and has competitive results with current
interpretability approaches in state-of-the-art object datasets, highlighting
its robustness to perform in varying data aside from the biometric context.},
 author = {Tiago Roxo and Joana C. Costa and Pedro R. M. Inácio and Hugo Proença},
 citations = {},
 comment = {},
 doi = {10.1007/978-3-031-92089-9_16},
 eprint = {2412.05134v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {How to Squeeze An Explanation Out of Your Model},
 url = {http://arxiv.org/abs/2412.05134v1},
 year = {2024}
}

@article{2412.06494v1,
 abstract = {Background. Federated learning (FL) has gained wide popularity as a
collaborative learning paradigm enabling collaborative AI in sensitive
healthcare applications. Nevertheless, the practical implementation of FL
presents technical and organizational challenges, as it generally requires
complex communication infrastructures. In this context, consensus-based
learning (CBL) may represent a promising collaborative learning alternative,
thanks to the ability of combining local knowledge into a federated decision
system, while potentially reducing deployment overhead. Methods. In this work
we propose an extensive benchmark of the accuracy and cost-effectiveness of a
panel of FL and CBL methods in a wide range of collaborative medical data
analysis scenarios. The benchmark includes 7 different medical datasets,
encompassing 3 machine learning tasks, 8 different data modalities, and
multi-centric settings involving 3 to 23 clients. Findings. Our results reveal
that CBL is a cost-effective alternative to FL. When compared across the panel
of medical dataset in the considered benchmark, CBL methods provide equivalent
accuracy to the one achieved by FL.Nonetheless, CBL significantly reduces
training time and communication cost (resp. 15 fold and 60 fold decrease) (p <
0.05). Interpretation. This study opens a novel perspective on the deployment
of collaborative AI in real-world applications, whereas the adoption of
cost-effective methods is instrumental to achieve sustainability and
democratisation of AI by alleviating the need for extensive computational
resources.},
 author = {Francesco Cremonesi and Lucia Innocenti and Sebastien Ourselin and Vicky Goh and Michela Antonelli and Marco Lorenzi},
 citations = {0},
 comment = {},
 doi = {10.1101/2024.05.27.596048},
 eprint = {2412.06494v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A cautionary tale on the cost-effectiveness of collaborative AI in real-world medical applications},
 url = {http://arxiv.org/abs/2412.06494v1},
 year = {2024}
}

@article{2412.08012v1,
 abstract = {Cost-sensitive loss functions are crucial in many real-world prediction
problems, where different types of errors are penalized differently; for
example, in medical diagnosis, a false negative prediction can lead to worse
consequences than a false positive prediction. However, traditional PAC
learning theory has mostly focused on the symmetric 0-1 loss, leaving
cost-sensitive losses largely unaddressed. In this work, we extend the
celebrated theory of boosting to incorporate both cost-sensitive and
multi-objective losses. Cost-sensitive losses assign costs to the entries of a
confusion matrix, and are used to control the sum of prediction errors
accounting for the cost of each error type. Multi-objective losses, on the
other hand, simultaneously track multiple cost-sensitive losses, and are useful
when the goal is to satisfy several criteria at once (e.g., minimizing false
positives while keeping false negatives below a critical threshold). We develop
a comprehensive theory of cost-sensitive and multi-objective boosting,
providing a taxonomy of weak learning guarantees that distinguishes which
guarantees are trivial (i.e., can always be achieved), which ones are boostable
(i.e., imply strong learning), and which ones are intermediate, implying
non-trivial yet not arbitrarily accurate learning. For binary classification,
we establish a dichotomy: a weak learning guarantee is either trivial or
boostable. In the multiclass setting, we describe a more intricate landscape of
intermediate weak learning guarantees. Our characterization relies on a
geometric interpretation of boosting, revealing a surprising equivalence
between cost-sensitive and multi-objective losses.},
 author = {Marco Bressan and Nataly Brukhim and Nicolò Cesa-Bianchi and Emmanuel Esposito and Yishay Mansour and Shay Moran and Maximilian Thiessen},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2412.08012v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Of Dice and Games: A Theory of Generalized Boosting},
 url = {http://arxiv.org/abs/2412.08012v1},
 year = {2024}
}

@article{2412.09000v1,
 abstract = {As artificial intelligence (AI) becomes increasingly embedded in daily life,
designing intuitive, trustworthy, and emotionally resonant AI-human interfaces
has emerged as a critical challenge. This editorial introduces a Special Issue
that explores the psychology of AI experience design, focusing on how
interfaces can foster seamless collaboration between humans and machines.
Drawing on insights from diverse fields (healthcare, consumer technology,
workplace dynamics, and cultural sector), the papers in this collection
highlight the complexities of trust, transparency, and emotional sensitivity in
human-AI interaction. Key themes include designing AI systems that align with
user perceptions and expectations, overcoming resistance through transparency
and trust, and framing AI capabilities to reduce user anxiety. By synthesizing
findings from eight diverse studies, this editorial underscores the need for AI
interfaces to balance efficiency with empathy, addressing both functional and
emotional dimensions of user experience. Ultimately, it calls for actionable
frameworks to bridge research and practice, ensuring that AI systems enhance
human lives through thoughtful, human-centered design.},
 author = {Aparna Sundar and Tony Russell-Rose and Udo Kruschwitz and Karen Machleit},
 citations = {4},
 comment = {8 pages},
 doi = {10.1016/j.chb.2024.108539},
 eprint = {2412.09000v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {The AI Interface: Designing for the Ideal Machine-Human Experience (Editorial)},
 url = {http://arxiv.org/abs/2412.09000v1},
 year = {2024}
}

@article{2412.12386v3,
 abstract = {Interpretability in Table Question Answering (Table QA) is critical,
especially in high-stakes domains like finance and healthcare. While recent
Table QA approaches based on Large Language Models (LLMs) achieve high
accuracy, they often produce ambiguous explanations of how answers are derived.
  We propose Plan-of-SQLs (POS), a new Table QA method that makes the model's
decision-making process interpretable. POS decomposes a question into a
sequence of atomic steps, each directly translated into an executable SQL
command on the table, thereby ensuring that every intermediate result is
transparent. Through extensive experiments, we show that: First, POS generates
the highest-quality explanations among compared methods, which markedly
improves the users' ability to simulate and verify the model's decisions.
Second, when evaluated on standard Table QA benchmarks (TabFact, WikiTQ, and
FeTaQA), POS achieves QA accuracy that is competitive to existing methods,
while also offering greater efficiency-requiring significantly fewer LLM calls
and table database queries (up to 25x fewer)-and more robust performance on
large-sized tables. Finally, we observe high agreement (up to 90.59% in forward
simulation) between LLMs and human users when making decisions based on the
same explanations, suggesting that LLMs could serve as an effective proxy for
humans in evaluating Table QA explanations.},
 author = {Giang Nguyen and Ivan Brugere and Shubham Sharma and Sanjay Kariyappa and Anh Totti Nguyen and Freddy Lecue},
 citations = {},
 comment = {Published in Transactions on Machine Learning Research (TMLR) in
  06/2025. Reviews at: https://openreview.net/forum?id=2eTsZBoU2W},
 doi = {},
 eprint = {2412.12386v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable LLM-based Table Question Answering},
 url = {http://arxiv.org/abs/2412.12386v3},
 year = {2024}
}

@article{2412.12597v1,
 abstract = {Mechanical Ventilation (MV) is a critical life-support intervention in
intensive care units (ICUs). However, optimal ventilator settings are
challenging to determine because of the complexity of balancing
patient-specific physiological needs with the risks of adverse outcomes that
impact morbidity, mortality, and healthcare costs. This study introduces
ConformalDQN, a novel distribution-free conformal deep Q-learning approach for
optimizing mechanical ventilation in intensive care units. By integrating
conformal prediction with deep reinforcement learning, our method provides
reliable uncertainty quantification, addressing the challenges of Q-value
overestimation and out-of-distribution actions in offline settings. We trained
and evaluated our model using ICU patient records from the MIMIC-IV database.
ConformalDQN extends the Double DQN architecture with a conformal predictor and
employs a composite loss function that balances Q-learning with well-calibrated
probability estimation. This enables uncertainty-aware action selection,
allowing the model to avoid potentially harmful actions in unfamiliar states
and handle distribution shifts by being more conservative in
out-of-distribution scenarios. Evaluation against baseline models, including
physician policies, policy constraint methods, and behavior cloning,
demonstrates that ConformalDQN consistently makes recommendations within
clinically safe and relevant ranges, outperforming other methods by increasing
the 90-day survival rate. Notably, our approach provides an interpretable
measure of confidence in its decisions, which is crucial for clinical adoption
and potential human-in-the-loop implementations.},
 author = {Niloufar Eghbali and Tuka Alhanai and Mohammad M. Ghassemi},
 citations = {0},
 comment = {},
 doi = {10.1609/aaai.v39i27.35013},
 eprint = {2412.12597v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Distribution-Free Uncertainty Quantification in Mechanical Ventilation Treatment: A Conformal Deep Q-Learning Framework},
 url = {http://arxiv.org/abs/2412.12597v1},
 year = {2024}
}

@article{2412.14097v1,
 abstract = {Advancements in foundation models (FMs) have led to a paradigm shift in
machine learning. The rich, expressive feature representations from these
pre-trained, large-scale FMs are leveraged for multiple downstream tasks,
usually via lightweight fine-tuning of a shallow fully-connected network
following the representation. However, the non-interpretable, black-box nature
of this prediction pipeline can be a challenge, especially in critical domains
such as healthcare, finance, and security. In this paper, we explore the
potential of Concept Bottleneck Models (CBMs) for transforming complex,
non-interpretable foundation models into interpretable decision-making
pipelines using high-level concept vectors. Specifically, we focus on the
test-time deployment of such an interpretable CBM pipeline "in the wild", where
the input distribution often shifts from the original training distribution. We
first identify the potential failure modes of such a pipeline under different
types of distribution shifts. Then we propose an adaptive concept bottleneck
framework to address these failure modes, that dynamically adapts the
concept-vector bank and the prediction layer based solely on unlabeled data
from the target domain, without access to the source (training) dataset.
Empirical evaluations with various real-world distribution shifts show that our
adaptation method produces concept-based interpretations better aligned with
the test data and boosts post-deployment accuracy by up to 28%, aligning the
CBM performance with that of non-interpretable classification.},
 author = {Jihye Choi and Jayaram Raghuram and Yixuan Li and Somesh Jha},
 citations = {7},
 comment = {The preliminary version of the work appeared in the ICML 2024
  Workshop on Foundation Models in the Wild},
 doi = {},
 eprint = {2412.14097v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts},
 url = {http://arxiv.org/abs/2412.14097v1},
 year = {2024}
}

@article{2412.14572v1,
 abstract = {One of the goals of personalized medicine is to tailor diagnostics to
individual patients. Diagnostics are performed in practice by measuring
quantities, called biomarkers, that indicate the existence and progress of a
disease. In common cardiovascular diseases, such as hypertension, biomarkers
that are closely related to the clinical representation of a patient can be
predicted using computational models. Personalizing computational models
translates to considering patient-specific flow conditions, for example, the
compliance of blood vessels that cannot be a priori known and quantities such
as the patient geometry that can be measured using imaging. Therefore, a
patient is identified by a set of measurable and nonmeasurable parameters
needed to well-define a computational model; else, the computational model is
not personalized, meaning it is prone to large prediction errors. Therefore, to
personalize a computational model, sufficient information needs to be extracted
from the data. The current methods by which this is done are either
inefficient, due to relying on slow-converging optimization methods, or hard to
interpret, due to using `black box` deep-learning algorithms. We propose a
personalized diagnostic procedure based on a differentiable 0D-1D Navier-Stokes
reduced order model solver and fast parameter inference methods that take
advantage of gradients through the solver. By providing a faster method for
performing parameter inference and sensitivity analysis through
differentiability while maintaining the interpretability of well-understood
mathematical models and numerical methods, the best of both worlds is combined.
The performance of the proposed solver is validated against a well-established
process on different geometries, and different parameter inference processes
are successfully performed.},
 author = {Diego Renner and Georgios Kissas},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2412.14572v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Accelerated Patient-Specific Calibration via Differentiable Hemodynamics Simulations},
 url = {http://arxiv.org/abs/2412.14572v1},
 year = {2024}
}

@article{2412.15748v2,
 abstract = {Background: Despite the current ubiquity of Large Language Models (LLMs)
across the medical domain, there is a surprising lack of studies which address
their reasoning behaviour. We emphasise the importance of understanding
reasoning behaviour as opposed to high-level prediction accuracies, since it is
equivalent to explainable AI (XAI) in this context. In particular, achieving
XAI in medical LLMs used in the clinical domain will have a significant impact
across the healthcare sector. Results: Therefore, in this work, we adapt the
existing concept of reasoning behaviour and articulate its interpretation
within the specific context of medical LLMs. We survey and categorise current
state-of-the-art approaches for modeling and evaluating reasoning reasoning in
medical LLMs. Additionally, we propose theoretical frameworks which can empower
medical professionals or machine learning engineers to gain insight into the
low-level reasoning operations of these previously obscure models. We also
outline key open challenges facing the development of Large Reasoning Models.
Conclusion: The subsequent increased transparency and trust in medical machine
learning models by clinicians as well as patients will accelerate the
integration, application as well as further development of medical AI for the
healthcare system as a whole.},
 author = {Shamus Sim and Tyrone Chen},
 citations = {1},
 comment = {25 pages, 7 figures, 3 tables. Conceptualization, both authors.
  formal analysis, both authors. funding acquisition, both authors.
  investigation, both authors. resources, both authors. supervision, T.C..
  validation, both authors. visualization, both authors. writing original
  draft, both authors. writing review and editing, both authors},
 doi = {},
 eprint = {2412.15748v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models},
 url = {http://arxiv.org/abs/2412.15748v2},
 year = {2024}
}

@article{2412.15828v2,
 abstract = {Integrating AI in healthcare can greatly improve patient care and system
efficiency. However, the lack of explainability in AI systems (XAI) hinders
their clinical adoption, especially in multimodal settings that use
increasingly complex model architectures. Most existing XAI methods focus on
unimodal models, which fail to capture cross-modal interactions crucial for
understanding the combined impact of multiple data sources. Existing methods
for quantifying cross-modal interactions are limited to two modalities, rely on
labelled data, and depend on model performance. This is problematic in
healthcare, where XAI must handle multiple data sources and provide
individualised explanations. This paper introduces InterSHAP, a cross-modal
interaction score that addresses the limitations of existing approaches.
InterSHAP uses the Shapley interaction index to precisely separate and quantify
the contributions of the individual modalities and their interactions without
approximations. By integrating an open-source implementation with the SHAP
package, we enhance reproducibility and ease of use. We show that InterSHAP
accurately measures the presence of cross-modal interactions, can handle
multiple modalities, and provides detailed explanations at a local level for
individual samples. Furthermore, we apply InterSHAP to multimodal medical
datasets and demonstrate its applicability for individualised explanations.},
 author = {Laura Wenderoth and Konstantin Hemker and Nikola Simidjievski and Mateja Jamnik},
 citations = {4},
 comment = {},
 doi = {10.1609/aaai.v39i20.35452},
 eprint = {2412.15828v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Measuring Cross-Modal Interactions in Multimodal Models},
 url = {http://arxiv.org/abs/2412.15828v2},
 year = {2024}
}

@article{2412.16003v2,
 abstract = {Explaining machine learning (ML) models using eXplainable AI (XAI) techniques
has become essential to make them more transparent and trustworthy. This is
especially important in high-stakes domains like healthcare, where
understanding model decisions is critical to ensure ethical, sound, and
trustworthy outcome predictions. However, users are often confused about which
explanability method to choose for their specific use case. We present a
comparative analysis of widely used explainability methods, Shapley Additive
Explanations (SHAP) and Gradient-weighted Class Activation Mapping (Grad-CAM),
within the domain of human activity recognition (HAR) utilizing graph
convolutional networks (GCNs). By evaluating these methods on skeleton-based
data from two real-world datasets, including a healthcare-critical cerebral
palsy (CP) case, this study provides vital insights into both approaches'
strengths, limitations, and differences, offering a roadmap for selecting the
most appropriate explanation method based on specific models and applications.
We quantitatively and quantitatively compare these methods, focusing on feature
importance ranking, interpretability, and model sensitivity through
perturbation experiments. While SHAP provides detailed input feature
attribution, Grad-CAM delivers faster, spatially oriented explanations, making
both methods complementary depending on the application's requirements. Given
the importance of XAI in enhancing trust and transparency in ML models,
particularly in sensitive environments like healthcare, our research
demonstrates how SHAP and Grad-CAM could complement each other to provide more
interpretable and actionable model explanations.},
 author = {Felix Tempel and Daniel Groos and Espen Alexander F. Ihlen and Lars Adde and Inga Strümke},
 citations = {3},
 comment = {},
 doi = {},
 eprint = {2412.16003v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Choose Your Explanation: A Comparison of SHAP and GradCAM in Human Activity Recognition},
 url = {http://arxiv.org/abs/2412.16003v2},
 year = {2024}
}

@article{2412.16277v1,
 abstract = {Despite recent advancements in Instruct-based Image Editing models for
generating high-quality images, they are known as black boxes and a significant
barrier to transparency and user trust. To solve this issue, we introduce SMILE
(Statistical Model-agnostic Interpretability with Local Explanations), a novel
model-agnostic for localized interpretability that provides a visual heatmap to
clarify the textual elements' influence on image-generating models. We applied
our method to various Instruction-based Image Editing models like Pix2Pix,
Image2Image-turbo and Diffusers-Inpaint and showed how our model can improve
interpretability and reliability. Also, we use stability, accuracy, fidelity,
and consistency metrics to evaluate our method. These findings indicate the
exciting potential of model-agnostic interpretability for reliability and
trustworthiness in critical applications such as healthcare and autonomous
driving while encouraging additional investigation into the significance of
interpretability in enhancing dependable image editing models.},
 author = {Zeinab Dehghani and Koorosh Aslansefat and Adil Khan and Adín Ramírez Rivera and Franky George and Muhammad Khalid},
 citations = {},
 comment = {},
 doi = {10.21203/rs.3.rs-5943708/v1},
 eprint = {2412.16277v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Mapping the Mind of an Instruction-based Image Editing using SMILE},
 url = {http://arxiv.org/abs/2412.16277v1},
 year = {2024}
}

@article{2412.17258v1,
 abstract = {Vertebral compression fractures (VCFs) are a common and potentially serious
consequence of osteoporosis. Yet, they often remain undiagnosed. Opportunistic
screening, which involves automated analysis of medical imaging data acquired
primarily for other purposes, is a cost-effective method to identify
undiagnosed VCFs. In high-stakes scenarios like opportunistic medical
diagnosis, model interpretability is a key factor for the adoption of AI
recommendations. Rule-based methods are inherently explainable and closely
align with clinical guidelines, but they are not immediately applicable to
high-dimensional data such as CT scans. To address this gap, we introduce a
neurosymbolic approach for VCF detection in CT volumes. The proposed model
combines deep learning (DL) for vertebral segmentation with a shape-based
algorithm (SBA) that analyzes vertebral height distributions in salient
anatomical regions. This allows for the definition of a rule set over the
height distributions to detect VCFs. Evaluation of VerSe19 dataset shows that
our method achieves an accuracy of 96% and a sensitivity of 91% in VCF
detection. In comparison, a black box model, DenseNet, achieved an accuracy of
95% and sensitivity of 91% in the same dataset. Our results demonstrate that
our intrinsically explainable approach can match or surpass the performance of
black box deep neural networks while providing additional insights into why a
prediction was made. This transparency can enhance clinician's trust thus,
supporting more informed decision-making in VCF diagnosis and treatment
planning.},
 author = {Blanca Inigo and Yiqing Shen and Benjamin D. Killeen and Michelle Song and Axel Krieger and Christopher Bradley and Mathias Unberath},
 citations = {1},
 comment = {},
 doi = {10.1117/12.3047426},
 eprint = {2412.17258v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {An Intrinsically Explainable Approach to Detecting Vertebral Compression Fractures in CT Scans via Neurosymbolic Modeling},
 url = {http://arxiv.org/abs/2412.17258v1},
 year = {2024}
}

@article{2412.17505v1,
 abstract = {Multimodal machine learning models, such as those that combine text and image
modalities, are increasingly used in critical domains including public safety,
security, and healthcare. However, these systems inherit biases from their
single modalities. This study proposes a systemic framework for analyzing
dynamic multimodal bias interactions. Using the MMBias dataset, which
encompasses categories prone to bias such as religion, nationality, and sexual
orientation, this study adopts a simulation-based heuristic approach to compute
bias scores for text-only, image-only, and multimodal embeddings. A framework
is developed to classify bias interactions as amplification (multimodal bias
exceeds both unimodal biases), mitigation (multimodal bias is lower than both),
and neutrality (multimodal bias lies between unimodal biases), with
proportional analyzes conducted to identify the dominant mode and dynamics in
these interactions. The findings highlight that amplification (22\%) occurs
when text and image biases are comparable, while mitigation (11\%) arises under
the dominance of text bias, highlighting the stabilizing role of image bias.
Neutral interactions (67\%) are related to a higher text bias without
divergence. Conditional probabilities highlight the text's dominance in
mitigation and mixed contributions in neutral and amplification cases,
underscoring complex modality interplay. In doing so, the study encourages the
use of this heuristic, systemic, and interpretable framework to analyze
multimodal bias interactions, providing insight into how intermodal biases
dynamically interact, with practical applications for multimodal modeling and
transferability to context-based datasets, all essential for developing fair
and equitable AI models.},
 author = {Mounia Drissi},
 citations = {},
 comment = {9 pages, 3 figures},
 doi = {},
 eprint = {2412.17505v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {More is Less? A Simulation-Based Approach to Dynamic Interactions between Biases in Multimodal Models},
 url = {http://arxiv.org/abs/2412.17505v1},
 year = {2024}
}

@article{2412.17527v1,
 abstract = {This research presents an innovative approach to cancer diagnosis and
prediction using explainable Artificial Intelligence (XAI) and deep learning
techniques. With cancer causing nearly 10 million deaths globally in 2020,
early and accurate diagnosis is crucial. Traditional methods often face
challenges in cost, accuracy, and efficiency. Our study develops an AI model
that provides precise outcomes and clear insights into its decision-making
process, addressing the "black box" problem of deep learning models. By
employing XAI techniques, we enhance interpretability and transparency,
building trust among healthcare professionals and patients. Our approach
leverages neural networks to analyse extensive datasets, identifying patterns
for cancer detection. This model has the potential to revolutionise diagnosis
by improving accuracy, accessibility, and clarity in medical decision-making,
possibly leading to earlier detection and more personalised treatment
strategies. Furthermore, it could democratise access to high-quality
diagnostics, particularly in resource-limited settings, contributing to global
health equity. The model's applications extend beyond cancer diagnosis,
potentially transforming various aspects of medical decision-making and saving
millions of lives worldwide.},
 author = {Badaru I. Olumuyiwa and The Anh Han and Zia U. Shamszaman},
 citations = {2},
 comment = {},
 doi = {},
 eprint = {2412.17527v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Enhancing Cancer Diagnosis with Explainable & Trustworthy Deep Learning Models},
 url = {http://arxiv.org/abs/2412.17527v1},
 year = {2024}
}

@article{2412.18059v1,
 abstract = {Concept bottleneck models are interpretable predictive models that are often
used in domains where model trust is a key priority, such as healthcare. They
identify a small number of human-interpretable concepts in the data, which they
then use to make predictions. Learning relevant concepts from data proves to be
a challenging task. The most predictive concepts may not align with expert
intuition, thus, failing interpretability with no recourse. Our proposed
approach identifies a number of predictive concepts that explain the data. By
offering multiple alternative explanations, we allow the human expert to choose
the one that best aligns with their expectation. To demonstrate our method, we
show that it is able discover all possible concept representations on a
synthetic dataset. On EHR data, our model was able to identify 4 out of the 5
pre-defined concepts without supervision.},
 author = {Katrina Brown and Marton Havasi and Finale Doshi-Velez},
 citations = {0},
 comment = {Accepted to the ICML 2022 Workshop on Human-Machine Collaboration and
  Teaming},
 doi = {},
 eprint = {2412.18059v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Diverse Concept Proposals for Concept Bottleneck Models},
 url = {http://arxiv.org/abs/2412.18059v1},
 year = {2024}
}

@article{2412.18706v1,
 abstract = {Survival analysis (SA) models have been widely studied in mining electronic
health records (EHRs), particularly in forecasting the risk of critical
conditions for prioritizing high-risk patients. However, their vulnerability to
adversarial attacks is much less explored in the literature. Developing
black-box perturbation algorithms and evaluating their impact on
state-of-the-art survival models brings two benefits to medical applications.
First, it can effectively evaluate the robustness of models in pre-deployment
testing. Also, exploring how subtle perturbations would result in significantly
different outcomes can provide counterfactual insights into the clinical
interpretation of model prediction. In this work, we introduce SurvAttack, a
novel black-box adversarial attack framework leveraging subtle clinically
compatible, and semantically consistent perturbations on longitudinal EHRs to
degrade survival models' predictive performance. We specifically develop a
greedy algorithm to manipulate medical codes with various adversarial actions
throughout a patient's medical history. Then, these adversarial actions are
prioritized using a composite scoring strategy based on multi-aspect
perturbation quality, including saliency, perturbation stealthiness, and
clinical meaningfulness. The proposed adversarial EHR perturbation algorithm is
then used in an efficient SA-specific strategy to attack a survival model when
estimating the temporal ranking of survival urgency for patients. To
demonstrate the significance of our work, we conduct extensive experiments,
including baseline comparisons, explainability analysis, and case studies. The
experimental results affirm our research's effectiveness in illustrating the
vulnerabilities of patient survival models, model interpretation, and
ultimately contributing to healthcare quality.},
 author = {Mohsen Nayebi Kerdabadi and Arya Hadizadeh Moghaddam and Bin Liu and Mei Liu and Zijun Yao},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2412.18706v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SurvAttack: Black-Box Attack On Survival Models through Ontology-Informed EHR Perturbation},
 url = {http://arxiv.org/abs/2412.18706v1},
 year = {2024}
}

@article{2412.20733v1,
 abstract = {The purpose of this paper is to contribute towards the near-future
privacy-preserving big data analytical healthcare platforms, capable of
processing streamed or uploaded timeseries data or videos from patients. The
experimental work includes a real-life knee rehabilitation video dataset
capturing a set of exercises from simple and personalised to more general and
challenging movements aimed for returning to sport. To convert video from
mobile into privacy-preserving diagnostic timeseries data, we employed Google
MediaPipe pose estimation. The developed proof-of-concept algorithms can
augment knee exercise videos by overlaying the patient with stick figure
elements while updating generated timeseries plot with knee angle estimation
streamed as CSV file format. For patients and physiotherapists, video with
side-to-side timeseries visually indicating potential issues such as excessive
knee flexion or unstable knee movements or stick figure overlay errors is
possible by setting a-priori knee-angle parameters. To address adherence to
rehabilitation programme and quantify exercise sets and repetitions, our
adaptive algorithm can correctly identify (91.67%-100%) of all exercises from
side- and front-view videos. Transparent algorithm design for adaptive visual
analysis of various knee exercise patterns contributes towards the
interpretable AI and will inform near-future privacy-preserving, non-vendor
locking, open-source developments for both end-user computing devices and as
on-premises non-proprietary cloud platforms that can be deployed within the
national healthcare system.},
 author = {Boris Bačić and Claudiu Vasile and Chengwei Feng and Marian G. Ciucă},
 citations = {8},
 comment = {The original work citation: Ba\v{c}i\'c, B., Claudiu Vasile, Feng,
  C., & Ciuc\u{a}, M. G. (2024, 13-15 Dec.). Towards nation-wide analytical
  healthcare infrastructures: A privacy-preserving augmented knee
  rehabilitation case study. Presented at the Conference on Innovative
  Technologies in Intelligent Systems & Industrial Applications (CITISIA 2024),
  Sydney, NSW},
 doi = {},
 eprint = {2412.20733v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards nation-wide analytical healthcare infrastructures: A privacy-preserving augmented knee rehabilitation case study},
 url = {http://arxiv.org/abs/2412.20733v1},
 year = {2024}
}

@article{2501.00755v2,
 abstract = {Causal inference in observational studies with high-dimensional covariates
presents significant challenges. We introduce CausalBGM, an AI-powered Bayesian
generative modeling approach that captures the causal relationship among
covariates, treatment, and outcome variables. The core innovation of CausalBGM
lies in its ability to estimate the individual treatment effect (ITE) by
learning individual-specific distributions of a low-dimensional latent feature
set (e.g., latent confounders) that drives changes in both treatment and
outcome. This approach not only effectively mitigates confounding effects but
also provides comprehensive uncertainty quantification, offering reliable and
interpretable causal effect estimates at the individual level. CausalBGM adopts
a Bayesian model and uses a novel iterative algorithm to update the model
parameters and the posterior distribution of latent features until convergence.
This framework leverages the power of AI to capture complex dependencies among
variables while adhering to the Bayesian principles. Extensive experiments
demonstrate that CausalBGM consistently outperforms state-of-the-art methods,
particularly in scenarios with high-dimensional covariates and large-scale
datasets. Its Bayesian foundation ensures statistical rigor, providing robust
and well-calibrated posterior intervals. By addressing key limitations of
existing methods, CausalBGM emerges as a robust and promising framework for
advancing causal inference in modern applications in fields such as genomics,
healthcare, and social sciences. CausalBGM is maintained at the website
https://causalbgm.readthedocs.io/.},
 author = {Qiao Liu and Wing Hung Wong},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2501.00755v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {An AI-powered Bayesian generative modeling approach for causal inference in observational studies},
 url = {http://arxiv.org/abs/2501.00755v2},
 year = {2025}
}

@article{2501.01447v2,
 abstract = {The COVID-19 vaccine development, manufacturing, transportation, and
administration proved an extreme logistics operation of global magnitude.
Global vaccination levels, however, remain a key concern in preventing the
emergence of new strains and minimizing the impact of the pandemic's disruption
of daily life. In this paper, country-level vaccination rates are analyzed
through a queuing framework to extract service rates that represent the
practical capacity of a country to administer vaccines. These rates are further
characterized through regression and interpretable machine learning methods
with country-level demographic, governmental, and socio-economic variates.
Model results show that participation in multi-governmental collaborations such
as COVAX may improve the ability to vaccinate. Similarly, improved
transportation and accessibility variates such as roads per area for low-income
countries and rail lines per area for high-income countries can improve rates.
It was also found that for low-income countries specifically, improvements in
basic and health infrastructure (as measured through spending on healthcare,
number of doctors and hospital beds per 100k, population percent with access to
electricity, life expectancy, and vehicles per 1000 people) resulted in higher
vaccination rates. Of the high-income countries, those with larger 65-plus
populations struggled to vaccinate at high rates, indicating potential
accessibility issues for the elderly. This study finds that improving basic and
health infrastructure, focusing on accessibility in the last mile, particularly
for the elderly, and fostering global partnerships can improve logistical
operations of such a scale. Such structural impediments and inequities in
global health care must be addressed in preparation for future global public
health crises.},
 author = {Sharika J. Hegde and Max T. M. Ng and Marcos Rios and Hani S. Mahmassani and Ying Chen and Karen Smilowitz},
 citations = {0},
 comment = {Under consideration for more thorough analysis},
 doi = {},
 eprint = {2501.01447v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Analyzing Country-Level Vaccination Rates and Determinants of Practical Capacity to Administer COVID-19 Vaccines},
 url = {http://arxiv.org/abs/2501.01447v2},
 year = {2024}
}

@article{2501.01639v2,
 abstract = {The rapid integration of artificial intelligence (AI) in healthcare is
revolutionizing medical diagnostics, personalized medicine, and operational
efficiency. However, alongside these advancements, significant challenges arise
concerning patient data privacy, ethical considerations, and regulatory
compliance. This paper examines the dual impact of AI on healthcare,
highlighting its transformative potential and the critical need for
safeguarding sensitive health information. It explores the role of the Health
Insurance Portability and Accountability Act (HIPAA) as a regulatory framework
for ensuring data privacy and security, emphasizing the importance of robust
safeguards and ethical standards in AI-driven healthcare. Through case studies,
including AI applications in diabetic retinopathy, oncology, and the
controversies surrounding data sharing, this study underscores the ethical and
legal complexities of AI implementation. A balanced approach that fosters
innovation while maintaining patient trust and privacy is imperative. The
findings emphasize the importance of continuous education, transparency, and
adherence to regulatory frameworks to harness AI's full potential responsibly
and ethically in healthcare.},
 author = {Ahmad Momani},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2501.01639v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Implications of Artificial Intelligence on Health Data Privacy and Confidentiality},
 url = {http://arxiv.org/abs/2501.01639v2},
 year = {2025}
}

@article{2501.01984v1,
 abstract = {The AUTO-PCOS Classification Challenge seeks to advance the diagnostic
capabilities of artificial intelligence (AI) in identifying Polycystic Ovary
Syndrome (PCOS) through automated classification of healthy and unhealthy
ultrasound frames. This report outlines our methodology for building a robust
AI pipeline utilizing transfer learning with the InceptionV3 architecture to
achieve high accuracy in binary classification. Preprocessing steps ensured the
dataset was optimized for training, validation, and testing, while
interpretability methods like LIME and saliency maps provided valuable insights
into the model's decision-making. Our approach achieved an accuracy of 90.52%,
with precision, recall, and F1-score metrics exceeding 90% on validation data,
demonstrating its efficacy. The project underscores the transformative
potential of AI in healthcare, particularly in addressing diagnostic challenges
like PCOS. Key findings, challenges, and recommendations for future
enhancements are discussed, highlighting the pathway for creating reliable,
interpretable, and scalable AI-driven medical diagnostic tools.},
 author = {Atharva Divekar and Atharva Sonawane},
 citations = {1},
 comment = {Code available at: https://github.com/ATHdevs/Auto-PCOS},
 doi = {},
 eprint = {2501.01984v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Leveraging AI for Automatic Classification of PCOS Using Ultrasound Imaging},
 url = {http://arxiv.org/abs/2501.01984v1},
 year = {2024}
}

@article{2501.02087v2,
 abstract = {In domains such as finance, healthcare, and robotics, managing worst-case
scenarios is critical, as failure to do so can lead to catastrophic outcomes.
Distributional Reinforcement Learning (DRL) provides a natural framework to
incorporate risk sensitivity into decision-making processes. However, existing
approaches face two key limitations: (1) the use of fixed risk measures at each
decision step often results in overly conservative policies, and (2) the
interpretation and theoretical properties of the learned policies remain
unclear. While optimizing a static risk measure addresses these issues, its use
in the DRL framework has been limited to the simple static CVaR risk measure.
In this paper, we present a novel DRL algorithm with convergence guarantees
that optimizes for a broader class of static Spectral Risk Measures (SRM).
Additionally, we provide a clear interpretation of the learned policy by
leveraging the distribution of returns in DRL and the decomposition of static
coherent risk measures. Extensive experiments demonstrate that our model learns
policies aligned with the SRM objective, and outperforms existing risk-neutral
and risk-sensitive DRL models in various settings.},
 author = {Mehrdad Moghimi and Hyejin Ku},
 citations = {1},
 comment = {Accepted at ICML 2025},
 doi = {},
 eprint = {2501.02087v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Beyond CVaR: Leveraging Static Spectral Risk Measures for Enhanced Decision-Making in Distributional Reinforcement Learning},
 url = {http://arxiv.org/abs/2501.02087v2},
 year = {2025}
}

@article{2501.03058v1,
 abstract = {This paper explores foundational and applied aspects of survival analysis,
using fall risk assessment as a case study. It revisits key time-related
probability distributions and statistical methods, including logistic
regression, Poisson regression, Exponential regression, and the Cox
Proportional Hazards model, offering a unified perspective on their
relationships within the survival analysis framework. A contribution of this
work is the step-by-step derivation and clarification of the relationships
among these models, particularly demonstrating that Poisson regression in the
survival context is a specific case of the Cox model. These insights address
gaps in understanding and reinforce the simplicity and interpretability of
survival models. The paper also emphasizes the practical utility of survival
analysis by connecting theoretical insights with real-world applications. In
the context of fall detection, it demonstrates how these models can
simultaneously predict fall risk, analyze contributing factors, and estimate
time-to-event outcomes within a single streamlined framework. In contrast,
advanced deep learning methods often require complex post-hoc interpretation
and separate training for different tasks particularly when working with
structured numerical data. This highlights the enduring relevance of classical
statistical frameworks and makes survival models especially valuable in
healthcare settings, where explainability and robustness are critical. By
unifying foundational concepts and offering a cohesive perspective on
time-to-event analysis, this work serves as an accessible resource for
understanding survival models and applying them effectively to diverse
analytical challenges.},
 author = {Tianhua Chen},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2501.03058v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Survival Analysis Revisited: Understanding and Unifying Poisson, Exponential, and Cox Models in Fall Risk Analysis},
 url = {http://arxiv.org/abs/2501.03058v1},
 year = {2025}
}

@article{2501.03282v1,
 abstract = {Uncertainty quantification (UQ) is a critical aspect of artificial
intelligence (AI) systems, particularly in high-risk domains such as
healthcare, autonomous systems, and financial technology, where decision-making
processes must account for uncertainty. This review explores the evolution of
uncertainty quantification techniques in AI, distinguishing between aleatoric
and epistemic uncertainties, and discusses the mathematical foundations and
methods used to quantify these uncertainties. We provide an overview of
advanced techniques, including probabilistic methods, ensemble learning,
sampling-based approaches, and generative models, while also highlighting
hybrid approaches that integrate domain-specific knowledge. Furthermore, we
examine the diverse applications of UQ across various fields, emphasizing its
impact on decision-making, predictive accuracy, and system robustness. The
review also addresses key challenges such as scalability, efficiency, and
integration with explainable AI, and outlines future directions for research in
this rapidly developing area. Through this comprehensive survey, we aim to
provide a deeper understanding of UQ's role in enhancing the reliability,
safety, and trustworthiness of AI systems.},
 author = {Tianyang Wang and Yunze Wang and Jun Zhou and Benji Peng and Xinyuan Song and Charles Zhang and Xintian Sun and Qian Niu and Junyu Liu and Silin Chen and Keyu Chen and Ming Li and Pohsun Feng and Ziqian Bi and Ming Liu and Yichao Zhang and Cheng Fei and Caitlyn Heqi Yin and Lawrence KQ Yan},
 citations = {8},
 comment = {14 pages},
 doi = {},
 eprint = {2501.03282v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {From Aleatoric to Epistemic: Exploring Uncertainty Quantification Techniques in Artificial Intelligence},
 url = {http://arxiv.org/abs/2501.03282v1},
 year = {2025}
}

@article{2501.04206v2,
 abstract = {Explainable AI (XAI) in medical histopathology is essential for enhancing the
interpretability and clinical trustworthiness of deep learning models in cancer
diagnosis. However, the black-box nature of these models often limits their
clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue
Examination), a post-hoc explainable framework designed for breast cancer
tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach,
extracting patches at various magnification levels, constructing an
hierarchical graph, and utilising graph attention networks (GAT) with scalewise
attention (SAN) to capture scale-dependent features. We trained the model on
140 tumour TMA cores and four benign whole slide images from which 140 benign
samples were created, and tested it on 53 pathologist-annotated TMA samples.
GRAPHITE outperformed traditional XAI methods, achieving a mean average
precision (mAP) of 0.56, an area under the receiver operating characteristic
curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating
that the model maintains high performance across a wide range of thresholds. In
clinical utility, GRAPHITE achieved the highest area under the decision curve
(AUDC) of 4.17e+5, indicating reliable decision support across thresholds.
These results highlight GRAPHITE's potential as a clinically valuable tool in
computational pathology, providing interpretable visualisations that align with
the pathologists' diagnostic reasoning and support precision medicine.},
 author = {Raktim Kumar Mondol and Ewan K. A. Millar and Peter H. Graham and Lois Browne and Arcot Sowmya and Erik Meijering},
 citations = {},
 comment = {25 Pages, 10 Figures, 1 Tables},
 doi = {10.1016/j.compbiomed.2025.111106},
 eprint = {2501.04206v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced Explainability in Breast Cancer Histopathology},
 url = {http://arxiv.org/abs/2501.04206v2},
 year = {2025}
}

@article{2501.05617v1,
 abstract = {The use of AI in healthcare has the potential to improve patient care,
optimize clinical workflows, and enhance decision-making. However, bias, data
incompleteness, and inaccuracies in training datasets can lead to unfair
outcomes and amplify existing disparities. This research investigates the
current state of dataset documentation practices, focusing on their ability to
address these challenges and support ethical AI development. We identify
shortcomings in existing documentation methods, which limit the recognition and
mitigation of bias, incompleteness, and other issues in datasets. We propose
the 'Healthcare AI Datasheet' to address these gaps, a dataset documentation
framework that promotes transparency and ensures alignment with regulatory
requirements. Additionally, we demonstrate how it can be expressed in a
machine-readable format, facilitating its integration with datasets and
enabling automated risk assessments. The findings emphasise the importance of
dataset documentation in fostering responsible AI development.},
 author = {Marjia Siddik and Harshvardhan J. Pandit},
 citations = {},
 comment = {Irish Conference on Artificial Intelligence and Cognitive Science
  (AICS), December 2024, Ireland},
 doi = {10.31219/osf.io/69ykb},
 eprint = {2501.05617v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Datasheets for Healthcare AI: A Framework for Transparency and Bias Mitigation},
 url = {http://arxiv.org/abs/2501.05617v1},
 year = {2025}
}

@article{2501.06077v1,
 abstract = {Causal inference has recently gained notable attention across various fields
like biology, healthcare, and environmental science, especially within
explainable artificial intelligence (xAI) systems, for uncovering the causal
relationships among multiple variables and outcomes. Yet, it has not been fully
recognized and deployed in the manufacturing systems. In this paper, we
introduce an explainable, scalable, and flexible federated Bayesian learning
framework, \texttt{xFBCI}, designed to explore causality through treatment
effect estimation in distributed manufacturing systems. By leveraging federated
Bayesian learning, we efficiently estimate posterior of local parameters to
derive the propensity score for each client without accessing local private
data. These scores are then used to estimate the treatment effect using
propensity score matching (PSM). Through simulations on various datasets and a
real-world Electrohydrodynamic (EHD) printing data, we demonstrate that our
approach outperforms standard Bayesian causal inference methods and several
state-of-the-art federated learning benchmarks.},
 author = {Xiaofeng Xiao and Khawlah Alharbi and Pengyu Zhang and Hantang Qin and Xubo Yue},
 citations = {0},
 comment = {26 pages},
 doi = {10.1007/s10845-025-02665-7},
 eprint = {2501.06077v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable Federated Bayesian Causal Inference and Its Application in Advanced Manufacturing},
 url = {http://arxiv.org/abs/2501.06077v1},
 year = {2025}
}

@article{2501.06269v2,
 abstract = {OpenAI released version GPT-4 on March 14, 2023, following the success of
ChatGPT, which was announced in November 2022. In addition to the existing
GPT-3 features, GPT-4 can interpret images. To achieve this, the processing
power and model have been significantly improved. The ability to process and
interpret images goes far beyond the applications and effectiveness of
artificial intelligence. In this study, we first explored the interpretation of
radiological images in healthcare using artificial intelligence (AI). Then, we
experimented with the image interpretation capability of the GPT-4. In this
way, we addressed the question of whether artificial intelligence (AI) can
replace a healthcare professional (e.g., a medical doctor) or whether it can be
used as a decision-support tool that makes decisions easier and more reliable.
Our results showed that ChatGPT is not sufficient and accurate to analyze chest
X-ray images, but it can provide interpretations that can assist medical
doctors or clinicians.},
 author = {Omer Aydin and Enis Karaarslan},
 citations = {},
 comment = {},
 doi = {10.2139/ssrn.5090227},
 eprint = {2501.06269v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {OpenAI ChatGPT interprets Radiological Images: GPT-4 as a Medical Doctor for a Fast Check-Up},
 url = {http://arxiv.org/abs/2501.06269v2},
 year = {2025}
}

@article{2501.07109v1,
 abstract = {Visual Question Answering (VQA) is an interdisciplinary field that bridges
the gap between computer vision (CV) and natural language processing(NLP),
enabling Artificial Intelligence(AI) systems to answer questions about images.
Since its inception in 2015, VQA has rapidly evolved, driven by advances in
deep learning, attention mechanisms, and transformer-based models. This survey
traces the journey of VQA from its early days, through major breakthroughs,
such as attention mechanisms, compositional reasoning, and the rise of
vision-language pre-training methods. We highlight key models, datasets, and
techniques that shaped the development of VQA systems, emphasizing the pivotal
role of transformer architectures and multimodal pre-training in driving recent
progress. Additionally, we explore specialized applications of VQA in domains
like healthcare and discuss ongoing challenges, such as dataset bias, model
interpretability, and the need for common-sense reasoning. Lastly, we discuss
the emerging trends in large multimodal language models and the integration of
external knowledge, offering insights into the future directions of VQA. This
paper aims to provide a comprehensive overview of the evolution of VQA,
highlighting both its current state and potential advancements.},
 author = {Anupam Pandey and Deepjyoti Bodo and Arpan Phukan and Asif Ekbal},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2501.07109v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {The Quest for Visual Understanding: A Journey Through the Evolution of Visual Question Answering},
 url = {http://arxiv.org/abs/2501.07109v1},
 year = {2025}
}

@article{2501.07611v1,
 abstract = {Personalized cancer treatment is revolutionizing oncology by leveraging
precision medicine and advanced computational techniques to tailor therapies to
individual patients. Despite its transformative potential, challenges such as
limited generalizability, interpretability, and reproducibility of predictive
models hinder its integration into clinical practice. Current methodologies
often rely on black-box machine learning models, which, while accurate, lack
the transparency needed for clinician trust and real-world application. This
paper proposes the development of an innovative framework that bridges
Kolmogorov-Arnold Networks (KANs) and Evolutionary Game Theory (EGT) to address
these limitations. Inspired by the Kolmogorov-Arnold representation theorem,
KANs offer interpretable, edge-based neural architectures capable of modeling
complex biological systems with unprecedented adaptability. Their integration
into the EGT framework enables dynamic modeling of cancer progression and
treatment responses. By combining KAN's computational precision with EGT's
mechanistic insights, this hybrid approach promises to enhance predictive
accuracy, scalability, and clinical usability.},
 author = {Sepinoud Azimi and Louise Spekking and Kateřina Staňková},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2501.07611v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Kolmogorov-Arnold Networks and Evolutionary Game Theory for More Personalized Cancer Treatment},
 url = {http://arxiv.org/abs/2501.07611v1},
 year = {2025}
}

@article{2501.08169v1,
 abstract = {This study introduces an integrated approach to recognizing Arabic Sign
Language (ArSL) using state-of-the-art deep learning models such as
MobileNetV3, ResNet50, and EfficientNet-B2. These models are further enhanced
by explainable AI (XAI) techniques to boost interpretability. The ArSL2018 and
RGB Arabic Alphabets Sign Language (AASL) datasets are employed, with
EfficientNet-B2 achieving peak accuracies of 99.48\% and 98.99\%, respectively.
Key innovations include sophisticated data augmentation methods to mitigate
class imbalance, implementation of stratified 5-fold cross-validation for
better generalization, and the use of Grad-CAM for clear model decision
transparency. The proposed system not only sets new benchmarks in recognition
accuracy but also emphasizes interpretability, making it suitable for
applications in healthcare, education, and inclusive communication
technologies.},
 author = {Mazen Balat and Rewaa Awaad and Ahmed B. Zaky and Salah A. Aly},
 citations = {4},
 comment = {13 pages, 25 figures, 16 tables},
 doi = {10.1109/icdsns62112.2024.10690996},
 eprint = {2501.08169v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Revolutionizing Communication with Deep Learning and XAI for Enhanced Arabic Sign Language Recognition},
 url = {http://arxiv.org/abs/2501.08169v1},
 year = {2025}
}

@article{2501.09217v1,
 abstract = {Time series classification (TSC) is fundamental in numerous domains,
including finance, healthcare, and environmental monitoring. However,
traditional TSC methods often struggle with the inherent complexity and
variability of time series data. Building on our previous work with the linear
law-based transformation (LLT) - which improved classification accuracy by
transforming the feature space based on key data patterns - we introduce
adaptive law-based transformation (ALT). ALT enhances LLT by incorporating
variable-length shifted time windows, enabling it to capture distinguishing
patterns of various lengths and thereby handle complex time series more
effectively. By mapping features into a linearly separable space, ALT provides
a fast, robust, and transparent solution that achieves state-of-the-art
performance with only a few hyperparameters.},
 author = {Marcell T. Kurbucz and Balázs Hajós and Balázs P. Halmos and Vince Á. Molnár and Antal Jakovác},
 citations = {1},
 comment = {8 pages, 1 figure, 5 tables},
 doi = {},
 eprint = {2501.09217v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Adaptive Law-Based Transformation (ALT): A Lightweight Feature Representation for Time Series Classification},
 url = {http://arxiv.org/abs/2501.09217v1},
 year = {2025}
}

@article{2501.09967v1,
 abstract = {Artificial Intelligence (AI) has continued to achieve tremendous success in
recent times. However, the decision logic of these frameworks is often not
transparent, making it difficult for stakeholders to understand, interpret or
explain their behavior. This limitation hinders trust in machine learning
systems and causes a general reluctance towards their adoption in practical
applications, particularly in mission-critical domains like healthcare and
autonomous driving. Explainable AI (XAI) techniques facilitate the
explainability or interpretability of machine learning models, enabling users
to discern the basis of the decision and possibly avert undesirable behavior.
This comprehensive survey details the advancements of explainable AI methods,
from inherently interpretable models to modern approaches for achieving
interpretability of various black box models, including large language models
(LLMs). Additionally, we review explainable AI techniques that leverage LLM and
vision-language model (VLM) frameworks to automate or improve the
explainability of other machine learning models. The use of LLM and VLM as
interpretability methods particularly enables high-level, semantically
meaningful explanations of model decisions and behavior. Throughout the paper,
we highlight the scientific principles, strengths and weaknesses of
state-of-the-art methods and outline different areas of improvement. Where
appropriate, we also present qualitative and quantitative comparison results of
various methods to show how they compare. Finally, we discuss the key
challenges of XAI and directions for future research.},
 author = {Fuseini Mumuni and Alhassan Mumuni},
 citations = {16},
 comment = {},
 doi = {},
 eprint = {2501.09967v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable artificial intelligence (XAI): from inherent explainability to large language models},
 url = {http://arxiv.org/abs/2501.09967v1},
 year = {2025}
}

@article{2501.10453v1,
 abstract = {Bias in Foundation Models (FMs) - trained on vast datasets spanning societal
and historical knowledge - poses significant challenges for fairness and equity
across fields such as healthcare, education, and finance. These biases, rooted
in the overrepresentation of stereotypes and societal inequalities in training
data, exacerbate real-world discrimination, reinforce harmful stereotypes, and
erode trust in AI systems. To address this, we introduce Trident Probe Testing
(TriProTesting), a systematic testing method that detects explicit and implicit
biases using semantically designed probes. Here we show that FMs, including
CLIP, ALIGN, BridgeTower, and OWLv2, demonstrate pervasive biases across single
and mixed social attributes (gender, race, age, and occupation). Notably, we
uncover mixed biases when social attributes are combined, such as gender x
race, gender x age, and gender x occupation, revealing deeper layers of
discrimination. We further propose Adaptive Logit Adjustment
(AdaLogAdjustment), a post-processing technique that dynamically redistributes
probability power to mitigate these biases effectively, achieving significant
improvements in fairness without retraining models. These findings highlight
the urgent need for ethical AI practices and interdisciplinary solutions to
address biases not only at the model level but also in societal structures. Our
work provides a scalable and interpretable solution that advances fairness in
AI systems while offering practical insights for future research on fair AI
technologies.},
 author = {Shuzhou Sun and Li Liu and Yongxiang Liu and Zhen Liu and Shuanghui Zhang and Janne Heikkilä and Xiang Li},
 citations = {1},
 comment = {60 pages, 5 figures},
 doi = {},
 eprint = {2501.10453v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Uncovering Bias in Foundation Models: Impact, Testing, Harm, and Mitigation},
 url = {http://arxiv.org/abs/2501.10453v1},
 year = {2025}
}

@article{2501.10897v1,
 abstract = {We use a tensor unfolding technique to prove a new identifiability result for
discrete bipartite graphical models, which have a bipartite graph between an
observed and a latent layer. This model family includes popular models such as
Noisy-Or Bayesian networks for medical diagnosis and Restricted Boltzmann
Machines in machine learning. These models are also building blocks for deep
generative models. Our result on identifying the graph structure enjoys the
following nice properties. First, our identifiability proof is constructive, in
which we innovatively unfold the population tensor under the model into
matrices and inspect the rank properties of the resulting matrices to uncover
the graph. This proof itself gives a population-level structure learning
algorithm that outputs both the number of latent variables and the bipartite
graph. Second, we allow various forms of nonlinear dependence among the
variables, unlike many continuous latent variable graphical models that rely on
linearity to show identifiability. Third, our identifiability condition is
interpretable, only requiring each latent variable to connect to at least two
"pure" observed variables in the bipartite graph. The new result not only
brings novel advances in algebraic statistics, but also has useful implications
for these models' trustworthy applications in scientific disciplines and
interpretable machine learning.},
 author = {Yuqi Gu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2501.10897v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Unfolding Tensors to Identify the Graph in Discrete Latent Bipartite Graphical Models},
 url = {http://arxiv.org/abs/2501.10897v1},
 year = {2025}
}

@article{2501.11430v5,
 abstract = {Diffusion models (DMs) have emerged as a powerful class of generative AI
models, showing remarkable potential in anomaly detection (AD) tasks across
various domains, such as cybersecurity, fraud detection, healthcare, and
manufacturing. The intersection of these two fields, termed diffusion models
for anomaly detection (DMAD), offers promising solutions for identifying
deviations in increasingly complex and high-dimensional data. In this survey,
we review recent advances in DMAD research. We begin by presenting the
fundamental concepts of AD and DMs, followed by a comprehensive analysis of
classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further
categorize existing DMAD methods into reconstruction-based, density-based, and
hybrid approaches, providing detailed examinations of their methodological
innovations. We also explore the diverse tasks across different data
modalities, encompassing image, time series, video, and multimodal data
analysis. Furthermore, we discuss critical challenges and emerging research
directions, including computational efficiency, model interpretability,
robustness enhancement, edge-cloud collaboration, and integration with large
language models. The collection of DMAD research papers and resources is
available at https://github.com/fdjingliu/DMAD.},
 author = {Jing Liu and Zhenchao Ma and Zepu Wang and Chenxuanyin Zou and Jiayang Ren and Zehua Wang and Liang Song and Bo Hu and Yang Liu and Victor C. M. Leung},
 citations = {9},
 comment = {},
 doi = {},
 eprint = {2501.11430v5},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Survey on Diffusion Models for Anomaly Detection},
 url = {http://arxiv.org/abs/2501.11430v5},
 year = {2025}
}

@article{2501.11632v2,
 abstract = {Biomedical knowledge graphs (BKGs) have emerged as powerful tools for
organizing and leveraging the vast and complex data found across the biomedical
field. Yet, current reviews of BKGs often limit their scope to specific domains
or methods, overlooking the broader landscape and the rapid technological
progress reshaping it. In this survey, we address this gap by offering a
systematic review of BKGs from three core perspectives: domains, tasks, and
applications. We begin by examining how BKGs are constructed from diverse data
sources, including molecular interactions, pharmacological datasets, and
clinical records. Next, we discuss the essential tasks enabled by BKGs,
focusing on knowledge management, retrieval, reasoning, and interpretation.
Finally, we highlight real-world applications in precision medicine, drug
discovery, and scientific research, illustrating the translational impact of
BKGs across multiple sectors. By synthesizing these perspectives into a unified
framework, this survey not only clarifies the current state of BKG research but
also establishes a foundation for future exploration, enabling both innovative
methodological advances and practical implementations.},
 author = {Yuxing Lu and Sin Yee Goi and Xukai Zhao and Jinzhuo Wang},
 citations = {},
 comment = {45 pages, 4 figures, 3 tables. Updated figures},
 doi = {},
 eprint = {2501.11632v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Biomedical Knowledge Graph: A Survey of Domains, Tasks, and Real-World Applications},
 url = {http://arxiv.org/abs/2501.11632v2},
 year = {2025}
}

@article{2501.12706v1,
 abstract = {Explainability techniques hold significant potential for enhancing the causal
discovery process, which is crucial for understanding complex systems in areas
like healthcare, economics, and artificial intelligence. However, no causal
discovery methods currently incorporate explainability into their models to
derive causal graphs. Thus, in this paper we explore this innovative approach,
as it offers substantial potential and represents a promising new direction
worth investigating. Specifically, we introduce REX, a causal discovery method
that leverages machine learning (ML) models coupled with explainability
techniques, specifically Shapley values, to identify and interpret significant
causal relationships among variables.
  Comparative evaluations on synthetic datasets comprising continuous tabular
data reveal that REX outperforms state-of-the-art causal discovery methods
across diverse data generation processes, including non-linear and additive
noise models. Moreover, REX was tested on the Sachs single-cell
protein-signaling dataset, achieving a precision of 0.952 and recovering key
causal relationships with no incorrect edges. Taking together, these results
showcase REX's effectiveness in accurately recovering true causal structures
while minimizing false positive predictions, its robustness across diverse
datasets, and its applicability to real-world problems. By combining ML and
explainability techniques with causal discovery, REX bridges the gap between
predictive modeling and causal inference, offering an effective tool for
understanding complex causal structures. REX is publicly available at
https://github.com/renero/causalgraph.},
 author = {Jesus Renero and Idoia Ochoa and Roberto Maestre},
 citations = {0},
 comment = {22 pages, 30 figures, Submitted to Elsevier's Pattern Recognition},
 doi = {10.1016/j.patcog.2025.112491},
 eprint = {2501.12706v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {REX: Causal Discovery based on Machine Learning and Explainability techniques},
 url = {http://arxiv.org/abs/2501.12706v1},
 year = {2025}
}

@article{2501.13479v1,
 abstract = {Few-shot learning (FSL) enables machine learning models to generalize
effectively with minimal labeled data, making it crucial for data-scarce
domains such as healthcare, robotics, and natural language processing. Despite
its potential, FSL faces challenges including sensitivity to initialization,
difficulty in adapting to diverse domains, and vulnerability to noisy datasets.
To address these issues, this paper introduces Adaptive Few-Shot Learning
(AFSL), a framework that integrates advancements in meta-learning, domain
alignment, noise resilience, and multi-modal integration. AFSL consists of four
key modules: a Dynamic Stability Module for performance consistency, a
Contextual Domain Alignment Module for domain adaptation, a Noise-Adaptive
Resilience Module for handling noisy data, and a Multi-Modal Fusion Module for
integrating diverse modalities. This work also explores strategies such as
task-aware data augmentation, semi-supervised learning, and explainable AI
techniques to enhance the applicability and robustness of FSL. AFSL provides
scalable, reliable, and impactful solutions for real-world, high-stakes
domains.},
 author = {Rishabh Agrawal},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2501.13479v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility},
 url = {http://arxiv.org/abs/2501.13479v1},
 year = {2025}
}

@article{2501.13687v1,
 abstract = {Healthcare systems continuously generate vast amounts of electronic health
records (EHRs), commonly stored in the Fast Healthcare Interoperability
Resources (FHIR) standard. Despite the wealth of information in these records,
their complexity and volume make it difficult for users to retrieve and
interpret crucial health insights. Recent advances in Large Language Models
(LLMs) offer a solution, enabling semantic question answering (QA) over medical
data, allowing users to interact with their health records more effectively.
However, ensuring privacy and compliance requires edge and private deployments
of LLMs.
  This paper proposes a novel approach to semantic QA over EHRs by first
identifying the most relevant FHIR resources for a user query (Task1) and
subsequently answering the query based on these resources (Task2). We explore
the performance of privately hosted, fine-tuned LLMs, evaluating them against
benchmark models such as GPT-4 and GPT-4o. Our results demonstrate that
fine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by
0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we
examine advanced aspects of LLM usage, including sequential fine-tuning, model
self-evaluation (narcissistic evaluation), and the impact of training data size
on performance. The models and datasets are available here:
https://huggingface.co/genloop},
 author = {Sara Kothari and Ayush Gupta},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2501.13687v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Question Answering on Patient Medical Records with Private Fine-Tuned LLMs},
 url = {http://arxiv.org/abs/2501.13687v1},
 year = {2025}
}

@article{2501.13936v1,
 abstract = {Large Language Models (LLMs) have emerged as transformative tools in the
healthcare sector, demonstrating remarkable capabilities in natural language
understanding and generation. However, their proficiency in numerical
reasoning, particularly in high-stakes domains like in clinical applications,
remains underexplored. Numerical reasoning is critical in healthcare
applications, influencing patient outcomes, treatment planning, and resource
allocation. This study investigates the computational accuracy of LLMs in
numerical reasoning tasks within healthcare contexts. Using a curated dataset
of 1,000 numerical problems, encompassing real-world scenarios such as dosage
calculations and lab result interpretations, the performance of a refined LLM
based on the GPT-3 architecture was evaluated. The methodology includes prompt
engineering, integration of fact-checking pipelines, and application of
regularization techniques to enhance model accuracy and generalization. Key
metrics such as precision, recall, and F1-score were utilized to assess the
model's efficacy. The results indicate an overall accuracy of 84.10%, with
improved performance in straightforward numerical tasks and challenges in
multi-step reasoning. The integration of a fact-checking pipeline improved
accuracy by 11%, underscoring the importance of validation mechanisms. This
research highlights the potential of LLMs in healthcare numerical reasoning and
identifies avenues for further refinement to support critical decision-making
in clinical environments. The findings aim to contribute to the development of
reliable, interpretable, and contextually relevant AI tools for healthcare.},
 author = {Arjun R. Malghan},
 citations = {0},
 comment = {13 pages, 1 figure, 2 tables},
 doi = {},
 eprint = {2501.13936v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Evaluating Computational Accuracy of Large Language Models in Numerical Reasoning Tasks for Healthcare Applications},
 url = {http://arxiv.org/abs/2501.13936v1},
 year = {2025}
}

@article{2501.14985v1,
 abstract = {In today's interconnected society, social media platforms have become an
important part of our lives, where individuals virtually express their
thoughts, emotions, and moods. These expressions offer valuable insights into
their mental health. This paper explores the use of platforms like Facebook,
$\mathbb{X}$ (formerly Twitter), and Reddit for mental health assessments. We
propose a domain knowledge-infused residual attention model called DepressionX
for explainable depression severity detection. Existing deep learning models on
this problem have shown considerable performance, but they often lack
transparency in their decision-making processes. In healthcare, where decisions
are critical, the need for explainability is crucial. In our model, we address
the critical gap by focusing on the explainability of depression severity
detection while aiming for a high performance accuracy. In addition to being
explainable, our model consistently outperforms the state-of-the-art models by
over 7% in terms of $\text{F}_1$ score on balanced as well as imbalanced
datasets. Our ultimate goal is to establish a foundation for trustworthy and
comprehensible analysis of mental disorders via social media.},
 author = {Yusif Ibrahimov and Tarique Anwar and Tommy Yuan},
 citations = {3},
 comment = {The paper is accepted for AAAI Workshop on Health Intelligence
  (W3PHIAI-25)},
 doi = {},
 eprint = {2501.14985v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DepressionX: Knowledge Infused Residual Attention for Explainable Depression Severity Assessment},
 url = {http://arxiv.org/abs/2501.14985v1},
 year = {2025}
}

@article{2501.15973v1,
 abstract = {Healthcare decision-making requires not only accurate predictions but also
insights into how factors influence patient outcomes. While traditional Machine
Learning (ML) models excel at predicting outcomes, such as identifying high
risk patients, they are limited in addressing what-if questions about
interventions. This study introduces the Probabilistic Causal Fusion (PCF)
framework, which integrates Causal Bayesian Networks (CBNs) and Probability
Trees (PTrees) to extend beyond predictions. PCF leverages causal relationships
from CBNs to structure PTrees, enabling both the quantification of factor
impacts and simulation of hypothetical interventions. PCF was validated on
three real-world healthcare datasets i.e. MIMIC-IV, Framingham Heart Study, and
Diabetes, chosen for their clinically diverse variables. It demonstrated
predictive performance comparable to traditional ML models while providing
additional causal reasoning capabilities. To enhance interpretability, PCF
incorporates sensitivity analysis and SHapley Additive exPlanations (SHAP).
Sensitivity analysis quantifies the influence of causal parameters on outcomes
such as Length of Stay (LOS), Coronary Heart Disease (CHD), and Diabetes, while
SHAP highlights the importance of individual features in predictive modeling.
By combining causal reasoning with predictive modeling, PCF bridges the gap
between clinical intuition and data-driven insights. Its ability to uncover
relationships between modifiable factors and simulate hypothetical scenarios
provides clinicians with a clearer understanding of causal pathways. This
approach supports more informed, evidence-based decision-making, offering a
robust framework for addressing complex questions in diverse healthcare
settings.},
 author = {Sheresh Zahoor and Pietro Liò and Gaël Dias and Mohammed Hasanuzzaman},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2501.15973v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Integrating Probabilistic Trees and Causal Networks for Clinical and Epidemiological Data},
 url = {http://arxiv.org/abs/2501.15973v1},
 year = {2025}
}

@article{2501.16357v1,
 abstract = {The widespread use of artificial intelligence deep neural networks in fields
such as medicine and engineering necessitates understanding their
decision-making processes. Current explainability methods often produce
inconsistent results and struggle to highlight essential signals influencing
model inferences. This paper introduces the Evolutionary Independent
Deterministic Explanation (EVIDENCE) theory, a novel approach offering a
deterministic, model-independent method for extracting significant signals from
black-box models. EVIDENCE theory, grounded in robust mathematical
formalization, is validated through empirical tests on diverse datasets,
including COVID-19 audio diagnostics, Parkinson's disease voice recordings, and
the George Tzanetakis music classification dataset (GTZAN). Practical
applications of EVIDENCE include improving diagnostic accuracy in healthcare
and enhancing audio signal analysis. For instance, in the COVID-19 use case,
EVIDENCE-filtered spectrograms fed into a frozen Residual Network with 50
layers improved precision by 32% for positive cases and increased the area
under the curve (AUC) by 16% compared to baseline models. For Parkinson's
disease classification, EVIDENCE achieved near-perfect precision and
sensitivity, with a macro average F1-Score of 0.997. In the GTZAN, EVIDENCE
maintained a high AUC of 0.996, demonstrating its efficacy in filtering
relevant features for accurate genre classification. EVIDENCE outperformed
other Explainable Artificial Intelligence (XAI) methods such as LIME, SHAP, and
GradCAM in almost all metrics. These findings indicate that EVIDENCE not only
improves classification accuracy but also provides a transparent and
reproducible explanation mechanism, crucial for advancing the trustworthiness
and applicability of AI systems in real-world settings.},
 author = {Vincenzo Dentamaro and Paolo Giglio and Donato Impedovo and Giuseppe Pirlo},
 citations = {},
 comment = {20 pages, 4 figures},
 doi = {10.1016/j.engappai.2025.111008},
 eprint = {2501.16357v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {EVolutionary Independent DEtermiNistiC Explanation},
 url = {http://arxiv.org/abs/2501.16357v1},
 year = {2025}
}

@article{2501.16627v1,
 abstract = {As reliance on AI systems for decision-making grows, it becomes critical to
ensure that human users can appropriately balance trust in AI suggestions with
their own judgment, especially in high-stakes domains like healthcare. However,
human + AI teams have been shown to perform worse than AI alone, with evidence
indicating automation bias as the reason for poorer performance, particularly
because humans tend to follow AI's recommendations even when they are
incorrect. In many existing human + AI systems, decision-making support is
typically provided in the form of text explanations (XAI) to help users
understand the AI's reasoning. Since human decision-making often relies on
System 1 thinking, users may ignore or insufficiently engage with the
explanations, leading to poor decision-making. Previous research suggests that
there is a need for new approaches that encourage users to engage with the
explanations and one proposed method is the use of cognitive forcing functions
(CFFs). In this work, we examine how various decision-support mechanisms impact
user engagement, trust, and human-AI collaborative task performance in a
diabetes management decision-making scenario. In a controlled experiment with
108 participants, we evaluated the effects of six decision-support mechanisms
split into two categories of explanations (text, visual) and four CFFs. Our
findings reveal that mechanisms like AI confidence levels, text explanations,
and performance visualizations enhanced human-AI collaborative task
performance, and improved trust when AI reasoning clues were provided.
Mechanisms like human feedback and AI-driven questions encouraged deeper
reflection but often reduced task performance by increasing cognitive effort,
which in turn affected trust. Simple mechanisms like visual explanations had
little effect on trust, highlighting the importance of striking a balance in
CFF and XAI design.},
 author = {Zichen Chen and Yunhao Luo and Misha Sra},
 citations = {2},
 comment = {36 pages, 6 figures, 6 tables. Preprint version},
 doi = {},
 eprint = {2501.16627v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Engaging with AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision-Making},
 url = {http://arxiv.org/abs/2501.16627v1},
 year = {2025}
}

@article{2501.16693v1,
 abstract = {Artificial Intelligence (AI) has demonstrated potential in healthcare,
particularly in enhancing diagnostic accuracy and decision-making through
Clinical Decision Support Systems (CDSSs). However, the successful
implementation of these systems relies on user trust and reliance, which can be
influenced by explainable AI. This study explores the impact of varying
explainability levels on clinicians trust, cognitive load, and diagnostic
performance in breast cancer detection. Utilizing an interrupted time series
design, we conducted a web-based experiment involving 28 healthcare
professionals. The results revealed that high confidence scores substantially
increased trust but also led to overreliance, reducing diagnostic accuracy. In
contrast, low confidence scores decreased trust and agreement while increasing
diagnosis duration, reflecting more cautious behavior. Some explainability
features influenced cognitive load by increasing stress levels. Additionally,
demographic factors such as age, gender, and professional role shaped
participants' perceptions and interactions with the system. This study provides
valuable insights into how explainability impact clinicians' behavior and
decision-making. The findings highlight the importance of designing AI-driven
CDSSs that balance transparency, usability, and cognitive demands to foster
trust and improve integration into clinical workflows.},
 author = {Olya Rezaeian and Alparslan Emrah Bayrak and Onur Asan},
 citations = {},
 comment = {},
 doi = {10.1080/10447318.2025.2539458},
 eprint = {2501.16693v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainability and AI Confidence in Clinical Decision Support Systems: Effects on Trust, Diagnostic Performance, and Cognitive Load in Breast Cancer Care},
 url = {http://arxiv.org/abs/2501.16693v1},
 year = {2025}
}

@article{2501.17726v2,
 abstract = {As artificial intelligence (AI) becomes increasingly central to healthcare,
the demand for explainable and trustworthy models is paramount. Current report
generation systems for chest X-rays (CXR) often lack mechanisms for validating
outputs without expert oversight, raising concerns about reliability and
interpretability. To address these challenges, we propose a novel multimodal
framework designed to enhance the semantic alignment and localization accuracy
of AI-generated medical reports. Our framework integrates two key modules: a
Phrase Grounding Model, which identifies and localizes pathologies in CXR
images based on textual prompts, and a Text-to-Image Diffusion Module, which
generates synthetic CXR images from prompts while preserving anatomical
fidelity. By comparing features between the original and generated images, we
introduce a dual-scoring system: one score quantifies localization accuracy,
while the other evaluates semantic consistency. This approach significantly
outperforms existing methods, achieving state-of-the-art results in pathology
localization and text-to-image alignment. The integration of phrase grounding
with diffusion models, coupled with the dual-scoring evaluation system,
provides a robust mechanism for validating report quality, paving the way for
more trustworthy and transparent AI in medical imaging.},
 author = {Sayeh Gholipour Picha and Dawood Al Chanti and Alice Caplier},
 citations = {0},
 comment = {},
 doi = {10.1016/j.mlwa.2025.100684},
 eprint = {2501.17726v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies in Generated Report Without Human Feedback},
 url = {http://arxiv.org/abs/2501.17726v2},
 year = {2025}
}

@article{2501.18071v2,
 abstract = {Diabetes mellitus (DM) is a global health issue of significance that must be
diagnosed as early as possible and managed well. This study presents a
framework for diabetes prediction using Machine Learning (ML) models,
complemented with eXplainable Artificial Intelligence (XAI) tools, to
investigate both the predictive accuracy and interpretability of the
predictions from ML models. Data Preprocessing is based on the Synthetic
Minority Oversampling Technique (SMOTE) and feature scaling used on the
Diabetes Binary Health Indicators dataset to deal with class imbalance and
variability of clinical features. The ensemble model provided high accuracy,
with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General
Health, Income, and Physical Activity were the most influential predictors
obtained from the model explanations. The results of this study suggest that ML
combined with XAI is a promising means of developing accurate and
computationally transparent tools for use in healthcare systems.},
 author = {Pir Bakhsh Khokhar and Viviana Pentangelo and Fabio Palomba and Carmine Gravino},
 citations = {2},
 comment = {},
 doi = {10.61356/j.scin.2024.1306},
 eprint = {2501.18071v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence},
 url = {http://arxiv.org/abs/2501.18071v2},
 year = {2025}
}

@article{2502.00025v4,
 abstract = {Importance: Emergency department (ED) returns for mental health conditions
pose a major healthcare burden, with 24-27% of patients returning within 30
days. Traditional machine learning models for predicting these returns often
lack interpretability for clinical use.
  Objective: To assess whether integrating large language models (LLMs) with
machine learning improves predictive accuracy and clinical interpretability of
ED mental health return risk models.
  Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904
unique mental health patients at an academic medical center in the Deep South
from January 2018 to December 2022.
  Main Outcomes and Measures: Two primary outcomes were evaluated: (1) 30-day
ED return prediction accuracy and (2) model interpretability using a novel
LLM-enhanced framework integrating SHAP (SHapley Additive exPlanations) values
with clinical knowledge.
  Results: For chief complaint classification, LLaMA 3 (8B) with 10-shot
learning outperformed traditional models (accuracy: 0.882, F1-score: 0.86). In
SDoH classification, LLM-based models achieved 0.95 accuracy and 0.96 F1-score,
with Alcohol, Tobacco, and Substance Abuse performing best (F1: 0.96-0.89),
while Exercise and Home Environment showed lower performance (F1: 0.70-0.67).
The LLM-based interpretability framework achieved 99% accuracy in translating
model predictions into clinically relevant explanations. LLM-extracted features
improved XGBoost AUC from 0.74 to 0.76 and AUC-PR from 0.58 to 0.61.
  Conclusions and Relevance: Integrating LLMs with machine learning models
yielded modest but consistent accuracy gains while significantly enhancing
interpretability through automated, clinically relevant explanations. This
approach provides a framework for translating predictive analytics into
actionable clinical insights.},
 author = {Abdulaziz Ahmed and Mohammad Saleem and Mohammed Alzeen and Badari Birur and Rachel E Fargason and Bradley G Burk and Ahmed Alhassan and Mohammed Ali Al-Garadi},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.00025v4},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable AI for Mental Health Emergency Returns: Integrating LLMs with Predictive Modeling},
 url = {http://arxiv.org/abs/2502.00025v4},
 year = {2025}
}

@article{2502.00568v3,
 abstract = {Emerging research has highlighted that artificial intelligence based
multimodal fusion of digital pathology and transcriptomic features can improve
cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction.
However, such direct fusion for joint decision is impractical in real clinical
settings, where histopathology is still the gold standard for diagnosis and
transcriptomic tests are rarely requested, at least in the public healthcare
system. With our novel diffusion based crossmodal generative AI model PathGen,
we show that genomic expressions synthesized from digital histopathology
jointly predicts cancer grading and patient survival risk with high accuracy
(state-of-the-art performance), certainty (through conformal coverage
guarantee) and interpretability (through distributed attention maps). PathGen
code is available for open use by the research community through GitHub at
https://github.com/Samiran-Dey/PathGen.},
 author = {Samiran Dey and Christopher R. S. Banerji and Partha Basuchowdhuri and Sanjoy K. Saha and Deepak Parashar and Tapabrata Chakraborti},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2502.00568v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions},
 url = {http://arxiv.org/abs/2502.00568v3},
 year = {2025}
}

@article{2502.00837v2,
 abstract = {Natural Language Processing (NLP) has become a cornerstone in many critical
sectors, including healthcare, finance, and customer relationship management.
This is especially true with the development and use of advanced models such as
GPT-based architectures and BERT, which are widely used in decision-making
processes. However, the black-box nature of these advanced NLP models has
created an urgent need for transparency and explainability. This review
explores explainable NLP (XNLP) with a focus on its practical deployment and
real-world applications, examining its implementation and the challenges faced
in domain-specific contexts. The paper underscores the importance of
explainability in NLP and provides a comprehensive perspective on how XNLP can
be designed to meet the unique demands of various sectors, from healthcare's
need for clear insights to finance's emphasis on fraud detection and risk
assessment. Additionally, this review aims to bridge the knowledge gap in XNLP
literature by offering a domain-specific exploration and discussing
underrepresented areas such as real-world applicability, metric evaluation, and
the role of human interaction in model assessment. The paper concludes by
suggesting future research directions that could enhance the understanding and
broader application of XNLP.},
 author = {Hadi Mohammadi and Ayoub Bagheri and Anastasia Giachanou and Daniel L. Oberski},
 citations = {9},
 comment = {},
 doi = {},
 eprint = {2502.00837v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainability in Practice: A Survey of Explainable NLP Across Various Domains},
 url = {http://arxiv.org/abs/2502.00837v2},
 year = {2025}
}

@article{2502.01012v1,
 abstract = {Recent technological advances have introduced new high-throughput methods for
studying host-virus interactions, but testing synergistic interactions between
host gene pairs during infection remains relatively slow and labor intensive.
Identification of multiple gene knockdowns that effectively inhibit viral
replication requires a search over the combinatorial space of all possible
target gene pairs and is infeasible via brute-force experiments. Although
active learning methods for sequential experimental design have shown promise,
existing approaches have generally been restricted to single-gene knockdowns or
small-scale double knockdown datasets. In this study, we present an integrated
Deep Active Learning (DeepAL) framework that incorporates information from a
biological knowledge graph (SPOKE, the Scalable Precision Medicine Open
Knowledge Engine) to efficiently search the configuration space of a large
dataset of all pairwise knockdowns of 356 human genes in HIV infection. Through
graph representation learning, the framework is able to generate task-specific
representations of genes while also balancing the exploration-exploitation
trade-off to pinpoint highly effective double-knockdown pairs. We additionally
present an ensemble method for uncertainty quantification and an interpretation
of the gene pairs selected by our algorithm via pathway analysis. To our
knowledge, this is the first work to show promising results on double-gene
knockdown experimental data of appreciable scale (356 by 356 matrix).},
 author = {Haonan Zhu and Mary Silva and Jose Cadena and Braden Soper and Michał Lisicki and Braian Peetoom and Sergio E. Baranzini and Shivshankar Sundaram and Priyadip Ray and Jeff Drocco},
 citations = {},
 comment = {},
 doi = {10.1093/bioadv/vbaf228},
 eprint = {2502.01012v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Deep Active Learning based Experimental Design to Uncover Synergistic Genetic Interactions for Host Targeted Therapeutics},
 url = {http://arxiv.org/abs/2502.01012v1},
 year = {2025}
}

@article{2502.01677v2,
 abstract = {AI Scaling has traditionally been synonymous with Scaling Up, which builds
larger and more powerful models. However, the growing demand for efficiency,
adaptability, and collaboration across diverse applications necessitates a
broader perspective. This position paper presents a holistic framework for AI
scaling, encompassing Scaling Up, Scaling Down, and Scaling Out. It argues that
while Scaling Up of models faces inherent bottlenecks, the future trajectory of
AI scaling lies in Scaling Down and Scaling Out. These paradigms address
critical technical and societal challenges, such as reducing carbon footprint,
ensuring equitable access, and enhancing cross-domain collaboration. We explore
transformative applications in healthcare, smart manufacturing, and content
creation, demonstrating how AI Scaling can enable breakthroughs in efficiency,
personalization, and global connectivity. Additionally, we highlight key
challenges, including balancing model complexity with interpretability,
managing resource constraints, and fostering ethical development. By
synthesizing these approaches, we propose a unified roadmap that redefines the
future of AI research and application, paving the way for advancements toward
Artificial General Intelligence (AGI).},
 author = {Yunke Wang and Yanxi Li and Chang Xu},
 citations = {},
 comment = {ICML 2025},
 doi = {},
 eprint = {2502.01677v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Position: AI Scaling: From Up to Down and Out},
 url = {http://arxiv.org/abs/2502.01677v2},
 year = {2025}
}

@article{2502.01689v1,
 abstract = {The rise of single-cell sequencing technologies has revolutionized the
exploration of drug resistance, revealing the crucial role of cellular
heterogeneity in advancing precision medicine. By building computational models
from existing single-cell drug response data, we can rapidly annotate cellular
responses to drugs in subsequent trials. To this end, we developed scGSDR, a
model that integrates two computational pipelines grounded in the knowledge of
cellular states and gene signaling pathways, both essential for understanding
biological gene semantics. scGSDR enhances predictive performance by
incorporating gene semantics and employs an interpretability module to identify
key pathways contributing to drug resistance phenotypes. Our extensive
validation, which included 16 experiments covering 11 drugs, demonstrates
scGSDR's superior predictive accuracy, when trained with either bulk-seq or
scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's
application has extended from single-drug predictions to scenarios involving
drug combinations. Leveraging pathways of known drug target genes, we found
that scGSDR's cell-pathway attention scores are biologically interpretable,
which helped us identify other potential drug-related genes. Literature review
of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,
and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for
Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating
gene semantics, enhances predictive modeling of cellular responses to diverse
drugs, proving invaluable for scenarios involving both single drug and
combination therapies and effectively identifying key resistance-related
pathways, thus advancing precision medicine and targeted therapy development.},
 author = {Yu-An Huang and Xiyue Cao and Zhu-Hong You and Yue-Chao Li and Xuequn Shang and Zhi-An Huang},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.01689v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological Profiling},
 url = {http://arxiv.org/abs/2502.01689v1},
 year = {2025}
}

@article{2502.01702v2,
 abstract = {Inferring physical laws from data is a central challenge in science and
engineering, including but not limited to healthcare, physical sciences,
biosciences, social sciences, sustainability, climate, and robotics. Deep
networks offer high-accuracy results but lack interpretability, prompting
interest in models built from simple components. The Sparse Identification of
Nonlinear Dynamics (SINDy) method has become the go-to approach for building
such modular and interpretable models. SINDy leverages sparse regression with
L1 regularization to identify key terms from a library of candidate functions.
However, SINDy's choice of candidate library and optimization method requires
significant technical expertise, limiting its widespread applicability. This
work introduces Al-Khwarizmi, a novel agentic framework for physical law
discovery from data, which integrates foundational models with SINDy.
Leveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach
automates physical law discovery, incorporating prior knowledge and iteratively
refining candidate solutions via reflection. Al-Khwarizmi operates in two
steps: it summarizes system observations-comprising textual descriptions, raw
data, and plots-followed by a secondary step that generates candidate feature
libraries and optimizer configurations to identify hidden physics laws
correctly. Evaluating our algorithm on over 198 models, we demonstrate
state-of-the-art performance compared to alternatives, reaching a 20 percent
increase against the best-performing alternative.},
 author = {Christopher E. Mower and Haitham Bou-Ammar},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.01702v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Al-Khwarizmi: Discovering Physical Laws with Foundation Models},
 url = {http://arxiv.org/abs/2502.01702v2},
 year = {2025}
}

@article{2502.02438v1,
 abstract = {Medical multimodal large language models (MLLMs) are becoming an instrumental
part of healthcare systems, assisting medical personnel with decision making
and results analysis. Models for radiology report generation are able to
interpret medical imagery, thus reducing the workload of radiologists. As
medical data is scarce and protected by privacy regulations, medical MLLMs
represent valuable intellectual property. However, these assets are potentially
vulnerable to model stealing, where attackers aim to replicate their
functionality via black-box access. So far, model stealing for the medical
domain has focused on classification; however, existing attacks are not
effective against MLLMs. In this paper, we introduce Adversarial Domain
Alignment (ADA-STEAL), the first stealing attack against medical MLLMs.
ADA-STEAL relies on natural images, which are public and widely available, as
opposed to their medical counterparts. We show that data augmentation with
adversarial noise is sufficient to overcome the data distribution gap between
natural images and the domain-specific distribution of the victim MLLM.
Experiments on the IU X-RAY and MIMIC-CXR radiology datasets demonstrate that
Adversarial Domain Alignment enables attackers to steal the medical MLLM
without any access to medical data.},
 author = {Yaling Shen and Zhixiong Zhuang and Kun Yuan and Maria-Irina Nicolae and Nassir Navab and Nicolas Padoy and Mario Fritz},
 citations = {},
 comment = {Accepted at AAAI 2025},
 doi = {10.1609/aaai.v39i7.32734},
 eprint = {2502.02438v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Medical Multimodal Model Stealing Attacks via Adversarial Domain Alignment},
 url = {http://arxiv.org/abs/2502.02438v1},
 year = {2025}
}

@article{2502.02618v1,
 abstract = {The rapid aging of the global population has highlighted the need for
technologies to support elderly, particularly in healthcare and emotional
well-being. Facial expression recognition (FER) systems offer a non-invasive
means of monitoring emotional states, with applications in assisted living,
mental health support, and personalized care. This study presents a systematic
review of deep learning-based FER systems, focusing on their applications for
the elderly population. Following a rigorous methodology, we analyzed 31
studies published over the last decade, addressing challenges such as the
scarcity of elderly-specific datasets, class imbalances, and the impact of
age-related facial expression differences. Our findings show that convolutional
neural networks remain dominant in FER, and especially lightweight versions for
resource-constrained environments. However, existing datasets often lack
diversity in age representation, and real-world deployment remains limited.
Additionally, privacy concerns and the need for explainable artificial
intelligence emerged as key barriers to adoption. This review underscores the
importance of developing age-inclusive datasets, integrating multimodal
solutions, and adopting XAI techniques to enhance system usability,
reliability, and trustworthiness. We conclude by offering recommendations for
future research to bridge the gap between academic progress and real-world
implementation in elderly care.},
 author = {F. Xavier Gaya-Morey and Jose M. Buades-Rubio and Philippe Palanque and Raquel Lacuesta and Cristina Manresa-Yee},
 citations = {},
 comment = {},
 doi = {10.2139/ssrn.4884262},
 eprint = {2502.02618v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review},
 url = {http://arxiv.org/abs/2502.02618v1},
 year = {2025}
}

@article{2502.02786v1,
 abstract = {Personalization in machine learning involves tailoring models to individual
users by incorporating personal attributes such as demographic or medical data.
While personalization can improve prediction accuracy, it may also amplify
biases and reduce explainability. This work introduces a unified framework to
evaluate the impact of personalization on both prediction accuracy and
explanation quality across classification and regression tasks. We derive novel
upper bounds for the number of personal attributes that can be used to reliably
validate benefits of personalization. Our analysis uncovers key trade-offs. We
show that regression models can potentially utilize more personal attributes
than classification models. We also demonstrate that improvements in prediction
accuracy due to personalization do not necessarily translate to enhanced
explainability -- underpinning the importance to evaluate both metrics when
personalizing machine learning models in critical settings such as healthcare.
Validated with a real-world dataset, this framework offers practical guidance
for balancing accuracy, fairness, and interpretability in personalized models.},
 author = {Louisa Cornelis and Guillermo Bernárdez and Haewon Jeong and Nina Miolane},
 citations = {0},
 comment = {35 pages, 9 figures, submitted to ICML 2025},
 doi = {},
 eprint = {2502.02786v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {When Machine Learning Gets Personal: Understanding Fairness of Personalized Models},
 url = {http://arxiv.org/abs/2502.02786v1},
 year = {2025}
}

@article{2502.03895v1,
 abstract = {Fuzzy rule-based systems interpret data in low-dimensional domains, providing
transparency and interpretability. In contrast, deep learning excels in complex
tasks like image and speech recognition but is prone to overfitting in sparse,
unstructured, or low-dimensional data. This interpretability is crucial in
fields like healthcare and finance. Traditional rule-based systems, especially
ANFIS with grid partitioning, suffer from exponential rule growth as
dimensionality increases. We propose a strategic rule-reduction model that
applies Principal Component Analysis (PCA) on normalized firing strengths to
obtain linearly uncorrelated components. Binary Particle Swarm Optimization
(BPSO) selectively refines these components, significantly reducing the number
of rules while preserving precision in decision-making. A custom parameter
update mechanism fine-tunes specific ANFIS layers by dynamically adjusting BPSO
parameters, avoiding local minima. We validated our approach on standard UCI
respiratory, keel classification, regression datasets, and a real-world
ischemic stroke dataset, demonstrating adaptability and practicality. Results
indicate fewer rules, shorter training, and high accuracy, underscoring the
methods effectiveness for low-dimensional interpretability and complex data
scenarios. This synergy of fuzzy logic and optimization fosters robust
solutions. Our method contributes a powerful framework for interpretable AI in
multiple domains. It addresses dimensionality, ensuring a rule base.},
 author = {Afnan Al-Ali and Uvais Qidwai},
 citations = {},
 comment = {41 pages, 9 figures},
 doi = {10.2139/ssrn.4789178},
 eprint = {2502.03895v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Rule-Based Modeling of Low-Dimensional Data with PCA and Binary Particle Swarm Optimization (BPSO) in ANFIS},
 url = {http://arxiv.org/abs/2502.03895v1},
 year = {2025}
}

@article{2502.03952v1,
 abstract = {From medical diagnosis to autonomous vehicles, critical applications rely on
the integration of multiple heterogeneous data modalities. Multimodal
Variational Autoencoders offer versatile and scalable methods for generating
unobserved modalities from observed ones. Recent models using
mixturesof-experts aggregation suffer from theoretically grounded limitations
that restrict their generation quality on complex datasets. In this article, we
propose a novel interpretable model able to learn both joint and conditional
distributions without introducing mixture aggregation. Our model follows a
multistage training process: first modeling the joint distribution with
variational inference and then modeling the conditional distributions with
Normalizing Flows to better approximate true posteriors. Importantly, we also
propose to extract and leverage the information shared between modalities to
improve the conditional coherence of generated samples. Our method achieves
state-of-the-art results on several benchmark datasets.},
 author = {Agathe Senellart and Stéphanie Allassonnière},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.03952v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Bridging the inference gap in Mutimodal Variational Autoencoders},
 url = {http://arxiv.org/abs/2502.03952v1},
 year = {2025}
}

@article{2502.04356v1,
 abstract = {In response to the success of proprietary Large Language Models (LLMs) such
as OpenAI's GPT-4, there is a growing interest in developing open,
non-proprietary LLMs and AI foundation models (AIFMs) for transparent use in
academic, scientific, and non-commercial applications. Despite their inability
to match the refined functionalities of their proprietary counterparts, open
models hold immense potential to revolutionize healthcare applications. In this
paper, we examine the prospects of open-source LLMs and AIFMs for developing
healthcare applications and make two key contributions. Firstly, we present a
comprehensive survey of the current state-of-the-art open-source healthcare
LLMs and AIFMs and introduce a taxonomy of these open AIFMs, categorizing their
utility across various healthcare tasks. Secondly, to evaluate the
general-purpose applications of open LLMs in healthcare, we present a case
study on personalized prescriptions. This task is particularly significant due
to its critical role in delivering tailored, patient-specific medications that
can greatly improve treatment outcomes. In addition, we compare the performance
of open-source models with proprietary models in settings with and without
Retrieval-Augmented Generation (RAG). Our findings suggest that, although less
refined, open LLMs can achieve performance comparable to proprietary models
when paired with grounding techniques such as RAG. Furthermore, to highlight
the clinical significance of LLMs-empowered personalized prescriptions, we
perform subjective assessment through an expert clinician. We also elaborate on
ethical considerations and potential risks associated with the misuse of
powerful LLMs and AIFMs, highlighting the need for a cautious and responsible
implementation in healthcare.},
 author = {Mahdi Alkaeed and Sofiat Abioye and Adnan Qayyum and Yosra Magdi Mekki and Ilhem Berrou and Mohamad Abdallah and Ala Al-Fuqaha and Muhammad Bilal and Junaid Qadir},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.04356v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Open Foundation Models in Healthcare: Challenges, Paradoxes, and Opportunities with GenAI Driven Personalized Prescription},
 url = {http://arxiv.org/abs/2502.04356v1},
 year = {2025}
}

@article{2502.04658v2,
 abstract = {The integration of human and artificial intelligence offers a powerful avenue
for advancing our understanding of information processing, as each system
provides unique computational insights. However, despite the promise of
human-AI integration, current AI models are largely trained on massive
datasets, optimized for population-level performance, lacking mechanisms to
align their computations with individual users' perceptual semantics and neural
dynamics. Here we show that integrating human behavioral insights and
millisecond scale neural data within a fine tuned CLIP based model not only
captures generalized and individualized aspects of perception but also over
doubles behavioral performance compared to the unmodified CLIP baseline. By
embedding human inductive biases and mirroring dynamic neural processes during
training, personalized neural fine tuning improves predictions of human
similarity judgments and tracks the temporal evolution of individual neural
responses. Our work establishes a novel, interpretable framework for designing
adaptive AI systems, with broad implications for neuroscience, personalized
medicine, and human-computer interaction.},
 author = {Stephen Chong Zhao and Yang Hu and Jason Lee and Andrew Bender and Trisha Mazumdar and Mark Wallace and David A. Tovar},
 citations = {},
 comment = {7 Figures, 3 Tables, 3 Supplemental Figures, 1 Supplemental Table},
 doi = {},
 eprint = {2502.04658v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Shifting Attention to You: Personalized Brain-Inspired AI Models},
 url = {http://arxiv.org/abs/2502.04658v2},
 year = {2025}
}

@article{2502.04695v1,
 abstract = {This position paper emphasizes the critical gap in the evaluation of
Explainable AI (XAI) due to the lack of standardized and reliable metrics,
which diminishes its practical value, trustworthiness, and ability to meet
regulatory requirements. Current evaluation methods are often fragmented,
subjective, and biased, making them prone to manipulation and complicating the
assessment of complex models. A central issue is the absence of a ground truth
for explanations, complicating comparisons across various XAI approaches. To
address these challenges, we advocate for widespread research into developing
robust, context-sensitive evaluation metrics. These metrics should be resistant
to manipulation, relevant to each use case, and based on human judgment and
real-world applicability. We also recommend creating domain-specific evaluation
benchmarks that align with the user and regulatory needs of sectors such as
healthcare and finance. By encouraging collaboration among academia, industry,
and regulators, we can create standards that balance flexibility and
consistency, ensuring XAI explanations are meaningful, trustworthy, and
compliant with evolving regulations.},
 author = {Pratinav Seth and Vinay Kumar Sankarapu},
 citations = {2},
 comment = {},
 doi = {},
 eprint = {2502.04695v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Bridging the Gap in XAI-Why Reliable Metrics Matter for Explainability and Compliance},
 url = {http://arxiv.org/abs/2502.04695v1},
 year = {2025}
}

@article{2502.05879v1,
 abstract = {Depression is one of the leading causes of disability worldwide, posing a
severe burden on individuals, healthcare systems, and society at large. Recent
advancements in Large Language Models (LLMs) have shown promise in addressing
mental health challenges, including the detection of depression through
text-based analysis. However, current LLM-based methods often struggle with
nuanced symptom identification and lack a transparent, step-by-step reasoning
process, making it difficult to accurately classify and explain mental health
conditions. To address these challenges, we propose a Chain-of-Thought
Prompting approach that enhances both the performance and interpretability of
LLM-based depression detection. Our method breaks down the detection process
into four stages: (1) sentiment analysis, (2) binary depression classification,
(3) identification of underlying causes, and (4) assessment of severity. By
guiding the model through these structured reasoning steps, we improve
interpretability and reduce the risk of overlooking subtle clinical indicators.
We validate our method on the E-DAIC dataset, where we test multiple
state-of-the-art large language models. Experimental results indicate that our
Chain-of-Thought Prompting technique yields superior performance in both
classification accuracy and the granularity of diagnostic insights, compared to
baseline approaches.},
 author = {Shiyu Teng and Jiaqing Liu and Rahul Kumar Jain and Shurong Chai and Ruibo Hou and Tomoko Tateyama and Lanfen Lin and Yen-wei Chen},
 citations = {2},
 comment = {},
 doi = {},
 eprint = {2502.05879v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Enhancing Depression Detection with Chain-of-Thought Prompting: From Emotion to Reasoning Using Large Language Models},
 url = {http://arxiv.org/abs/2502.05879v1},
 year = {2025}
}

@article{2502.06243v1,
 abstract = {This study introduces an AI-driven skin lesion classification algorithm built
on an enhanced Transformer architecture, addressing the challenges of accuracy
and robustness in medical image analysis. By integrating a multi-scale feature
fusion mechanism and refining the self-attention process, the model effectively
extracts both global and local features, enhancing its ability to detect
lesions with ambiguous boundaries and intricate structures. Performance
evaluation on the ISIC 2017 dataset demonstrates that the improved Transformer
surpasses established AI models, including ResNet50, VGG19, ResNext, and Vision
Transformer, across key metrics such as accuracy, AUC, F1-Score, and Precision.
Grad-CAM visualizations further highlight the interpretability of the model,
showcasing strong alignment between the algorithm's focus areas and actual
lesion sites. This research underscores the transformative potential of
advanced AI models in medical imaging, paving the way for more accurate and
reliable diagnostic tools. Future work will explore the scalability of this
approach to broader medical imaging tasks and investigate the integration of
multimodal data to enhance AI-driven diagnostic frameworks for intelligent
healthcare.},
 author = {Jiacheng Hu and Yanlin Xiang and Yang Lin and Junliang Du and Hanchao Zhang and Houze Liu},
 citations = {},
 comment = {},
 doi = {10.1145/3730436.3730505},
 eprint = {2502.06243v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Multi-Scale Transformer Architecture for Accurate Medical Image Classification},
 url = {http://arxiv.org/abs/2502.06243v1},
 year = {2025}
}

@article{2502.06842v3,
 abstract = {Healthcare systems are struggling to meet the growing demand for neurological
care, particularly in Alzheimer's disease and related dementias (ADRD). We
propose that LLM-based generative AI systems can enhance clinician capabilities
to approach specialist-level assessment and decision-making in ADRD care at
scale. This article presents a comprehensive six-phase roadmap for responsible
design and integration of such systems into ADRD care: (1) high-quality
standardized data collection across modalities; (2) decision support; (3)
clinical integration enhancing workflows; (4) rigorous validation and
monitoring protocols; (5) continuous learning through clinical feedback; and
(6) robust ethics and risk management frameworks. This human centered approach
optimizes clinicians' capabilities in comprehensive data collection,
interpretation of complex clinical information, and timely application of
relevant medical knowledge while prioritizing patient safety, healthcare
equity, and transparency. Though focused on ADRD, these principles offer broad
applicability across medical specialties facing similar systemic challenges.},
 author = {Andrew G. Breithaupt and Michael Weiner and Alice Tang and Katherine L. Possin and Marina Sirota and James Lah and Allan I. Levey and Pascal Van Hentenryck and Reza Zandehshahvar and Marilu Luisa Gorno-Tempini and Joseph Giorgio and Jingshen Wang and Andreas M. Rauschecker and Howard J. Rosen and Rachel L. Nosheny and Bruce L. Miller and Pedro Pinheiro-Chagas},
 citations = {0},
 comment = {27 pages, 2 figures, 1 table},
 doi = {},
 eprint = {2502.06842v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Integrating Generative Artificial Intelligence in ADRD: A Roadmap for Streamlining Diagnosis and Care in Neurodegenerative Diseases},
 url = {http://arxiv.org/abs/2502.06842v3},
 year = {2025}
}

@article{2502.06866v2,
 abstract = {The drastic changes in the global economy, geopolitical conditions, and
disruptions such as the COVID-19 pandemic have impacted the cost of living and
quality of life. It is important to understand the long-term nature of the cost
of living and quality of life in major economies. A transparent and
comprehensive living index must include multiple dimensions of living
conditions. In this study, we present an approach to quantifying the quality of
life through the Global Ease of Living Index that combines various
socio-economic and infrastructural factors into a single composite score. Our
index utilises economic indicators that define living standards, which could
help in targeted interventions to improve specific areas. We present a machine
learning framework for addressing the problem of missing data for some of the
economic indicators for specific countries. We then curate and update the data
and use a dimensionality reduction approach (principal component analysis) to
create the Ease of Living Index for major economies since 1970. Our work
significantly adds to the literature by offering a practical tool for
policymakers to identify areas needing improvement, such as healthcare systems,
employment opportunities, and public safety. Our approach with open data and
code can be easily reproduced and applied to various contexts. This
transparency and accessibility make our work a valuable resource for ongoing
research and policy development in quality-of-life assessment.},
 author = {Tanay Panat and Rohitash Chandra},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2502.06866v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Global Ease of Living Index: a machine learning framework for longitudinal analysis of major economies},
 url = {http://arxiv.org/abs/2502.06866v2},
 year = {2025}
}

@article{2502.06911v2,
 abstract = {As data continues to grow in volume and complexity across domains such as
finance, manufacturing, and healthcare, effective anomaly detection is
essential for identifying irregular patterns that may signal critical issues.
Recently, foundation models (FMs) have emerged as a powerful tool for advancing
anomaly detection. They have demonstrated unprecedented capabilities in
enhancing anomaly identification, generating detailed data descriptions, and
providing visual explanations. This survey presents the first comprehensive
review of recent advancements in FM-based anomaly detection. We propose a novel
taxonomy that classifies FMs into three categories based on their roles in
anomaly detection tasks, i.e., as encoders, detectors, or interpreters. We
provide a systematic analysis of state-of-the-art methods and discuss key
challenges in leveraging FMs for improved anomaly detection. We also outline
future research directions in this rapidly evolving field.},
 author = {Jing Ren and Tao Tang and Hong Jia and Ziqi Xu and Haytham Fayek and Xiaodong Li and Suyu Ma and Xiwei Xu and Feng Xia},
 citations = {1},
 comment = {11 pages, 4 figures},
 doi = {},
 eprint = {2502.06911v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Foundation Models for Anomaly Detection: Vision and Challenges},
 url = {http://arxiv.org/abs/2502.06911v2},
 year = {2025}
}

@article{2502.07039v1,
 abstract = {High-risk artificial intelligence and machine learning classification tasks,
such as healthcare diagnosis, require accurate and interpretable prediction
models. However, classifier algorithms typically sacrifice individual
case-accuracy for overall model accuracy, limiting analysis of class overlap
areas regardless of task significance. The Adaptive Boosting meta-algorithm,
which won the 2003 G\"odel Prize, analytically assigns higher weights to
misclassified cases to reclassify. However, it relies on weaker base
classifiers that are iteratively strengthened, limiting improvements from base
classifiers. Combining visual and computational approaches enables selecting
stronger base classifiers before boosting. This paper proposes moving boosting
methodology from focusing on only misclassified cases to all cases in the class
overlap areas using Computational and Interactive Visual Learning (CIVL) with a
Human-in-the-Loop. It builds classifiers in lossless visualizations integrating
human domain expertise and visual insights. A Divide and Classify process
splits cases to simple and complex, classifying these individually through
computational analysis and data visualization with lossless visualization
spaces of Parallel Coordinates or other General Line Coordinates. After finding
pure and overlap class areas simple cases in pure areas are classified,
generating interpretable sub-models like decision rules in Propositional and
First-order Logics. Only multidimensional cases in the overlap areas are
losslessly visualized simplifying end-user cognitive tasks to identify
difficult case patterns, including engineering features to form new
classifiable patterns. Demonstration shows a perfectly accurate and losslessly
interpretable model of the Iris dataset, and simulated data shows generalized
benefits to accuracy and interpretability of models, increasing end-user
confidence in discovered models.},
 author = {Alice Williams and Boris Kovalerchuk},
 citations = {1},
 comment = {Preprint},
 doi = {10.1007/978-3-031-93429-2_26},
 eprint = {2502.07039v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Boosting of Classification Models with Human-in-the-Loop Computational Visual Knowledge Discovery},
 url = {http://arxiv.org/abs/2502.07039v1},
 year = {2025}
}

@article{2502.07347v5,
 abstract = {As artificial intelligence (AI) systems become increasingly embedded in
ethically sensitive domains such as education, healthcare, and transportation,
the need to balance accuracy and interpretability in decision-making has become
a central concern. Coarse Ethics (CE) is a theoretical framework that justifies
coarse-grained evaluations, such as letter grades or warning labels, as
ethically appropriate under cognitive and contextual constraints. However, CE
has lacked mathematical formalization. This paper introduces Coarse Set Theory
(CST), a novel mathematical framework that models coarse-grained
decision-making using totally ordered structures and coarse partitions. CST
defines hierarchical relations among sets and uses information-theoretic tools,
such as Kullback-Leibler Divergence, to quantify the trade-off between
simplification and information loss. We demonstrate CST through applications in
educational grading and explainable AI (XAI), showing how it enables more
transparent and context-sensitive evaluations. By grounding coarse evaluations
in set theory and probabilistic reasoning, CST contributes to the ethical
design of interpretable AI systems. This work bridges formal methods and
human-centered ethics, offering a principled approach to balancing
comprehensibility, fairness, and informational integrity in AI-driven
decisions.},
 author = {Takashi Izumo},
 citations = {},
 comment = {28 pages, 2 figures},
 doi = {},
 eprint = {2502.07347v5},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Coarse Set Theory for AI Ethics and Decision-Making: A Mathematical Framework for Granular Evaluations},
 url = {http://arxiv.org/abs/2502.07347v5},
 year = {2025}
}

@article{2502.07389v1,
 abstract = {Cardiovascular diseases, a leading cause of noncommunicable disease-related
deaths, require early and accurate detection to improve patient outcomes.
Taking advantage of advances in machine learning and deep learning, multiple
approaches have been proposed in the literature to address the challenge of
detecting ECG anomalies. Typically, these methods are based on the manual
interpretation of ECG signals, which is time consuming and depends on the
expertise of healthcare professionals. The objective of this work is to propose
a deep learning system, FADE, designed for normal ECG forecasting and anomaly
detection, which reduces the need for extensive labeled datasets and manual
interpretation. FADE has been trained in a self-supervised manner with a novel
morphological inspired loss function. Unlike conventional models that learn
from labeled anomalous ECG waveforms, our approach predicts the future of
normal ECG signals, thus avoiding the need for extensive labeled datasets.
Using a novel distance function to compare forecasted ECG signals with actual
sensor data, our method effectively identifies cardiac anomalies. Additionally,
this approach can be adapted to new contexts through domain adaptation
techniques. To evaluate our proposal, we performed a set of experiments using
two publicly available datasets: MIT-BIH NSR and MIT-BIH Arrythmia. The results
demonstrate that our system achieves an average accuracy of 83.84% in anomaly
detection, while correctly classifying normal ECG signals with an accuracy of
85.46%. Our proposed approach exhibited superior performance in the early
detection of cardiac anomalies in ECG signals, surpassing previous methods that
predominantly identify a limited range of anomalies. FADE effectively detects
both abnormal heartbeats and arrhythmias, offering significant advantages in
healthcare through cost reduction or processing of large-scale ECG data.},
 author = {Paula Ruiz-Barroso and Francisco M. Castro and José Miranda and Denisa-Andreea Constantinescu and David Atienza and Nicolás Guil},
 citations = {},
 comment = {},
 doi = {10.1016/j.cmpb.2025.108780},
 eprint = {2502.07389v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {FADE: Forecasting for Anomaly Detection on ECG},
 url = {http://arxiv.org/abs/2502.07389v1},
 year = {2025}
}

@article{2502.08651v1,
 abstract = {The development and deployment of artificial intelligence (AI) systems, with
their profound societal impacts, raise critical challenges for governance.
Historically, technological innovations have been governed by concentrated
expertise with limited public input. However, AI's pervasive influence across
domains such as healthcare, employment, and justice necessitates inclusive
governance approaches. This article explores the tension between expert-led
oversight and democratic participation, analyzing models of participatory and
deliberative democracy. Using case studies from France and Brazil, we highlight
how inclusive frameworks can bridge the gap between technical complexity and
public accountability. Recommendations are provided for integrating these
approaches into a balanced governance model tailored to the European Union,
emphasizing transparency, diversity, and adaptive regulation to ensure that AI
governance reflects societal values while maintaining technical rigor. This
analysis underscores the importance of hybrid frameworks that unite expertise
and public voice in shaping the future of AI policy.},
 author = {Lucile Ter-Minassian},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2502.08651v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Democratizing AI Governance: Balancing Expertise and Public Participation},
 url = {http://arxiv.org/abs/2502.08651v1},
 year = {2025}
}

@article{2502.09173v1,
 abstract = {In remote healthcare monitoring, time series representation learning reveals
critical patient behavior patterns from high-frequency data. This study
analyzes home activity data from individuals living with dementia by proposing
a two-stage, self-supervised learning approach tailored to uncover low-rank
structures. The first stage converts time-series activities into text sequences
encoded by a pre-trained language model, providing a rich, high-dimensional
latent state space using a PageRank-based method. This PageRank vector captures
latent state transitions, effectively compressing complex behaviour data into a
succinct form that enhances interpretability. This low-rank representation not
only enhances model interpretability but also facilitates clustering and
transition analysis, revealing key behavioral patterns correlated with
clinicalmetrics such as MMSE and ADAS-COG scores. Our findings demonstrate the
framework's potential in supporting cognitive status prediction, personalized
care interventions, and large-scale health monitoring.},
 author = {Jin Cui and Alexander Capstick and Payam Barnaghi and Gregory Scott},
 citations = {0},
 comment = {AAAI 2025 Workshop on Large Language Models and Generative AI for
  Health},
 doi = {},
 eprint = {2502.09173v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia},
 url = {http://arxiv.org/abs/2502.09173v1},
 year = {2025}
}

@article{2502.09242v1,
 abstract = {Generative artificial intelligence (AI) models, such as diffusion models and
OpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy
and automating clinical workflows. The field has advanced rapidly, evolving
from text-only large language models for tasks such as clinical documentation
and decision support to multimodal AI systems capable of integrating diverse
data modalities, including imaging, text, and structured data, within a single
model. The diverse landscape of these technologies, along with rising interest,
highlights the need for a comprehensive review of their applications and
potential. This scoping review explores the evolution of multimodal AI,
highlighting its methods, applications, datasets, and evaluation in clinical
settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed,
IEEE Xplore, and Web of Science, prioritizing recent studies published up to
the end of 2024. After rigorous screening, 144 papers were included, revealing
key trends and challenges in this dynamic field. Our findings underscore a
shift from unimodal to multimodal approaches, driving innovations in diagnostic
support, medical report generation, drug discovery, and conversational AI.
However, critical challenges remain, including the integration of heterogeneous
data types, improving model interpretability, addressing ethical concerns, and
validating AI systems in real-world clinical settings. This review summarizes
the current state of the art, identifies critical gaps, and provides insights
to guide the development of scalable, trustworthy, and clinically impactful
multimodal AI solutions in healthcare.},
 author = {Lukas Buess and Matthias Keicher and Nassir Navab and Andreas Maier and Soroosh Tayebi Arasteh},
 citations = {5},
 comment = {},
 doi = {10.1007/s13534-025-00497-1},
 eprint = {2502.09242v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine},
 url = {http://arxiv.org/abs/2502.09242v1},
 year = {2025}
}

@article{2502.09849v2,
 abstract = {Explainable AI (XAI) has become a crucial component of Clinical Decision
Support Systems (CDSS) to enhance transparency, trust, and clinical adoption.
However, while many XAI methods have been proposed, their effectiveness in
real-world medical settings remains underexplored. This paper provides a survey
of human-centered evaluations of Explainable AI methods in Clinical Decision
Support Systems. By categorizing existing works based on XAI methodologies,
evaluation frameworks, and clinical adoption challenges, we offer a structured
understanding of the landscape. Our findings reveal key challenges in the
integration of XAI into healthcare workflows and propose a structured framework
to align the evaluation methods of XAI with the clinical needs of stakeholders.},
 author = {Alessandro Gambetti and Qiwei Han and Hong Shen and Claudia Soares},
 citations = {},
 comment = {13 pages, 1 table, 1 figure},
 doi = {},
 eprint = {2502.09849v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems},
 url = {http://arxiv.org/abs/2502.09849v2},
 year = {2025}
}

@article{2502.10108v1,
 abstract = {The early diagnosis of Alzheimer's Disease (AD) through non invasive methods
remains a significant healthcare challenge. We present NeuroXVocal, a novel
dual-component system that not only classifies but also explains potential AD
cases through speech analysis. The classification component (Neuro) processes
three distinct data streams: acoustic features capturing speech patterns and
voice characteristics, textual features extracted from speech transcriptions,
and precomputed embeddings representing linguistic patterns. These streams are
fused through a custom transformer-based architecture that enables robust
cross-modal interactions. The explainability component (XVocal) implements a
Retrieval-Augmented Generation (RAG) approach, leveraging Large Language Models
combined with a domain-specific knowledge base of AD research literature. This
architecture enables XVocal to retrieve relevant clinical studies and research
findings to generate evidence-based context-sensitive explanations of the
acoustic and linguistic markers identified in patient speech. Using the IS2021
ADReSSo Challenge benchmark dataset, our system achieved state-of-the-art
performance with 95.77% accuracy in AD classification, significantly
outperforming previous approaches. The explainability component was
qualitatively evaluated using a structured questionnaire completed by medical
professionals, validating its clinical relevance. NeuroXVocal's unique
combination of high-accuracy classification and interpretable,
literature-grounded explanations demonstrates its potential as a practical tool
for supporting clinical AD diagnosis.},
 author = {Nikolaos Ntampakis and Konstantinos Diamantaras and Ioanna Chouvarda and Magda Tsolaki and Vasileios Argyriou and Panagiotis Sarigianndis},
 citations = {},
 comment = {},
 doi = {10.1007/978-3-032-05185-1_40},
 eprint = {2502.10108v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {NeuroXVocal: Detection and Explanation of Alzheimer's Disease through Non-invasive Analysis of Picture-prompted Speech},
 url = {http://arxiv.org/abs/2502.10108v1},
 year = {2025}
}

@article{2502.10689v2,
 abstract = {The burgeoning volume of electronic health records (EHRs) has enabled deep
learning models to excel in predictive healthcare. However, for high-stakes
applications such as diagnosis prediction, model interpretability remains
paramount. Existing deep learning diagnosis prediction models with intrinsic
interpretability often assign attention weights to every past diagnosis or
hospital visit, providing explanations lacking flexibility and succinctness. In
this paper, we introduce SHy, a self-explaining hypergraph neural network
model, designed to offer personalized, concise and faithful explanations that
allow for interventions from clinical experts. By modeling each patient as a
unique hypergraph and employing a message-passing mechanism, SHy captures
higher-order disease interactions and extracts distinct temporal phenotypes as
personalized explanations. It also addresses the incompleteness of the EHR data
by accounting for essential false negatives in the original diagnosis record. A
qualitative case study and extensive quantitative evaluations on two real-world
EHR datasets demonstrate the superior predictive performance and
interpretability of SHy over existing state-of-the-art models.},
 author = {Leisheng Yu and Yanxiao Cai and Minxing Zhang and Xia Hu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.10689v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Self-Explaining Hypergraph Neural Networks for Diagnosis Prediction},
 url = {http://arxiv.org/abs/2502.10689v2},
 year = {2025}
}

@article{2502.10732v1,
 abstract = {Deep Reinforcement Learning (RL) is remarkably effective in addressing
sequential resource allocation problems in domains such as healthcare, public
policy, and resource management. However, deep RL policies often lack
transparency and adaptability, challenging their deployment alongside human
decision-makers. In contrast, Language Agents, powered by large language models
(LLMs), provide human-understandable reasoning but may struggle with effective
decision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement
Learning (RBRL), a novel framework that jointly optimizes decision and
explanations. At each step, RBRL generates candidate rules with an LLM, selects
among them using an attention-based RL policy, and determines the environment
action with an explanation via chain-of-thought reasoning. The RL rule
selection is optimized using the environment rewards and an explainability
metric judged by the LLM. Evaluations in real-world scenarios highlight RBRL's
competitive performance with deep RL and efficiency gains over LLM fine-tuning.
A survey further confirms the enhanced quality of its explanations.},
 author = {Mauricio Tec and Guojun Xiong and Haichuan Wang and Francesca Dominici and Milind Tambe},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.10732v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents},
 url = {http://arxiv.org/abs/2502.10732v1},
 year = {2025}
}

@article{2502.12753v2,
 abstract = {In artificial intelligence (AI), the complexity of many models and processes
surpasses human understanding, making it challenging to determine why a
specific prediction is made. This lack of transparency is particularly
problematic in critical fields like healthcare, where trust in a model's
predictions is paramount. As a result, the explainability of machine learning
(ML) and other complex models has become a key area of focus. Efforts to
improve model explainability often involve experimenting with AI systems and
approximating their behavior through interpretable surrogate mechanisms.
However, these procedures can be resource-intensive. Optimal design of
experiments, which seeks to maximize the information obtained from a limited
number of observations, offers promising methods for improving the efficiency
of these explainability techniques. To demonstrate this potential, we explore
Local Interpretable Model-agnostic Explanations (LIME), a widely used method
introduced by Ribeiro et al. (2016). LIME provides explanations by generating
new data points near the instance of interest and passing them through the
model. While effective, this process can be computationally expensive,
especially when predictions are costly or require many samples. LIME is highly
versatile and can be applied to a wide range of models and datasets. In this
work, we focus on models involving tabular data, regression tasks, and linear
models as interpretable local approximations. By utilizing optimal design of
experiments' techniques, we reduce the number of function evaluations of the
complex model, thereby reducing the computational effort of LIME by a
significant amount. We consider this modified version of LIME to be
energy-efficient or "green".},
 author = {Alexandra Stadler and Werner G. Müller and Radoslav Harman},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.12753v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Green LIME: Improving AI Explainability through Design of Experiments},
 url = {http://arxiv.org/abs/2502.12753v2},
 year = {2025}
}

@article{2502.13108v2,
 abstract = {Clinical Question Answering (CQA) plays a crucial role in medical
decision-making, enabling physicians to extract relevant information from
Electronic Medical Records (EMRs). While transformer-based models such as BERT,
BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in
CQA, existing models lack the ability to categorize extracted answers, which is
critical for structured retrieval, content filtering, and medical decision
support.
  To address this limitation, we introduce a Multi-Task Learning (MTL)
framework that jointly trains CQA models for both answer extraction and medical
categorization. In addition to predicting answer spans, our model classifies
responses into five standardized medical categories: Diagnosis, Medication,
Symptoms, Procedure, and Lab Reports. This categorization enables more
structured and interpretable outputs, making clinical QA models more useful in
real-world healthcare settings.
  We evaluate our approach on emrQA, a large-scale dataset for medical question
answering. Results show that MTL improves F1-score by 2.2% compared to standard
fine-tuning, while achieving 90.7% accuracy in answer categorization. These
findings suggest that MTL not only enhances CQA performance but also introduces
an effective mechanism for categorization and structured medical information
retrieval.},
 author = {Priyaranjan Pattnayak and Hitesh Laxmichand Patel and Amit Agarwal and Bhargava Kumar and Srikant Panda and Tejaswini Kumar},
 citations = {6},
 comment = {},
 doi = {10.1109/eit64391.2025.11103631},
 eprint = {2502.13108v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization},
 url = {http://arxiv.org/abs/2502.13108v2},
 year = {2025}
}

@article{2502.13290v1,
 abstract = {Predicting medical events in advance within critical care settings is
paramount for patient outcomes and resource management. Utilizing predictive
models, healthcare providers can anticipate issues such as cardiac arrest,
sepsis, or respiratory failure before they manifest. Recently, there has been a
surge in research focusing on forecasting adverse medical event onsets prior to
clinical manifestation using machine learning. However, while these models
provide temporal prognostic predictions for the occurrence of a specific
adverse event of interest within defined time intervals, their interpretability
often remains a challenge. In this work, we explore the applicability of neural
temporal point processes in the context of adverse event onset prediction, with
the aim of explaining clinical pathways and providing interpretable insights.
Our experiments span six state-of-the-art neural point processes and six
critical care datasets, each focusing on the onset of distinct adverse events.
This work represents a novel application class of neural temporal point
processes in event prediction.},
 author = {Sachini Weerasekara and Sagar Kamarthi and Jacqueline Isaacs},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.13290v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Prediction of Clinical Complication Onset using Neural Point Processes},
 url = {http://arxiv.org/abs/2502.13290v1},
 year = {2025}
}

@article{2502.13524v4,
 abstract = {Efficient evaluation of three-dimensional (3D) medical images is crucial for
diagnostic and therapeutic practices in healthcare. Recent years have seen a
substantial uptake in applying deep learning and computer vision to analyse and
interpret medical images. Traditional approaches, such as convolutional neural
networks (CNNs) and vision transformers (ViTs), face significant computational
challenges, prompting the need for architectural advancements. Recent efforts
have led to the introduction of novel architectures like the ``Mamba'' model as
alternative solutions to traditional CNNs or ViTs. The Mamba model excels in
the linear processing of one-dimensional data with low computational demands.
However, Mamba's potential for 3D medical image analysis remains underexplored
and could face significant computational challenges as the dimension increases.
This manuscript presents MobileViM, a streamlined architecture for efficient
segmentation of 3D medical images. In the MobileViM network, we invent a new
dimension-independent mechanism and a dual-direction traversing approach to
incorporate with a vision-Mamba-based framework. MobileViM also features a
cross-scale bridging technique to improve efficiency and accuracy across
various medical imaging modalities. With these enhancements, MobileViM achieves
segmentation speeds exceeding 90 frames per second (FPS) on a single graphics
processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster
than the state-of-the-art deep learning models for processing 3D images with
the same computational resources. In addition, experimental evaluations
demonstrate that MobileViM delivers superior performance, with Dice similarity
scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,
ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses
existing models.},
 author = {Wei Dai and Jun Liu},
 citations = {},
 comment = {The corresponding author disagrees with the manuscript submitted to
  arXiv},
 doi = {},
 eprint = {2502.13524v4},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis},
 url = {http://arxiv.org/abs/2502.13524v4},
 year = {2025}
}

@article{2502.14001v1,
 abstract = {Recent advancement in machine learning algorithms reaches a point where
medical devices can be equipped with artificial intelligence (AI) models for
diagnostic support and routine automation in clinical settings. In medicine and
healthcare, there is a particular demand for sufficient and objective
explainability of the outcome generated by AI models. However, AI models are
generally considered as black boxes due to their complexity, and the
computational process leading to their response is often opaque. Although
several methods have been proposed to explain the behavior of models by
evaluating the importance of each feature in discrimination and prediction,
they may suffer from biases and opacities arising from the scale and sampling
protocol of the dataset used for training or testing. To overcome the
shortcomings of existing methods, we explore an alternative approach to provide
an objective explanation of AI models that can be defined independently of the
learning process and does not require additional data. As a preliminary study
for this direction of research, this work examines a numerical availability of
the Jacobian matrix of deep learning models that measures how stably a model
responses against small perturbations added to the input. The indicator, if
available, are calculated from a trained AI model for a given target input.
This is a first step towards a perturbation-based explanation, which will
assist medical practitioners in understanding and interpreting the response of
the AI model in its clinical application.},
 author = {Takeshi Abe and Yoshiyuki Asai},
 citations = {0},
 comment = {7 pages, 1 figure},
 doi = {},
 eprint = {2502.14001v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards a perturbation-based explanation for medical AI as differentiable programs},
 url = {http://arxiv.org/abs/2502.14001v1},
 year = {2025}
}

@article{2502.14116v1,
 abstract = {Hardware Trojans are malicious modifications in digital designs that can be
inserted by untrusted supply chain entities. Hardware Trojans can give rise to
diverse attack vectors such as information leakage (e.g. MOLES Trojan) and
denial-of-service (rarely triggered bit flip). Such an attack in critical
systems (e.g. healthcare and aviation) can endanger human lives and lead to
catastrophic financial loss. Several techniques have been developed to detect
such malicious modifications in digital designs, particularly for designs
sourced from third-party intellectual property (IP) vendors. However, most
techniques have scalability concerns (due to unsound assumptions during
evaluation) and lead to large number of false positive detections (false
alerts). Our framework (SALTY) mitigates these concerns through the use of a
novel Graph Neural Network architecture (using Jumping-Knowledge mechanism) for
generating initial predictions and an Explainable Artificial Intelligence (XAI)
approach for fine tuning the outcomes (post-processing). Experiments show 98%
True Positive Rate (TPR) and True Negative Rate (TNR), significantly
outperforming state-of-the-art techniques across a large set of standard
benchmarks.},
 author = {Tanzim Mahfuz and Pravin Gaikwad and Tasneem Suha and Swarup Bhunia and Prabuddha Chakraborty},
 citations = {0},
 comment = {},
 doi = {10.1109/vts65138.2025.11022818},
 eprint = {2502.14116v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SALTY: Explainable Artificial Intelligence Guided Structural Analysis for Hardware Trojan Detection},
 url = {http://arxiv.org/abs/2502.14116v1},
 year = {2025}
}

@article{2502.14868v1,
 abstract = {The lack of explainability of Artificial Intelligence (AI) is one of the
first obstacles that the industry and regulators must overcome to mitigate the
risks associated with the technology. The need for eXplainable AI (XAI) is
evident in fields where accountability, ethics and fairness are critical, such
as healthcare, credit scoring, policing and the criminal justice system. At the
EU level, the notion of explainability is one of the fundamental principles
that underpin the AI Act, though the exact XAI techniques and requirements are
still to be determined and tested in practice. This paper explores various
approaches and techniques that promise to advance XAI, as well as the
challenges of implementing the principle of explainability in AI governance and
policies. Finally, the paper examines the integration of XAI into EU law,
emphasising the issues of standard setting, oversight, and enforcement.},
 author = {Georgios Pavlidis},
 citations = {},
 comment = {},
 doi = {10.2139/ssrn.5067916},
 eprint = {2502.14868v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's Framework for Explainability in AI},
 url = {http://arxiv.org/abs/2502.14868v1},
 year = {2025}
}

@article{2502.14889v1,
 abstract = {The task of identifying multimodal image-text representations has garnered
increasing attention, particularly with models such as CLIP (Contrastive
Language-Image Pretraining), which demonstrate exceptional performance in
learning complex associations between images and text. Despite these
advancements, ensuring the interpretability of such models is paramount for
their safe deployment in real-world applications, such as healthcare. While
numerous interpretability methods have been developed for unimodal tasks, these
approaches often fail to transfer effectively to multimodal contexts due to
inherent differences in the representation structures. Bottleneck methods,
well-established in information theory, have been applied to enhance CLIP's
interpretability. However, they are often hindered by strong assumptions or
intrinsic randomness. To overcome these challenges, we propose the Narrowing
Information Bottleneck Theory, a novel framework that fundamentally redefines
the traditional bottleneck approach. This theory is specifically designed to
satisfy contemporary attribution axioms, providing a more robust and reliable
solution for improving the interpretability of multimodal models. In our
experiments, compared to state-of-the-art methods, our approach enhances image
interpretability by an average of 9%, text interpretability by an average of
58.83%, and accelerates processing speed by 63.95%. Our code is publicly
accessible at https://github.com/LMBTough/NIB.},
 author = {Zhiyu Zhu and Zhibo Jin and Jiayu Zhang and Nan Yang and Jiahao Huang and Jianlong Zhou and Fang Chen},
 citations = {1},
 comment = {Accepted by ICLR 2025},
 doi = {},
 eprint = {2502.14889v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability},
 url = {http://arxiv.org/abs/2502.14889v1},
 year = {2025}
}

@article{2502.15871v2,
 abstract = {The application of large language models (LLMs) in healthcare holds
significant promise for enhancing clinical decision-making, medical research,
and patient care. However, their integration into real-world clinical settings
raises critical concerns around trustworthiness, particularly around dimensions
of truthfulness, privacy, safety, robustness, fairness, and explainability.
These dimensions are essential for ensuring that LLMs generate reliable,
unbiased, and ethically sound outputs. While researchers have recently begun
developing benchmarks and evaluation frameworks to assess LLM trustworthiness,
the trustworthiness of LLMs in healthcare remains underexplored, lacking a
systematic review that provides a comprehensive understanding and future
insights. This survey addresses that gap by providing a comprehensive review of
current methodologies and solutions aimed at mitigating risks across key trust
dimensions. We analyze how each dimension affects the reliability and ethical
deployment of healthcare LLMs, synthesize ongoing research efforts, and
identify critical gaps in existing approaches. We also identify emerging
challenges posed by evolving paradigms, such as multi-agent collaboration,
multi-modal reasoning, and the development of small open-source medical models.
Our goal is to guide future research toward more trustworthy, transparent, and
clinically viable LLMs.},
 author = {Manar Aljohani and Jun Hou and Sindhura Kommu and Xuan Wang},
 citations = {0},
 comment = {},
 doi = {10.7753/ijsea1406.1001},
 eprint = {2502.15871v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare},
 url = {http://arxiv.org/abs/2502.15871v2},
 year = {2025}
}

@article{2502.15898v1,
 abstract = {Medicare fraud poses a substantial challenge to healthcare systems, resulting
in significant financial losses and undermining the quality of care provided to
legitimate beneficiaries. This study investigates the use of machine learning
(ML) to enhance Medicare fraud detection, addressing key challenges such as
class imbalance, high-dimensional data, and evolving fraud patterns. A dataset
comprising inpatient claims, outpatient claims, and beneficiary details was
used to train and evaluate five ML models: Random Forest, KNN, LDA, Decision
Tree, and AdaBoost. Data preprocessing techniques included resampling SMOTE
method to address the class imbalance, feature selection for dimensionality
reduction, and aggregation of diagnostic and procedural codes. Random Forest
emerged as the best-performing model, achieving a training accuracy of 99.2%
and validation accuracy of 98.8%, and F1-score (98.4%). The Decision Tree also
performed well, achieving a validation accuracy of 96.3%. KNN and AdaBoost
demonstrated moderate performance, with validation accuracies of 79.2% and
81.1%, respectively, while LDA struggled with a validation accuracy of 63.3%
and a low recall of 16.6%. The results highlight the importance of advanced
resampling techniques, feature engineering, and adaptive learning in detecting
Medicare fraud effectively. This study underscores the potential of machine
learning in addressing the complexities of fraud detection. Future work should
explore explainable AI and hybrid models to improve interpretability and
performance, ensuring scalable and reliable fraud detection systems that
protect healthcare resources and beneficiaries.},
 author = {Dorsa Farahmandazad and Kasra Danesh},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2502.15898v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ML-Driven Approaches to Combat Medicare Fraud: Advances in Class Imbalance Solutions, Feature Engineering, Adaptive Learning, and Business Impact},
 url = {http://arxiv.org/abs/2502.15898v1},
 year = {2025}
}

@article{2502.16732v2,
 abstract = {The rapid integration of artificial intelligence (AI) into healthcare is
transforming clinical decision-making and hospital operations. DeepSeek has
emerged as a leading AI system, widely deployed across China's tertiary
hospitals since January 2025. Initially implemented in Shanghai's major medical
institutions, it has since expanded nationwide, enhancing diagnostic accuracy,
streamlining workflows, and improving patient management. AI-powered pathology,
imaging analysis, and clinical decision support systems have demonstrated
significant potential in optimizing medical processes and reducing the
cognitive burden on healthcare professionals. However, the widespread adoption
of AI in healthcare raises critical regulatory and ethical challenges,
particularly regarding accountability in AI-assisted diagnosis and the risk of
automation bias. The absence of a well-defined liability framework underscores
the need for policies that ensure AI functions as an assistive tool rather than
an autonomous decision-maker. With continued technological advancements, AI is
expected to integrate multimodal data sources, such as genomics and radiomics,
paving the way for precision medicine and personalized treatment strategies.
The future of AI in healthcare depends on the development of transparent
regulatory structures, industry collaboration, and adaptive governance
frameworks that balance innovation with responsibility, ensuring equitable and
effective AI-driven medical services.},
 author = {Jishizhan Chen and Qingzeng Zhang},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.16732v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DeepSeek reshaping healthcare in China's tertiary hospitals},
 url = {http://arxiv.org/abs/2502.16732v2},
 year = {2025}
}

@article{2502.17061v1,
 abstract = {Time-series classification is essential across diverse domains, including
medical diagnosis, industrial monitoring, financial forecasting, and human
activity recognition. The Rocket algorithm has emerged as a simple yet powerful
method, achieving state-of-the-art performance through random convolutional
kernels applied to time-series data, followed by non-linear transformation. Its
architecture approximates a one-hidden-layer convolutional neural network while
eliminating parameter training, ensuring computational efficiency. Despite its
empirical success, fundamental questions about its theoretical foundations
remain unexplored. We bridge theory and practice by formalizing Rocket's random
convolutional filters within the compressed sensing framework, proving that
random projections preserve discriminative patterns in time-series data. This
analysis reveals relationships between kernel parameters and input signal
characteristics, enabling more principled approaches to algorithm
configuration. Moreover, we demonstrate that its non-linearity, based on the
proportion of positive values after convolutions, expresses the inherent
sparsity of time-series data. Our theoretical investigation also proves that
Rocket satisfies two critical conditions: translation invariance and noise
robustness. These findings enhance interpretability and provide guidance for
parameter optimization in extreme cases, advancing both theoretical
understanding and practical application of time-series classification.},
 author = {Jorge Marco-Blanco and Rubén Cuevas},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2502.17061v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Random Projections and Natural Sparsity in Time-Series Classification: A Theoretical Analysis},
 url = {http://arxiv.org/abs/2502.17061v1},
 year = {2025}
}

@article{2502.17487v2,
 abstract = {Large language models (LLMs) increasingly serve as interactive healthcare
resources, yet user acceptance remains underexplored. This study examines how
ease of use, perceived usefulness, trust, and risk perception interact to shape
intentions to adopt DeepSeek, an emerging LLM-based platform, for healthcare
purposes. A cross-sectional survey of 556 participants from India, the United
Kingdom, and the United States was conducted to measure perceptions and usage
patterns. Structural equation modeling assessed both direct and indirect
effects, including potential quadratic relationships. Results revealed that
trust plays a pivotal mediating role: ease of use exerts a significant indirect
effect on usage intentions through trust, while perceived usefulness
contributes to both trust development and direct adoption. By contrast, risk
perception negatively affects usage intent, emphasizing the importance of
robust data governance and transparency. Notably, significant non-linear paths
were observed for ease of use and risk, indicating threshold or plateau
effects. The measurement model demonstrated strong reliability and validity,
supported by high composite reliabilities, average variance extracted, and
discriminant validity measures. These findings extend technology acceptance and
health informatics research by illuminating the multifaceted nature of user
adoption in sensitive domains. Stakeholders should invest in trust-building
strategies, user-centric design, and risk mitigation measures to encourage
sustained and safe uptake of LLMs in healthcare. Future work can employ
longitudinal designs or examine culture-specific variables to further clarify
how user perceptions evolve over time and across different regulatory
environments. Such insights are critical for harnessing AI to enhance outcomes.},
 author = {Avishek Choudhury and Yeganeh Shahsavar and Hamid Shamszare},
 citations = {},
 comment = {},
 doi = {10.36227/techrxiv.173933103.36386053/v1},
 eprint = {2502.17487v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {User Intent to Use DeepSeek for Healthcare Purposes and their Trust in the Large Language Model: Multinational Survey Study},
 url = {http://arxiv.org/abs/2502.17487v2},
 year = {2025}
}

@article{2502.17999v1,
 abstract = {Sensor-based Human Activity Recognition (HAR) in smart home environments is
crucial for several applications, especially in the healthcare domain. The
majority of the existing approaches leverage deep learning models. While these
approaches are effective, the rationale behind their outputs is opaque.
Recently, eXplainable Artificial Intelligence (XAI) approaches emerged to
provide intuitive explanations to the output of HAR models. To the best of our
knowledge, these approaches leverage classic deep models like CNNs or RNNs.
Recently, Graph Neural Networks (GNNs) proved to be effective for sensor-based
HAR. However, existing approaches are not designed with explainability in mind.
In this work, we propose the first explainable Graph Neural Network explicitly
designed for smart home HAR. Our results on two public datasets show that this
approach provides better explanations than state-of-the-art methods while also
slightly improving the recognition rate.},
 author = {Michele Fiori and Davide Mor and Gabriele Civitarese and Claudio Bettini},
 citations = {},
 comment = {This is a preprint. Paper accepted for publication at the 21st EAI
  International Conference on Mobile and Ubiquitous Systems: Computing,
  Networking and Services (Mobiquitous)},
 doi = {},
 eprint = {2502.17999v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {GNN-XAR: A Graph Neural Network for Explainable Activity Recognition in Smart Homes},
 url = {http://arxiv.org/abs/2502.17999v1},
 year = {2025}
}

@article{2502.18050v1,
 abstract = {This study addresses the critical issue of reliability for AI-assisted
medical diagnosis. We focus on the selection prediction approach that allows
the diagnosis system to abstain from providing the decision if it is not
confident in the diagnosis. Such selective prediction (or abstention)
approaches are usually based on the modeling predictive uncertainty of machine
learning models involved.
  This study explores uncertainty quantification in machine learning models for
medical text analysis, addressing diverse tasks across multiple datasets. We
focus on binary mortality prediction from textual data in MIMIC-III,
multi-label medical code prediction using ICD-10 codes from MIMIC-IV, and
multi-class classification with a private outpatient visits dataset.
Additionally, we analyze mental health datasets targeting depression and
anxiety detection, utilizing various text-based sources, such as essays, social
media posts, and clinical descriptions.
  In addition to comparing uncertainty methods, we introduce HUQ-2, a new
state-of-the-art method for enhancing reliability in selective prediction
tasks. Our results provide a detailed comparison of uncertainty quantification
methods. They demonstrate the effectiveness of HUQ-2 in capturing and
evaluating uncertainty, paving the way for more reliable and interpretable
applications in medical text analysis.},
 author = {Artem Vazhentsev and Ivan Sviridov and Alvard Barseghyan and Gleb Kuzmin and Alexander Panchenko and Aleksandr Nesterov and Artem Shelmanov and Maxim Panov},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2502.18050v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Uncertainty-aware abstention in medical diagnosis based on medical texts},
 url = {http://arxiv.org/abs/2502.18050v1},
 year = {2025}
}

@article{2502.18733v1,
 abstract = {Deep learning's growing prevalence has driven its widespread use in
healthcare, where AI and sensor advancements enhance diagnosis, treatment, and
monitoring. In mobile health, AI-powered tools enable early diagnosis and
continuous monitoring of conditions like stress. Wearable technologies and
multimodal physiological data have made stress detection increasingly viable,
but model efficacy depends on data quality, quantity, and modality. This study
develops transformer models for stress detection using the WESAD dataset,
training on electrocardiograms (ECG), electrodermal activity (EDA),
electromyography (EMG), respiration rate (RESP), temperature (TEMP), and 3-axis
accelerometer (ACC) signals. The results demonstrate the effectiveness of
single-modality transformers in analyzing physiological signals, achieving
state-of-the-art performance with accuracy, precision and recall values in the
range of $99.73\%$ to $99.95\%$ for stress detection. Furthermore, this study
explores cross-modal performance and also explains the same using 2D
visualization of the learned embedding space and quantitative analysis based on
data variance. Despite the large body of work on stress detection and
monitoring, the robustness and generalization of these models across different
modalities has not been explored. This research represents one of the initial
efforts to interpret embedding spaces for stress detection, providing valuable
information on cross-modal performance.},
 author = {Eric Oliver and Sagnik Dakshit},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2502.18733v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Cross-Modality Investigation on WESAD Stress Classification},
 url = {http://arxiv.org/abs/2502.18733v1},
 year = {2025}
}

@article{2502.20513v1,
 abstract = {The emergence of Large Language Models (LLMs) has revolutionized
Conversational User Interfaces (CUIs), enabling more dynamic, context-aware,
and human-like interactions across diverse domains, from social sciences to
healthcare. However, the rapid adoption of LLM-based personas raises critical
ethical and practical concerns, including bias, manipulation, and unforeseen
social consequences. Unlike traditional CUIs, where personas are carefully
designed with clear intent, LLM-based personas generate responses dynamically
from vast datasets, making their behavior less predictable and harder to
govern. This workshop aims to bridge the gap between CUI and broader AI
communities by fostering a cross-disciplinary dialogue on the responsible
design and evaluation of LLM-based personas. Bringing together researchers,
designers, and practitioners, we will explore best practices, develop ethical
guidelines, and promote frameworks that ensure transparency, inclusivity, and
user-centered interactions. By addressing these challenges collaboratively, we
seek to shape the future of LLM-driven CUIs in ways that align with societal
values and expectations.},
 author = {Smit Desai and Mateusz Dubiel and Nima Zargham and Thomas Mildner and Laura Spillner},
 citations = {},
 comment = {},
 doi = {10.1145/3719160.3728624},
 eprint = {2502.20513v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Personas Evolved: Designing Ethical LLM-Based Conversational Agent Personalities},
 url = {http://arxiv.org/abs/2502.20513v1},
 year = {2025}
}

@article{2502.20719v2,
 abstract = {Generating realistic synthetic electronic health records (EHRs) holds
tremendous promise for accelerating healthcare research, facilitating AI model
development and enhancing patient privacy. However, existing generative methods
typically treat EHRs as flat sequences of discrete medical codes. This approach
overlooks two critical aspects: the inherent hierarchical organization of
clinical coding systems and the rich semantic context provided by code
descriptions. Consequently, synthetic patient sequences often lack high
clinical fidelity and have limited utility in downstream clinical tasks. In
this paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT),
a novel framework that leverages both hierarchical and semantic information for
the generative process. HiSGT constructs a hierarchical graph to encode
parent-child and sibling relationships among clinical codes and employs a graph
neural network to derive hierarchy-aware embeddings. These are then fused with
semantic embeddings extracted from a pre-trained clinical language model (e.g.,
ClinicalBERT), enabling the Transformer-based generator to more accurately
model the nuanced clinical patterns inherent in real EHRs. Extensive
experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that HiSGT
significantly improves the statistical alignment of synthetic data with real
patient records, as well as supports robust downstream applications such as
chronic disease classification. By addressing the limitations of conventional
raw code-based generative models, HiSGT represents a significant step toward
clinically high-fidelity synthetic data generation and a general paradigm
suitable for interpretable medical code representation, offering valuable
applications in data augmentation and privacy-preserving healthcare analytics.},
 author = {Guanglin Zhou and Sebastiano Barbieri},
 citations = {1},
 comment = {The camera ready version for ECAI-2025},
 doi = {},
 eprint = {2502.20719v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer},
 url = {http://arxiv.org/abs/2502.20719v2},
 year = {2025}
}

@article{2503.00025v2,
 abstract = {This study presents a comparative evaluation of 22 large language models LLMs
on the Spanish Medical Intern Resident MIR examinations for 2024 and 2025 with
a focus on clinical reasoning domain specific expertise and multimodal
processing capabilities The MIR exam consisting of 210 multiple choice
questions some requiring image interpretation serves as a stringent benchmark
for assessing both factual recall and complex clinical problem solving skills
Our investigation encompasses general purpose models such as GPT4 Claude LLaMA
and Gemini as well as specialized fine tuned systems like Miri Pro which
leverages proprietary Spanish healthcare data to excel in medical contexts
  Recent market entries Deepseek and Grok have further enriched the evaluation
landscape particularly for tasks that demand advanced visual and semantic
analysis The findings indicate that while general purpose LLMs perform robustly
overall fine tuned models consistently achieve superior accuracy especially in
addressing nuanced domain specific challenges A modest performance decline
observed between the two exam cycles appears attributable to the implementation
of modified questions designed to mitigate reliance on memorization
  The results underscore the transformative potential of domain specific fine
tuning and multimodal integration in advancing medical AI applications They
also highlight critical implications for the future integration of LLMs into
medical education training and clinical decision making emphasizing the
importance of balancing automated reasoning with ethical and context aware
judgment},
 author = {Carlos Luengo Vera and Ignacio Ferro Picon and M. Teresa del Val Nunez and Jose Andres Gomez Gandia and Antonio de Lucas Ancillo and Victor Ramos Arroyo and Carlos Milan Figueredo},
 citations = {0},
 comment = {26 pages, 1 table, 7 figures},
 doi = {},
 eprint = {2503.00025v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Evaluating Large Language Models on the Spanish Medical Intern Resident (MIR) Examination 2024/2025:A Comparative Analysis of Clinical Reasoning and Knowledge Application},
 url = {http://arxiv.org/abs/2503.00025v2},
 year = {2025}
}

@article{2503.00565v2,
 abstract = {The multi-armed bandits (MAB) framework is a widely used approach for
sequential decision-making, where a decision-maker selects an arm in each round
with the goal of maximizing long-term rewards. Moreover, in many practical
applications, such as personalized medicine and recommendation systems,
feedback is provided in batches, contextual information is available at the
time of decision-making, and rewards from different arms are related rather
than independent. We propose a novel semi-parametric framework for batched
bandits with covariates and a shared parameter across arms, leveraging the
single-index regression (SIR) model to capture relationships between arm
rewards while balancing interpretability and flexibility. Our algorithm,
Batched single-Index Dynamic binning and Successive arm elimination (BIDS),
employs a batched successive arm elimination strategy with a dynamic binning
mechanism guided by the single-index direction. We consider two settings: one
where a pilot direction is available and another where the direction is
estimated from data, deriving theoretical regret bounds for both cases. When a
pilot direction is available with sufficient accuracy, our approach achieves
minimax-optimal rates (with $d = 1$) for nonparametric batched bandits,
circumventing the curse of dimensionality. Extensive experiments on simulated
and real-world datasets demonstrate the effectiveness of our algorithm compared
to the nonparametric batched bandit method introduced by
\cite{jiang2024batched}.},
 author = {Sakshi Arya and Hyebin Song},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2503.00565v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Semi-Parametric Batched Global Multi-Armed Bandits with Covariates},
 url = {http://arxiv.org/abs/2503.00565v2},
 year = {2025}
}

@article{2503.01037v1,
 abstract = {Chest diseases rank among the most prevalent and dangerous global health
issues. Object detection and phrase grounding deep learning models interpret
complex radiology data to assist healthcare professionals in diagnosis. Object
detection locates abnormalities for classes, while phrase grounding locates
abnormalities for textual descriptions. This paper investigates how text
enhances abnormality localization in chest X-rays by comparing the performance
and explainability of these two tasks. To establish an explainability baseline,
we proposed an automatic pipeline to generate image regions for report
sentences using radiologists' eye-tracking data. The better performance - mIoU
= 0.36 vs. 0.20 - and explainability - Containment ratio 0.48 vs. 0.26 - of the
phrase grounding model infers the effectiveness of text in enhancing chest
X-ray abnormality localization.},
 author = {Elham Ghelichkhan and Tolga Tasdizen},
 citations = {0},
 comment = {Accepted in 2025 IEEE International Symposium on Biomedical Imaging
  (ISBI 2025)},
 doi = {10.1109/isbi60581.2025.10981290},
 eprint = {2503.01037v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Comparison of Object Detection and Phrase Grounding Models in Chest X-ray Abnormality Localization using Eye-tracking Data},
 url = {http://arxiv.org/abs/2503.01037v1},
 year = {2025}
}

@article{2503.01863v1,
 abstract = {With the advent of Vision-Language Models (VLMs), medical artificial
intelligence (AI) has experienced significant technological progress and
paradigm shifts. This survey provides an extensive review of recent
advancements in Medical Vision-Language Models (Med-VLMs), which integrate
visual and textual data to enhance healthcare outcomes. We discuss the
foundational technology behind Med-VLMs, illustrating how general models are
adapted for complex medical tasks, and examine their applications in
healthcare. The transformative impact of Med-VLMs on clinical practice,
education, and patient care is highlighted, alongside challenges such as data
scarcity, narrow task generalization, interpretability issues, and ethical
concerns like fairness, accountability, and privacy. These limitations are
exacerbated by uneven dataset distribution, computational demands, and
regulatory hurdles. Rigorous evaluation methods and robust regulatory
frameworks are essential for safe integration into healthcare workflows. Future
directions include leveraging large-scale, diverse datasets, improving
cross-modal generalization, and enhancing interpretability. Innovations like
federated learning, lightweight architectures, and Electronic Health Record
(EHR) integration are explored as pathways to democratize access and improve
clinical relevance. This review aims to provide a comprehensive understanding
of Med-VLMs' strengths and limitations, fostering their ethical and balanced
adoption in healthcare.},
 author = {Beria Chingnabe Kalpelbe and Angel Gabriel Adaambiik and Wei Peng},
 citations = {4},
 comment = {},
 doi = {},
 eprint = {2503.01863v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Vision Language Models in Medicine},
 url = {http://arxiv.org/abs/2503.01863v1},
 year = {2025}
}

@article{2503.02034v2,
 abstract = {Medical imaging plays a pivotal role in modern healthcare, with computed
tomography pulmonary angiography (CTPA) being a critical tool for diagnosing
pulmonary embolism and other thoracic conditions. However, the complexity of
interpreting CTPA scans and generating accurate radiology reports remains a
significant challenge. This paper introduces Abn-BLIP (Abnormality-aligned
Bootstrapping Language-Image Pretraining), an advanced diagnosis model designed
to align abnormal findings to generate the accuracy and comprehensiveness of
radiology reports. By leveraging learnable queries and cross-modal attention
mechanisms, our model demonstrates superior performance in detecting
abnormalities, reducing missed findings, and generating structured reports
compared to existing methods. Our experiments show that Abn-BLIP outperforms
state-of-the-art medical vision-language models and 3D report generation
methods in both accuracy and clinical relevance. These results highlight the
potential of integrating multimodal learning strategies for improving radiology
reporting. The source code is available at https://github.com/zzs95/abn-blip.},
 author = {Zhusi Zhong and Yuli Wang and Lulu Bi and Zhuoqi Ma and Sun Ho Ahn and Christopher J. Mullin and Colin F. Greineder and Michael K. Atalay and Scott Collins and Grayson L. Baird and Cheng Ting Lin and Webster Stayman and Todd M. Kolb and Ihab Kamel and Harrison X. Bai and Zhicheng Jiao},
 citations = {},
 comment = {},
 doi = {10.1016/j.media.2025.103786},
 eprint = {2503.02034v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA},
 url = {http://arxiv.org/abs/2503.02034v2},
 year = {2025}
}

@article{2503.02675v1,
 abstract = {The recent tremendous advancements in the areas of Artificial Intelligence
(AI) and Deep Learning (DL) have also resulted into corresponding remarkable
progress in the field of Computer Vision (CV), showcasing robust technological
solutions in a wide range of application sectors of high industrial interest
(e.g., healthcare, autonomous driving, automation, etc.). Despite the
outstanding performance of CV systems in specific domains, their development
and exploitation at industrial-scale necessitates, among other, the addressing
of requirements related to the reliability, transparency, trustworthiness,
security, safety, and robustness of the developed AI models. The latter raises
the imperative need for the development of efficient, comprehensive and
widely-adopted industrial standards. In this context, this study investigates
the current state of play regarding the development of industrial computer
vision AI standards, emphasizing on critical aspects, like model
interpretability, data quality, and regulatory compliance. In particular, a
systematic analysis of launched and currently developing CV standards, proposed
by the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN,
etc.) is performed. The latter is complemented by a comprehensive discussion on
the current challenges and future directions observed in this regularization
endeavor.},
 author = {Artemis Stefanidou and Panagiotis Radoglou-Grammatikis and Vasileios Argyriou and Panagiotis Sarigiannidis and Iraklis Varlamis and Georgios Th. Papadopoulos},
 citations = {0},
 comment = {},
 doi = {10.1109/cai64502.2025.00288},
 eprint = {2503.02675v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {State of play and future directions in industrial computer vision AI standards},
 url = {http://arxiv.org/abs/2503.02675v1},
 year = {2025}
}

@article{2503.03129v1,
 abstract = {Deep Learning has emerged as one of the most significant innovations in
machine learning. However, a notable limitation of this field lies in the
``black box" decision-making processes, which have led to skepticism within
groups like healthcare and scientific communities regarding its applicability.
In response, this study introduces a interpretable approach using Neural
Ordinary Differential Equations (NODEs), a category of neural network models
that exploit the dynamics of differential equations for representation
learning. Leveraging their foundation in differential equations, we illustrate
the capability of these models to continuously process textual data, marking
the first such model of its kind, and thereby proposing a promising direction
for future research in this domain. The primary objective of this research is
to propose a novel architecture for groups like healthcare that require the
predictive capabilities of deep learning while emphasizing the importance of
model transparency demonstrated in NODEs.},
 author = {Shi Li},
 citations = {},
 comment = {ACL SRW Submission},
 doi = {10.21203/rs.3.rs-6150705/v1},
 eprint = {2503.03129v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Exploring Neural Ordinary Differential Equations as Interpretable Healthcare classifiers},
 url = {http://arxiv.org/abs/2503.03129v1},
 year = {2025}
}

@article{2503.03152v2,
 abstract = {Pathology image analysis plays a pivotal role in medical diagnosis, with deep
learning techniques significantly advancing diagnostic accuracy and research.
While numerous studies have been conducted to address specific pathological
tasks, the lack of standardization in pre-processing methods and model/database
architectures complicates fair comparisons across different approaches. This
highlights the need for a unified pipeline and comprehensive benchmarks to
enable consistent evaluation and accelerate research progress. In this paper,
we present UnPuzzle, a novel and unified framework for pathological AI research
that covers a broad range of pathology tasks with benchmark results. From
high-level to low-level, upstream to downstream tasks, UnPuzzle offers a
modular pipeline that encompasses data pre-processing, model
composition,taskconfiguration,andexperimentconduction.Specifically, it
facilitates efficient benchmarking for both Whole Slide Images (WSIs) and
Region of Interest (ROI) tasks. Moreover, the framework supports
variouslearningparadigms,includingself-supervisedlearning,multi-task
learning,andmulti-modallearning,enablingcomprehensivedevelopment of pathology
AI models. Through extensive benchmarking across multiple datasets, we
demonstrate the effectiveness of UnPuzzle in streamlining pathology AI research
and promoting reproducibility. We envision UnPuzzle as a cornerstone for future
advancements in pathology AI, providing a more accessible, transparent, and
standardized approach to model evaluation. The UnPuzzle repository is publicly
available at https://github.com/Puzzle-AI/UnPuzzle.},
 author = {Dankai Liao and Sicheng Chen and Nuwa Xi and Qiaochu Xue and Jieyu Li and Lingxuan Hou and Zeyu Liu and Chang Han Low and Yufeng Wu and Yiling Liu and Yanqin Jiang and Dandan Li and Shangqing Lyu},
 citations = {0},
 comment = {11 pages,2 figures},
 doi = {},
 eprint = {2503.03152v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {UnPuzzle: A Unified Framework for Pathology Image Analysis},
 url = {http://arxiv.org/abs/2503.03152v2},
 year = {2025}
}

@article{2503.03724v1,
 abstract = {We present a deep learning-based approach to studying dynamic clinical
behavioral regimes in diverse non-randomized healthcare settings. Our proposed
methodology - deep causal behavioral policy learning (DC-BPL) - uses deep
learning algorithms to learn the distribution of high-dimensional clinical
action paths, and identifies the causal link between these action paths and
patient outcomes. Specifically, our approach: (1) identifies the causal effects
of provider assignment on clinical outcomes; (2) learns the distribution of
clinical actions a given provider would take given evolving patient
information; (3) and combines these steps to identify the optimal provider for
a given patient type and emulate that provider's care decisions. Underlying
this strategy, we train a large clinical behavioral model (LCBM) on electronic
health records data using a transformer architecture, and demonstrate its
ability to estimate clinical behavioral policies. We propose a novel
interpretation of a behavioral policy learned using the LCBM: that it is an
efficient encoding of complex, often implicit, knowledge used to treat a
patient. This allows us to learn a space of policies that are critical to a
wide range of healthcare applications, in which the vast majority of clinical
knowledge is acquired tacitly through years of practice and only a tiny
fraction of information relevant to patient care is written down (e.g. in
textbooks, studies or standardized guidelines).},
 author = {Jonas Knecht and Anna Zink and Jonathan Kolstad and Maya Petersen},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2503.03724v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Deep Causal Behavioral Policy Learning: Applications to Healthcare},
 url = {http://arxiv.org/abs/2503.03724v1},
 year = {2025}
}

@article{2503.04509v1,
 abstract = {Recent improvements in the expressive power of spatio-temporal models have
led to performance gains in many real-world applications, such as traffic
forecasting and social network modelling. However, understanding the
predictions from a model is crucial to ensure reliability and trustworthiness,
particularly for high-risk applications, such as healthcare and transport. Few
existing methods are able to generate explanations for models trained on
continuous-time dynamic graph data and, of these, the computational complexity
and lack of suitable explanation objectives pose challenges. In this paper, we
propose $\textbf{S}$patio-$\textbf{T}$emporal E$\textbf{X}$planation
$\textbf{Search}$ (STX-Search), a novel method for generating instance-level
explanations that is applicable to static and dynamic temporal graph
structures. We introduce a novel search strategy and objective function, to
find explanations that are highly faithful and interpretable. When compared
with existing methods, STX-Search produces explanations of higher fidelity
whilst optimising explanation size to maintain interpretability.},
 author = {Saif Anwar and Nathan Griffiths and Thomas Popham and Abhir Bhalerao},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2503.04509v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {STX-Search: Explanation Search for Continuous Dynamic Spatio-Temporal Models},
 url = {http://arxiv.org/abs/2503.04509v1},
 year = {2025}
}

@article{2503.05773v1,
 abstract = {As artificial intelligence (AI) technologies increasingly enter important
sectors like healthcare, transportation, and finance, the development of
effective governance frameworks is crucial for dealing with ethical, security,
and societal risks. This paper conducts a comparative analysis of AI risk
management strategies across the European Union (EU), United States (U.S.),
United Kingdom (UK), and China. A multi-method qualitative approach, including
comparative policy analysis, thematic analysis, and case studies, investigates
how these regions classify AI risks, implement compliance measures, structure
oversight, prioritize transparency, and respond to emerging innovations.
Examples from high-risk contexts like healthcare diagnostics, autonomous
vehicles, fintech, and facial recognition demonstrate the advantages and
limitations of different regulatory models. The findings show that the EU
implements a structured, risk-based framework that prioritizes transparency and
conformity assessments, while the U.S. uses decentralized, sector-specific
regulations that promote innovation but may lead to fragmented enforcement. The
flexible, sector-specific strategy of the UK facilitates agile responses but
may lead to inconsistent coverage across domains. China's centralized
directives allow rapid large-scale implementation while constraining public
transparency and external oversight. These insights show the necessity for AI
regulation that is globally informed yet context-sensitive, aiming to balance
effective risk management with technological progress. The paper concludes with
policy recommendations and suggestions for future research aimed at enhancing
effective, adaptive, and inclusive AI governance globally.},
 author = {Amir Al-Maamari},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2503.05773v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Between Innovation and Oversight: A Cross-Regional Study of AI Risk Management Frameworks in the EU, U.S., UK, and China},
 url = {http://arxiv.org/abs/2503.05773v1},
 year = {2025}
}

@article{2503.05789v1,
 abstract = {Algorithmic solutions have significant potential to improve decision-making
across various domains, from healthcare to e-commerce. However, the widespread
adoption of these solutions is hindered by a critical challenge: the lack of
human-interpretable explanations. Current approaches to Explainable AI (XAI)
predominantly focus on complex machine learning models, often producing brittle
and non-intuitive explanations. This project proposes a novel approach to
developing explainable algorithms by starting with optimization problems,
specifically the assignment problem. The developed software library enriches
basic algorithms with human-understandable explanations through four key
methodologies: generating meaningful alternative solutions, creating robust
solutions through input perturbation, generating concise decision trees and
providing reports with comprehensive explanation of the results. Currently
developed tools are often designed with specific clustering algorithms in mind,
which limits their adaptability and flexibility to incorporate alternative
techniques. Additionally, many of these tools fail to integrate expert
knowledge, which could enhance the clustering process by providing valuable
insights and context. This lack of adaptability and integration can hinder the
effectiveness and robustness of the clustering outcomes in various
applications. The represents a step towards making algorithmic solutions more
transparent, trustworthy, and accessible. By collaborating with industry
partners in sectors such as sales, we demonstrate the practical relevance and
transformative potential of our approach.},
 author = {Zuzanna Bączek and Michał Bizoń and Aneta Pawelec and Piotr Sankowski},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2503.05789v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {EXALT: EXplainable ALgorithmic Tools for Optimization Problems},
 url = {http://arxiv.org/abs/2503.05789v1},
 year = {2025}
}

@article{2503.06054v1,
 abstract = {Recent advancements in Artificial Intelligence, particularly in Large
Language Models (LLMs), have transformed natural language processing by
improving generative capabilities. However, detecting biases embedded within
these models remains a challenge. Subtle biases can propagate misinformation,
influence decision-making, and reinforce stereotypes, raising ethical concerns.
This study presents a detection framework to identify nuanced biases in LLMs.
The approach integrates contextual analysis, interpretability via attention
mechanisms, and counterfactual data augmentation to capture hidden biases
across linguistic contexts. The methodology employs contrastive prompts and
synthetic datasets to analyze model behaviour across cultural, ideological, and
demographic scenarios.
  Quantitative analysis using benchmark datasets and qualitative assessments
through expert reviews validate the effectiveness of the framework. Results
show improvements in detecting subtle biases compared to conventional methods,
which often fail to highlight disparities in model responses to race, gender,
and socio-political contexts. The framework also identifies biases arising from
imbalances in training data and model architectures. Continuous user feedback
ensures adaptability and refinement. This research underscores the importance
of proactive bias mitigation strategies and calls for collaboration between
policymakers, AI developers, and regulators. The proposed detection mechanisms
enhance model transparency and support responsible LLM deployment in sensitive
applications such as education, legal systems, and healthcare. Future work will
focus on real-time bias monitoring and cross-linguistic generalization to
improve fairness and inclusivity in AI-driven communication tools.},
 author = {Suvendu Mohanty},
 citations = {0},
 comment = {Bias detection, Large Language Models, nuanced biases, fine-grained
  mechanisms, model transparency, ethical AI},
 doi = {},
 eprint = {2503.06054v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases},
 url = {http://arxiv.org/abs/2503.06054v1},
 year = {2025}
}

@article{2503.06463v1,
 abstract = {As cannabis use has increased in recent years, researchers have come to rely
on sophisticated machine learning models to predict cannabis use behavior and
its impact on health. However, many artificial intelligence (AI) models lack
transparency and interpretability due to their opaque nature, limiting their
trust and adoption in real-world medical applications, such as clinical
decision support systems (CDSS). To address this issue, this paper enhances
algorithm explainability underlying CDSS by integrating multiple Explainable
Artificial Intelligence (XAI) methods and applying causal inference techniques
to clarify the model' predictive decisions under various scenarios. By
providing deeper interpretability of the XAI outputs using Large Language
Models (LLMs), we provide users with more personalized and accessible insights
to overcome the challenges posed by AI's "black box" nature. Our system
dynamically adjusts feedback based on user queries and emotional states,
combining text-based sentiment analysis with real-time facial emotion
recognition to ensure responses are empathetic, context-adaptive, and
user-centered. This approach bridges the gap between the learning demands of
interpretability and the need for intuitive understanding, enabling
non-technical users such as clinicians and clinical researchers to interact
effectively with AI models.} Ultimately, this approach improves usability,
enhances perceived trustworthiness, and increases the impact of CDSS in
healthcare applications.},
 author = {Tongze Zhang and Tammy Chung and Anind Dey and Sang Won Bae},
 citations = {0},
 comment = {},
 doi = {10.1109/abc64332.2025.11118599},
 eprint = {2503.06463v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AXAI-CDSS : An Affective Explainable AI-Driven Clinical Decision Support System for Cannabis Use},
 url = {http://arxiv.org/abs/2503.06463v1},
 year = {2025}
}

@article{2503.06571v2,
 abstract = {Patient-ventilator asynchrony (PVA) is a common and critical issue during
mechanical ventilation, affecting up to 85% of patients. PVA can result in
clinical complications such as discomfort, sleep disruption, and potentially
more severe conditions like ventilator-induced lung injury and diaphragm
dysfunction. Traditional PVA management, which relies on manual adjustments by
healthcare providers, is often inadequate due to delays and errors. While
various computational methods, including rule-based, statistical, and deep
learning approaches, have been developed to detect PVA events, they face
challenges related to dataset imbalances and lack of interpretability. In this
work, we propose a shapelet-based approach SHIP for PVA detection, utilizing
shapelets - discriminative subsequences in time-series data - to enhance
detection accuracy and interpretability. Our method addresses dataset
imbalances through shapelet-based data augmentation and constructs a shapelet
pool to transform the dataset for more effective classification. The combined
shapelet and statistical features are then used in a classifier to identify PVA
events. Experimental results on medical datasets show that SHIP significantly
improves PVA detection while providing interpretable insights into model
decisions.},
 author = {Xuan-May Le and Ling Luo and Uwe Aickelin and Minh-Tuan Tran and David Berlowitz and Mark Howard},
 citations = {},
 comment = {Accepted at PAKDD 2025},
 doi = {10.1007/978-981-96-8173-0_34},
 eprint = {2503.06571v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SHIP: A Shapelet-based Approach for Interpretable Patient-Ventilator Asynchrony Detection},
 url = {http://arxiv.org/abs/2503.06571v2},
 year = {2025}
}

@article{2503.06690v1,
 abstract = {Dynamic Treatment Regimes (DTRs) provide a systematic approach for making
sequential treatment decisions that adapt to individual patient
characteristics, particularly in clinical contexts where survival outcomes are
of interest. Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL) is a
novel framework to address the complexities associated with censored data when
estimating optimal DTRs. We explore ways to learn effective DTRs, from
observational data. By enhancing traditional tree-based reinforcement learning
methods with augmented inverse probability weighting (AIPW) and censoring-aware
modifications, CA-TRL delivers robust and interpretable treatment strategies.
We demonstrate its effectiveness through extensive simulations and real-world
applications using the SANAD epilepsy dataset, where it outperformed the
recently proposed ASCL method in key metrics such as restricted mean survival
time (RMST) and decision-making accuracy. This work represents a step forward
in advancing personalized and data-driven treatment strategies across diverse
healthcare settings.},
 author = {Animesh Kumar Paul and Russell Greiner},
 citations = {50},
 comment = {},
 doi = {10.1214/18-aoas1137},
 eprint = {2503.06690v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Censoring-Aware Tree-Based Reinforcement Learning for Estimating Dynamic Treatment Regimes with Censored Outcomes},
 url = {http://arxiv.org/abs/2503.06690v1},
 year = {2025}
}

@article{2503.08699v1,
 abstract = {As artificial intelligence (AI) systems become increasingly complex and
autonomous, concerns over transparency and accountability have intensified. The
"black box" problem in AI decision-making limits stakeholders' ability to
understand, trust, and verify outcomes, particularly in high-stakes sectors
such as healthcare, finance, and autonomous systems. Blockchain technology,
with its decentralized, immutable, and transparent characteristics, presents a
potential solution to enhance AI transparency and auditability. This paper
explores the integration of blockchain with AI to improve decision
traceability, data provenance, and model accountability. By leveraging
blockchain as an immutable record-keeping system, AI decision-making can become
more interpretable, fostering trust among users and regulatory compliance.
However, challenges such as scalability, integration complexity, and
computational overhead must be addressed to fully realize this synergy. This
study discusses existing research, proposes a framework for blockchain-enhanced
AI transparency, and highlights practical applications, benefits, and
limitations. The findings suggest that blockchain could be a foundational
technology for ensuring AI systems remain accountable, ethical, and aligned
with regulatory standards.},
 author = {Afroja Akther and Ayesha Arobee and Abdullah Al Adnan and Omum Auyon and ASM Johirul Islam and Farhad Akter},
 citations = {},
 comment = {14 pages, 2 figures, 5 tables},
 doi = {},
 eprint = {2503.08699v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Blockchain As a Platform For Artificial Intelligence (AI) Transparency},
 url = {http://arxiv.org/abs/2503.08699v1},
 year = {2025}
}

@article{2503.09743v1,
 abstract = {Foodborne gastrointestinal (GI) illness is a common cause of ill health in
the UK. However, many cases do not interact with the healthcare system, posing
significant challenges for traditional surveillance methods. The growth of
publicly available online restaurant reviews and advancements in large language
models (LLMs) present potential opportunities to extend disease surveillance by
identifying public reports of GI illness. In this study, we introduce a novel
annotation schema, developed with experts in GI illness, applied to the Yelp
Open Dataset of reviews. Our annotations extend beyond binary disease
detection, to include detailed extraction of information on symptoms and foods.
We evaluate the performance of open-weight LLMs across these three tasks: GI
illness detection, symptom extraction, and food extraction. We compare this
performance to RoBERTa-based classification models fine-tuned specifically for
these tasks. Our results show that using prompt-based approaches, LLMs achieve
micro-F1 scores of over 90% for all three of our tasks. Using prompting alone,
we achieve micro-F1 scores that exceed those of smaller fine-tuned models. We
further demonstrate the robustness of LLMs in GI illness detection across three
bias-focused experiments. Our results suggest that publicly available review
text and LLMs offer substantial potential for public health surveillance of GI
illness by enabling highly effective extraction of key information. While LLMs
appear to exhibit minimal bias in processing, the inherent limitations of
restaurant review data highlight the need for cautious interpretation of
results.},
 author = {Timothy Laurence and Joshua Harris and Leo Loman and Amy Douglas and Yung-Wai Chan and Luke Hounsome and Lesley Larkin and Michael Borowitz},
 citations = {2},
 comment = {20 pages},
 doi = {},
 eprint = {2503.09743v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Review GIDE -- Restaurant Review Gastrointestinal Illness Detection and Extraction with Large Language Models},
 url = {http://arxiv.org/abs/2503.09743v1},
 year = {2025}
}

@article{2503.09910v1,
 abstract = {Constraining deep neural networks (DNNs) to learn individual logic types per
node, as performed using the DiffLogic network architecture, opens the door to
model-specific explanation techniques that quell the complexity inherent to
DNNs. Inspired by principles of circuit analysis from computer engineering,
this work presents an algorithm (eXpLogic) for producing saliency maps which
explain input patterns that activate certain functions. The eXpLogic
explanations: (1) show the exact set of inputs responsible for a decision,
which helps interpret false negative and false positive predictions, (2)
highlight common input patterns that activate certain outputs, and (3) help
reduce the network size to improve class-specific inference. To evaluate the
eXpLogic saliency map, we introduce a metric that quantifies how much an input
changes before switching a model's class prediction (the SwitchDist) and use
this metric to compare eXpLogic against the Vanilla Gradients (VG) and
Integrated Gradient (IG) methods. Generally, we show that eXpLogic saliency
maps are better at predicting which inputs will change the class score. These
maps help reduce the network size and inference times by 87\% and 8\%,
respectively, while having a limited impact (-3.8\%) on class-specific
predictions. The broader value of this work to machine learning is in
demonstrating how certain DNN architectures promote explainability, which is
relevant to healthcare, defense, and law.},
 author = {Stephen Wormald and David Koblah and Matheus Kunzler Maldaner and Domenic Forte and Damon L. Woodard},
 citations = {},
 comment = {Conference submission, 6 pages, 2 figures},
 doi = {10.1007/978-3-031-89063-5_24},
 eprint = {2503.09910v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {eXpLogic: Explaining Logic Types and Patterns in DiffLogic Networks},
 url = {http://arxiv.org/abs/2503.09910v1},
 year = {2025}
}

@article{2503.10486v2,
 abstract = {Large Language Models (LLMs) are revolutionizing medical diagnostics by
enhancing both disease classification and clinical decision-making. In this
study, we evaluate the performance of two LLM- based diagnostic tools, DeepSeek
R1 and O3 Mini, using a structured dataset of symptoms and diagnoses. We
assessed their predictive accuracy at both the disease and category levels, as
well as the reliability of their confidence scores. DeepSeek R1 achieved a
disease-level accuracy of 76% and an overall accuracy of 82%, outperforming O3
Mini, which attained 72% and 75% respectively. Notably, DeepSeek R1
demonstrated exceptional performance in Mental Health, Neurological Disorders,
and Oncology, where it reached 100% accuracy, while O3 Mini excelled in
Autoimmune Disease classification with 100% accuracy. Both models, however,
struggled with Respiratory Disease classification, recording accuracies of only
40% for DeepSeek R1 and 20% for O3 Mini. Additionally, the analysis of
confidence scores revealed that DeepSeek R1 provided high-confidence
predictions in 92% of cases, compared to 68% for O3 Mini. Ethical
considerations regarding bias, model interpretability, and data privacy are
also discussed to ensure the responsible integration of LLMs into clinical
practice. Overall, our findings offer valuable insights into the strengths and
limitations of LLM-based diagnostic systems and provide a roadmap for future
enhancements in AI-driven healthcare.},
 author = {Gaurav Kumar Gupta and Pranal Pande and Nirajan Acharya and Aniket Kumar Singh and Suman Niroula},
 citations = {},
 comment = {12 pages, 3 figures},
 doi = {},
 eprint = {2503.10486v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions},
 url = {http://arxiv.org/abs/2503.10486v2},
 year = {2025}
}

@article{2503.11684v1,
 abstract = {One of the primary goals of Human-Robot Interaction (HRI) research is to
develop robots that can interpret human behavior and adapt their responses
accordingly. Adaptive learning models, such as continual and reinforcement
learning, play a crucial role in improving robots' ability to interact
effectively in real-world settings. However, these models face significant
challenges due to the limited availability of real-world data, particularly in
sensitive domains like healthcare and well-being. This data scarcity can hinder
a robot's ability to adapt to new situations. To address these challenges,
causality provides a structured framework for understanding and modeling the
underlying relationships between actions, events, and outcomes. By moving
beyond mere pattern recognition, causality enables robots to make more
explainable and generalizable decisions. This paper presents an exploratory
causality-based analysis through a case study of an adaptive robotic coach
delivering positive psychology exercises over four weeks in a workplace
setting. The robotic coach autonomously adapts to multimodal human behaviors,
such as facial valence and speech duration. By conducting both macro- and
micro-level causal analyses, this study aims to gain deeper insights into how
adaptability can enhance well-being during interactions. Ultimately, this
research seeks to advance our understanding of how causality can help overcome
challenges in HRI, particularly in real-world applications.},
 author = {Micol Spitale and Srikar Babu and Serhan Cakmak and Jiaee Cheong and Hatice Gunes},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2503.11684v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching},
 url = {http://arxiv.org/abs/2503.11684v1},
 year = {2025}
}

@article{2503.14217v1,
 abstract = {Decision trees are a crucial class of models offering robust predictive
performance and inherent interpretability across various domains, including
healthcare, finance, and logistics. However, current tree induction methods
often face limitations such as suboptimal solutions from greedy methods or
prohibitive computational costs and limited applicability of exact optimization
approaches. To address these challenges, we propose an evolutionary
optimization method for decision tree induction based on genetic programming
(GP). Our key innovation is the integration of semantic priors and
domain-specific knowledge about the search space into the optimization
algorithm. To this end, we introduce $\texttt{LLEGO}$, a framework that
incorporates semantic priors into genetic search operators through the use of
Large Language Models (LLMs), thereby enhancing search efficiency and targeting
regions of the search space that yield decision trees with superior
generalization performance. This is operationalized through novel genetic
operators that work with structured natural language prompts, effectively
utilizing LLMs as conditional generative models and sources of semantic
knowledge. Specifically, we introduce $\textit{fitness-guided}$ crossover to
exploit high-performing regions, and $\textit{diversity-guided}$ mutation for
efficient global exploration of the search space. These operators are
controlled by corresponding hyperparameters that enable a more nuanced balance
between exploration and exploitation across the search space. Empirically, we
demonstrate across various benchmarks that $\texttt{LLEGO}$ evolves
superior-performing trees compared to existing tree induction methods, and
exhibits significantly more efficient search performance compared to
conventional GP approaches.},
 author = {Tennison Liu and Nicolas Huynh and Mihaela van der Schaar},
 citations = {},
 comment = {*Liu and Huynh contributed equally. Published as a conference paper
  at ICLR 2025},
 doi = {},
 eprint = {2503.14217v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Decision Tree Induction Through LLMs via Semantically-Aware Evolution},
 url = {http://arxiv.org/abs/2503.14217v1},
 year = {2025}
}

@article{2503.14442v1,
 abstract = {Causal abstraction techniques such as Interchange Intervention Training (IIT)
have been proposed to infuse neural network with expert knowledge encoded in
causal models, but their application to real-world problems remains limited.
This article explores the application of IIT in predicting blood glucose levels
in Type 1 Diabetes Mellitus (T1DM) patients. The study utilizes an acyclic
version of the simglucose simulator approved by the FDA to train a Multi-Layer
Perceptron (MLP) model, employing IIT to impose causal relationships. Results
show that the model trained with IIT effectively abstracted the causal
structure and outperformed the standard one in terms of predictive performance
across different prediction horizons (PHs) post-meal. Furthermore, the
breakdown of the counterfactual loss can be leveraged to explain which part of
the causal mechanism are more or less effectively captured by the model. These
preliminary results suggest the potential of IIT in enhancing predictive models
in healthcare by effectively complying with expert knowledge.},
 author = {Ana Esponera and Giovanni Cinà},
 citations = {0},
 comment = {27 pages, 10 pages, to be published in the Proceedings of Machine
  Learning Research (PMLR), to be presented at the conference CLeaR 2025},
 doi = {},
 eprint = {2503.14442v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Inducing Causal Structure for Interpretable Neural Networks Applied to Glucose Prediction for T1DM Patients},
 url = {http://arxiv.org/abs/2503.14442v1},
 year = {2025}
}

@article{2503.14536v2,
 abstract = {Background: This study proposes a Vision-Language Model (VLM) leveraging the
SIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic
tuberculosis (TB) screening. By integrating chest X-ray images with clinical
data, the model addresses the challenges of manual interpretation, improving
diagnostic consistency and accessibility, particularly in resource-constrained
settings.
  Methods: The VLM architecture combines a Vision Transformer (ViT) for visual
encoding and a transformer-based text encoder to process clinical context, such
as patient histories and treatment records. Cross-modal attention mechanisms
align radiographic features with textual information, while the Gemma-3b
decoder generates comprehensive diagnostic reports. The model was pre-trained
on 5 million paired medical images and texts and fine-tuned using 100,000
chronic TB-specific chest X-rays.
  Results: The model demonstrated high precision (94 percent) and recall (94
percent) for detecting key chronic TB pathologies, including fibrosis,
calcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores
exceeded 0.93, and Intersection over Union (IoU) values were above 0.91,
validating its effectiveness in detecting and localizing TB-related
abnormalities.
  Conclusion: The VLM offers a robust and scalable solution for automated
chronic TB diagnosis, integrating radiographic and clinical data to deliver
actionable and context-aware insights. Future work will address subtle
pathologies and dataset biases to enhance the model's generalizability,
ensuring equitable performance across diverse populations and healthcare
settings.},
 author = {Praveen Shastry and Sowmya Chowdary Muthulur and Naveen Kumarasami and Anandakumar D and Mounigasri M and Keerthana R and Kishore Prasath Venkatesh and Bargava Subramanian and Kalyan Sivasailam and Revathi Ezhumalai and Abitha Marimuthu},
 citations = {0},
 comment = {10 pages , 3 figures},
 doi = {},
 eprint = {2503.14536v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models: A Multi modal Framework for Precision Analysis},
 url = {http://arxiv.org/abs/2503.14536v2},
 year = {2025}
}

@article{2503.14567v1,
 abstract = {Raman spectroscopy is becoming more common for medical diagnostics with deep
learning models being increasingly used to leverage its full potential.
However, the opaque nature of such models and the sensitivity of medical
diagnosis together with regulatory requirements necessitate the need for
explainable AI tools. We introduce SpecReX, specifically adapted to explaining
Raman spectra. SpecReX uses the theory of actual causality to rank causal
responsibility in a spectrum, quantified by iteratively refining mutated
versions of the spectrum and testing if it retains the original classification.
The explanations provided by SpecReX take the form of a responsibility map,
highlighting spectral regions most responsible for the model to make a correct
classification. To assess the validity of SpecReX, we create increasingly
complex simulated spectra, in which a "ground truth" signal is seeded, to train
a classifier. We then obtain SpecReX explanations and compare the results with
another explainability tool. By using simulated spectra we establish that
SpecReX localizes to the known differences between classes, under a number of
conditions. This provides a foundation on which we can find the spectral
features which differentiate disease classes. This is an important first step
in proving the validity of SpecReX.},
 author = {Nathan Blake and David A. Kelly and Akchunya Chanchal and Sarah Kapllani-Mucaj and Geraint Thomas and Hana Chockler},
 citations = {1},
 comment = {AAAI Workshop on Health Intelligencee (W3PHIAI-25)},
 doi = {},
 eprint = {2503.14567v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SpecReX: Explainable AI for Raman Spectroscopy},
 url = {http://arxiv.org/abs/2503.14567v1},
 year = {2025}
}

@article{2503.14869v1,
 abstract = {Time series data is one of the most ubiquitous data modalities existing in a
diverse critical domains such as healthcare, seismology, manufacturing and
energy. Recent years, there are increasing interest of the data mining
community to develop time series deep learning models to pursue better
performance. The models performance often evaluate by certain evaluation
metrics such as RMSE, Accuracy, and F1-score. Yet time series data are often
hard to interpret and are collected with unknown environmental factors, sensor
configuration, latent physic mechanisms, and non-stationary evolving behavior.
As a result, a model that is better on standard metric-based evaluation may not
always perform better in real-world tasks. In this blue sky paper, we aim to
explore the challenge that exists in the metric-based evaluation framework for
time series data mining and propose a potential blue-sky idea -- developing a
knowledge-discovery-based evaluation framework, which aims to effectively
utilize domain-expertise knowledge to evaluate a model. We demonstrate that an
evidence-seeking explanation can potentially have stronger persuasive power
than metric-based evaluation and obtain better generalization ability for time
series data mining tasks.},
 author = {Li Zhang},
 citations = {},
 comment = {accepted in SIAM SDM 2025 - Blue Sky Track (to appear)},
 doi = {10.1137/1.9781611978520.36},
 eprint = {2503.14869v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Evaluating Time Series Models with Knowledge Discovery},
 url = {http://arxiv.org/abs/2503.14869v1},
 year = {2025}
}

@article{2503.14973v2,
 abstract = {Building trust in reinforcement learning (RL) agents requires understanding
why they make certain decisions, especially in high-stakes applications like
robotics, healthcare, and finance. Existing explainability methods often focus
on single states or entire trajectories, either providing only local, step-wise
insights or attributing decisions to coarse, episodelevel summaries. Both
approaches miss the recurring strategies and temporally extended patterns that
actually drive agent behavior across multiple decisions. We address this gap by
proposing a fully offline, reward-free framework for behavior discovery and
segmentation, enabling the attribution of actions to meaningful and
interpretable behavior segments that capture recurring patterns appearing
across multiple trajectories. Our method identifies coherent behavior clusters
from state-action sequences and attributes individual actions to these clusters
for fine-grained, behavior-centric explanations. Evaluations on four diverse
offline RL environments show that our approach discovers meaningful behaviors
and outperforms trajectory-level baselines in fidelity, human preference, and
cluster coherence. Our code is publicly available.},
 author = {Rishav Rishav and Somjit Nath and Vincent Michalski and Samira Ebrahimi Kahou},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2503.14973v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Behaviour Discovery and Attribution for Explainable Reinforcement Learning},
 url = {http://arxiv.org/abs/2503.14973v2},
 year = {2025}
}

@article{2503.15821v2,
 abstract = {Aggressive behavior, including aggression towards others and self-injury,
occurs in up to 80% of children and adolescents with autism, making it a
leading cause of behavioral health referrals and a major driver of healthcare
costs. Predicting when autistic youth will exhibit aggression can be
challenging due to their communication difficulties. Many are minimally verbal
or have poor emotional insight. Recent advances in Machine Learning and
wearable biosensing demonstrate the ability to predict aggression within a
limited future window (typically one to three minutes) in autistic individuals.
However, existing works don't estimate aggression onset probability or the
expected number of aggression onsets over longer periods, nor do they provide
interpretable insights into onset dynamics. To address these limitations, we
apply Temporal Point Processes (TPPs) - particularly self-exciting Hawkes
processes - to model the timing of aggressive behavior onsets in psychiatric
inpatient autistic youth. We benchmark several TPP models by evaluating their
goodness-of-fit and predictive metrics. Our results demonstrate that
self-exciting TPPs more accurately captures the irregular and clustered nature
of aggression onsets, especially compared to traditional Poisson models. These
incipient findings suggest that TPPs can provide interpretable, probabilistic
forecasts of aggression onset along a time continuum, supporting future
clinical decision-making and preemptive intervention.},
 author = {Michael Potter and Michael Everett and Ashutosh Singh and Georgios Stratis and Yuna Watanabe and Ahmet Demirkaya and Deniz Erdogmus and Tales Imbiriba and Matthew S. Goodwin},
 citations = {1},
 comment = {Submitted to Nature Scientific Reports},
 doi = {},
 eprint = {2503.15821v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Temporal Point Process Modeling of Aggressive Behavior Onset in Psychiatric Inpatient Youths with Autism},
 url = {http://arxiv.org/abs/2503.15821v2},
 year = {2025}
}

@article{2503.16938v1,
 abstract = {Decision-making processes in healthcare can be highly complex and
challenging. Machine Learning tools offer significant potential to assist in
these processes. However, many current methodologies rely on complex models
that are not easily interpretable by experts. This underscores the need to
develop interpretable models that can provide meaningful support in clinical
decision-making. When approaching such tasks, humans typically compare the
situation at hand to a few key examples and representative cases imprinted in
their memory. Using an approach which selects such exemplary cases and grounds
its predictions on them could contribute to obtaining high-performing
interpretable solutions to such problems. To this end, we evaluate PivotTree,
an interpretable prototype selection model, on an oral lesion detection
problem, specifically trying to detect the presence of neoplastic, aphthous and
traumatic ulcerated lesions from oral cavity images. We demonstrate the
efficacy of using such method in terms of performance and offer a qualitative
and quantitative comparison between exemplary cases and ground-truth prototypes
selected by experts.},
 author = {Alessio Cascione and Mattia Setzu and Federico A. Galatolo and Mario G. C. A. Cimino and Riccardo Guidotti},
 citations = {0},
 comment = {},
 doi = {10.1007/978-3-031-78980-9_20},
 eprint = {2503.16938v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable Machine Learning for Oral Lesion Diagnosis through Prototypical Instances Identification},
 url = {http://arxiv.org/abs/2503.16938v1},
 year = {2025}
}

@article{2503.18213v1,
 abstract = {Over the last few decades, Artificial Intelligence (AI) scientists have been
conducting investigations to attain human-level performance by a machine in
accomplishing a cognitive task. Within machine learning, the ultimate
aspiration is to attain Artificial General Intelligence (AGI) through a
machine. This pursuit has led to the exploration of two distinct AI paradigms.
Symbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and
Connectionist (Sub-symbolic) AI, represented by Neural Systems, are two
mutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,
and knowledge representation but faces challenges in processing complex
real-world data with noise. Conversely, deep learning (Black-Box systems)
research breakthroughs in neural networks are notable, yet they lack reasoning
and interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI
research, attempts to bridge this gap by integrating logical reasoning into
neural networks, enabling them to learn and reason with symbolic
representations. While a long path, this strategy has made significant progress
towards achieving common sense reasoning by systems. This article conducts an
extensive review of over 977 studies from prominent scientific databases (DBLP,
ACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the
multifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its
healthcare applications, particularly in drug discovery, and Protein
engineering research. The survey addresses vital themes, including reasoning,
explainability, integration strategies, 41 healthcare-related use cases,
benchmarking, datasets, current approach limitations from both healthcare and
broader perspectives, and proposed novel approaches for future experiments.},
 author = {Delower Hossain and Jake Y Chen},
 citations = {},
 comment = {18 pages},
 doi = {},
 eprint = {2503.18213v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Study on Neuro-Symbolic Artificial Intelligence: Healthcare Perspectives},
 url = {http://arxiv.org/abs/2503.18213v1},
 year = {2025}
}

@article{2503.18994v1,
 abstract = {This paper introduces the HH4AI Methodology, a structured approach to
assessing the impact of AI systems on human rights, focusing on compliance with
the EU AI Act and addressing technical, ethical, and regulatory challenges. The
paper highlights AIs transformative nature, driven by autonomy, data, and
goal-oriented design, and how the EU AI Act promotes transparency,
accountability, and safety. A key challenge is defining and assessing
"high-risk" AI systems across industries, complicated by the lack of
universally accepted standards and AIs rapid evolution.
  To address these challenges, the paper explores the relevance of ISO/IEC and
IEEE standards, focusing on risk management, data quality, bias mitigation, and
governance. It proposes a Fundamental Rights Impact Assessment (FRIA)
methodology, a gate-based framework designed to isolate and assess risks
through phases including an AI system overview, a human rights checklist, an
impact assessment, and a final output phase. A filtering mechanism tailors the
assessment to the system's characteristics, targeting areas like
accountability, AI literacy, data governance, and transparency.
  The paper illustrates the FRIA methodology through a fictional case study of
an automated healthcare triage service. The structured approach enables
systematic filtering, comprehensive risk assessment, and mitigation planning,
effectively prioritizing critical risks and providing clear remediation
strategies. This promotes better alignment with human rights principles and
enhances regulatory compliance.},
 author = {Paolo Ceravolo and Ernesto Damiani and Maria Elisa D'Amico and Bianca de Teffe Erb and Simone Favaro and Nannerel Fiano and Paolo Gambatesa and Simone La Porta and Samira Maghool and Lara Mauri and Niccolo Panigada and Lorenzo Maria Ratto Vaquer and Marta A. Tamborini},
 citations = {1},
 comment = {19 pages, 7 figures, 1 table},
 doi = {},
 eprint = {2503.18994v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {HH4AI: A methodological Framework for AI Human Rights impact assessment under the EUAI ACT},
 url = {http://arxiv.org/abs/2503.18994v1},
 year = {2025}
}

@article{2503.19285v3,
 abstract = {Despite the outstanding performance of deep learning models in clinical
prediction tasks, explainability remains a significant challenge. Inspired by
transformer architectures, we introduce the Temporal-Feature Cross Attention
Mechanism (TFCAM), a novel deep learning framework designed to capture dynamic
interactions among clinical features across time, enhancing both predictive
accuracy and interpretability. In an experiment with 1,422 patients with
Chronic Kidney Disease, predicting progression to End-Stage Renal Disease,
TFCAM outperformed LSTM and RETAIN baselines, achieving an AUROC of 0.95 and an
F1-score of 0.69. Beyond performance gains, TFCAM provides multi-level
explainability by identifying critical temporal periods, ranking feature
importance, and quantifying how features influence each other across time
before affecting predictions. Our approach addresses the "black box"
limitations of deep learning in healthcare, offering clinicians transparent
insights into disease progression mechanisms while maintaining state-of-the-art
predictive performance.},
 author = {Yubo Li and Xinyu Yao and Rema Padman},
 citations = {0},
 comment = {10 pages, 3 figures, submitted to AMIA 2025},
 doi = {},
 eprint = {2503.19285v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {No Black Box Anymore: Demystifying Clinical Predictive Modeling with Temporal-Feature Cross Attention Mechanism},
 url = {http://arxiv.org/abs/2503.19285v3},
 year = {2025}
}

@article{2503.19394v1,
 abstract = {Current machine learning approaches to medical diagnosis often rely on
correlational patterns between symptoms and diseases, risking misdiagnoses when
symptoms are ambiguous or common across multiple conditions. In this work, we
move beyond correlation to investigate the causal influence of key
symptoms-specifically "chest pain" on diagnostic predictions. Leveraging the
CausaLM framework, we generate counterfactual text representations in which
target concepts are effectively "forgotten" enabling a principled estimation of
the causal effect of that concept on a model's predicted disease distribution.
By employing Textual Representation-based Average Treatment Effect (TReATE), we
quantify how the presence or absence of a symptom shapes the model's diagnostic
outcomes, and contrast these findings against correlation-based baselines such
as CONEXP. Our results offer deeper insight into the decision-making behavior
of clinical NLP models and have the potential to inform more trustworthy,
interpretable, and causally-grounded decision support tools in medical
practice.},
 author = {Mehul Shetty and Connor Jordan},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2503.19394v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Quantifying Symptom Causality in Clinical Decision Making: An Exploration Using CausaLM},
 url = {http://arxiv.org/abs/2503.19394v1},
 year = {2025}
}

@article{2503.20758v1,
 abstract = {Ensuring transparency in machine learning decisions is critically important,
especially in sensitive sectors such as healthcare, finance, and justice.
Despite this, some popular explainable algorithms, such as Local Interpretable
Model-agnostic Explanations (LIME), often produce unstable explanations due to
the random generation of perturbed samples. Random perturbation introduces
small changes or noise to modified instances of the original data, leading to
inconsistent explanations. Even slight variations in the generated samples
significantly affect the explanations provided by such models, undermining
trust and hindering the adoption of interpretable models. To address this
challenge, we propose MindfulLIME, a novel algorithm that intelligently
generates purposive samples using a graph-based pruning algorithm and
uncertainty sampling. MindfulLIME substantially improves the consistency of
visual explanations compared to random sampling approaches. Our experimental
evaluation, conducted on a widely recognized chest X-ray dataset, confirms
MindfulLIME's stability with a 100% success rate in delivering reliable
explanations under identical conditions. Additionally, MindfulLIME improves the
localization precision of visual explanations by reducing the distance between
the generated explanations and the actual local annotations compared to LIME.
We also performed comprehensive experiments considering various segmentation
algorithms and sample numbers, focusing on stability, quality, and efficiency.
The results demonstrate the outstanding performance of MindfulLIME across
different segmentation settings, generating fewer high-quality samples within a
reasonable processing time. By addressing the stability limitations of LIME in
image data, MindfulLIME enhances the trustworthiness and interpretability of
machine learning models in specific medical imaging applications, a critical
domain.},
 author = {Shakiba Rahimiaghdam and Hande Alemdar},
 citations = {},
 comment = {},
 doi = {10.1007/s00521-025-11583-x},
 eprint = {2503.20758v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MindfulLIME: A Stable Solution for Explanations of Machine Learning Models with Enhanced Localization Precision -- A Medical Image Case Study},
 url = {http://arxiv.org/abs/2503.20758v1},
 year = {2025}
}

@article{2503.22257v1,
 abstract = {Learning from longitudinal electronic health records is limited if it does
not capture the temporal trajectories of the patient's state in a clinical
setting. Graph models allow us to capture the hidden dependencies of the
multivariate time-series when the graphs are constructed in a similar dynamic
manner. Previous dynamic graph models require a pre-defined and/or static graph
structure, which is unknown in most cases, or they only capture the spatial
relations between the features. Furthermore in healthcare, the interpretability
of the model is an essential requirement to build trust with clinicians. In
addition to previously proposed attention mechanisms, there has not been an
interpretable dynamic graph framework for data from multivariate electronic
health records (EHRs). Here, we propose DynaGraph, an end-to-end interpretable
contrastive graph model that learns the dynamics of multivariate time-series
EHRs as part of optimisation. We validate our model in four real-world clinical
datasets, ranging from primary care to secondary care settings with broad
demographics, in challenging settings where tasks are imbalanced and
multi-labelled. Compared to state-of-the-art models, DynaGraph achieves
significant improvements in balanced accuracy and sensitivity over the nearest
complex competitors in time-series or dynamic graph modelling across three ICU
and one primary care datasets. Through a pseudo-attention approach to graph
construction, our model also indicates the importance of clinical covariates
over time, providing means for clinical validation.},
 author = {Munib Mesinovic and Soheila Molaei and Peter Watkinson and Tingting Zhu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2503.22257v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DynaGraph: Interpretable Multi-Label Prediction from EHRs via Dynamic Graph Learning and Contrastive Augmentation},
 url = {http://arxiv.org/abs/2503.22257v1},
 year = {2025}
}

@article{2503.23819v1,
 abstract = {Deep learning based diagnostic AI systems based on medical images are
starting to provide similar performance as human experts. However these data
hungry complex systems are inherently black boxes and therefore slow to be
adopted for high risk applications like healthcare. This problem of lack of
transparency is exacerbated in the case of recent large foundation models,
which are trained in a self supervised manner on millions of data points to
provide robust generalisation across a range of downstream tasks, but the
embeddings generated from them happen through a process that is not
interpretable, and hence not easily trustable for clinical applications. To
address this timely issue, we deploy conformal analysis to quantify the
predictive uncertainty of a vision transformer (ViT) based foundation model
across patient demographics with respect to sex, age and ethnicity for the
tasks of skin lesion classification using several public benchmark datasets.
The significant advantage of this method is that conformal analysis is method
independent and it not only provides a coverage guarantee at population level
but also provides an uncertainty score for each individual. We used a
model-agnostic dynamic F1-score-based sampling during model training, which
helped to stabilize the class imbalance and we investigate the effects on
uncertainty quantification (UQ) with or without this bias mitigation step. Thus
we show how this can be used as a fairness metric to evaluate the robustness of
the feature embeddings of the foundation model (Google DermFoundation) and thus
advance the trustworthiness and fairness of clinical AI.},
 author = {Swarnava Bhattacharyya and Umapada Pal and Tapabrata Chakraborti},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2503.23819v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics},
 url = {http://arxiv.org/abs/2503.23819v1},
 year = {2025}
}

@article{2504.00053v1,
 abstract = {Objective: Electronic health records (EHR) are widely available to complement
administrative data-based disease surveillance and healthcare performance
evaluation. Defining conditions from EHR is labour-intensive and requires
extensive manual labelling of disease outcomes. This study developed an
efficient strategy based on advanced large language models to identify multiple
conditions from EHR clinical notes. Methods: We linked a cardiac registry
cohort in 2015 with an EHR system in Alberta, Canada. We developed a pipeline
that leveraged a generative large language model (LLM) to analyze, understand,
and interpret EHR notes by prompts based on specific diagnosis, treatment
management, and clinical guidelines. The pipeline was applied to detect acute
myocardial infarction (AMI), diabetes, and hypertension. The performance was
compared against clinician-validated diagnoses as the reference standard and
widely adopted International Classification of Diseases (ICD) codes-based
methods. Results: The study cohort accounted for 3,088 patients and 551,095
clinical notes. The prevalence was 55.4%, 27.7%, 65.9% and for AMI, diabetes,
and hypertension, respectively. The performance of the LLM-based pipeline for
detecting conditions varied: AMI had 88% sensitivity, 63% specificity, and 77%
positive predictive value (PPV); diabetes had 91% sensitivity, 86% specificity,
and 71% PPV; and hypertension had 94% sensitivity, 32% specificity, and 72%
PPV. Compared with ICD codes, the LLM-based method demonstrated improved
sensitivity and negative predictive value across all conditions. The monthly
percentage trends from the detected cases by LLM and reference standard showed
consistent patterns.},
 author = {Jie Pan and Seungwon Lee and Cheligeer Cheligeer and Elliot A. Martin and Kiarash Riazi and Hude Quan and Na Li},
 citations = {1},
 comment = {},
 doi = {10.1016/j.compbiomed.2025.110161},
 eprint = {2504.00053v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Integrating Large Language Models with Human Expertise for Disease Detection in Electronic Health Records},
 url = {http://arxiv.org/abs/2504.00053v1},
 year = {2025}
}

@article{2504.00060v2,
 abstract = {As deep learning continues to advance, the transparency of neural network
decision-making remains a critical challenge, limiting trust and applicability
in high-stakes domains. Class Activation Mapping (CAM) techniques have emerged
as a key approach toward visualizing model decisions, yet existing methods face
inherent trade-offs. Gradient-based CAM variants suffer from sensitivity to
gradient perturbations due to gradient noise, leading to unstable and
unreliable explanations. Conversely, gradient-free approaches mitigate gradient
instability but incur significant computational overhead and inference latency.
To address these limitations, we propose a Cluster Filter Class Activation Map
(CF-CAM) technique, a novel framework that reintroduces gradient-based
weighting while enhancing robustness against gradient noise. CF-CAM utilizes
hierarchical importance weighting strategy to balance discriminative feature
preservation and noise elimination. A density-aware channel clustering method
via Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups
semantically relevant feature channels and discard noise-prone activations.
Additionally, cluster-conditioned gradient filtering leverages Gaussian filters
to refine gradient signals, preserving edge-aware localization while
suppressing noise impact. Experiment results demonstrate that CF-CAM achieves
superior interpretability performance while enhancing computational efficiency,
outperforming state-of-the-art CAM methods in faithfulness and robustness. By
effectively mitigating gradient instability without excessive computational
cost, CF-CAM provides a competitive solution for enhancing the interpretability
of deep neural networks in critical applications such as autonomous driving and
medical diagnosis.},
 author = {Hongjie He and Xu Pan and Yudong Yao},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2504.00060v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {CF-CAM: Cluster Filter Class Activation Mapping for Reliable Gradient-Based Interpretability},
 url = {http://arxiv.org/abs/2504.00060v2},
 year = {2025}
}

@article{2504.00485v3,
 abstract = {Heart disease remains a leading cause of mortality and morbidity worldwide,
necessitating the development of accurate and reliable predictive models to
facilitate early detection and intervention. While state of the art work has
focused on various machine learning approaches for predicting heart disease,
but they could not able to achieve remarkable accuracy. In response to this
need, we applied nine machine learning algorithms XGBoost, logistic regression,
decision tree, random forest, k-nearest neighbors (KNN), support vector machine
(SVM), gaussian na\"ive bayes (NB gaussian), adaptive boosting, and linear
regression to predict heart disease based on a range of physiological
indicators. Our approach involved feature selection techniques to identify the
most relevant predictors, aimed at refining the models to enhance both
performance and interpretability. The models were trained, incorporating
processes such as grid search hyperparameter tuning, and cross-validation to
minimize overfitting. Additionally, we have developed a novel voting system
with feature selection techniques to advance heart disease classification.
Furthermore, we have evaluated the models using key performance metrics
including accuracy, precision, recall, F1-score, and the area under the
receiver operating characteristic curve (ROC AUC). Among the models, XGBoost
demonstrated exceptional performance, achieving 99% accuracy, precision,
F1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach
to early heart disease diagnosis and preventive healthcare.},
 author = {Mahade Hasan and Farhana Yasmin and Xue Yu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2504.00485v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Stroke Disease Classification Using Machine Learning with Feature Selection Techniques},
 url = {http://arxiv.org/abs/2504.00485v3},
 year = {2025}
}

@article{2504.04243v2,
 abstract = {The design of AI systems to assist human decision-making typically requires
the availability of labels to train and evaluate supervised models. Frequently,
however, these labels are unknown, and different ways of estimating them
involve unverifiable assumptions or arbitrary choices. In this work, we
introduce the concept of label indeterminacy and derive important implications
in high-stakes AI-assisted decision-making. We present an empirical study in a
healthcare context, focusing specifically on predicting the recovery of
comatose patients after resuscitation from cardiac arrest. Our study shows that
label indeterminacy can result in models that perform similarly when evaluated
on patients with known labels, but vary drastically in their predictions for
patients where labels are unknown. After demonstrating crucial ethical
implications of label indeterminacy in this high-stakes context, we discuss
takeaways for evaluation, reporting, and design.},
 author = {Jakob Schoeffer and Maria De-Arteaga and Jonathan Elmer},
 citations = {1},
 comment = {The 2025 ACM Conference on Fairness, Accountability, and Transparency
  (FAccT '25)},
 doi = {10.1145/3715275.3732070},
 eprint = {2504.04243v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest},
 url = {http://arxiv.org/abs/2504.04243v2},
 year = {2025}
}

@article{2504.04262v1,
 abstract = {Chronic Kidney Disease (CKD) is a major global health issue which is
affecting million people around the world and with increasing rate of
mortality. Mitigation of progression of CKD and better patient outcomes
requires early detection. Nevertheless, limitations lie in traditional
diagnostic methods, especially in resource constrained settings. This study
proposes an advanced machine learning approach to enhance CKD detection by
evaluating four models: Random Forest (RF), Multi-Layer Perceptron (MLP),
Logistic Regression (LR), and a fine-tuned CatBoost algorithm. Specifically,
among these, the fine-tuned CatBoost model demonstrated the best overall
performance having an accuracy of 98.75%, an AUC of 0.9993 and a Kappa score of
97.35% of the studies. The proposed CatBoost model has used a nature inspired
algorithm such as Simulated Annealing to select the most important features,
Cuckoo Search to adjust outliers and grid search to fine tune its settings in
such a way to achieve improved prediction accuracy. Features significance is
explained by SHAP-a well-known XAI technique-for gaining transparency in the
decision-making process of proposed model and bring up trust in diagnostic
systems. Using SHAP, the significant clinical features were identified as
specific gravity, serum creatinine, albumin, hemoglobin, and diabetes mellitus.
The potential of advanced machine learning techniques in CKD detection is shown
in this research, particularly for low income and middle-income healthcare
settings where prompt and correct diagnoses are vital. This study seeks to
provide a highly accurate, interpretable, and efficient diagnostic tool to add
to efforts for early intervention and improved healthcare outcomes for all CKD
patients.},
 author = {Md. Ehsanul Haque and S. M. Jahidul Islam and Jeba Maliha and Md. Shakhauat Hossan Sumon and Rumana Sharmin and Sakib Rokoni},
 citations = {3},
 comment = {8 page, 8 figures , conference : 14th IEEE International Conference
  on Communication Systems and Network Technologies (CSNT2025)},
 doi = {10.1109/csnt64827.2025.10968421},
 eprint = {2504.04262v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Improving Chronic Kidney Disease Detection Efficiency: Fine Tuned CatBoost and Nature-Inspired Algorithms with Explainable AI},
 url = {http://arxiv.org/abs/2504.04262v1},
 year = {2025}
}

@article{2504.04276v1,
 abstract = {This paper compares model-agnostic and model-specific approaches to
explainable AI (XAI) in deep learning image classification. I examine how LIME
and SHAP (model-agnostic methods) differ from Grad-CAM and Guided
Backpropagation (model-specific methods) when interpreting ResNet50 predictions
across diverse image categories. Through extensive testing with various species
from dogs and birds to insects I found that each method reveals different
aspects of the models decision-making process. Model-agnostic techniques
provide broader feature attribution that works across different architectures,
while model-specific approaches excel at highlighting precise activation
regions with greater computational efficiency. My analysis shows there is no
"one-size-fits-all" solution for model interpretability. Instead, combining
multiple XAI methods offers the most comprehensive understanding of complex
models particularly valuable in high-stakes domains like healthcare, autonomous
vehicles, and financial services where transparency is crucial. This
comparative framework provides practical guidance for selecting appropriate
interpretability techniques based on specific application needs and
computational constraints.},
 author = {Keerthi Devireddy},
 citations = {2},
 comment = {},
 doi = {},
 eprint = {2504.04276v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific Approaches},
 url = {http://arxiv.org/abs/2504.04276v1},
 year = {2025}
}

@article{2504.04462v1,
 abstract = {Large Language Models (LLMs) have significantly advanced sentiment analysis,
yet their inherent uncertainty and variability pose critical challenges to
achieving reliable and consistent outcomes. This paper systematically explores
the Model Variability Problem (MVP) in LLM-based sentiment analysis,
characterized by inconsistent sentiment classification, polarization, and
uncertainty arising from stochastic inference mechanisms, prompt sensitivity,
and biases in training data. We analyze the core causes of MVP, presenting
illustrative examples and a case study to highlight its impact. In addition, we
investigate key challenges and mitigation strategies, paying particular
attention to the role of temperature as a driver of output randomness and
emphasizing the crucial role of explainability in improving transparency and
user trust. By providing a structured perspective on stability,
reproducibility, and trustworthiness, this study helps develop more reliable,
explainable, and robust sentiment analysis models, facilitating their
deployment in high-stakes domains such as finance, healthcare, and
policymaking, among others.},
 author = {David Herrera-Poyatos and Carlos Peláez-González and Cristina Zuheros and Andrés Herrera-Poyatos and Virilo Tejedor and Francisco Herrera and Rosana Montes},
 citations = {3},
 comment = {25 pages and 3 figures},
 doi = {10.3389/frai.2025.1609097},
 eprint = {2504.04462v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitigation strategies and the role of explainability},
 url = {http://arxiv.org/abs/2504.04462v1},
 year = {2025}
}

@article{2504.04505v1,
 abstract = {Contextual multi-armed bandits are a popular choice to model sequential
decision-making. E.g., in a healthcare application we may perform various tests
to asses a patient condition (exploration) and then decide on the best
treatment to give (exploitation). When humans design strategies, they aim for
the exploration to be fast, since the patient's health is at stake, and easy to
interpret for a physician overseeing the process. However, common bandit
algorithms are nothing like that: The regret caused by exploration scales with
$\sqrt{H}$ over $H$ rounds and decision strategies are based on opaque
statistical considerations. In this paper, we use an original classification
view to meta learn interpretable and fast exploration plans for a fixed
collection of bandits $\mathbb{M}$. The plan is prescribed by an interpretable
decision tree probing decisions' payoff to classify the test bandit. The test
regret of the plan in the stochastic and contextual setting scales with $O
(\lambda^{-2} C_{\lambda} (\mathbb{M}) \log^2 (MH))$, being $M$ the size of
$\mathbb{M}$, $\lambda$ a separation parameter over the bandits, and $C_\lambda
(\mathbb{M})$ a novel classification-coefficient that fundamentally links meta
learning bandits with classification. Through a nearly matching lower bound, we
show that $C_\lambda (\mathbb{M})$ inherently captures the complexity of the
setting.},
 author = {Mirco Mutti and Jeongyeol Kwon and Shie Mannor and Aviv Tamar},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2504.04505v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Classification View on Meta Learning Bandits},
 url = {http://arxiv.org/abs/2504.04505v1},
 year = {2025}
}

@article{2504.04549v1,
 abstract = {While deep learning has exhibited remarkable predictive capabilities in
various medical image tasks, its inherent black-box nature has hindered its
widespread implementation in real-world healthcare settings. Our objective is
to unveil the decision-making processes of deep learning models in the context
of glaucoma classification by employing several Class Activation Map (CAM)
techniques to generate model focus regions and comparing them with clinical
domain knowledge of the anatomical area (optic cup, optic disk, and blood
vessels). Four deep neural networks, including VGG-11, ResNet-18, DeiT-Tiny,
and Swin Transformer-Tiny, were developed using binary diagnostic labels of
glaucoma and five CAM methods (Grad-CAM, XGrad-CAM, Score-CAM, Eigen-CAM, and
Layer-CAM) were employed to highlight the model focus area. We applied the
paired-sample t-test to compare the percentage of anatomies in the model focus
area to the proportion of anatomies in the entire image. After that, Pearson's
and Spearman's correlation tests were implemented to examine the relationship
between model predictive ability and the percentage of anatomical structures in
the model focus area. On five public glaucoma datasets, all deep learning
models consistently displayed statistically significantly higher percentages of
anatomical structures in the focus area than the proportions of anatomical
structures in the entire image. Also, we validated the positive relationship
between the percentage of anatomical structures in the focus area and model
predictive performance. Our study provides evidence of the convergence of
decision logic between deep neural networks and human clinicians through
rigorous statistical tests. We anticipate that it can help alleviate
clinicians' concerns regarding the trustworthiness of deep learning in
healthcare. For reproducibility, the code and dataset have been released at
GitHub.},
 author = {Han Yuan and Lican Kang and Yong Li},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2504.04549v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Opening the black box of deep learning: Validating the statistical association between explainable artificial intelligence (XAI) and clinical domain knowledge in fundus image-based glaucoma diagnosis},
 url = {http://arxiv.org/abs/2504.04549v1},
 year = {2025}
}

@article{2504.04703v1,
 abstract = {Artificial intelligence-augmented technology represents a considerable
opportunity for improving healthcare delivery. Significant progress has been
made to demonstrate the value of complex models to enhance clinicians`
efficiency in decision-making. However, the clinical adoption of such models is
scarce due to multifaceted implementation issues, with the explainability of AI
models being among them. One of the substantially documented areas of concern
is the unclear AI explainability that negatively influences clinicians`
considerations for accepting the complex model. With a usability study engaging
20 U.S.-based clinicians and following the qualitative reflexive thematic
analysis, this study develops and presents a concrete framework and an
operational definition of explainability. The framework can inform the required
customizations and feature developments in AI tools to support clinicians`
preferences and enhance their acceptance.},
 author = {Mohammad Golam Kibria and Lauren Kucirka and Javed Mostafa},
 citations = {},
 comment = {10 pages, 4 figures},
 doi = {},
 eprint = {2504.04703v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Usability Testing of an Explainable AI-enhanced Tool for Clinical Decision Support: Insights from the Reflexive Thematic Analysis},
 url = {http://arxiv.org/abs/2504.04703v1},
 year = {2025}
}

@article{2504.04814v2,
 abstract = {Trustworthy artificial intelligence (AI) is essential in healthcare,
particularly for high-stakes tasks like medical image segmentation. Explainable
AI and uncertainty quantification significantly enhance AI reliability by
addressing key attributes such as robustness, usability, and explainability.
Despite extensive technical advances in uncertainty quantification for medical
imaging, understanding the clinical informativeness and interpretability of
uncertainty remains limited. This study introduces a novel framework to explain
the potential sources of predictive uncertainty, specifically in cortical
lesion segmentation in multiple sclerosis using deep ensembles. The proposed
analysis shifts the focus from the uncertainty-error relationship towards
relevant medical and engineering factors. Our findings reveal that
instance-wise uncertainty is strongly related to lesion size, shape, and
cortical involvement. Expert rater feedback confirms that similar factors
impede annotator confidence. Evaluations conducted on two datasets (206
patients, almost 2000 lesions) under both in-domain and distribution-shift
conditions highlight the utility of the framework in different scenarios.},
 author = {Nataliia Molchanova and Pedro M. Gordaliza and Alessandro Cagol and Mario Ocampo--Pineda and Po--Jui Lu and Matthias Weigel and Xinjie Chen and Erin S. Beck and Haris Tsagkas and Daniel Reich and Anna Stölting and Pietro Maggi and Delphine Ribes and Adrien Depeursinge and Cristina Granziera and Henning Müller and Meritxell Bach Cuadra},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2504.04814v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explaining Uncertainty in Multiple Sclerosis Lesion Segmentation Beyond Prediction Errors},
 url = {http://arxiv.org/abs/2504.04814v2},
 year = {2025}
}

@article{2504.05278v1,
 abstract = {This study investigates uncertainty quantification in large language models
(LLMs) for medical applications, emphasizing both technical innovations and
philosophical implications. As LLMs become integral to clinical
decision-making, accurately communicating uncertainty is crucial for ensuring
reliable, safe, and ethical AI-assisted healthcare. Our research frames
uncertainty not as a barrier but as an essential part of knowledge that invites
a dynamic and reflective approach to AI design. By integrating advanced
probabilistic methods such as Bayesian inference, deep ensembles, and Monte
Carlo dropout with linguistic analysis that computes predictive and semantic
entropy, we propose a comprehensive framework that manages both epistemic and
aleatoric uncertainties. The framework incorporates surrogate modeling to
address limitations of proprietary APIs, multi-source data integration for
better context, and dynamic calibration via continual and meta-learning.
Explainability is embedded through uncertainty maps and confidence metrics to
support user trust and clinical interpretability. Our approach supports
transparent and ethical decision-making aligned with Responsible and Reflective
AI principles. Philosophically, we advocate accepting controlled ambiguity
instead of striving for absolute predictability, recognizing the inherent
provisionality of medical knowledge.},
 author = {Zahra Atf and Seyed Amir Ahmad Safavi-Naini and Peter R. Lewis and Aref Mahjoubfar and Nariman Naderi and Thomas R. Savage and Ali Soroush},
 citations = {10},
 comment = {25 pages, 11 figures},
 doi = {},
 eprint = {2504.05278v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {The challenge of uncertainty quantification of large language models in medicine},
 url = {http://arxiv.org/abs/2504.05278v1},
 year = {2025}
}

@article{2504.05331v3,
 abstract = {As artificial intelligence (AI) becomes embedded in healthcare, trust in
medical decision-making is changing fast. Nowhere is this shift more visible
than in radiology, where AI tools are increasingly embedded across the imaging
workflow - from scheduling and acquisition to interpretation, reporting, and
communication with referrers and patients. This opinion paper argues that trust
in AI isn't a simple transfer from humans to machines - it is a dynamic,
evolving relationship that must be built and maintained. Rather than debating
whether AI belongs in medicine, it asks: what kind of trust must AI earn, and
how? Drawing from philosophy, bioethics, and system design, it explores the key
differences between human trust and machine reliability - emphasizing
transparency, accountability, and alignment with the values of good care. It
argues that trust in AI should not be built on mimicking empathy or intuition,
but on thoughtful design, responsible deployment, and clear moral
responsibility. The goal is a balanced view - one that avoids blind optimism
and reflexive fear. Trust in AI must be treated not as a given, but as
something to be earned over time.},
 author = {Jan Beger},
 citations = {},
 comment = {},
 doi = {10.1016/j.ejrai.2025.100038},
 eprint = {2504.05331v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Not someone, but something: Rethinking trust in the age of medical AI},
 url = {http://arxiv.org/abs/2504.05331v3},
 year = {2025}
}

@article{2504.08150v1,
 abstract = {This study addresses the challenge of predicting post-stroke rigidity by
emphasizing feature interactions through graph-based explainable AI.
Post-stroke rigidity, characterized by increased muscle tone and stiffness,
significantly affects survivors' mobility and quality of life. Despite its
prevalence, early prediction remains limited, delaying intervention. We analyze
519K stroke hospitalization records from the Healthcare Cost and Utilization
Project dataset, where 43% of patients exhibited rigidity. We compare
traditional approaches such as Logistic Regression, XGBoost, and Transformer
with graph-based models like Graphormer and Graph Attention Network. These
graph models inherently capture feature interactions and incorporate intrinsic
or post-hoc explainability. Our results show that graph-based methods
outperform others (AUROC 0.75), identifying key predictors such as NIH Stroke
Scale and APR-DRG mortality risk scores. They also uncover interactions missed
by conventional models. This research provides a novel application of
graph-based XAI in stroke prognosis, with potential to guide early
identification and personalized rehabilitation strategies.},
 author = {Jiawei Xu and Yonggeon Lee and Anthony Elkommos Youssef and Eunjin Yun and Tinglin Huang and Tianjian Guo and Hamidreza Saber and Rex Ying and Ying Ding},
 citations = {},
 comment = {Jiawei Xu and Yonggeon Lee contributed equally to this work},
 doi = {},
 eprint = {2504.08150v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Beyond Feature Importance: Feature Interactions in Predicting Post-Stroke Rigidity with Graph Explainable AI},
 url = {http://arxiv.org/abs/2504.08150v1},
 year = {2025}
}

@article{2504.08418v1,
 abstract = {Fairness in artificial intelligence (AI) prediction models is increasingly
emphasized to support responsible adoption in high-stakes domains such as
health care and criminal justice. Guidelines and implementation frameworks
highlight the importance of both predictive accuracy and equitable outcomes.
However, current fairness toolkits often evaluate classification performance
disparities in isolation, with limited attention to other critical aspects such
as calibration. To address these gaps, we present seeBias, an R package for
comprehensive evaluation of model fairness and predictive performance. seeBias
offers an integrated evaluation across classification, calibration, and other
performance domains, providing a more complete view of model behavior. It
includes customizable visualizations to support transparent reporting and
responsible AI implementation. Using public datasets from criminal justice and
healthcare, we demonstrate how seeBias supports fairness evaluations, and
uncovers disparities that conventional fairness metrics may overlook. The R
package is available on GitHub, and a Python version is under development.},
 author = {Yilin Ning and Yian Ma and Mingxuan Liu and Xin Li and Nan Liu},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2504.08418v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness},
 url = {http://arxiv.org/abs/2504.08418v1},
 year = {2025}
}

@article{2504.08552v1,
 abstract = {The integration of Artificial Intelligence in the development of computer
systems presents a new challenge: make intelligent systems explainable to
humans. This is especially vital in the field of health and well-being, where
transparency in decision support systems enables healthcare professionals to
understand and trust automated decisions and predictions. To address this need,
tools are required to guide the development of explainable AI systems. In this
paper, we introduce an evaluation framework designed to support the development
of explainable AI systems for health and well-being. Additionally, we present a
case study that illustrates the application of the framework in practice. We
believe that our framework can serve as a valuable tool not only for developing
explainable AI systems in healthcare but also for any AI system that has a
significant impact on individuals.},
 author = {Esperança Amengual-Alcover and Antoni Jaume-i-Capó and Miquel Miró-Nicolau and Gabriel Moyà-Alcover and Antonia Paniza-Fullana},
 citations = {1},
 comment = {},
 doi = {10.5220/0013289600003928},
 eprint = {2504.08552v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards an Evaluation Framework for Explainable Artificial Intelligence Systems for Health and Well-being},
 url = {http://arxiv.org/abs/2504.08552v1},
 year = {2025}
}

@article{2504.08713v5,
 abstract = {Deep learning-based electrocardiogram (ECG) classification has shown
impressive performance but clinical adoption has been slowed by the lack of
transparent and faithful explanations. Post hoc methods such as saliency maps
may fail to reflect a model's true decision process. Prototype-based reasoning
offers a more transparent alternative by grounding decisions in similarity to
learned representations of real ECG segments, enabling faithful, case-based
explanations. We introduce ProtoECGNet, a prototype-based deep learning model
for interpretable, multi-label ECG classification. ProtoECGNet employs a
structured, multi-branch architecture that reflects clinical interpretation
workflows: it integrates a 1D CNN with global prototypes for rhythm
classification, a 2D CNN with time-localized prototypes for morphology-based
reasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each
branch is trained with a prototype loss designed for multi-label learning,
combining clustering, separation, diversity, and a novel contrastive loss that
encourages appropriate separation between prototypes of unrelated classes while
allowing clustering for frequently co-occurring diagnoses. We evaluate
ProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating
competitive performance relative to state-of-the-art black-box models while
providing structured, case-based explanations. To assess prototype quality, we
conduct a structured clinician review of the final model's projected
prototypes, finding that they are rated as representative and clear.
ProtoECGNet shows that prototype learning can be effectively scaled to complex,
multi-label time-series classification, offering a practical path toward
transparent and trustworthy deep learning models for clinical decision support.},
 author = {Sahil Sethi and David Chen and Thomas Statchen and Michael C. Burkhart and Nipun Bhandari and Bashar Ramadan and Brett Beaulieu-Jones},
 citations = {2},
 comment = {Accepted to PMLR 298, 10th Machine Learning for Healthcare Conference
  (MLHC)},
 doi = {},
 eprint = {2504.08713v5},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning},
 url = {http://arxiv.org/abs/2504.08713v5},
 year = {2025}
}

@article{2504.09635v1,
 abstract = {Matching in causal inference from observational data aims to construct
treatment and control groups with similar distributions of covariates, thereby
reducing confounding and ensuring an unbiased estimation of treatment effects.
This matched sample closely mimics a randomized controlled trial (RCT), thus
improving the quality of causal estimates. We introduce a novel Two-stage
Interpretable Matching (TIM) framework for transparent and interpretable
covariate matching. In the first stage, we perform exact matching across all
available covariates. For treatment and control units without an exact match in
the first stage, we proceed to the second stage. Here, we iteratively refine
the matching process by removing the least significant confounder in each
iteration and attempting exact matching on the remaining covariates. We learn a
distance metric for the dropped covariates to quantify closeness to the
treatment unit(s) within the corresponding strata. We used these high- quality
matches to estimate the conditional average treatment effects (CATEs). To
validate TIM, we conducted experiments on synthetic datasets with varying
association structures and correlations. We assessed its performance by
measuring bias in CATE estimation and evaluating multivariate overlap between
treatment and control groups before and after matching. Additionally, we apply
TIM to a real-world healthcare dataset from the Centers for Disease Control and
Prevention (CDC) to estimate the causal effect of high cholesterol on diabetes.
Our results demonstrate that TIM improves CATE estimates, increases
multivariate overlap, and scales effectively to high-dimensional data, making
it a robust tool for causal inference in observational data.},
 author = {Sahil Shikalgar and Md. Noor-E-Alam},
 citations = {},
 comment = {},
 doi = {10.32614/cran.package.flame},
 eprint = {2504.09635v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Two-Stage Interpretable Matching Framework for Causal Inference},
 url = {http://arxiv.org/abs/2504.09635v1},
 year = {2025}
}

@article{2504.10397v1,
 abstract = {Objective: This study investigates the potential of Large Language Models
(LLMs) as an alternative to human expert elicitation for extracting structured
causal knowledge and facilitating causal modeling in biometric and healthcare
applications.
  Material and Methods: LLM-generated causal structures, specifically Bayesian
networks (BNs), were benchmarked against traditional statistical methods (e.g.,
Bayesian Information Criterion) using healthcare datasets. Validation
techniques included structural equation modeling (SEM) to verifying
relationships, and measures such as entropy, predictive accuracy, and
robustness to compare network structures.
  Results and Discussion: LLM-generated BNs demonstrated lower entropy than
expert-elicited and statistically generated BNs, suggesting higher confidence
and precision in predictions. However, limitations such as contextual
constraints, hallucinated dependencies, and potential biases inherited from
training data require further investigation.
  Conclusion: LLMs represent a novel frontier in expert elicitation for
probabilistic causal modeling, promising to improve transparency and reduce
uncertainty in the decision-making using such models.},
 author = {Olha Shaposhnyk and Daria Zahorska and Svetlana Yanushkevich},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2504.10397v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?},
 url = {http://arxiv.org/abs/2504.10397v1},
 year = {2025}
}

@article{2504.10708v1,
 abstract = {Explanations for artificial intelligence (AI) systems are intended to support
the people who are impacted by AI systems in high-stakes decision-making
environments, such as doctors, patients, teachers, students, housing
applicants, and many others. To protect people and support the responsible
development of AI, explanations need to be actionable--helping people take
pragmatic action in response to an AI system--and contestable--enabling people
to push back against an AI system and its determinations. For many high-stakes
domains, such as healthcare, education, and finance, the sociotechnical
environment includes significant legal implications that impact how people use
AI explanations. For example, physicians who use AI decision support systems
may need information on how accepting or rejecting an AI determination will
protect them from lawsuits or help them advocate for their patients. In this
paper, we make the case for Legally-Informed Explainable AI, responding to the
need to integrate and design for legal considerations when creating AI
explanations. We describe three stakeholder groups with different informational
and actionability needs, and provide practical recommendations to tackle design
challenges around the design of explainable AI systems that incorporate legal
considerations.},
 author = {Gennie Mansi and Naveena Karusala and Mark Riedl},
 citations = {0},
 comment = {9 pages},
 doi = {10.5281/zenodo.15170444},
 eprint = {2504.10708v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Legally-Informed Explainable AI},
 url = {http://arxiv.org/abs/2504.10708v1},
 year = {2025}
}

@article{2504.11159v1,
 abstract = {Time series are ubiquitous in domains such as energy forecasting, healthcare,
and industry. Using AI systems, some tasks within these domains can be
efficiently handled. Explainable AI (XAI) aims to increase the reliability of
AI solutions by explaining model reasoning. For time series, many XAI methods
provide point- or sequence-based attribution maps. These methods explain model
reasoning in terms of low-level patterns. However, they do not capture
high-level patterns that may also influence model reasoning. We propose a
concept-based method to provide explanations in terms of these high-level
patterns. In this paper, we present C-SHAP for time series, an approach which
determines the contribution of concepts to a model outcome. We provide a
general definition of C-SHAP and present an example implementation using time
series decomposition. Additionally, we demonstrate the effectiveness of the
methodology through a use case from the energy domain.},
 author = {Annemarie Jutte and Faizan Ahmed and Jeroen Linssen and Maurice van Keulen},
 citations = {1},
 comment = {10 pages, 6 figures},
 doi = {},
 eprint = {2504.11159v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {C-SHAP for time series: An approach to high-level temporal explanations},
 url = {http://arxiv.org/abs/2504.11159v1},
 year = {2025}
}

@article{2504.11264v2,
 abstract = {The rapid accumulation of Electronic Health Records (EHRs) has transformed
healthcare by providing valuable data that enhance clinical predictions and
diagnoses. While conventional machine learning models have proven effective,
they often lack robust representation learning and depend heavily on
expert-crafted features. Although deep learning offers powerful solutions, it
is often criticized for its lack of interpretability. To address these
challenges, we propose DeepSelective, a novel end to end deep learning
framework for predicting patient prognosis using EHR data, with a strong
emphasis on enhancing model interpretability. DeepSelective combines data
compression techniques with an innovative feature selection approach,
integrating custom-designed modules that work together to improve both accuracy
and interpretability. Our experiments demonstrate that DeepSelective not only
enhances predictive accuracy but also significantly improves interpretability,
making it a valuable tool for clinical decision-making. The source code is
freely available at http://www.healthinformaticslab.org/supp/resources.php .},
 author = {Ruochi Zhang and Qian Yang and Xiaoyang Wang and Tian Wang and Qiong Zhou and Ziqi Deng and Kewei Li and Yueying Wang and Yusi Fan and Jiale Zhang and Lan Huang and Chang Liu and Fengfeng Zhou},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2504.11264v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DeepSelective: Interpretable Prognosis Prediction via Feature Selection and Compression in EHR Data},
 url = {http://arxiv.org/abs/2504.11264v2},
 year = {2025}
}

@article{2504.11344v2,
 abstract = {Temporal Point Processes (TPPs) are widely used for modeling event sequences
in various medical domains, such as disease onset prediction, progression
analysis, and clinical decision support. Although TPPs effectively capture
temporal dynamics, their lack of interpretability remains a critical challenge.
Recent advancements have introduced interpretable TPPs. However, these methods
fail to incorporate numerical features, thereby limiting their ability to
generate precise predictions. To address this issue, we propose Hybrid-Rule
Temporal Point Processes (HRTPP), a novel framework that integrates temporal
logic rules with numerical features, improving both interpretability and
predictive accuracy in event modeling. HRTPP comprises three key components:
basic intensity for intrinsic event likelihood, rule-based intensity for
structured temporal dependencies, and numerical feature intensity for dynamic
probability modulation. To effectively discover valid rules, we introduce a
two-phase rule mining strategy with Bayesian optimization. To evaluate our
method, we establish a multi-criteria assessment framework, incorporating rule
validity, model fitting, and temporal predictive accuracy. Experimental results
on real-world medical datasets demonstrate that HRTPP outperforms
state-of-the-art interpretable TPPs in terms of predictive performance and
clinical interpretability. In case studies, the rules extracted by HRTPP
explain the disease progression, offering valuable contributions to medical
diagnosis.},
 author = {Yunyang Cao and Juekai Lin and Hongye Wang and Wenhao Li and Bo Jin},
 citations = {},
 comment = {},
 doi = {10.1007/978-3-032-06066-2_26},
 eprint = {2504.11344v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable Hybrid-Rule Temporal Point Processes},
 url = {http://arxiv.org/abs/2504.11344v2},
 year = {2025}
}

@article{2504.11511v2,
 abstract = {The rise of reinforcement learning (RL) in critical real-world applications
demands a fundamental rethinking of privacy in AI systems. Traditional privacy
frameworks, designed to protect isolated data points, fall short for sequential
decision-making systems where sensitive information emerges from temporal
patterns, behavioral strategies, and collaborative dynamics. Modern RL
paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in
large language models (LLMs), exacerbate these challenges by introducing
complex, interactive, and context-dependent learning environments that
traditional methods do not address. In this position paper, we argue for a new
privacy paradigm built on four core principles: multi-scale protection,
behavioral pattern protection, collaborative privacy preservation, and
context-aware adaptation. These principles expose inherent tensions between
privacy, utility, and interpretability that must be navigated as RL systems
become more pervasive in high-stakes domains like healthcare, autonomous
vehicles, and decision support systems powered by LLMs. To tackle these
challenges, we call for the development of new theoretical frameworks,
practical mechanisms, and rigorous evaluation methodologies that collectively
enable effective privacy protection in sequential decision-making systems.},
 author = {Flint Xiaofeng Fan and Cheston Tan and Roger Wattenhofer and Yew-Soon Ong},
 citations = {},
 comment = {IJCNN 2025 Position Paper Track},
 doi = {10.2139/ssrn.5219045},
 eprint = {2504.11511v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs},
 url = {http://arxiv.org/abs/2504.11511v2},
 year = {2025}
}

@article{2504.12338v1,
 abstract = {There is a long history of building predictive models in healthcare using
tabular data from electronic medical records. However, these models fail to
extract the information found in unstructured clinical notes, which document
diagnosis, treatment, progress, medications, and care plans. In this study, we
investigate how answers generated by GPT-4o-mini (ChatGPT) to simple clinical
questions about patients, when given access to the patient's discharge summary,
can support patient-level mortality prediction. Using data from 14,011
first-time admissions to the Coronary Care or Cardiovascular Intensive Care
Units in the MIMIC-IV Note dataset, we implement a transparent framework that
uses GPT responses as input features in logistic regression models. Our
findings demonstrate that GPT-based models alone can outperform models trained
on standard tabular data, and that combining both sources of information yields
even greater predictive power, increasing AUC by an average of 5.1 percentage
points and increasing positive predictive value by 29.9 percent for the
highest-risk decile. These results highlight the value of integrating large
language models (LLMs) into clinical prediction tasks and underscore the
broader potential for using LLMs in any domain where unstructured text data
remains an underutilized resource.},
 author = {David Anderson and Michaela Anderson and Margret Bjarnadottir and Stephen Mahar and Shriyan Reyya},
 citations = {1},
 comment = {Paper and Online Supplement combined into one PDF. 26 pages. 2
  figures},
 doi = {},
 eprint = {2504.12338v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions},
 url = {http://arxiv.org/abs/2504.12338v1},
 year = {2025}
}

@article{2504.12417v1,
 abstract = {Objective: Create precise, structured, data-backed guidelines for type 2
diabetes treatment progression, suitable for clinical adoption.
  Research Design and Methods: Our training cohort was composed of patient
(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to
2014. We divide visits into 4 groups based on the patient's treatment regimen
before the visit, and further divide them into subgroups based on the
recommended treatment during the visit. Since each subgroup has observational
data, which has confounding bias (sicker patients are prescribed more
aggressive treatments), we used machine learning and optimization to remove
some datapoints so that the remaining data resembles a randomized trial. On
each subgroup, we train AI-backed tree-based models to prescribe treatment
changes. Once we train these tree models, we manually combine the models for
every group to create an end-to-end prescription pipeline for all patients in
that group. In this process, we prioritize stepping up to a more aggressive
treatment before considering less aggressive options. We tested this pipeline
on unseen data from BMC, and an external dataset from Hartford healthcare (type
2 diabetes patient visits from January 2020 to May 2024).
  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more
than what the doctors achieved on the unseen BMC patients. For the Hartford
cohort, our pipelines were better by 0.13%.
  Conclusions: This precise, interpretable, and efficient AI-backed approach to
treatment progression in type 2 diabetes is predicted to outperform the current
practice and can be deployed to improve patient outcomes.},
 author = {Dewang Kumar Agarwal and Dimitris J. Bertsimas},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2504.12417v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data},
 url = {http://arxiv.org/abs/2504.12417v1},
 year = {2025}
}

@article{2504.12529v1,
 abstract = {This study critically examines the commonly held assumption that
explicability in artificial intelligence (AI) systems inherently boosts user
trust. Utilizing a meta-analytical approach, we conducted a comprehensive
examination of the existing literature to explore the relationship between AI
explainability and trust. Our analysis, incorporating data from 90 studies,
reveals a statistically significant but moderate positive correlation between
the explainability of AI systems and the trust they engender among users. This
indicates that while explainability contributes to building trust, it is not
the sole or predominant factor in this equation. In addition to academic
contributions to the field of Explainable AI (XAI), this research highlights
its broader socio-technical implications, particularly in promoting
accountability and fostering user trust in critical domains such as healthcare
and justice. By addressing challenges like algorithmic bias and ethical
transparency, the study underscores the need for equitable and sustainable AI
adoption. Rather than focusing solely on immediate trust, we emphasize the
normative importance of fostering authentic and enduring trustworthiness in AI
systems.},
 author = {Zahra Atf and Peter R. Lewis},
 citations = {6},
 comment = {9 Page, 1 Figure},
 doi = {10.1109/tts.2025.3558448},
 eprint = {2504.12529v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Is Trust Correlated With Explainability in AI? A Meta-Analysis},
 url = {http://arxiv.org/abs/2504.12529v1},
 year = {2025}
}

@article{2504.12803v1,
 abstract = {Swarm intelligence effectively optimizes complex systems across fields like
engineering and healthcare, yet algorithm solutions often suffer from low
reliability due to unclear configurations and hyperparameters. This study
analyzes Particle Swarm Optimization (PSO), focusing on how different
communication topologies Ring, Star, and Von Neumann affect convergence and
search behaviors. Using an adapted IOHxplainer , an explainable benchmarking
tool, we investigate how these topologies influence information flow,
diversity, and convergence speed, clarifying the balance between exploration
and exploitation. Through visualization and statistical analysis, the research
enhances interpretability of PSO's decisions and provides practical guidelines
for choosing suitable topologies for specific optimization tasks. Ultimately,
this contributes to making swarm based optimization more transparent, robust,
and trustworthy.},
 author = {Nitin Gupta and Indu Bala and Bapi Dutta and Luis Martínez and Anupam Yadav},
 citations = {1},
 comment = {},
 doi = {10.1145/3712255.3726658},
 eprint = {2504.12803v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies},
 url = {http://arxiv.org/abs/2504.12803v1},
 year = {2025}
}

@article{2504.13186v1,
 abstract = {The rapid advancement of deep learning (DL) has transformed healthcare,
particularly in cancer detection and diagnosis. DL surpasses traditional
machine learning and human accuracy, making it a critical tool for identifying
diseases. Despite numerous reviews on DL in healthcare, a comprehensive
analysis of its role in cancer detection remains limited. Existing studies
focus on specific aspects, leaving gaps in understanding its broader impact.
This paper addresses these gaps by reviewing advanced DL techniques, including
transfer learning (TL), reinforcement learning (RL), federated learning (FL),
Transformers, and large language models (LLMs). These approaches enhance
accuracy, tackle data scarcity, and enable decentralized learning while
maintaining data privacy. TL adapts pre-trained models to new datasets,
improving performance with limited labeled data. RL optimizes diagnostic
pathways and treatment strategies, while FL fosters collaborative model
development without sharing sensitive data. Transformers and LLMs,
traditionally used in natural language processing, are now applied to medical
data for improved interpretability. Additionally, this review examines these
techniques' efficiency in cancer diagnosis, addresses challenges like data
imbalance, and proposes solutions. It serves as a resource for researchers and
practitioners, providing insights into current trends and guiding future
research in advanced DL for cancer detection.},
 author = {Yassine Habchi and Hamza Kheddar and Yassine Himeur and Adel Belouchrani and Erchin Serpedin and Fouad Khelifi and Muhammad E. H. Chowdhury},
 citations = {7},
 comment = {},
 doi = {10.1016/j.imavis.2025.105495},
 eprint = {2504.13186v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Advanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detection},
 url = {http://arxiv.org/abs/2504.13186v1},
 year = {2025}
}

@article{2504.13926v2,
 abstract = {The integration of Artificial Intelligence (AI) into high-stakes domains such
as healthcare, finance, and autonomous systems is often constrained by concerns
over transparency, interpretability, and trust. While Human-Centered AI (HCAI)
emphasizes alignment with human values, Explainable AI (XAI) enhances
transparency by making AI decisions more understandable. However, the lack of a
unified approach limits AI's effectiveness in critical decision-making
scenarios. This paper presents a novel three-layered framework that bridges
HCAI and XAI to establish a structured explainability paradigm. The framework
comprises (1) a foundational AI model with built-in explainability mechanisms,
(2) a human-centered explanation layer that tailors explanations based on
cognitive load and user expertise, and (3) a dynamic feedback loop that refines
explanations through real-time user interaction. The framework is evaluated
across healthcare, finance, and software development, demonstrating its
potential to enhance decision-making, regulatory compliance, and public trust.
Our findings advance Human-Centered Explainable AI (HCXAI), fostering AI
systems that are transparent, adaptable, and ethically aligned.},
 author = {Chameera De Silva and Thilina Halloluwa and Dhaval Vyas},
 citations = {},
 comment = {I am requesting this withdrawal because I believe the current version
  requires significant revisions and restructuring to better reflect the
  intended research contributions. I plan to substantially improve the work and
  may resubmit a revised version in the future. Thank you for your
  understanding and support},
 doi = {},
 eprint = {2504.13926v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust},
 url = {http://arxiv.org/abs/2504.13926v2},
 year = {2025}
}

@article{2504.14068v1,
 abstract = {Understanding patient feedback is crucial for improving healthcare services,
yet analyzing unlabeled short-text feedback presents significant challenges due
to limited data and domain-specific nuances. Traditional supervised learning
approaches require extensive labeled datasets, making unsupervised methods more
viable for uncovering meaningful insights from patient feedback. This study
explores unsupervised methods to extract meaningful topics from 439 survey
responses collected from a healthcare system in Wisconsin, USA. A keyword-based
filtering approach was applied to isolate complaint-related feedback using a
domain-specific lexicon. To delve deeper and analyze dominant topics in
feedback, we explored traditional topic modeling methods, including Latent
Dirichlet Allocation (LDA) and Gibbs Sampling Dirichlet Multinomial Mixture
(GSDMM), alongside BERTopic, an advanced neural embedding-based clustering
approach. To improve coherence and interpretability where data are scarce and
consist of short-texts, we propose kBERT, an integration of BERT embeddings
with k-means clustering. Model performance was assessed using coherence scores
(Cv ) for topic interpretability and average Inverted Rank-Biased Overlap
(IRBOavg) for topic diversity. Results indicate that kBERT achieves the highest
coherence (Cv = 0.53) and distinct topic separation (IRBOavg = 1.00),
outperforming all other models in short-text healthcare feedback analysis. Our
findings emphasize the importance of embedding-based techniques for topic
identification and highlight the need for context-aware models in healthcare
analytics.},
 author = {K M Sajjadul Islam and Ravi Teja Karri and Srujan Vegesna and Jiawei Wu and Praveen Madiraju},
 citations = {0},
 comment = {Full version of the paper accepted at the 2025 IEEE COMPSAC, Toronto,
  Canada},
 doi = {10.1109/compsac65507.2025.00106},
 eprint = {2504.14068v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Contextual Embedding-based Clustering to Identify Topics for Healthcare Service Improvement},
 url = {http://arxiv.org/abs/2504.14068v1},
 year = {2025}
}

@article{2504.15290v1,
 abstract = {Accurate fetal birth weight prediction is a cornerstone of prenatal care, yet
traditional methods often rely on imaging technologies that remain inaccessible
in resource-limited settings. This study presents a novel machine
learning-based framework that circumvents these conventional dependencies,
using a diverse set of physiological, environmental, and parental factors to
refine birth weight estimation. A multi-stage feature selection pipeline
filters the dataset into an optimized subset, demonstrating previously
underexplored yet clinically relevant predictors of fetal growth. By
integrating advanced regression architectures and ensemble learning strategies,
the model captures non-linear relationships often overlooked by traditional
approaches, offering a predictive solution that is both interpretable and
scalable. Beyond predictive accuracy, this study addresses a question: whether
birth weight can be reliably estimated without conventional diagnostic tools.
The findings challenge entrenched methodologies by introducing an alternative
pathway that enhances accessibility without compromising clinical utility.
While limitations exist, the study lays the foundation for a new era in
prenatal analytics, one where data-driven inference competes with, and
potentially redefines, established medical assessments. By bridging
computational intelligence with obstetric science, this research establishes a
framework for equitable, technology-driven advancements in maternal-fetal
healthcare.},
 author = {Rajeshwari Mistri and Harsh Joshi and Nachiket Kapure and Parul Kumari and Manasi Mali and Seema Purohit and Neha Sharma and Mrityunjoy Panday and Chittaranjan S. Yajnik},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2504.15290v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Parental Imprints On Birth Weight: A Data-Driven Model For Neonatal Prediction In Low Resource Prenatal Care},
 url = {http://arxiv.org/abs/2504.15290v1},
 year = {2025}
}

@article{2504.15562v1,
 abstract = {In medical imaging, anomaly detection is a vital element of healthcare
diagnostics, especially for neurological conditions which can be
life-threatening. Conventional deterministic methods often fall short when it
comes to capturing the inherent uncertainty of anomaly detection tasks. This
paper introduces a Bayesian Variational Autoencoder (VAE) equipped with
multi-head attention mechanisms for detecting anomalies in brain magnetic
resonance imaging (MRI). For the purpose of improving anomaly detection
performance, we incorporate both epistemic and aleatoric uncertainty estimation
through Bayesian inference. The model was tested on the BraTS2020 dataset, and
the findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper
suggests that modeling uncertainty is an essential component of anomaly
detection, enhancing both performance and interpretability and providing
confidence estimates, as well as anomaly predictions, for clinicians to
leverage in making medical decisions.},
 author = {Dip Roy},
 citations = {},
 comment = {16 pages, 6 figures},
 doi = {10.21203/rs.3.rs-6488522/v1},
 eprint = {2504.15562v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis},
 url = {http://arxiv.org/abs/2504.15562v1},
 year = {2025}
}

@article{2504.16432v2,
 abstract = {As time evolves, data within specific domains exhibit predictability that
motivates time series forecasting to predict future trends from historical
data. However, current deep forecasting methods can achieve promising
performance but generally lack interpretability, hindering trustworthiness and
practical deployment in safety-critical applications such as auto-driving and
healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for
credible time series forecasting. iTFKAN enables further exploration of model
decision rationales and underlying data patterns due to its interpretability
achieved through model symbolization. Besides, iTFKAN develops two strategies,
prior knowledge injection, and time-frequency synergy learning, to effectively
guide model learning under complex intertwined time series data. Extensive
experimental results demonstrated that iTFKAN can achieve promising forecasting
performance while simultaneously possessing high interpretive capabilities.},
 author = {Ziran Liang and Rui An and Wenqi Fan and Yanghui Rao and Yuxuan Liang},
 citations = {},
 comment = {Currently under review at IEEE Transactions on Knowledge and Data
  Engineering},
 doi = {},
 eprint = {2504.16432v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network},
 url = {http://arxiv.org/abs/2504.16432v2},
 year = {2025}
}

@article{2504.17166v1,
 abstract = {Heterogeneous treatment effect (HTE) estimation is critical in medical
research. It provides insights into how treatment effects vary among
individuals, which can provide statistical evidence for precision medicine.
While most existing methods focus on binary treatment situations, real-world
applications often involve multiple interventions. However, current HTE
estimation methods are primarily designed for binary comparisons and often rely
on black-box models, which limit their applicability and interpretability in
multi-arm settings. To address these challenges, we propose an interpretable
machine learning framework for HTE estimation in multi-arm trials. Our method
employs a rule-based ensemble approach consisting of rule generation, rule
ensemble, and HTE estimation, ensuring both predictive accuracy and
interpretability. Through extensive simulation studies and real data
applications, the performance of our method was evaluated against
state-of-the-art multi-arm HTE estimation approaches. The results indicate that
our approach achieved lower bias and higher estimation accuracy compared with
those of existing methods. Furthermore, the interpretability of our framework
allows clearer insights into how covariates influence treatment effects,
facilitating clinical decision making. By bridging the gap between accuracy and
interpretability, our study contributes a valuable tool for multi-arm HTE
estimation, supporting precision medicine.},
 author = {Ke Wan and Kensuke Tanioka and Toshio Shimokawa},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2504.17166v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Causal rule ensemble approach for multi-arm data},
 url = {http://arxiv.org/abs/2504.17166v1},
 year = {2025}
}

@article{2504.17540v1,
 abstract = {The recent global spread of monkeypox, particularly in regions where it has
not historically been prevalent, has raised significant public health concerns.
Early and accurate diagnosis is critical for effective disease management and
control. In response, this study proposes a novel deep learning-based framework
for the automated detection of monkeypox from skin lesion images, leveraging
the power of transfer learning, dimensionality reduction, and advanced machine
learning techniques. We utilize the newly developed Monkeypox Skin Lesion
Dataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to
train and evaluate our models. The proposed framework employs the Xception
architecture for deep feature extraction, followed by Principal Component
Analysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting
(NGBoost) algorithm for classification. To optimize the model's performance and
generalization, we introduce the African Vultures Optimization Algorithm (AVOA)
for hyperparameter tuning, ensuring efficient exploration of the parameter
space. Our results demonstrate that the proposed AVOA-NGBoost model achieves
state-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%
and an AUC of 97.47%. Additionally, we enhance model interpretability using
Grad-CAM and LIME techniques, providing insights into the decision-making
process and highlighting key features influencing classification. This
framework offers a highly precise and efficient diagnostic tool, potentially
aiding healthcare providers in early detection and diagnosis, particularly in
resource-constrained environments.},
 author = {Ahmadreza Shateri and Negar Nourani and Morteza Dorrigiv and Hamid Nasiri},
 citations = {3},
 comment = {},
 doi = {},
 eprint = {2504.17540v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm},
 url = {http://arxiv.org/abs/2504.17540v1},
 year = {2025}
}

@article{2504.17717v1,
 abstract = {Background and Objectives: Multidrug Resistance (MDR) is a critical global
health issue, causing increased hospital stays, healthcare costs, and
mortality. This study proposes an interpretable Machine Learning (ML) framework
for MDR prediction, aiming for both accurate inference and enhanced
explainability.
  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing
clinical progression and patient-to-patient interactions. Similarity among
patients is quantified using MTS-based methods: descriptive statistics, Dynamic
Time Warping, and Time Cluster Kernel. These similarity measures serve as
inputs for MDR classification via Logistic Regression, Random Forest, and
Support Vector Machines, with dimensionality reduction and kernel
transformations improving model performance. For explainability, patient
similarity networks are constructed from these metrics. Spectral clustering and
t-SNE are applied to identify MDR-related subgroups and visualize high-risk
clusters, enabling insight into clinically relevant patterns.
  Results: The framework was validated on ICU Electronic Health Records from
the University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms
baseline ML and deep learning models by leveraging graph-based patient
similarity. The approach identifies key risk factors -- prolonged antibiotic
use, invasive procedures, co-infections, and extended ICU stays -- and reveals
clinically meaningful clusters. Code and results are available at
\https://github.com/oscarescuderoarnanz/DM4MTS.
  Conclusions: Patient similarity representations combined with graph-based
analysis provide accurate MDR prediction and interpretable insights. This
method supports early detection, risk factor identification, and patient
stratification, highlighting the potential of explainable ML in critical care.},
 author = {Óscar Escudero-Arnanz and Antonio G. Marques and Inmaculada Mora-Jiménez and Joaquín Álvarez-Rodríguez and Cristina Soguero-Ruiz},
 citations = {},
 comment = {},
 doi = {10.1016/j.cmpb.2025.108920},
 eprint = {2504.17717v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations},
 url = {http://arxiv.org/abs/2504.17717v1},
 year = {2025}
}

@article{2504.18007v1,
 abstract = {With the rapid digitalization of healthcare systems, there has been a
substantial increase in the generation and sharing of private health data.
Safeguarding patient information is essential for maintaining consumer trust
and ensuring compliance with legal data protection regulations. Machine
learning is critical in healthcare, supporting personalized treatment, early
disease detection, predictive analytics, image interpretation, drug discovery,
efficient operations, and patient monitoring. It enhances decision-making,
accelerates research, reduces errors, and improves patient outcomes. In this
paper, we utilize machine learning methodologies, including differential
privacy and federated learning, to develop privacy-preserving models that
enable healthcare stakeholders to extract insights without compromising
individual privacy. Differential privacy introduces noise to data to guarantee
statistical privacy, while federated learning enables collaborative model
training across decentralized datasets. We explore applying these technologies
to Heart Disease Data, demonstrating how they preserve privacy while delivering
valuable insights and comprehensive analysis. Our results show that using a
federated learning model with differential privacy achieved a test accuracy of
85%, ensuring patient data remained secure and private throughout the process.},
 author = {Yazan Otoum and Amiya Nayak},
 citations = {1},
 comment = {\c{opyright} 2025 IEEE. Accepted to IEEE International Conference on
  Communications ICC 2025. Final version to appear in IEEE Xplore},
 doi = {10.1109/icc52391.2025.11161731},
 eprint = {2504.18007v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Differential Privacy-Driven Framework for Enhancing Heart Disease Prediction},
 url = {http://arxiv.org/abs/2504.18007v1},
 year = {2025}
}

@article{2504.18044v1,
 abstract = {Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social
Computing requires the examination of ethical and social norms to ensure safe
incorporation into human life. We conducted a mixed-method study, including an
online survey with 111 participants and an interview study with 38 experts, to
investigate the AI ethics and social norms in ChatGPT as everyday life tools.
This study aims to evaluate whether ChatGPT in an empirical context operates
following ethics and social norms, which is critical for understanding actions
in industrial and academic research and achieving machine ethics. The findings
of this study provide initial insights into six important aspects of AI ethics,
including bias, trustworthiness, security, toxicology, social norms, and
ethical data. Significant obstacles related to transparency and bias in
unsupervised data collection methods are identified as ChatGPT's ethical
concerns.},
 author = {Omid Veisi and Sasan Bahrami and Roman Englert and Claudia Müller},
 citations = {},
 comment = {Accepted for presentation at the ACM Conference on Computer-Supported
  Cooperative Work and Social Computing (CSCW) 2025. To appear in Proceedings
  of the ACM on Human-Computer Interaction (PACM HCI)},
 doi = {},
 eprint = {2504.18044v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How},
 url = {http://arxiv.org/abs/2504.18044v1},
 year = {2025}
}

@article{2504.18671v1,
 abstract = {Mild Traumatic Brain Injury (TBI) detection presents significant challenges
due to the subtle and often ambiguous presentation of symptoms in medical
imaging, making accurate diagnosis a complex task. To address these challenges,
we propose Proof-of-TBI, a medical diagnosis support system that integrates
multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large
language model (LLM). Our approach fine-tunes multiple vision-language models
using a labeled dataset of TBI MRI scans, training them to diagnose TBI
symptoms effectively. The predictions from these models are aggregated through
a consensus-based decision-making process. The system evaluates the predictions
from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a
model that has demonstrated remarkable reasoning performance, to produce the
most accurate final diagnosis. The LLM Agents orchestrates interactions between
the vision-language models and the reasoning LLM, managing the final
decision-making process with transparency, reliability, and automation. This
end-to-end decision-making workflow combines the vision-language model
consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt
engineering by the LLM agents. The prototype for the proposed platform was
developed in collaboration with the U.S. Army Medical Research team in Newport
News, Virginia, incorporating five fine-tuned vision-language models. The
results demonstrate the transformative potential of combining fine-tuned
vision-language model inputs with the OpenAI-o3 reasoning LLM to create a
robust, secure, and highly accurate diagnostic system for mild TBI prediction.
To the best of our knowledge, this research represents the first application of
fine-tuned vision-language models integrated with a reasoning LLM for TBI
prediction tasks.},
 author = {Ross Gore and Eranga Bandara and Sachin Shetty and Alberto E. Musto and Pratip Rana and Ambrosio Valencia-Romero and Christopher Rhea and Lobat Tayebi and Heather Richter and Atmaram Yarlagadda and Donna Edmonds and Steven Wallace and Donna Broshek},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2504.18671v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction},
 url = {http://arxiv.org/abs/2504.18671v1},
 year = {2025}
}

@article{2504.18858v1,
 abstract = {Context: ChatGPT and other large language models (LLMs) are widely used
across healthcare, business, economics, engineering, and software engineering
(SE). Despite their popularity, concerns persist about their reliability,
especially their error rates across domains and the software development
lifecycle (SDLC).
  Objective: This study synthesizes and quantifies ChatGPT's reported error
rates across major domains and SE tasks aligned with SDLC phases. It provides
an evidence-based view of where ChatGPT excels, where it fails, and how
reliability varies by task, domain, and model version (GPT-3.5, GPT-4,
GPT-4-turbo, GPT-4o).
  Method: A Multivocal Literature Review (MLR) was conducted, gathering data
from academic studies, reports, benchmarks, and grey literature up to 2025.
Factual, reasoning, coding, and interpretive errors were considered. Data were
grouped by domain and SE phase and visualized using boxplots to show error
distributions.
  Results: Error rates vary across domains and versions. In healthcare, rates
ranged from 8% to 83%. Business and economics saw error rates drop from ~50%
with GPT-3.5 to 15-20% with GPT-4. Engineering tasks averaged 20-30%.
Programming success reached 87.5%, though complex debugging still showed over
50% errors. In SE, requirements and design phases showed lower error rates
(~5-20%), while coding, testing, and maintenance phases had higher variability
(10-50%). Upgrades from GPT-3.5 to GPT-4 improved reliability.
  Conclusion: Despite improvements, ChatGPT still exhibits non-negligible error
rates varying by domain, task, and SDLC phase. Full reliance without human
oversight remains risky, especially in critical settings. Continuous evaluation
and critical validation are essential to ensure reliability and
trustworthiness.},
 author = {Vahid Garousi},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2504.18858v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle},
 url = {http://arxiv.org/abs/2504.18858v1},
 year = {2025}
}

@article{2504.19027v2,
 abstract = {Explainable artificial intelligence (XAI) has become increasingly important
in decision-critical domains such as healthcare, finance, and law.
Counterfactual (CF) explanations, a key approach in XAI, provide users with
actionable insights by suggesting minimal modifications to input features that
lead to different model outcomes. Despite significant advancements, existing CF
generation methods often struggle to balance proximity, diversity, and
robustness, limiting their real-world applicability. A widely adopted
framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but
lacks robustness, making CF explanations sensitive to perturbations and domain
constraints. To address these challenges, we introduce DiCE-Extended, an
enhanced CF explanation framework that integrates multi-objective optimization
techniques to improve robustness while maintaining interpretability. Our
approach introduces a novel robustness metric based on the Dice-S{\o}rensen
coefficient, enabling stability under small input variations. Additionally, we
refine CF generation using weighted loss components (lambda_p, lambda_d,
lambda_r) to balance proximity, diversity, and robustness. We empirically
validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German
Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch,
TensorFlow). Results demonstrate improved CF validity, stability, and alignment
with decision boundaries compared to standard DiCE-generated explanations. Our
findings highlight the potential of DiCE-Extended in generating more reliable
and interpretable CFs for high-stakes applications. Future work could explore
adaptive optimization techniques and domain-specific constraints to further
enhance CF generation in real-world scenarios},
 author = {Volkan Bakir and Polat Goktas and Sureyya Akyuz},
 citations = {0},
 comment = {5th international Conference on Modelling, Computation and
  Optimization in Information Systems and Management Sciences (MCO 2025), June
  4-6, 2025, Metz, France},
 doi = {},
 eprint = {2504.19027v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning},
 url = {http://arxiv.org/abs/2504.19027v2},
 year = {2025}
}

@article{2504.19179v1,
 abstract = {Artificial Intelligence (AI) holds great promise for transforming healthcare,
particularly in disease diagnosis, prognosis, and patient care. The increasing
availability of digital medical data, such as images, omics, biosignals, and
electronic health records, combined with advances in computing, has enabled AI
models to approach expert-level performance. However, widespread clinical
adoption remains limited, primarily due to challenges beyond technical
performance, including ethical concerns, regulatory barriers, and lack of
trust. To address these issues, AI systems must align with the principles of
Trustworthy AI (TAI), which emphasize human agency and oversight, algorithmic
robustness, privacy and data governance, transparency, bias and discrimination
avoidance, and accountability. Yet, the complexity of healthcare processes
(e.g., screening, diagnosis, prognosis, and treatment) and the diversity of
stakeholders (clinicians, patients, providers, regulators) complicate the
integration of TAI principles. To bridge the gap between TAI theory and
practical implementation, this paper proposes a design framework to support
developers in embedding TAI principles into medical AI systems. Thus, for each
stakeholder identified across various healthcare processes, we propose a
disease-agnostic collection of requirements that medical AI systems should
incorporate to adhere to the principles of TAI. Additionally, we examine the
challenges and tradeoffs that may arise when applying these principles in
practice. To ground the discussion, we focus on cardiovascular diseases, a
field marked by both high prevalence and active AI innovation, and demonstrate
how TAI principles have been applied and where key obstacles persist.},
 author = {Pedro A. Moreno-Sánchez and Javier Del Ser and Mark van Gils and Jussi Hernesniemi},
 citations = {},
 comment = {},
 doi = {10.2139/ssrn.5249603},
 eprint = {2504.19179v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption},
 url = {http://arxiv.org/abs/2504.19179v1},
 year = {2025}
}

@article{2504.19300v1,
 abstract = {Coronary artery disease (CAD) remains a leading cause of mortality worldwide,
requiring accurate segmentation and stenosis detection using Coronary Computed
Tomography angiography (CCTA). Existing methods struggle with challenges such
as low contrast, morphological variability and small vessel segmentation. To
address these limitations, we propose the Myocardial Region-guided Feature
Aggregation Net, a novel U-shaped dual-encoder architecture that integrates
anatomical prior knowledge to enhance robustness in coronary artery
segmentation. Our framework incorporates three key innovations: (1) a
Myocardial Region-guided Module that directs attention to coronary regions via
myocardial contour expansion and multi-scale feature fusion, (2) a Residual
Feature Extraction Encoding Module that combines parallel spatial channel
attention with residual blocks to enhance local-global feature discrimination,
and (3) a Multi-scale Feature Fusion Module for adaptive aggregation of
hierarchical vascular features. Additionally, Monte Carlo dropout f quantifies
prediction uncertainty, supporting clinical interpretability. For stenosis
detection, a morphology-based centerline extraction algorithm separates the
vascular tree into anatomical branches, enabling cross-sectional area
quantification and stenosis grading. The superiority of MGFA-Net was
demonstrated by achieving an Dice score of 85.04%, an accuracy of 84.24%, an
HD95 of 6.1294 mm, and an improvement of 5.46% in true positive rate for
stenosis detection compared to3D U-Net. The integrated segmentation-to-stenosis
pipeline provides automated, clinically interpretable CAD assessment, bridging
deep learning with anatomical prior knowledge for precision medicine. Our code
is publicly available at http://github.com/chenzhao2023/MGFA_CCTA},
 author = {Ni Yao and Xiangyu Liu and Danyang Sun and Chuang Han and Yanting Li and Jiaofen Nan and Chengyang Li and Fubao Zhu and Weihua Zhou and Chen Zhao},
 citations = {0},
 comment = {31 pages, 12 figures},
 doi = {},
 eprint = {2504.19300v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Myocardial Region-guided Feature Aggregation Net for Automatic Coronary artery Segmentation and Stenosis Assessment using Coronary Computed Tomography Angiography},
 url = {http://arxiv.org/abs/2504.19300v1},
 year = {2025}
}

@article{2504.20103v1,
 abstract = {Drug-target interaction (DTI) prediction is a core task in drug development
and precision medicine in the biomedical field. However, traditional machine
learning methods generally have the black box problem, which makes it difficult
to reveal the deep correlation between the model decision mechanism and the
interaction pattern between biological molecules. This study proposes a
heterogeneous network drug target interaction prediction framework, integrating
graph neural network and multi scale signal processing technology to construct
a model with both efficient prediction and multi level interpretability. Its
technical breakthroughs are mainly reflected in the following three
dimensions:Local global feature collaborative perception module. Based on
heterogeneous graph convolutional neural network (HGCN), a multi order neighbor
aggregation strategy is designed.Multi scale graph signal decomposition and
biological interpretation module. A deep hierarchical node feature transform
(GWT) architecture is proposed.Contrastive learning combining multi dimensional
perspectives and hierarchical representations. By comparing the learning
models, the node representations from the two perspectives of HGCN and GWT are
aligned and fused, so that the model can integrate multi dimensional
information and improve the prediction robustness. Experimental results show
that our framework shows excellent prediction performance on all datasets. This
study provides a complete solution for drug target discovery from black box
prediction to mechanism decoding, and its methodology has important reference
value for modeling complex biomolecular interaction systems.},
 author = {Wenfeng Dai and Yanhong Wang and Shuai Yan and Qingzhi Yu and Xiang Cheng},
 citations = {0},
 comment = {},
 doi = {10.1038/s41598-025-16098-y},
 eprint = {2504.20103v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning},
 url = {http://arxiv.org/abs/2504.20103v1},
 year = {2025}
}

@article{2504.20118v4,
 abstract = {Traditional Chinese Medicine (TCM) represents a rich repository of ancient
medical knowledge that continues to play an important role in modern
healthcare. Due to the complexity and breadth of the TCM literature, the
integration of AI technologies is critical for its modernization and broader
accessibility. However, this integration poses considerable challenges,
including the interpretation of obscure classical Chinese texts and the
modeling of intricate semantic relationships among TCM concepts. In this paper,
we develop OpenTCM, an LLM-based system that combines a domain-specific TCM
knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).
First, we extract more than 3.73 million classical Chinese characters from 68
gynecological books in the Chinese Medical Classics Database, with the help of
TCM and gynecology experts. Second, we construct a comprehensive
multi-relational knowledge graph comprising more than 48,000 entities and
152,000 interrelationships, using customized prompts and Chinese-oriented LLMs
such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,
we empower OpenTCM with GraphRAG, enabling high-fidelity ingredient knowledge
retrieval and diagnostic question-answering without model fine-tuning.
Experimental evaluations demonstrate that OpenTCM achieves mean expert scores
(MES) of 4.378 in ingredient information retrieval and 4.045 in diagnostic
question-answering tasks, outperforming state-of-the-art solutions in
real-world TCM use cases.},
 author = {Jinglin He and Yunqi Guo and Lai Kwan Lam and Waikei Leung and Lixing He and Yuanan Jiang and Chi Chiu Wang and Guoliang Xing and Hongkai Chen},
 citations = {},
 comment = {8 pages, 5 figures, 7 tables},
 doi = {},
 eprint = {2504.20118v4},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis},
 url = {http://arxiv.org/abs/2504.20118v4},
 year = {2025}
}

@article{2504.20635v1,
 abstract = {Ensuring the generalisability of clinical machine learning (ML) models across
diverse healthcare settings remains a significant challenge due to variability
in patient demographics, disease prevalence, and institutional practices.
Existing model evaluation approaches often rely on real-world datasets, which
are limited in availability, embed confounding biases, and lack the flexibility
needed for systematic experimentation. Furthermore, while generative models aim
for statistical realism, they often lack transparency and explicit control over
factors driving distributional shifts. In this work, we propose a novel
structured synthetic data framework designed for the controlled benchmarking of
model robustness, fairness, and generalisability. Unlike approaches focused
solely on mimicking observed data, our framework provides explicit control over
the data generating process, including site-specific prevalence variations,
hierarchical subgroup effects, and structured feature interactions. This
enables targeted investigation into how models respond to specific
distributional shifts and potential biases. Through controlled experiments, we
demonstrate the framework's ability to isolate the impact of site variations,
support fairness-aware audits, and reveal generalisation failures, particularly
highlighting how model complexity interacts with site-specific effects. This
work contributes a reproducible, interpretable, and configurable tool designed
to advance the reliable deployment of ML in clinical settings.},
 author = {Bradley Segal and Joshua Fieggen and David Clifton and Lei Clifton},
 citations = {},
 comment = {7 pages, 4 figures},
 doi = {},
 eprint = {2504.20635v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation},
 url = {http://arxiv.org/abs/2504.20635v1},
 year = {2025}
}

@article{2504.21795v3,
 abstract = {The Hawkes process (HP) is commonly used to model event sequences with
self-reinforcing dynamics, including electronic health records (EHRs).
Traditional HPs capture self-reinforcement via parametric impact functions that
can be inspected to understand how each event modulates the intensity of
others. Neural network-based HPs offer greater flexibility, resulting in
improved fit and prediction performance, but at the cost of interpretability,
which is often critical in healthcare. In this work, we aim to understand and
improve upon this tradeoff. We propose a novel HP formulation in which impact
functions are modeled by defining a flexible impact kernel, instantiated as a
neural network, in event embedding space, which allows us to model large-scale
event sequences with many event types. This approach is more flexible than
traditional HPs yet more interpretable than other neural network approaches,
and allows us to explicitly trade flexibility for interpretability by adding
transformer encoder layers to further contextualize the event embeddings.
Results show that our method accurately recovers impact functions in
simulations, achieves competitive performance on MIMIC-IV procedure dataset,
and gains clinically meaningful interpretation on Duke-EHR with children
diagnosis dataset even without transformer layers. This suggests that our
flexible impact kernel is often sufficient to capture self-reinforcing dynamics
in EHRs and other data effectively, implying that interpretability can be
maintained without loss of performance.},
 author = {Yuankang Zhao and Matthew Engelhard},
 citations = {},
 comment = {Machine Learning for Healthcare 2025},
 doi = {},
 eprint = {2504.21795v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model},
 url = {http://arxiv.org/abs/2504.21795v3},
 year = {2025}
}

@article{2505.00008v2,
 abstract = {Objective: This review aims to explore the potential and challenges of using
Natural Language Processing (NLP) to detect, correct, and mitigate medically
inaccurate information, including errors, misinformation, and hallucination. By
unifying these concepts, the review emphasizes their shared methodological
foundations and their distinct implications for healthcare. Our goal is to
advance patient safety, improve public health communication, and support the
development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines,
analyzing studies from 2020 to 2024 across five databases. Studies were
selected based on their use of NLP to address medically inaccurate information
and were categorized by topic, tasks, document types, datasets, models, and
evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate
information on the following tasks: (1) error detection (2) error correction
(3) misinformation detection (4) misinformation correction (5) hallucination
detection (6) hallucination mitigation. However, challenges remain with data
privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle
medically inaccurate information while underscoring the need to address
persistent challenges. Future efforts should focus on developing real-world
datasets, refining contextual methods, and improving hallucination management
to ensure reliable and transparent healthcare applications.},
 author = {Zhaoyi Sun and Wen-Wai Yim and Ozlem Uzuner and Fei Xia and Meliha Yetisgen},
 citations = {1},
 comment = {This paper has been accepted by the Journal of Biomedical Informatics},
 doi = {10.1016/j.jbi.2025.104866},
 eprint = {2505.00008v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination},
 url = {http://arxiv.org/abs/2505.00008v2},
 year = {2025}
}

@article{2505.00171v1,
 abstract = {Non-muscle-invasive bladder cancer (NMIBC) is a relentless challenge in
oncology, with recurrence rates soaring as high as 70-80%. Each recurrence
triggers a cascade of invasive procedures, lifelong surveillance, and
escalating healthcare costs - affecting 460,000 individuals worldwide. However,
existing clinical prediction tools remain fundamentally flawed, often
overestimating recurrence risk and failing to provide personalized insights for
patient management. In this work, we propose an interpretable deep learning
framework that integrates vector embeddings and attention mechanisms to improve
NMIBC recurrence prediction performance. We incorporate vector embeddings for
categorical variables such as smoking status and intravesical treatments,
allowing the model to capture complex relationships between patient attributes
and recurrence risk. These embeddings provide a richer representation of the
data, enabling improved feature interactions and enhancing prediction
performance. Our approach not only enhances performance but also provides
clinicians with patient-specific insights by highlighting the most influential
features contributing to recurrence risk for each patient. Our model achieves
accuracy of 70% with tabular data, outperforming conventional statistical
methods while providing clinician-friendly patient-level explanations through
feature attention. Unlike previous studies, our approach identifies new
important factors influencing recurrence, such as surgical duration and
hospital stay, which had not been considered in existing NMIBC prediction
models.},
 author = {Saram Abbas and Naeem Soomro and Rishad Shafik and Rakesh Heer and Kabita Adhikari},
 citations = {},
 comment = {7 pages, 5 figures, Accepted to be presented at the 47th Annual
  International Conference of the IEEE Engineering in Medicine and Biology
  Society (EMBC 2025)},
 doi = {},
 eprint = {2505.00171v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Attention-enabled Explainable AI for Bladder Cancer Recurrence Prediction},
 url = {http://arxiv.org/abs/2505.00171v1},
 year = {2025}
}

@article{2505.00189v1,
 abstract = {Chronic diseases, such as cardiovascular disease, diabetes, chronic kidney
disease, and thyroid disorders, are the leading causes of premature mortality
worldwide. Early detection and intervention are crucial for improving patient
outcomes, yet traditional diagnostic methods often fail due to the complex
nature of these conditions. This study explores the application of machine
learning (ML) and deep learning (DL) techniques to predict chronic disease and
thyroid disorders. We used a variety of models, including Logistic Regression
(LR), Random Forest (RF), Gradient Boosted Trees (GBT), Neural Networks (NN),
Decision Trees (DT) and Native Bayes (NB), to analyze and predict disease
outcomes. Our methodology involved comprehensive data pre-processing, including
handling missing values, categorical encoding, and feature aggregation,
followed by model training and evaluation. Performance metrics such ad
precision, recall, accuracy, F1-score, and Area Under the Curve (AUC) were used
to assess the effectiveness of each model. The results demonstrated that
ensemble methods like Random Forest and Gradient Boosted Trees consistently
outperformed. Neutral Networks also showed superior performance, particularly
in capturing complex data patterns. The findings highlight the potential of ML
and DL in revolutionizing chronic disease prediction, enabling early diagnosis
and personalized treatment strategies. However, challenges such as data
quality, model interpretability, and the need for advanced computational
techniques in healthcare to improve patient outcomes and reduce the burden of
chronic diseases. This study was conducted as part of Big Data class project
under the supervision of our professors Mr. Abderrahmane EZ-ZAHOUT and Mr.
Abdessamad ESSAIDI.},
 author = {Houda Belhad and Asmae Bourbia and Salma Boughanja},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2505.00189v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Chronic Diseases Prediction using Machine Learning and Deep Learning Methods},
 url = {http://arxiv.org/abs/2505.00189v1},
 year = {2025}
}

@article{2505.00275v1,
 abstract = {Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,
epilepsy, and tuberculosis, necessitate rigorous adherence to medication to
avert disease progression, manage symptoms, and decrease mortality rates.
Adherence is frequently undermined by factors including patient behavior,
caregiver support, elevated medical costs, and insufficient healthcare
infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based
multimodal large vision language model (LVLM) aimed at visual question
answering (VQA) concerning medication adherence through patient videos. We
employ a private dataset comprising 806 custom-annotated tuberculosis (TB)
medication monitoring videos, which have been labeled by clinical experts, to
fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a
detailed medical adherence VQA dataset that encompasses positive, negative, and
ambiguous adherence cases. Our method identifies correlations between visual
features, such as the clear visibility of the patient's face, medication, water
intake, and the act of ingestion, and their associated medical concepts in
captions. This facilitates the integration of aligned visual-linguistic
representations and improves multimodal interactions. Experimental results
indicate that our method surpasses parameter-efficient fine-tuning (PEFT)
enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute
improvements ranging from 3.1% to 3.54% across pre-trained, regular, and
low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and
attention map visualizations substantiate our approach, enhancing
interpretability.},
 author = {Md Asaduzzaman Jabin and Hanqi Jiang and Yiwei Li and Patrick Kaggwa and Eugene Douglass and Juliet N. Sekandi and Tianming Liu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.00275v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care},
 url = {http://arxiv.org/abs/2505.00275v1},
 year = {2025}
}

@article{2505.00410v2,
 abstract = {The present research tackles the difficulty of predicting osteoporosis risk
via machine learning (ML) approaches, emphasizing the use of explainable
artificial intelligence (XAI) to improve model transparency. Osteoporosis is a
significant public health concern, sometimes remaining untreated owing to its
asymptomatic characteristics, and early identification is essential to avert
fractures. The research assesses six machine learning classifiers: Random
Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting
and utilizes a dataset based on clinical, demographic, and lifestyle variables.
The models are refined using GridSearchCV to calibrate hyperparameters, with
the objective of enhancing predictive efficacy. XGBoost had the greatest
accuracy (91%) among the evaluated models, surpassing others in precision
(0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI
approaches, such as SHAP, LIME, and Permutation Feature Importance, to
elucidate the decision-making process of the optimal model. The study indicates
that age is the primary determinant in forecasting osteoporosis risk, followed
by hormonal alterations and familial history. These results corroborate
clinical knowledge and affirm the models' therapeutic significance. The
research underscores the significance of explainability in machine learning
models for healthcare applications, guaranteeing that physicians can rely on
the system's predictions. The report ultimately proposes directions for further
research, such as validation across varied populations and the integration of
supplementary biomarkers for enhanced predictive accuracy.},
 author = {Farhana Elias and Md Shihab Reza and Muhammad Zawad Mahmud and Samiha Islam and Shahran Rahman Alve},
 citations = {1},
 comment = {Submitted in an international conference},
 doi = {10.1109/QPAIN66474.2025.11172213},
 eprint = {2505.00410v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis},
 url = {http://arxiv.org/abs/2505.00410v2},
 year = {2025}
}

@article{2505.00616v2,
 abstract = {As artificial intelligence systems grow more capable and autonomous, frontier
AI development poses potential systemic risks that could affect society at a
massive scale. Current practices at many AI labs developing these systems lack
sufficient transparency around safety measures, testing procedures, and
governance structures. This opacity makes it challenging to verify safety
claims or establish appropriate liability when harm occurs. Drawing on
liability frameworks from nuclear energy, aviation software, cybersecurity, and
healthcare, we propose a comprehensive approach to safety documentation and
accountability in frontier AI development.},
 author = {Aidan Kierans and Kaley Rittichier and Utku Sonsayar and Avijit Ghosh},
 citations = {0},
 comment = {10 pages, 1 figure, 1 table, in review for AIES 2025, presented at
  TAIS 2025},
 doi = {},
 eprint = {2505.00616v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Catastrophic Liability: Managing Systemic Risks in Frontier AI Development},
 url = {http://arxiv.org/abs/2505.00616v2},
 year = {2025}
}

@article{2505.00827v1,
 abstract = {Clinical risk prediction based on machine learning algorithms plays a vital
role in modern healthcare. A crucial component in developing a reliable
prediction model is collecting high-quality time series clinical events. In
this work, we release such a dataset that consists of 22,588,586 Clinical Time
Series events, which we term MIMIC-\RNum{4}-Ext-22MCTS. Our source data are
discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note
\cite{Johnson2023-pg}. We then extract clinical events as short text span from
the discharge summaries, along with the timestamps of these events as temporal
information. The general-purpose MIMIC-IV-Note pose specific challenges for our
work: it turns out that the discharge summaries are too lengthy for typical
natural language models to process, and the clinical events of interest often
are not accompanied with explicit timestamps. Therefore, we propose a new
framework that works as follows: 1) we break each discharge summary into
manageably small text chunks; 2) we apply contextual BM25 and contextual
semantic search to retrieve chunks that have a high potential of containing
clinical events; and 3) we carefully design prompts to teach the recently
released Llama-3.1-8B \cite{touvron2023llama} model to identify or infer
temporal information of the chunks. We show that the obtained dataset is so
informative and transparent that standard models fine-tuned on our dataset are
achieving significant improvements in healthcare applications. In particular,
the BERT model fine-tuned based on our dataset achieves 10\% improvement in
accuracy on medical question answering task, and 3\% improvement in clinical
trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned
on our dataset, produces more clinically reliable results for clinical
questions.},
 author = {Jing Wang and Xing Niu and Juyong Kim and Jie Shen and Tong Zhang and Jeremy C. Weiss},
 citations = {},
 comment = {},
 doi = {10.21203/rs.3.rs-6347897/v1},
 eprint = {2505.00827v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MIMIC-\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction},
 url = {http://arxiv.org/abs/2505.00827v1},
 year = {2025}
}

@article{2505.01145v1,
 abstract = {In recent years, two parallel research trends have emerged in machine
learning, yet their intersections remain largely unexplored. On one hand, there
has been a significant increase in literature focused on Individual Treatment
Effect (ITE) modeling, particularly targeting the Conditional Average Treatment
Effect (CATE) using meta-learner techniques. These approaches often aim to
identify causal effects from observational data. On the other hand, the field
of Explainable Machine Learning (XML) has gained traction, with various
approaches developed to explain complex models and make their predictions more
interpretable. A prominent technique in this area is Shapley Additive
Explanations (SHAP), which has become mainstream in data science for analyzing
supervised learning models. However, there has been limited exploration of SHAP
application in identifying predictive biomarkers through CATE models, a crucial
aspect in pharmaceutical precision medicine. We address inherent challenges
associated with the SHAP concept in multi-stage CATE strategies and introduce a
surrogate estimation approach that is agnostic to the choice of CATE strategy,
effectively reducing computational burdens in high-dimensional data. Using this
approach, we conduct simulation benchmarking to evaluate the ability to
accurately identify biomarkers using SHAP values derived from various CATE
meta-learners and Causal Forest.},
 author = {David Svensson and Erik Hermansson and Nikolaos Nikolaou and Konstantinos Sechidis and Ilya Lipkovich},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.01145v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Overview and practical recommendations on using Shapley Values for identifying predictive biomarkers via CATE modeling},
 url = {http://arxiv.org/abs/2505.01145v1},
 year = {2025}
}

@article{2505.02062v1,
 abstract = {The adoption of Artificial Intelligence (AI) in the healthcare service
industry presents numerous ethical challenges, yet current frameworks often
fail to offer a comprehensive, empirical understanding of the multidimensional
factors influencing ethical AI integration. Addressing this critical research
gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model
(MEAAM), a novel theoretical framework that categorizes 13 critical ethical
variables across four foundational dimensions of Ethical AI Fair AI,
Responsible AI, Explainable AI, and Sustainable AI. These dimensions are
further analyzed through three core ethical lenses: epistemic concerns (related
to knowledge, transparency, and system trustworthiness), normative concerns
(focused on justice, autonomy, dignity, and moral obligations), and overarching
concerns (highlighting global, systemic, and long-term ethical implications).
This study adopts a quantitative, cross-sectional research design using survey
data collected from healthcare professionals and analyzed via Partial Least
Squares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study
empirically investigates the influence of these ethical constructs on two
outcomes Operational AI Adoption and Systemic AI Adoption. Results indicate
that normative concerns most significantly drive operational adoption
decisions, while overarching concerns predominantly shape systemic adoption
strategies and governance frameworks. Epistemic concerns play a facilitative
role, enhancing the impact of ethical design principles on trust and
transparency in AI systems. By validating the MEAAM framework, this research
advances a holistic, actionable approach to ethical AI adoption in healthcare
and provides critical insights for policymakers, technologists, and healthcare
administrators striving to implement ethically grounded AI solutions.},
 author = {Prathamesh Muzumdar and Apoorva Muley and Kuldeep Singh and Sumanth Cheemalapati},
 citations = {0},
 comment = {},
 doi = {10.9734/ajmah/2025/v23i51228},
 eprint = {2505.02062v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)},
 url = {http://arxiv.org/abs/2505.02062v1},
 year = {2025}
}

@article{2505.02874v1,
 abstract = {Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,
reliability, and interpretability of Machine Learning (ML) systems for
healthcare, optimizing resources and improving patient care. Despite the
emergence of ML-based clinical decision support tools, the lack of principled
quantification of uncertainty in ML models remains a major challenge. Current
reviews have a narrow focus on analyzing the state-of-the-art UQ in specific
healthcare domains without systematically evaluating method efficacy across
different stages of model development, and despite a growing body of research,
its implementation in healthcare applications remains limited. Therefore, in
this survey, we provide a comprehensive analysis of current UQ in healthcare,
offering an informed framework that highlights how different methods can be
integrated into each stage of the ML pipeline including data processing,
training and evaluation. We also highlight the most popular methods used in
healthcare and novel approaches from other domains that hold potential for
future adoption in the medical context. We expect this study will provide a
clear overview of the challenges and opportunities of implementing UQ in the ML
pipeline for healthcare, guiding researchers and practitioners in selecting
suitable techniques to enhance the reliability, safety and trust from patients
and clinicians on ML-driven healthcare solutions.},
 author = {L. Julián Lechuga López and Shaza Elsharief and Dhiyaa Al Jorf and Firas Darwish and Congbo Ma and Farah E. Shamout},
 citations = {0},
 comment = {46 pages, 3 figures, 2 tables, AHLI Conference on Health, Inference,
  and Learning (CHIL)},
 doi = {10.54195/9789465150475},
 eprint = {2505.02874v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Uncertainty Quantification for Machine Learning in Healthcare: A Survey},
 url = {http://arxiv.org/abs/2505.02874v1},
 year = {2025}
}

@article{2505.03315v1,
 abstract = {Understanding and predicting human behavior has emerged as a core capability
in various AI application domains such as autonomous driving, smart healthcare,
surveillance systems, and social robotics. This paper defines the technical
framework of Artificial Behavior Intelligence (ABI), which comprehensively
analyzes and interprets human posture, facial expressions, emotions, behavioral
sequences, and contextual cues. It details the essential components of ABI,
including pose estimation, face and emotion recognition, sequential behavior
analysis, and context-aware modeling. Furthermore, we highlight the
transformative potential of recent advances in large-scale pretrained models,
such as large language models (LLMs), vision foundation models, and multimodal
integration models, in significantly improving the accuracy and
interpretability of behavior recognition. Our research team has a strong
interest in the ABI domain and is actively conducting research, particularly
focusing on the development of intelligent lightweight models capable of
efficiently inferring complex human behaviors. This paper identifies several
technical challenges that must be addressed to deploy ABI in real-world
applications including learning behavioral intelligence from limited data,
quantifying uncertainty in complex behavior prediction, and optimizing model
structures for low-power, real-time inference. To tackle these challenges, our
team is exploring various optimization strategies including lightweight
transformers, graph-based recognition architectures, energy-aware loss
functions, and multimodal knowledge distillation, while validating their
applicability in real-time environments.},
 author = {Kanghyun Jo and Jehwan Choi and Kwanho Kim and Seongmin Kim and Duy-Linh Nguyen and Xuan-Thuy Vo and Adri Priadana and Tien-Dat Tran},
 citations = {},
 comment = {9 pages, 6 figures, Pre-print for IWIS2025},
 doi = {10.36227/techrxiv.175289368.86941511/v1},
 eprint = {2505.03315v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Artificial Behavior Intelligence: Technology, Challenges, and Future Directions},
 url = {http://arxiv.org/abs/2505.03315v1},
 year = {2025}
}

@article{2505.03380v1,
 abstract = {Medical AI assistants support doctors in disease diagnosis, medical image
analysis, and report generation. However, they still face significant
challenges in clinical use, including limited accuracy with multimodal content
and insufficient validation in real-world settings. We propose RCMed, a
full-stack AI assistant that improves multimodal alignment in both input and
output, enabling precise anatomical delineation, accurate localization, and
reliable diagnosis through hierarchical vision-language grounding. A
self-reinforcing correlation mechanism allows visual features to inform
language context, while language semantics guide pixel-wise attention, forming
a closed loop that refines both modalities. This correlation is enhanced by a
color region description strategy, translating anatomical structures into
semantically rich text to learn shape-location-text relationships across
scales. Trained on 20 million image-mask-description triplets, RCMed achieves
state-of-the-art precision in contextualizing irregular lesions and subtle
anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It
achieved a 23.5% relative improvement in cell segmentation from microscopy
images over prior methods. RCMed's strong vision-language alignment enables
exceptional generalization, with state-of-the-art performance in external
validation across 20 clinically significant cancer types, including novel
tasks. This work demonstrates how integrated multimodal models capture
fine-grained patterns, enabling human-level interpretation in complex scenarios
and advancing human-centric AI healthcare.},
 author = {Haonan Wang and Jiaji Mao and Lehan Wang and Qixiang Zhang and Marawan Elbatel and Yi Qin and Huijun Hu and Baoxun Li and Wenhui Deng and Weifeng Qin and Hongrui Li and Jialin Liang and Jun Shen and Xiaomeng Li},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.03380v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant},
 url = {http://arxiv.org/abs/2505.03380v1},
 year = {2025}
}

@article{2505.03781v1,
 abstract = {Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers
high accuracy and convenience. However, generating reliable, evidence-based
results in specialized fields like healthcare remains a challenge, as RAG alone
may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG
for ECG analysis that incorporates expert-curated knowledge to enhance
diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset
demonstrates the framework's effectiveness, highlighting the value of
structured domain expertise in automated ECG interpretation. Our framework is
designed to support comprehensive ECG analysis, addressing diverse diagnostic
needs with potential applications beyond the tested dataset.},
 author = {Jin Yu and JaeHo Park and TaeJun Park and Gyurin Kim and JiHyun Lee and Min Sung Lee and Joon-myoung Kwon and Jeong Min Son and Yong-Yeon Jo},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2505.03781v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ALFRED: Ask a Large-language model For Reliable ECG Diagnosis},
 url = {http://arxiv.org/abs/2505.03781v1},
 year = {2025}
}

@article{2505.03785v2,
 abstract = {Agentic systems built on large language models (LLMs) offer promising
capabilities for automating complex workflows in healthcare AI. We introduce
mAIstro, an open-source, autonomous multi-agentic framework for end-to-end
development and deployment of medical AI models. The system orchestrates
exploratory data analysis, radiomic feature extraction, image segmentation,
classification, and regression through a natural language interface, requiring
no coding from the user. Built on a modular architecture, mAIstro supports both
open- and closed-source LLMs, and was evaluated using a large and diverse set
of prompts across 16 open-source datasets, covering a wide range of imaging
modalities, anatomical regions, and data types. The agents successfully
executed all tasks, producing interpretable outputs and validated models. This
work presents the first agentic framework capable of unifying data analysis, AI
model development, and inference across varied healthcare applications,
offering a reproducible and extensible foundation for clinical and research AI
integration. The code is available at: https://github.com/eltzanis/mAIstro},
 author = {Eleftherios Tzanis and Michail E. Klontzas},
 citations = {},
 comment = {},
 doi = {10.1016/j.ejrai.2025.100044},
 eprint = {2505.03785v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging},
 url = {http://arxiv.org/abs/2505.03785v2},
 year = {2025}
}

@article{2505.03863v1,
 abstract = {Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as
healthcare, avionics, and autonomous vehicles. Formal verification of their
operational safety is, therefore, of utmost importance. In this paper, we
address the falsification problem, where the focus is on searching for an
unsafe execution in the system instead of proving their absence. The
contribution of this paper is a framework that (a) connects the falsification
of CPS with the falsification of deep neural networks (DNNs) and (b) leverages
the inherent interpretability of Decision Trees for faster falsification of
CPS. This is achieved by: (1) building a surrogate model of the CPS under test,
either as a DNN model or a Decision Tree, (2) application of various DNN
falsification tools to falsify CPS, and (3) a novel falsification algorithm
guided by the explanations of safety violations of the CPS model extracted from
its Decision Tree surrogate. The proposed framework has the potential to
exploit a repertoire of \emph{adversarial attack} algorithms designed to
falsify robustness properties of DNNs, as well as state-of-the-art
falsification algorithms for DNNs. Although the presented methodology is
applicable to systems that can be executed/simulated in general, we demonstrate
its effectiveness, particularly in CPS. We show that our framework, implemented
as a tool \textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS
that have linear and non-linear dynamics. Decision tree-guided falsification
shows promising results in efficiently finding multiple counterexamples in the
ARCH-COMP 2024 falsification benchmarks~\cite{khandait2024arch}.},
 author = {Atanu Kundu and Sauvik Gon and Rajarshi Ray},
 citations = {5},
 comment = {},
 doi = {10.1145/3641399.3641401},
 eprint = {2505.03863v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Data-Driven Falsification of Cyber-Physical Systems},
 url = {http://arxiv.org/abs/2505.03863v1},
 year = {2025}
}

@article{2505.04570v1,
 abstract = {Technological advances in Artificial Intelligence (AI) and Machine Learning
(ML) for the healthcare domain are rapidly arising, with a growing discussion
regarding the ethical management of their development. In general, ML
healthcare applications crucially require performance, interpretability of
data, and respect for data privacy. The latter is an increasingly debated topic
as commercial cloud computing services become more and more widespread.
Recently, dedicated methods are starting to be developed aiming to protect data
privacy. However, these generally result in a trade-off forcing one to balance
the level of data privacy and the algorithm performance. Here, a Support Vector
Machine (SVM) classifier model is proposed whose training is reformulated into
a Quadratic Unconstrained Binary Optimization (QUBO) problem, and adapted to a
neutral atom-based Quantum Processing Unit (QPU). Our final model does not
require anonymization techniques to protect data privacy since the sensitive
data are not needed to be transferred to the cloud-available QPU. Indeed, the
latter is used only during the training phase, hence allowing a future concrete
application in a real-world scenario. Finally, performance and scaling analyses
on a publicly available breast cancer dataset are discussed, both using ideal
and noisy simulations for the training process, and also successfully tested on
a currently available real neutral-atom QPU.},
 author = {Ettore Canonici and Filippo Caruso},
 citations = {},
 comment = {19 pages, 5 figures},
 doi = {},
 eprint = {2505.04570v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Privacy-preserving neutral atom-based quantum classifier towards real healthcare applications},
 url = {http://arxiv.org/abs/2505.04570v1},
 year = {2025}
}

@article{2505.05857v3,
 abstract = {In the last few decades, Machine Learning (ML) has achieved significant
success across domains ranging from healthcare, sustainability, and the social
sciences, to criminal justice and finance. But its deployment in increasingly
sophisticated, critical, and sensitive areas affecting individuals, the groups
they belong to, and society as a whole raises critical concerns around
fairness, transparency and robustness, among others. As the complexity and
scale of ML systems and of the settings in which they are deployed grow, so
does the need for responsible ML methods that address these challenges while
providing guaranteed performance in deployment.
  Mixed-integer optimization (MIO) offers a powerful framework for embedding
responsible ML considerations directly into the learning process while
maintaining performance. For example, it enables learning of inherently
transparent models that can conveniently incorporate fairness or other domain
specific constraints. This tutorial paper provides an accessible and
comprehensive introduction to this topic discussing both theoretical and
practical aspects. It outlines some of the core principles of responsible ML,
their importance in applications, and the practical utility of MIO for building
ML models that align with these principles. Through examples and mathematical
formulations, it illustrates practical strategies and available tools for
efficiently solving MIO problems for responsible ML. It concludes with a
discussion on current limitations and open research questions, providing
suggestions for future work.},
 author = {Nathan Justin and Qingshi Sun and Andrés Gómez and Phebe Vayanos},
 citations = {},
 comment = {69 pages, 12 figures},
 doi = {10.1287/educ.2025.0292},
 eprint = {2505.05857v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Responsible Machine Learning via Mixed-Integer Optimization},
 url = {http://arxiv.org/abs/2505.05857v3},
 year = {2025}
}

@article{2505.06620v1,
 abstract = {There is a growing demand for the use of Artificial Intelligence (AI) and
Machine Learning (ML) in healthcare, particularly as clinical decision support
systems to assist medical professionals. However, the complexity of many of
these models, often referred to as black box models, raises concerns about
their safe integration into clinical settings as it is difficult to understand
how they arrived at their predictions. This paper discusses insights and
recommendations derived from an expert working group convened by the UK
Medicine and Healthcare products Regulatory Agency (MHRA). The group consisted
of healthcare professionals, regulators, and data scientists, with a primary
focus on evaluating the outputs from different AI algorithms in clinical
decision-making contexts. Additionally, the group evaluated findings from a
pilot study investigating clinicians' behaviour and interaction with AI methods
during clinical diagnosis. Incorporating AI methods is crucial for ensuring the
safety and trustworthiness of medical AI devices in clinical settings. Adequate
training for stakeholders is essential to address potential issues, and further
insights and recommendations for safely adopting AI systems in healthcare
settings are provided.},
 author = {Dima Alattal and Asal Khoshravan Azar and Puja Myles and Richard Branson and Hatim Abdulhussein and Allan Tucker},
 citations = {0},
 comment = {47 pages},
 doi = {},
 eprint = {2505.06620v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations},
 url = {http://arxiv.org/abs/2505.06620v1},
 year = {2025}
}

@article{2505.06971v1,
 abstract = {What does Artificial Intelligence (AI) have to contribute to health care? And
what should we be looking out for if we are worried about its risks? In this
paper we offer a survey, and initial evaluation, of hopes and fears about the
applications of artificial intelligence in medicine. AI clearly has enormous
potential as a research tool, in genomics and public health especially, as well
as a diagnostic aid. It's also highly likely to impact on the organisational
and business practices of healthcare systems in ways that are perhaps
under-appreciated. Enthusiasts for AI have held out the prospect that it will
free physicians up to spend more time attending to what really matters to them
and their patients. We will argue that this claim depends upon implausible
assumptions about the institutional and economic imperatives operating in
contemporary healthcare settings. We will also highlight important concerns
about privacy, surveillance, and bias in big data, as well as the risks of over
trust in machines, the challenges of transparency, the deskilling of healthcare
practitioners, the way AI reframes healthcare, and the implications of AI for
the distribution of power in healthcare institutions. We will suggest that two
questions, in particular, are deserving of further attention from philosophers
and bioethicists. What does care look like when one is dealing with data as
much as people? And, what weight should we give to the advice of machines in
our own deliberations about medical decisions?},
 author = {Robert Sparrow and Joshua Hatherley},
 citations = {0},
 comment = {},
 doi = {10.1056/aie2401073},
 eprint = {2505.06971v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {The promise and perils of AI in medicine},
 url = {http://arxiv.org/abs/2505.06971v1},
 year = {2025}
}

@article{2505.07875v1,
 abstract = {Assessments of trustworthiness have become a cornerstone of responsible AI
development. Especially in high-stakes fields like healthcare, aligning
technical, evidence-based, and ethical practices with forthcoming legal
requirements is increasingly urgent. We argue that developers and deployers of
AI systems for the medical domain should be proactive and take steps to
progressively ensure that such systems, both those currently in use and those
being developed or planned, respect the requirements of the AI Act, which has
come into force in August 2024. This is necessary if full and effective
compliance is to be ensured when the most relevant provisions of the Act become
effective (August 2026). The engagement with the AI Act cannot be viewed as a
formalistic exercise. Compliance with the AI Act needs to be carried out
through the proactive commitment to the ethical principles of trustworthy AI.
These principles provide the background for the Act, which mentions them
several times and connects them to the protection of public interest. They can
be used to interpret and apply the Act's provisions and to identify good
practices, increasing the validity and sustainability of AI systems over time.},
 author = {John Brandt Brodersen and Ilaria Amelia Caggiano and Pedro Kringen and Vince Istvan Madai and Walter Osika and Giovanni Sartor and Ellen Svensson and Magnus Westerlund and Roberto V. Zicari},
 citations = {},
 comment = {8 pages, 1 table},
 doi = {},
 eprint = {2505.07875v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Getting Ready for the EU AI Act in Healthcare. A call for Sustainable AI Development and Deployment},
 url = {http://arxiv.org/abs/2505.07875v1},
 year = {2025}
}

@article{2505.08085v1,
 abstract = {Privacy and regulatory barriers often hinder centralized machine learning
solutions, particularly in sectors like healthcare where data cannot be freely
shared. Federated learning has emerged as a powerful paradigm to address these
concerns; however, existing frameworks primarily support gradient-based models,
leaving a gap for more interpretable, tree-based approaches. This paper
introduces a federated learning framework for Random Forest classifiers that
preserves data privacy and provides robust performance in distributed settings.
By leveraging PySyft for secure, privacy-aware computation, our method enables
multiple institutions to collaboratively train Random Forest models on locally
stored data without exposing sensitive information. The framework supports
weighted model averaging to account for varying data distributions, incremental
learning to progressively refine models, and local evaluation to assess
performance across heterogeneous datasets. Experiments on two real-world
healthcare benchmarks demonstrate that the federated approach maintains
competitive predictive accuracy - within a maximum 9\% margin of centralized
methods - while satisfying stringent privacy requirements. These findings
underscore the viability of tree-based federated learning for scenarios where
data cannot be centralized due to regulatory, competitive, or technical
constraints. The proposed solution addresses a notable gap in existing
federated learning libraries, offering an adaptable tool for secure distributed
machine learning tasks that demand both transparency and reliable performance.
The tool is available at https://github.com/ieeta-pt/fed_rf.},
 author = {Alexandre Cotorobai and Jorge Miguel Silva and Jose Luis Oliveira},
 citations = {1},
 comment = {},
 doi = {10.1109/cbms65348.2025.00159},
 eprint = {2505.08085v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Federated Random Forest Solution for Secure Distributed Machine Learning},
 url = {http://arxiv.org/abs/2505.08085v1},
 year = {2025}
}

@article{2505.08198v2,
 abstract = {Explainable artificial intelligence (XAI) is essential for trustworthy
machine learning (ML), particularly in high-stakes domains such as healthcare
and finance. Shapley value (SV) methods provide a principled framework for
feature attribution in complex models but incur high computational costs,
limiting their scalability in high-dimensional settings. We propose Stochastic
Iterative Momentum for Shapley Value Approximation (SIM-Shapley), a stable and
efficient SV approximation method inspired by stochastic optimization. We
analyze variance theoretically, prove linear $Q$-convergence, and demonstrate
improved empirical stability and low bias in practice on real-world datasets.
In our numerical experiments, SIM-Shapley reduces computation time by up to 85%
relative to state-of-the-art baselines while maintaining comparable feature
attribution quality. Beyond feature attribution, our stochastic mini-batch
iterative framework extends naturally to a broader class of sample average
approximation problems, offering a new avenue for improving computational
efficiency with stability guarantees. Code is publicly available at
https://github.com/nliulab/SIM-Shapley.},
 author = {Wangxuan Fan and Siqi Li and Doudou Zhou and Yohei Okada and Chuan Hong and Molei Liu and Nan Liu},
 citations = {0},
 comment = {21 pages, 6 figures, 5 tables},
 doi = {},
 eprint = {2505.08198v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SIM-Shapley: A Stable and Computationally Efficient Approach to Shapley Value Approximation},
 url = {http://arxiv.org/abs/2505.08198v2},
 year = {2025}
}

@article{2505.08445v1,
 abstract = {Large language models achieve high task performance yet often hallucinate or
rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses
these gaps by coupling generation with external search. We analyse how
hyperparameters influence speed and quality in RAG systems, covering Chroma and
Faiss vector stores, chunking policies, cross-encoder re-ranking, and
temperature, and we evaluate six metrics: faithfulness, answer correctness,
answer relevancy, context precision, context recall, and answer similarity.
Chroma processes queries 13% faster, whereas Faiss yields higher retrieval
precision, revealing a clear speed-accuracy trade-off. Naive fixed-length
chunking with small windows and minimal overlap outperforms semantic
segmentation while remaining the quickest option. Re-ranking provides modest
gains in retrieval quality yet increases runtime by roughly a factor of 5, so
its usefulness depends on latency constraints. These results help practitioners
balance computational cost and accuracy when tuning RAG systems for
transparent, up-to-date responses. Finally, we re-evaluate the top
configurations with a corrective RAG workflow and show that their advantages
persist when the model can iteratively request additional evidence. We obtain a
near-perfect context precision (99%), which demonstrates that RAG systems can
achieve extremely high retrieval accuracy with the right combination of
hyperparameters, with significant implications for applications where retrieval
quality directly impacts downstream task performance, such as clinical decision
support in healthcare.},
 author = {Adel Ammar and Anis Koubaa and Omer Nacar and Wadii Boulila},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2505.08445v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency},
 url = {http://arxiv.org/abs/2505.08445v1},
 year = {2025}
}

@article{2505.08508v1,
 abstract = {Patient recruitment remains a major bottleneck in clinical trials, calling
for scalable and automated solutions. We present TrialMatchAI, an AI-powered
recommendation system that automates patient-to-trial matching by processing
heterogeneous clinical data, including structured records and unstructured
physician notes. Built on fine-tuned, open-source large language models (LLMs)
within a retrieval-augmented generation framework, TrialMatchAI ensures
transparency and reproducibility and maintains a lightweight deployment
footprint suitable for clinical environments. The system normalizes biomedical
entities, retrieves relevant trials using a hybrid search strategy combining
lexical and semantic similarity, re-ranks results, and performs criterion-level
eligibility assessments using medical Chain-of-Thought reasoning. This pipeline
delivers explainable outputs with traceable decision rationales. In real-world
validation, 92 percent of oncology patients had at least one relevant trial
retrieved within the top 20 recommendations. Evaluation across synthetic and
real clinical datasets confirmed state-of-the-art performance, with expert
assessment validating over 90 percent accuracy in criterion-level eligibility
classification, particularly excelling in biomarker-driven matches. Designed
for modularity and privacy, TrialMatchAI supports Phenopackets-standardized
data, enables secure local deployment, and allows seamless replacement of LLM
components as more advanced models emerge. By enhancing efficiency and
interpretability and offering lightweight, open-source deployment, TrialMatchAI
provides a scalable solution for AI-driven clinical trial matching in precision
medicine.},
 author = {Majd Abdallah and Sigve Nakken and Mariska Bierkens and Johanna Galvis and Alexis Groppi and Slim Karkar and Lana Meiqari and Maria Alexandra Rujano and Steve Canham and Rodrigo Dienstmann and Remond Fijneman and Eivind Hovig and Gerrit Meijer and Macha Nikolski},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2505.08508v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation System to Streamline Patient-to-Trial Matching},
 url = {http://arxiv.org/abs/2505.08508v1},
 year = {2025}
}

@article{2505.08664v1,
 abstract = {We explore the use of inner speech as a mechanism to enhance transparency and
trust in social robots for dietary advice. In humans, inner speech structures
thought processes and decision-making; in robotics, it improves explainability
by making reasoning explicit. This is crucial in healthcare scenarios, where
trust in robotic assistants depends on both accurate recommendations and
human-like dialogue, which make interactions more natural and engaging.
Building on this, we developed a social robot that provides dietary advice, and
we provided the architecture with inner speech capabilities to validate user
input, refine reasoning, and generate clear justifications. The system
integrates large language models for natural language understanding and a
knowledge graph for structured dietary information. By making decisions more
transparent, our approach strengthens trust and improves human-robot
interaction in healthcare. We validated this by measuring the computational
efficiency of our architecture and conducting a small user study, which
assessed the reliability of inner speech in explaining the robot's behavior.},
 author = {Valerio Belcamino and Alessandro Carfì and Valeria Seidita and Fulvio Mastrogiovanni and Antonio Chella},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2505.08664v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Social Robot with Inner Speech for Dietary Guidance},
 url = {http://arxiv.org/abs/2505.08664v1},
 year = {2025}
}

@article{2505.09334v1,
 abstract = {Lung cancer is a leading cause of cancer-related deaths globally, where early
detection and accurate diagnosis are critical for improving survival rates.
While deep learning, particularly convolutional neural networks (CNNs), has
revolutionized medical image analysis by detecting subtle patterns indicative
of early-stage lung cancer, its adoption faces challenges. These models are
often computationally expensive and require significant resources, making them
unsuitable for resource constrained environments. Additionally, their lack of
transparency hinders trust and broader adoption in sensitive fields like
healthcare. Knowledge distillation addresses these challenges by transferring
knowledge from large, complex models (teachers) to smaller, lightweight models
(students). We propose a knowledge distillation-based approach for lung cancer
detection, incorporating explainable AI (XAI) techniques to enhance model
transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3,
and VGG16, are evaluated as teacher models. We developed and trained a
lightweight student model, Distilled Custom Student Network (DCSNet) using
ResNet50 as the teacher. This approach not only ensures high diagnostic
performance in resource-constrained settings but also addresses transparency
concerns, facilitating the adoption of AI-driven diagnostic tools in
healthcare.},
 author = {Sadman Sakib Alif and Nasim Anzum Promise and Fiaz Al Abid and Aniqua Nusrat Zereen},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.09334v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images},
 url = {http://arxiv.org/abs/2505.09334v1},
 year = {2025}
}

@article{2505.10188v1,
 abstract = {As the field of healthcare increasingly adopts artificial intelligence, it
becomes important to understand which types of explanations increase
transparency and empower users to develop confidence and trust in the
predictions made by machine learning (ML) systems. In shared decision-making
scenarios where doctors cooperate with ML systems to reach an appropriate
decision, establishing mutual trust is crucial. In this paper, we explore
different approaches to generating explanations in eXplainable AI (XAI) and
make their underlying arguments explicit so that they can be evaluated by
medical experts. In particular, we present the findings of a user study
conducted with physicians to investigate their perceptions of various types of
AI-generated explanations in the context of diagnostic decision support. The
study aims to identify the most effective and useful explanations that enhance
the diagnostic process. In the study, medical doctors filled out a survey to
assess different types of explanations. Further, an interview was carried out
post-survey to gain qualitative insights on the requirements of explanations
incorporated in diagnostic decision support. Overall, the insights gained from
this study contribute to understanding the types of explanations that are most
effective.},
 author = {Felix Liedeker and Olivia Sanchez-Graillet and Moana Seidler and Christian Brandt and Jörg Wellmer and Philipp Cimiano},
 citations = {},
 comment = {Presented at 'The First Workshop on Natural Language Argument-Based
  Explanations', co-located with ECAI 2024},
 doi = {},
 eprint = {2505.10188v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A User Study Evaluating Argumentative Explanations in Diagnostic Decision Support},
 url = {http://arxiv.org/abs/2505.10188v1},
 year = {2025}
}

@article{2505.11262v1,
 abstract = {Real-world problems are often dependent on multiple data modalities, making
multimodal fusion essential for leveraging diverse information sources. In
high-stakes domains, such as in healthcare, understanding how each modality
contributes to the prediction is critical to ensure trustworthy and
interpretable AI models. We present MultiFIX, an interpretability-driven
multimodal data fusion pipeline that explicitly engineers distinct features
from different modalities and combines them to make the final prediction.
Initially, only deep learning components are used to train a model from data.
The black-box (deep learning) components are subsequently either explained
using post-hoc methods such as Grad-CAM for images or fully replaced by
interpretable blocks, namely symbolic expressions for tabular data, resulting
in an explainable model. We study the use of MultiFIX using several training
strategies for feature extraction and predictive modeling. Besides highlighting
strengths and weaknesses of MultiFIX, experiments on a variety of synthetic
datasets with varying degrees of interaction between modalities demonstrate
that MultiFIX can generate multimodal models that can be used to accurately
explain both the extracted features and their integration without compromising
predictive performance.},
 author = {Mafalda Malafaia and Thalea Schlender and Tanja Alderliesten and Peter A. N. Bosman},
 citations = {},
 comment = {9 pages, 6 figures, submitted to GECCO conference},
 doi = {10.1145/3712255.3734292},
 eprint = {2505.11262v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Step towards Interpretable Multimodal AI Models with MultiFIX},
 url = {http://arxiv.org/abs/2505.11262v1},
 year = {2025}
}

@article{2505.11678v3,
 abstract = {Ensuring fairness in data driven decision making has become a central concern
across domains such as marketing, lending, and healthcare, but fairness
constraints often come at the cost of utility. We propose a statistical
hypothesis testing framework that jointly evaluates approximate fairness and
utility, relaxing strict fairness requirements while ensuring that overall
utility remains above a specified threshold. Our framework builds on the strong
demographic parity (SDP) criterion and incorporates a utility measure motivated
by the potential outcomes framework. The test statistic is constructed via
Wasserstein projections, enabling auditors to assess whether observed
fairness-utility tradeoffs are intrinsic to the algorithm or attributable to
randomness in the data. We show that the test is computationally tractable,
interpretable, broadly applicable across machine learning models, and
extendable to more general settings. We apply our approach to multiple
real-world datasets, offering new insights into the fairness-utility tradeoff
through the perspective of statistical hypothesis testing.},
 author = {Yan Chen and Zheng Tan and Jose Blanchet and Hanzhang Qin},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2505.11678v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Testing Fairness with Utility Tradeoffs: A Wasserstein Projection Approach},
 url = {http://arxiv.org/abs/2505.11678v3},
 year = {2025}
}

@article{2505.12192v1,
 abstract = {Parkinson's disease (PD) poses a growing global health challenge, with
Bangladesh experiencing a notable rise in PD-related mortality. Early detection
of PD remains particularly challenging in resource-constrained settings, where
voice-based analysis has emerged as a promising non-invasive and cost-effective
alternative. However, existing studies predominantly focus on English or other
major languages; notably, no voice dataset for PD exists for Bengali - posing a
significant barrier to culturally inclusive and accessible healthcare
solutions. Moreover, most prior studies employed only a narrow set of acoustic
features, with limited or no hyperparameter tuning and feature selection
strategies, and little attention to model explainability. This restricts the
development of a robust and generalizable machine learning model. To address
this gap, we present BenSparX, the first Bengali conversational speech dataset
for PD detection, along with a robust and explainable machine learning
framework tailored for early diagnosis. The proposed framework incorporates
diverse acoustic feature categories, systematic feature selection methods, and
state-of-the-art machine learning algorithms with extensive hyperparameter
optimization. Furthermore, to enhance interpretability and trust in model
predictions, the framework incorporates SHAP (SHapley Additive exPlanations)
analysis to quantify the contribution of individual acoustic features toward PD
detection. Our framework achieves state-of-the-art performance, yielding an
accuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further
externally validated our approach by applying the framework to existing PD
datasets in other languages, where it consistently outperforms state-of-the-art
approaches. To facilitate further research and reproducibility, the dataset has
been made publicly available at https://github.com/Riad071/BenSParX.},
 author = {Riad Hossain and Muhammad Ashad Kabir and Arat Ibne Golam Mowla and Animesh Chandra Roy and Ranjit Kumar Ghosh},
 citations = {},
 comment = {46 pages, 16 figures},
 doi = {},
 eprint = {2505.12192v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech},
 url = {http://arxiv.org/abs/2505.12192v1},
 year = {2025}
}

@article{2505.12701v1,
 abstract = {Reinforcement Learning (RL) has shown great promise in domains like
healthcare and robotics but often struggles with adoption due to its lack of
interpretability. Counterfactual explanations, which address "what if"
scenarios, provide a promising avenue for understanding RL decisions but remain
underexplored for continuous action spaces. We propose a novel approach for
generating counterfactual explanations in continuous action RL by computing
alternative action sequences that improve outcomes while minimizing deviations
from the original sequence. Our approach leverages a distance metric for
continuous actions and accounts for constraints such as adhering to predefined
policies in specific states. Evaluations in two RL domains, Diabetes Control
and Lunar Lander, demonstrate the effectiveness, efficiency, and generalization
of our approach, enabling more interpretable and trustworthy RL applications.},
 author = {Shuyang Dong and Shangtong Zhang and Lu Feng},
 citations = {},
 comment = {Accepted by International Joint Conference on Artificial Intelligence
  (IJCAI) 2025},
 doi = {10.24963/ijcai.2024/561},
 eprint = {2505.12701v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Counterfactual Explanations for Continuous Action Reinforcement Learning},
 url = {http://arxiv.org/abs/2505.12701v1},
 year = {2025}
}

@article{2505.13028v2,
 abstract = {Large Language Models (LLMs) are increasingly integrated into critical
systems in industries like healthcare and finance. Users can often submit
queries to LLM-enabled chatbots, some of which can enrich responses with
information retrieved from internal databases storing sensitive data. This
gives rise to a range of attacks in which a user submits a malicious query and
the LLM-system outputs a response that creates harm to the owner, such as
leaking internal data or creating legal liability by harming a third-party.
While security tools are being developed to counter these threats, there is
little formal evaluation of their effectiveness and usability. This study
addresses this gap by conducting a thorough comparative analysis of LLM
security tools. We identified 13 solutions (9 closed-source, 4 open-source),
but only 7 were evaluated due to a lack of participation by proprietary model
owners.To evaluate, we built a benchmark dataset of malicious prompts, and
evaluate these tools performance against a baseline LLM model
(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many
false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard
emerged as the best overall tools showcasing the tradeoff between usability and
performance. The study concluded with recommendations for greater transparency
among closed source providers, improved context-aware detections, enhanced
open-source engagement, increased user awareness, and the adoption of more
representative performance metrics.},
 author = {Sayon Palit and Daniel Woods},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.13028v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Evaluating the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset},
 url = {http://arxiv.org/abs/2505.13028v2},
 year = {2025}
}

@article{2505.13243v1,
 abstract = {High-stakes decisions in domains such as healthcare, energy, and public
policy are often made by human experts using domain knowledge and heuristics,
yet are increasingly supported by predictive and optimization-based tools. A
dominant approach in operations research is the predict-then-optimize paradigm,
where a predictive model estimates uncertain inputs, and an optimization model
recommends a decision. However, this approach often lacks interpretability and
can fail under distributional uncertainty -- particularly when the outcome
distribution is multi-modal or complex -- leading to brittle or misleading
decisions. In this paper, we introduce CREDO, a novel framework that
quantifies, for any candidate decision, a distribution-free upper bound on the
probability that the decision is suboptimal. By combining inverse optimization
geometry with conformal prediction and generative modeling, CREDO produces risk
certificates that are both statistically rigorous and practically
interpretable. This framework enables human decision-makers to audit and
validate their own decisions under uncertainty, bridging the gap between
algorithmic tools and real-world judgment.},
 author = {Wenbin Zhou and Agni Orfanoudaki and Shixiang Zhu},
 citations = {},
 comment = {36 pages, 17 figures},
 doi = {},
 eprint = {2505.13243v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Conformalized Decision Risk Assessment},
 url = {http://arxiv.org/abs/2505.13243v1},
 year = {2025}
}

@article{2505.13732v1,
 abstract = {We introduce $\textit{Backward Conformal Prediction}$, a method that
guarantees conformal coverage while providing flexible control over the size of
prediction sets. Unlike standard conformal prediction, which fixes the coverage
level and allows the conformal set size to vary, our approach defines a rule
that constrains how prediction set sizes behave based on the observed data, and
adapts the coverage level accordingly. Our method builds on two key
foundations: (i) recent results by Gauthier et al. [2025] on post-hoc validity
using e-values, which ensure marginal coverage of the form $\mathbb{P}(Y_{\rm
test} \in \hat C_n^{\tilde{\alpha}}(X_{\rm test})) \ge 1 -
\mathbb{E}[\tilde{\alpha}]$ up to a first-order Taylor approximation for any
data-dependent miscoverage $\tilde{\alpha}$, and (ii) a novel leave-one-out
estimator $\hat{\alpha}^{\rm LOO}$ of the marginal miscoverage
$\mathbb{E}[\tilde{\alpha}]$ based on the calibration set, ensuring that the
theoretical guarantees remain computable in practice. This approach is
particularly useful in applications where large prediction sets are impractical
such as medical diagnosis. We provide theoretical results and empirical
evidence supporting the validity of our method, demonstrating that it maintains
computable coverage guarantees while ensuring interpretable, well-controlled
prediction set sizes.},
 author = {Etienne Gauthier and Francis Bach and Michael I. Jordan},
 citations = {},
 comment = {Code available at: https://github.com/GauthierE/backward-cp},
 doi = {},
 eprint = {2505.13732v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Backward Conformal Prediction},
 url = {http://arxiv.org/abs/2505.13732v1},
 year = {2025}
}

@article{2505.14510v3,
 abstract = {As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.},
 author = {Haishi Bai and Jozo Dujmovic and Jianwu Wang},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.14510v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {BACON: A fully explainable AI model with graded logic for decision making problems},
 url = {http://arxiv.org/abs/2505.14510v3},
 year = {2025}
}

@article{2505.14659v1,
 abstract = {As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.},
 author = {Navneet Kaur and Lav Gupta},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.14659v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks},
 url = {http://arxiv.org/abs/2505.14659v1},
 year = {2025}
}

@article{2505.14715v1,
 abstract = {Multi-modal medical image fusion (MMIF) is increasingly recognized as an
essential technique for enhancing diagnostic precision and facilitating
effective clinical decision-making within computer-aided diagnosis systems.
MMIF combines data from X-ray, MRI, CT, PET, SPECT, and ultrasound to create
detailed, clinically useful images of patient anatomy and pathology. These
integrated representations significantly advance diagnostic accuracy, lesion
detection, and segmentation. This comprehensive review meticulously surveys the
evolution, methodologies, algorithms, current advancements, and clinical
applications of MMIF. We present a critical comparative analysis of traditional
fusion approaches, including pixel-, feature-, and decision-level methods, and
delves into recent advancements driven by deep learning, generative models, and
transformer-based architectures. A critical comparative analysis is presented
between these conventional methods and contemporary techniques, highlighting
differences in robustness, computational efficiency, and interpretability. The
article addresses extensive clinical applications across oncology, neurology,
and cardiology, demonstrating MMIF's vital role in precision medicine through
improved patient-specific therapeutic outcomes. Moreover, the review thoroughly
investigates the persistent challenges affecting MMIF's broad adoption,
including issues related to data privacy, heterogeneity, computational
complexity, interpretability of AI-driven algorithms, and integration within
clinical workflows. It also identifies significant future research avenues,
such as the integration of explainable AI, adoption of privacy-preserving
federated learning frameworks, development of real-time fusion systems, and
standardization efforts for regulatory compliance.},
 author = {Muhammad Zubair and Muzammil Hussai and Mousa Ahmad Al-Bashrawi and Malika Bendechache and Muhammad Owais},
 citations = {},
 comment = {computerized medical imaging and graphics Journal submission},
 doi = {10.1016/j.cmpb.2025.109014},
 eprint = {2505.14715v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Comprehensive Review of Techniques, Algorithms, Advancements, Challenges, and Clinical Applications of Multi-modal Medical Image Fusion for Improved Diagnosis},
 url = {http://arxiv.org/abs/2505.14715v1},
 year = {2025}
}

@article{2505.14716v1,
 abstract = {Bone fractures are a leading cause of morbidity and disability worldwide,
imposing significant clinical and economic burdens on healthcare systems.
Traditional X ray interpretation is time consuming and error prone, while
existing machine learning and deep learning solutions often demand extensive
feature engineering, large, annotated datasets, and high computational
resources. To address these challenges, a distributed hybrid quantum classical
pipeline is proposed that first applies Principal Component Analysis (PCA) for
dimensionality reduction and then leverages a 4 qubit quantum amplitude
encoding circuit for feature enrichment. By fusing eight PCA derived features
with eight quantum enhanced features into a 16 dimensional vector and then
classifying with different machine learning models achieving 99% accuracy using
a public multi region X ray dataset on par with state of the art transfer
learning models while reducing feature extraction time by 82%.},
 author = {Sahil Tomar and Rajeshwar Tripathi and Sandeep Kumar},
 citations = {0},
 comment = {8 pages},
 doi = {},
 eprint = {2505.14716v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Hybrid Quantum Classical Pipeline for X Ray Based Fracture Diagnosis},
 url = {http://arxiv.org/abs/2505.14716v1},
 year = {2025}
}

@article{2505.14803v2,
 abstract = {Survival analysis, which estimates the probability of event occurrence over
time from censored data, is fundamental in numerous real-world applications,
particularly in high-stakes domains such as healthcare and risk assessment.
Despite advances in numerous survival models, quantifying the uncertainty of
predictions from these models remains underexplored and challenging. The lack
of reliable uncertainty quantification limits the interpretability and
trustworthiness of survival models, hindering their adoption in clinical
decision-making and other sensitive applications. To bridge this gap, in this
work, we introduce SurvUnc, a novel meta-model based framework for post-hoc
uncertainty quantification for survival models. SurvUnc introduces an
anchor-based learning strategy that integrates concordance knowledge into
meta-model optimization, leveraging pairwise ranking performance to estimate
uncertainty effectively. Notably, our framework is model-agnostic, ensuring
compatibility with any survival model without requiring modifications to its
architecture or access to its internal parameters. Especially, we design a
comprehensive evaluation pipeline tailored to this critical yet overlooked
problem. Through extensive experiments on four publicly available benchmarking
datasets and five representative survival models, we demonstrate the
superiority of SurvUnc across multiple evaluation scenarios, including
selective prediction, misprediction detection, and out-of-domain detection. Our
results highlight the effectiveness of SurvUnc in enhancing model
interpretability and reliability, paving the way for more trustworthy survival
predictions in real-world applications.},
 author = {Yu Liu and Weiyao Tao and Tong Xia and Simon Knight and Tingting Zhu},
 citations = {},
 comment = {KDD 2025},
 doi = {10.1145/3711896.3737140},
 eprint = {2505.14803v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis},
 url = {http://arxiv.org/abs/2505.14803v2},
 year = {2025}
}

@article{2505.14850v1,
 abstract = {Acute pancreatitis (AP) is a common and potentially life-threatening
gastrointestinal disease that imposes a significant burden on healthcare
systems. ICU readmissions among AP patients are common, especially in severe
cases, with rates exceeding 40%. Identifying high-risk patients for readmission
is crucial for improving outcomes. This study used the MIMIC-III database to
identify ICU admissions for AP based on diagnostic codes.
  We applied a preprocessing pipeline including missing data imputation,
correlation analysis, and hybrid feature selection. Recursive Feature
Elimination with Cross-Validation (RFECV) and LASSO regression, supported by
expert review, reduced over 50 variables to 20 key predictors, covering
demographics, comorbidities, lab tests, and interventions. To address class
imbalance, we used the Synthetic Minority Over-sampling Technique (SMOTE) in a
five-fold cross-validation framework.
  We developed and optimized six machine learning models-Logistic Regression,
k-Nearest Neighbors, Naive Bayes, Random Forest, LightGBM, and XGBoost-using
grid search. Model performance was evaluated with AUROC, accuracy, F1 score,
sensitivity, specificity, PPV, and NPV. XGBoost performed best, with an AUROC
of 0.862 (95% CI: 0.800-0.920) and accuracy of 0.889 (95% CI: 0.858-0.923) on
the test set.
  An ablation study showed that removing any feature decreased performance.
SHAP analysis identified platelet count, age, and SpO2 as key predictors of
readmission. This study shows that ensemble learning, informed feature
selection, and handling class imbalance can improve ICU readmission prediction
in AP patients, supporting targeted post-discharge interventions.},
 author = {Shuheng Chen and Yong Si and Junyi Fan and Li Sun and Elham Pishgar and Kamiar Alaei and Greg Placencia and Maryam Pishgar},
 citations = {5},
 comment = {},
 doi = {10.1101/2025.05.11.25327405},
 eprint = {2505.14850v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Predicting ICU Readmission in Acute Pancreatitis Patients Using a Machine Learning-Based Model with Enhanced Clinical Interpretability},
 url = {http://arxiv.org/abs/2505.14850v1},
 year = {2025}
}

@article{2505.15083v1,
 abstract = {Time series forecasting plays a crucial role in various applications,
particularly in healthcare, where accurate predictions of future health
trajectories can significantly impact clinical decision-making. Ensuring
transparency and explainability of the models responsible for these tasks is
essential for their adoption in critical settings. Recent work has explored a
top-down approach to bi-level transparency, focusing on understanding trends
and properties of predicted time series using static features. In this work, we
extend this framework by incorporating exogenous time series features alongside
static features in a structured manner, while maintaining cohesive
interpretation. Our approach leverages the insights of trajectory comprehension
to introduce an encoding mechanism for exogenous time series, where they are
decomposed into meaningful trends and properties, enabling the extraction of
interpretable patterns. Through experiments on several synthetic datasets, we
demonstrate that our approach remains predictive while preserving
interpretability and robustness. This work represents a step towards developing
robust, and generalized time series forecasting models. The code is available
at https://github.com/jeremy-qin/TIMEVIEW},
 author = {Jeremy Qin},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.15083v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features},
 url = {http://arxiv.org/abs/2505.15083v1},
 year = {2025}
}

@article{2505.15354v1,
 abstract = {Time series forecasting models often produce systematic, predictable errors
even in critical domains such as energy, finance, and healthcare. We introduce
a novel post training adaptive optimization framework that improves forecast
accuracy without retraining or architectural changes. Our method automatically
applies expressive transformations optimized via reinforcement learning,
contextual bandits, or genetic algorithms to correct model outputs in a
lightweight and model agnostic way. Theoretically, we prove that affine
corrections always reduce the mean squared error; practically, we extend this
idea with dynamic action based optimization. The framework also supports an
optional human in the loop component: domain experts can guide corrections
using natural language, which is parsed into actions by a language model.
Across multiple benchmarks (e.g., electricity, weather, traffic), we observe
consistent accuracy gains with minimal computational overhead. Our interactive
demo shows the framework's real time usability. By combining automated post hoc
refinement with interpretable and extensible mechanisms, our approach offers a
powerful new direction for practical forecasting systems.},
 author = {Malik Tiomoko and Hamza Cherkaoui and Giuseppe Paolo and Zhang Yili and Yu Meng and Zhang Keli and Hafiz Tiomoko Ali},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.15354v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Human in the Loop Adaptive Optimization for Improved Time Series Forecasting},
 url = {http://arxiv.org/abs/2505.15354v1},
 year = {2025}
}

@article{2505.16288v2,
 abstract = {Deep learning models trained on extensive Electronic Health Records (EHR)
data have achieved high accuracy in diagnosis prediction, offering the
potential to assist clinicians in decision-making and treatment planning.
However, these models lack two crucial features that clinicians highly value:
interpretability and interactivity. The ``black-box'' nature of these models
makes it difficult for clinicians to understand the reasoning behind
predictions, limiting their ability to make informed decisions. Additionally,
the absence of interactive mechanisms prevents clinicians from incorporating
their own knowledge and experience into the decision-making process. To address
these limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal
discovery framework that integrates personalized knowledge databases and
agentic LLMs. II-KEA enhances interpretability through explicit reasoning and
causal analysis, while also improving interactivity by allowing clinicians to
inject their knowledge and experience through customized knowledge bases and
prompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating
superior performance along with enhanced interpretability and interactivity, as
evidenced by its strong results from extensive case studies.},
 author = {Xiaoxue Han and Pengfei Hu and Jun-En Ding and Chang Lu and Feng Liu and Yue Ning},
 citations = {},
 comment = {EMNLP 2025},
 doi = {},
 eprint = {2505.16288v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery},
 url = {http://arxiv.org/abs/2505.16288v2},
 year = {2025}
}

@article{2505.17105v2,
 abstract = {Artificial Intelligence (AI) plays an essential role in healthcare and is
pervasively incorporated into medical software and equipment. In the European
Union, healthcare is a high-risk application domain for AI, and providers must
prepare Instructions for Use (IFU) according to the European regulation
2024/1689 (AI Act). To this regulation, the principle of transparency is
cardinal and requires the IFU to be clear and relevant to the users. This study
tests whether these latter requirements are satisfied by the IFU structure. A
survey was administered online via the Qualtrics platform to four types of
direct stakeholders, i.e., managers (N = 238), healthcare professionals (N =
115), patients (N = 229), and Information Technology experts (N = 230). The
participants rated the relevance of a set of transparency needs and indicated
the IFU section addressing them. The results reveal differentiated priorities
across stakeholders and a troubled mapping of transparency needs onto the IFU
structure. Recommendations to build a locally meaningful IFU are derived.},
 author = {Anna Spagnolli and Cecilia Tolomini and Elisa Beretta and Claudio Sarra},
 citations = {0},
 comment = {22 pages, pre-review version},
 doi = {},
 eprint = {2505.17105v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Transparency in Healthcare AI: Testing European Regulatory Provisions against Users' Transparency Needs},
 url = {http://arxiv.org/abs/2505.17105v2},
 year = {2025}
}

@article{2505.17344v1,
 abstract = {Unattended scheduled appointments, defined as patient no-shows, adversely
affect both healthcare providers and patients' health, disrupting the
continuity of care, operational efficiency, and the efficient allocation of
medical resources. Accurate predictive modelling is needed to reduce the impact
of no-shows. Although machine learning methods, such as logistic regression,
random forest models, and decision trees, are widely used in predicting patient
no-shows, they often rely on hard decision splits and static feature
importance, limiting their adaptability to specific or complex patient
behaviors. To address this limitation, we propose a new hybrid Multi-Head
Attention Soft Random Forest (MHASRF) model that integrates attention
mechanisms into a random forest model using probabilistic soft splitting
instead of hard splitting. The MHASRF model assigns attention weights
differently across the trees, enabling attention on specific patient behaviors.
The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a
93.59% F1 score, surpassing the performance of decision tree, logistic
regression, random forest, and naive Bayes models. Furthermore, MHASRF was able
to identify key predictors of patient no-shows using two levels of feature
importance (tree level and attention mechanism level), offering deeper insights
into patient no-show predictors. The proposed model is a robust, adaptable, and
interpretable method for predicting patient no-shows that will help healthcare
providers in optimizing resources.},
 author = {Ninda Nurseha Amalina and Kwadwo Boateng Ofori-Amanfo and Heungjo An},
 citations = {0},
 comment = {14 pages, 6 figures},
 doi = {},
 eprint = {2505.17344v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction},
 url = {http://arxiv.org/abs/2505.17344v1},
 year = {2025}
}

@article{2505.17779v2,
 abstract = {Ultrasound is a widely-used imaging modality critical to global healthcare,
yet its interpretation remains challenging due to its varying image quality on
operators, noises, and anatomical structures. Although large vision-language
models (LVLMs) have demonstrated impressive multimodal capabilities across
natural and medical domains, their performance on ultrasound remains largely
unexplored. We introduce U2-BENCH, the first comprehensive benchmark to
evaluate LVLMs on ultrasound understanding across classification, detection,
regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning
15 anatomical regions and defines 8 clinically inspired tasks, such as
diagnosis, view recognition, lesion localization, clinical value estimation,
and report generation, across 50 ultrasound application scenarios. We evaluate
20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and
medical-specific. Our results reveal strong performance on image-level
classification, but persistent challenges in spatial reasoning and clinical
language generation. U2-BENCH establishes a rigorous and unified testbed to
assess and accelerate LVLM research in the uniquely multimodal domain of
medical ultrasound imaging.},
 author = {Anjie Le and Henan Liu and Yue Wang and Zhenyu Liu and Rongkun Zhu and Taohan Weng and Jinze Yu and Boyang Wang and Yalun Wu and Kaiwen Yan and Quanlin Sun and Meirui Jiang and Jialun Pei and Siya Liu and Haoyun Zheng and Zhoujun Li and Alison Noble and Jacques Souquet and Xiaoqing Guo and Manxi Lin and Hongcheng Guo},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.17779v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding},
 url = {http://arxiv.org/abs/2505.17779v2},
 year = {2025}
}

@article{2505.18169v1,
 abstract = {Understanding and predicting human emotional and physiological states using
wearable sensors has important applications in stress monitoring, mental health
assessment, and affective computing. This study presents a novel Multi-Task
Physics-Informed Neural Network (PINN) that performs Electrodermal Activity
(EDA) prediction and emotion classification simultaneously, using the publicly
available WESAD dataset. The model integrates psychological self-report
features (PANAS and SAM) with a physics-inspired differential equation
representing EDA dynamics, enforcing biophysically grounded constraints through
a custom loss function. This loss combines EDA regression, emotion
classification, and a physics residual term for improved interpretability.
  The architecture supports dual outputs for both tasks and is trained under a
unified multi-task framework. Evaluated using 5-fold cross-validation, the
model achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919,
and F1-score of 94.08 percent. These results outperform classical models such
as SVR and XGBoost, as well as ablated variants like emotion-only and EDA-only
models.
  In addition, the learned physical parameters including decay rate (alpha_0),
emotional sensitivity (beta), and time scaling (gamma) are interpretable and
stable across folds, aligning with known principles of human physiology. This
work is the first to introduce a multi-task PINN framework for wearable emotion
recognition, offering improved performance, generalizability, and model
transparency. The proposed system provides a foundation for future
interpretable and multimodal applications in healthcare and human-computer
interaction.},
 author = {Nischal Mandal},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2505.18169v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction},
 url = {http://arxiv.org/abs/2505.18169v1},
 year = {2025}
}

@article{2505.18893v4,
 abstract = {Conventional AI evaluation approaches concentrated within the AI stack
exhibit systemic limitations for exploring, navigating and resolving the human
and societal factors that play out in real world deployment such as in
education, finance, healthcare, and employment sectors. AI capability
evaluations can capture detail about first-order effects, such as whether
immediate system outputs are accurate, or contain toxic, biased or
stereotypical content, but AI's second-order effects, i.e. any long-term
outcomes and consequences that may result from AI use in the real world, have
become a significant area of interest as the technology becomes embedded in our
daily lives. These secondary effects can include shifts in user behavior,
societal, cultural and economic ramifications, workforce transformations, and
long-term downstream impacts that may result from a broad and growing set of
risks. This position paper argues that measuring the indirect and secondary
effects of AI will require expansion beyond static, single-turn approaches
conducted in silico to include testing paradigms that can capture what actually
materializes when people use AI technology in context. Specifically, we
describe the need for data and methods that can facilitate contextual awareness
and enable downstream interpretation and decision making about AI's secondary
effects, and recommend requirements for a new ecosystem.},
 author = {Reva Schwartz and Rumman Chowdhury and Akash Kundu and Heather Frase and Marzieh Fadaee and Tom David and Gabriella Waters and Afaf Taik and Morgan Briggs and Patrick Hall and Shomik Jain and Kyra Yee and Spencer Thomas and Sundeep Bhandari and Paul Duncan and Andrew Thompson and Maya Carlyle and Qinghua Lu and Matthew Holmes and Theodora Skeadas},
 citations = {4},
 comment = {9 pages},
 doi = {},
 eprint = {2505.18893v4},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects},
 url = {http://arxiv.org/abs/2505.18893v4},
 year = {2025}
}

@article{2505.19369v1,
 abstract = {Human Activity Recognition (HAR) using wearable sensor data has become a
central task in mobile computing, healthcare, and human-computer interaction.
Despite the success of traditional deep learning models such as CNNs and RNNs,
they often struggle to capture long-range temporal dependencies and contextual
relevance across multiple sensor channels. To address these limitations, we
propose SETransformer, a hybrid deep neural architecture that combines
Transformer-based temporal modeling with channel-wise squeeze-and-excitation
(SE) attention and a learnable temporal attention pooling mechanism. The model
takes raw triaxial accelerometer data as input and leverages global
self-attention to capture activity-specific motion dynamics over extended time
windows, while adaptively emphasizing informative sensor channels and critical
time steps.
  We evaluate SETransformer on the WISDM dataset and demonstrate that it
significantly outperforms conventional models including LSTM, GRU, BiLSTM, and
CNN baselines. The proposed model achieves a validation accuracy of 84.68\% and
a macro F1-score of 84.64\%, surpassing all baseline architectures by a notable
margin. Our results show that SETransformer is a competitive and interpretable
solution for real-world HAR tasks, with strong potential for deployment in
mobile and ubiquitous sensing applications.},
 author = {Yunbo Liu and Xukui Qin and Yifan Gao and Xiang Li and Chengwei Feng},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.19369v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition},
 url = {http://arxiv.org/abs/2505.19369v1},
 year = {2025}
}

@article{2505.19419v2,
 abstract = {The quality of training data is critical to the performance of machine
learning applications in domains like transportation, healthcare, and robotics.
Accurate image labeling, however, often relies on time-consuming, expert-driven
methods with limited feedback. This research introduces a sketch-based
annotation approach supported by large language models (LLMs) to reduce
technical barriers and enhance accessibility. Using a synthetic dataset, we
examine how sketch recognition features relate to LLM feedback metrics, aiming
to improve the reliability and interpretability of LLM-assisted labeling. We
also explore how prompting strategies and sketch variations influence feedback
quality. Our main contribution is a sketch-based virtual assistant that
simplifies annotation for non-experts and advances LLM-driven labeling tools in
terms of scalability, accessibility, and explainability.},
 author = {Baichuan Li and Larry Powell and Tracy Hammond},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.19419v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {It's Not Just Labeling -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features},
 url = {http://arxiv.org/abs/2505.19419v2},
 year = {2025}
}

@article{2505.19445v1,
 abstract = {The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains
like healthcare and finance demands reliable explanations of their
decision-making processes. While inherently interpretable GNN architectures
like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to
generating explanations based on spurious correlations, potentially undermining
trust in critical applications. We present MetaGMT, a meta-learning framework
that enhances explanation fidelity through a novel bi-level optimization
approach. We demonstrate that MetaGMT significantly improves both explanation
quality (AUC-ROC, Precision@K) and robustness to spurious patterns, across
BA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive
classification accuracy while producing more faithful explanations (with an
increase up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline
methods. These advancements in interpretability could enable safer deployment
of GNNs in sensitive domains by (1) facilitating model debugging through more
reliable explanations, (2) supporting targeted retraining when biases are
identified, and (3) enabling meaningful human oversight. By addressing the
critical challenge of explanation reliability, our work contributes to building
more trustworthy and actionable GNN systems for real-world applications.},
 author = {Rishabh Bhattacharya and Hari Shankar and Vaishnavi Shivkumar and Ponnurangam Kumaraguru},
 citations = {0},
 comment = {8 Pages Main Content, 10 Pages including Appendix. 1 Figure, 7 Tables},
 doi = {},
 eprint = {2505.19445v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration},
 url = {http://arxiv.org/abs/2505.19445v1},
 year = {2025}
}

@article{2505.19802v2,
 abstract = {Understanding pain-related facial behaviors is essential for digital
healthcare in terms of effective monitoring, assisted diagnostics, and
treatment planning, particularly for patients unable to communicate verbally.
Existing data-driven methods of detecting pain from facial expressions are
limited due to interpretability and severity quantification. To this end, we
propose GraphAU-Pain, leveraging a graph-based framework to model facial Action
Units (AUs) and their interrelationships for pain intensity estimation. AUs are
represented as graph nodes, with co-occurrence relationships as edges, enabling
a more expressive depiction of pain-related facial behaviors. By utilizing a
relational graph neural network, our framework offers improved interpretability
and significant performance gains. Experiments conducted on the publicly
available UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,
achieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity
estimation.},
 author = {Zhiyu Wang and Yang Liu and Hatice Gunes},
 citations = {},
 comment = {MiGA@IJCAI25},
 doi = {},
 eprint = {2505.19802v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation},
 url = {http://arxiv.org/abs/2505.19802v2},
 year = {2025}
}

@article{2505.20048v1,
 abstract = {Time series forecasting plays a critical role in domains such as energy,
finance, and healthcare, where accurate predictions inform decision-making
under uncertainty. Although Transformer-based models have demonstrated success
in sequential modeling, their adoption for time series remains limited by
challenges such as noise sensitivity, long-range dependencies, and a lack of
inductive bias for temporal structure. In this work, we present a unified and
principled framework for benchmarking three prominent Transformer forecasting
architectures-Autoformer, Informer, and Patchtst-each evaluated through three
architectural variants: Minimal, Standard, and Full, representing increasing
levels of complexity and modeling capacity.
  We conduct over 1500 controlled experiments on a suite of ten synthetic
signals, spanning five patch lengths and five forecast horizons under both
clean and noisy conditions. Our analysis reveals consistent patterns across
model families.
  To advance this landscape further, we introduce the Koopman-enhanced
Transformer framework, Deep Koopformer, which integrates operator-theoretic
latent state modeling to improve stability and interpretability. We demonstrate
its efficacy on nonlinear and chaotic dynamical systems. Our results highlight
Koopman based Transformer as a promising hybrid approach for robust,
interpretable, and theoretically grounded time series forecasting in noisy and
complex real-world conditions.},
 author = {Ali Forootani and Mohammad Khosravi},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.20048v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks},
 url = {http://arxiv.org/abs/2505.20048v1},
 year = {2025}
}

@article{2505.21360v5,
 abstract = {Competing risks are crucial considerations in survival modelling,
particularly in healthcare domains where patients may experience multiple
distinct event types. We propose CRISP-NAM (Competing Risks Interpretable
Survival Prediction with Neural Additive Models), an interpretable neural
additive model for competing risks survival analysis which extends the neural
additive architecture to model cause-specific hazards while preserving
feature-level interpretability. Each feature contributes independently to risk
estimation through dedicated neural networks, allowing for visualization of
complex non-linear relationships between covariates and each competing risk. We
demonstrate competitive performance on multiple datasets compared to existing
approaches.},
 author = {Dhanesh Ramachandram and Ananya Raval},
 citations = {0},
 comment = {Added missing subsections and minor bug fixes},
 doi = {},
 eprint = {2505.21360v5},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models},
 url = {http://arxiv.org/abs/2505.21360v5},
 year = {2025}
}

@article{2505.21570v1,
 abstract = {Artificial Knowledge (AK) systems are transforming decision-making across
critical domains such as healthcare, finance, and criminal justice. However,
their growing opacity presents governance challenges that current regulatory
approaches, focused predominantly on explainability, fail to address
adequately. This article argues for a shift toward validation as a central
regulatory pillar. Validation, ensuring the reliability, consistency, and
robustness of AI outputs, offers a more practical, scalable, and risk-sensitive
alternative to explainability, particularly in high-stakes contexts where
interpretability may be technically or economically unfeasible. We introduce a
typology based on two axes, validity and explainability, classifying AK systems
into four categories and exposing the trade-offs between interpretability and
output reliability. Drawing on comparative analysis of regulatory approaches in
the EU, US, UK, and China, we show how validation can enhance societal trust,
fairness, and safety even where explainability is limited. We propose a
forward-looking policy framework centered on pre- and post-deployment
validation, third-party auditing, harmonized standards, and liability
incentives. This framework balances innovation with accountability and provides
a governance roadmap for responsibly integrating opaque, high-performing AK
systems into society.},
 author = {Dalit Ken-Dror Feldman and Daniel Benoliel},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.21570v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Beyond Explainability: The Case for AI Validation},
 url = {http://arxiv.org/abs/2505.21570v1},
 year = {2025}
}

@article{2505.21824v2,
 abstract = {The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.},
 author = {Praveen Kumar and Vincent T. Metzger and Scott A. Malec},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.21824v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations},
 url = {http://arxiv.org/abs/2505.21824v2},
 year = {2025}
}

@article{2505.22306v2,
 abstract = {Cardiovascular signals such as photoplethysmography (PPG),
electrocardiography (ECG), and blood pressure (BP) are inherently correlated
and complementary, together reflecting the health of cardiovascular system.
However, their joint utilization in real-time monitoring is severely limited by
diverse acquisition challenges from noisy wearable recordings to burdened
invasive procedures. Here we propose UniCardio, a multi-modal diffusion
transformer that reconstructs low-quality signals and synthesizes unrecorded
signals in a unified generative framework. Its key innovations include a
specialized model architecture to manage the signal modalities involved in
generation tasks and a continual learning paradigm to incorporate varying
modality combinations. By exploiting the complementary nature of cardiovascular
signals, UniCardio clearly outperforms recent task-specific baselines in signal
denoising, imputation, and translation. The generated signals match the
performance of ground-truth signals in detecting abnormal health conditions and
estimating vital signs, even in unseen domains, while ensuring interpretability
for human experts. These advantages position UniCardio as a promising avenue
for advancing AI-assisted healthcare.},
 author = {Zehua Chen and Yuyang Miao and Liyuan Wang and Luyun Fan and Danilo P. Mandic and Jun Zhu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2505.22306v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer},
 url = {http://arxiv.org/abs/2505.22306v2},
 year = {2025}
}

@article{2505.22541v1,
 abstract = {Deep neural networks form the backbone of artificial intelligence research,
with potential to transform the human experience in areas ranging from
autonomous driving to personal assistants, healthcare to education. However,
their integration into the daily routines of real-world classrooms remains
limited. It is not yet common for a teacher to assign students individualized
homework targeting their specific weaknesses, provide students with instant
feedback, or simulate student responses to a new exam question. While these
models excel in predictive performance, this lack of adoption can be attributed
to a significant weakness: the lack of explainability of model decisions,
leading to a lack of trust from students, parents, and teachers. This thesis
aims to bring human needs to the forefront of eXplainable AI (XAI) research,
grounded in the concrete use case of personalized learning and teaching. We
frame the contributions along two verticals: technical advances in XAI and
their aligned human studies. We investigate explainability in AI for education,
revealing systematic disagreements between post-hoc explainers and identifying
a need for inherently interpretable model architectures. We propose four novel
technical contributions in interpretability with a multimodal modular
architecture (MultiModN), an interpretable mixture-of-experts model
(InterpretCC), adversarial training for explainer stability, and a
theory-driven LLM-XAI framework to present explanations to students
(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,
learning scientists, and university students. By combining empirical
evaluations of existing explainers with novel architectural designs and human
studies, our work lays a foundation for human-centric AI systems that balance
state-of-the-art performance with built-in transparency and trust.},
 author = {Vinitra Swamy},
 citations = {0},
 comment = {PhD Thesis, EPFL (Computer Science)},
 doi = {10.1109/icaccs60874.2024.10716907},
 eprint = {2505.22541v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Human-Centric Approach to Explainable AI for Personalized Education},
 url = {http://arxiv.org/abs/2505.22541v1},
 year = {2025}
}

@article{2505.24786v1,
 abstract = {Dynamic hand gestures play a pivotal role in assistive human-robot
interaction (HRI), facilitating intuitive, non-verbal communication,
particularly for individuals with mobility constraints or those operating
robots remotely. Current gesture recognition methods are mostly limited to
short-range interactions, reducing their utility in scenarios demanding robust
assistive communication from afar. In this paper, we introduce a novel approach
designed specifically for assistive robotics, enabling dynamic gesture
recognition at extended distances of up to 30 meters, thereby significantly
improving accessibility and quality of life. Our proposed Distance-aware
Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable
Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust
processing and classification of gesture sequences captured under challenging
conditions, including significant physical attenuation, reduced resolution, and
dynamic gesture variations commonly experienced in real-world assistive
environments. We further introduce the Radiometric Spatio-Temporal Depth
Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model
robustness across varying distances. Our model demonstrates significant
performance improvement over state-of-the-art gesture recognition frameworks,
achieving a recognition accuracy of 97.3% on a diverse dataset with challenging
hyper-range gestures. By effectively interpreting gestures from considerable
distances, DiG-Net significantly enhances the usability of assistive robots in
home healthcare, industrial safety, and remote assistance scenarios, enabling
seamless and intuitive interactions for users regardless of physical
limitations},
 author = {Eran Bamani Beeri and Eden Nissinman and Avishai Sintov},
 citations = {},
 comment = {arXiv admin note: substantial text overlap with arXiv:2411.18413},
 doi = {},
 eprint = {2505.24786v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics},
 url = {http://arxiv.org/abs/2505.24786v1},
 year = {2025}
}

@article{2505.24830v2,
 abstract = {Large language models (LLMs) exhibit extensive medical knowledge but are
prone to hallucinations and inaccurate citations, which pose a challenge to
their clinical adoption and regulatory compliance. Current methods, such as
Retrieval Augmented Generation, partially address these issues by grounding
answers in source documents, but hallucinations and low fact-level
explainability persist. In this work, we introduce a novel atomic fact-checking
framework designed to enhance the reliability and explainability of LLMs used
in medical long-form question answering. This method decomposes LLM-generated
responses into discrete, verifiable units called atomic facts, each of which is
independently verified against an authoritative knowledge base of medical
guidelines. This approach enables targeted correction of errors and direct
tracing to source literature, thereby improving the factual accuracy and
explainability of medical Q&A. Extensive evaluation using multi-reader
assessments by medical experts and an automated open Q&A benchmark demonstrated
significant improvements in factual accuracy and explainability. Our framework
achieved up to a 40% overall answer improvement and a 50% hallucination
detection rate. The ability to trace each atomic fact back to the most relevant
chunks from the database provides a granular, transparent explanation of the
generated responses, addressing a major gap in current medical AI applications.
This work represents a crucial step towards more trustworthy and reliable
clinical applications of LLMs, addressing key prerequisites for clinical
application and fostering greater confidence in AI-assisted healthcare.},
 author = {Juraj Vladika and Annika Domres and Mai Nguyen and Rebecca Moser and Jana Nano and Felix Busch and Lisa C. Adams and Keno K. Bressem and Denise Bernhardt and Stephanie E. Combs and Kai J. Borm and Florian Matthes and Jan C. Peeken},
 citations = {},
 comment = {15 pages, 5 figures and tables},
 doi = {},
 eprint = {2505.24830v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs},
 url = {http://arxiv.org/abs/2505.24830v2},
 year = {2025}
}

@article{2506.00200v2,
 abstract = {Radiology reports are critical for clinical decision-making but often lack a
standardized format, limiting both human interpretability and machine learning
(ML) applications. While large language models (LLMs) have shown strong
capabilities in reformatting clinical text, their high computational
requirements, lack of transparency, and data privacy concerns hinder practical
deployment. To address these challenges, we explore lightweight encoder-decoder
models (<300M parameters)-specifically T5 and BERT2BERT-for structuring
radiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark
these models against eight open-source LLMs (1B-70B), adapted using prefix
prompting, in-context learning (ICL), and low-rank adaptation (LoRA)
finetuning. Our best-performing lightweight model outperforms all LLMs adapted
using prompt-based techniques on a human-annotated test set. While some
LoRA-finetuned LLMs achieve modest gains over the lightweight model on the
Findings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,
GREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of
substantially greater computational resources. For example, LLaMA-3-70B
incurred more than 400 times the inference time, cost, and carbon emissions
compared to the lightweight model. These results underscore the potential of
lightweight, task-specific models as sustainable and privacy-preserving
solutions for structuring clinical text in resource-constrained healthcare
settings.},
 author = {Johannes Moll and Louisa Fay and Asfandyar Azhar and Sophie Ostmeier and Tim Lueth and Sergios Gatidis and Curtis Langlotz and Jean-Benoit Delbrouck},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2506.00200v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Structuring Radiology Reports: Challenging LLMs with Lightweight Models},
 url = {http://arxiv.org/abs/2506.00200v2},
 year = {2025}
}

@article{2506.00233v1,
 abstract = {Artificial Intelligence (AI) is transforming sectors such as healthcare,
finance, and autonomous systems, offering powerful tools for innovation. Yet
its rapid integration raises urgent ethical concerns related to data ownership,
privacy, and systemic bias. Issues like opaque decision-making, misleading
outputs, and unfair treatment in high-stakes domains underscore the need for
transparent and accountable AI systems. This article addresses these challenges
by proposing a modular ethical assessment framework built on ontological blocks
of meaning-discrete, interpretable units that encode ethical principles such as
fairness, accountability, and ownership. By integrating these blocks with FAIR
(Findable, Accessible, Interoperable, Reusable) principles, the framework
supports scalable, transparent, and legally aligned ethical evaluations,
including compliance with the EU AI Act. Using a real-world use case in
AI-powered investor profiling, the paper demonstrates how the framework enables
dynamic, behavior-informed risk classification. The findings suggest that
ontological blocks offer a promising path toward explainable and auditable AI
ethics, though challenges remain in automation and probabilistic reasoning.},
 author = {Aasish Kumar Sharma and Dimitar Kyosev and Julian Kunkel},
 citations = {0},
 comment = {6 pages, 3 figures, accepted at 8th IEEE International Workshop on
  Advances in Artificial Intelligence and Machine Learning (AIML 2025):
  Futuristic AI and ML models & Intelligent Systems},
 doi = {10.1109/compsac65507.2025.00344},
 eprint = {2506.00233v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Ethical AI: Towards Defining a Collective Evaluation Framework},
 url = {http://arxiv.org/abs/2506.00233v1},
 year = {2025}
}

@article{2506.00235v1,
 abstract = {Healthcare decision-making represents one of the most challenging domains for
Artificial Intelligence (AI), requiring the integration of diverse knowledge
sources, complex reasoning, and various external analytical tools. Current AI
systems often rely on either task-specific models, which offer limited
adaptability, or general language models without grounding with specialized
external knowledge and tools. We introduce MedOrch, a novel framework that
orchestrates multiple specialized tools and reasoning agents to provide
comprehensive medical decision support. MedOrch employs a modular, agent-based
architecture that facilitates the flexible integration of domain-specific tools
without altering the core system. Furthermore, it ensures transparent and
traceable reasoning processes, enabling clinicians to meticulously verify each
intermediate step underlying the system's recommendations. We evaluate MedOrch
across three distinct medical applications: Alzheimer's disease diagnosis,
chest X-ray interpretation, and medical visual question answering, using
authentic clinical datasets. The results demonstrate MedOrch's competitive
performance across these diverse medical tasks. Notably, in Alzheimer's disease
diagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the
state-of-the-art baseline by over four percentage points. For predicting
Alzheimer's disease progression, it attains a 50.35% accuracy, marking a
significant improvement. In chest X-ray analysis, MedOrch exhibits superior
performance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover,
in complex multimodal visual question answering (Image+Table), MedOrch achieves
an accuracy of 54.47%. These findings underscore MedOrch's potential to advance
healthcare AI by enabling reasoning-driven tool utilization for multimodal
medical data processing and supporting intricate cognitive tasks in clinical
decision-making.},
 author = {Yexiao He and Ang Li and Boyi Liu and Zhewei Yao and Yuxiong He},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.00235v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility},
 url = {http://arxiv.org/abs/2506.00235v1},
 year = {2025}
}

@article{2506.00587v1,
 abstract = {Stress significantly contributes to both mental and physical disorders, yet
traditional self-reported questionnaires are inherently subjective. In this
study, we introduce a novel framework that employs geometric machine learning
to detect stress from raw EEG recordings. Our approach constructs graphs by
integrating structural connectivity (derived from electrode spatial
arrangement) with functional connectivity from pairwise signal correlations. A
spatio-temporal graph convolutional network (ST-GCN) processes these graphs to
capture spatial and temporal dynamics. Experiments on the SAM-40 dataset show
that the ST-GCN outperforms standard machine learning models on all key
classification metrics and enhances interpretability, explored through ablation
analyses of key channels and brain regions. These results pave the way for more
objective and accurate stress detection methods.},
 author = {Sonia Koszut and Sam Nallaperuma-Herzberg and Pietro Lio},
 citations = {},
 comment = {12 pages, 5 figures. This version has been accepted as a full paper
  at the 2025 AI in Healthcare (AIiH) Conference},
 doi = {10.1007/978-3-032-00656-1_1},
 eprint = {2506.00587v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Decoding the Stressed Brain with Geometric Machine Learning},
 url = {http://arxiv.org/abs/2506.00587v1},
 year = {2025}
}

@article{2506.01257v1,
 abstract = {DeepSeek-R1 is a cutting-edge open-source large language model (LLM)
developed by DeepSeek, showcasing advanced reasoning capabilities through a
hybrid architecture that integrates mixture of experts (MoE), chain of thought
(CoT) reasoning, and reinforcement learning. Released under the permissive MIT
license, DeepSeek-R1 offers a transparent and cost-effective alternative to
proprietary models like GPT-4o and Claude-3 Opus; it excels in structured
problem-solving domains such as mathematics, healthcare diagnostics, code
generation, and pharmaceutical research. The model demonstrates competitive
performance on benchmarks like the United States Medical Licensing Examination
(USMLE) and American Invitational Mathematics Examination (AIME), with strong
results in pediatric and ophthalmologic clinical decision support tasks. Its
architecture enables efficient inference while preserving reasoning depth,
making it suitable for deployment in resource-constrained settings. However,
DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,
adversarial manipulation, and safety failures - especially in multilingual and
ethically sensitive contexts. This survey highlights the model's strengths,
including interpretability, scalability, and adaptability, alongside its
limitations in general language fluency and safety alignment. Future research
priorities include improving bias mitigation, natural language comprehension,
domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1
represents a major advance in open, scalable AI, underscoring the need for
collaborative governance to ensure responsible and equitable deployment.},
 author = {Jiancheng Ye and Sophie Bronstein and Jiarui Hai and Malak Abu Hashish},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.01257v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models},
 url = {http://arxiv.org/abs/2506.01257v1},
 year = {2025}
}

@article{2506.01326v2,
 abstract = {Operations research (OR) is widely deployed to solve critical decision-making
problems with complex objectives and constraints, impacting manufacturing,
logistics, finance, and healthcare outcomes. While Large Language Models (LLMs)
have shown promising results in various domains, their practical application in
industry-relevant operations research (OR) problems presents significant
challenges and opportunities. Preliminary industrial applications of LLMs for
operations research face two critical deployment challenges: 1) Self-correction
focuses on code syntax rather than mathematical accuracy, causing costly
errors; 2) Complex expert selection creates unpredictable workflows that reduce
transparency and increase maintenance costs, making them impractical for
time-sensitive business applications. To address these business limitations, we
introduce ORMind, a cognitive-inspired framework that enhances optimization
through counterfactual reasoning. Our approach emulates human cognition,
implementing an end-to-end workflow that systematically transforms requirements
into mathematical models and executable solver code. It is currently being
tested internally in Lenovo's AI Assistant, with plans to enhance optimization
capabilities for both business and consumer customers. Experiments demonstrate
that ORMind outperforms existing methods, achieving a 9.5\% improvement on the
NL4Opt dataset and a 14.6\% improvement on the ComplexOR dataset.},
 author = {Zhiyuan Wang and Bokui Chen and Yinya Huang and Qingxing Cao and Ming He and Jianping Fan and Xiaodan Liang},
 citations = {},
 comment = {Accepted by Annual Meetings of the Association for Computational
  Linguistics 2025},
 doi = {10.18653/v1/2025.acl-industry.10},
 eprint = {2506.01326v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ORMind: A Cognitive-Inspired End-to-End Reasoning Framework for Operations Research},
 url = {http://arxiv.org/abs/2506.01326v2},
 year = {2025}
}

@article{2506.02071v1,
 abstract = {Artificial intelligence has transformed numerous industries, from healthcare
to finance, enhancing decision-making through automated systems. However, the
reliability of these systems is mainly dependent on the quality of the
underlying datasets, raising ongoing concerns about transparency,
accountability, and potential biases. This paper introduces a scorecard
designed to evaluate the development of AI datasets, focusing on five key areas
from the system card framework data development life cycle: data dictionary,
collection process, composition, motivation, and pre-processing. The method
follows a structured approach, using an intake form and scoring criteria to
assess the quality and completeness of the data set. Applied to four diverse
datasets, the methodology reveals strengths and improvement areas. The results
are compiled using a scoring system that provides tailored recommendations to
enhance the transparency and integrity of the data set. The scorecard addresses
technical and ethical aspects, offering a holistic evaluation of data
practices. This approach aims to improve the quality of the data set. It offers
practical guidance to curators and researchers in developing responsible AI
systems, ensuring fairness and accountability in decision support systems.},
 author = {Tadesse K. Bahiru and Haileleol Tibebu and Ioannis A. Kakadiaris},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2506.02071v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AI Data Development: A Scorecard for the System Card Framework},
 url = {http://arxiv.org/abs/2506.02071v1},
 year = {2025}
}

@article{2506.02134v1,
 abstract = {Graph Neural Networks (GNNs) achieve high performance across many
applications but function as black-box models, limiting their use in critical
domains like healthcare and criminal justice. Explainability methods address
this by providing feature-level explanations that identify important node
attributes for predictions. These explanations create privacy risks. Combined
with auxiliary information, feature explanations can enable adversaries to
reconstruct graph structure, exposing sensitive relationships. Existing graph
reconstruction attacks assume access to original auxiliary data, but practical
systems use differential privacy to protect node features and labels while
providing explanations for transparency. We study a threat model where
adversaries access public feature explanations along with privatized node
features and labels. We show that existing explanation-based attacks like GSEF
perform poorly with privatized data due to noise from differential privacy
mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios
with public explanations and privatized auxiliary data. Our method adapts
explanation-based frameworks by incorporating denoising mechanisms that handle
differential privacy noise while exploiting structural signals in explanations.
Experiments across multiple datasets show ReconXF outperforms SoTA methods in
privatized settings, with improvements in AUC and average precision. Results
indicate that public explanations combined with denoising enable graph
structure recovery even under the privacy protection of auxiliary data. Code is
available at (link to be made public after acceptance).},
 author = {Rishi Raj Sahoo and Rucha Bhalchandra Joshi and Subhankar Mishra},
 citations = {},
 comment = {Under review},
 doi = {},
 eprint = {2506.02134v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels},
 url = {http://arxiv.org/abs/2506.02134v1},
 year = {2025}
}

@article{2506.03209v1,
 abstract = {Postoperative stroke remains a critical complication in elderly surgical
intensive care unit (SICU) patients, contributing to prolonged hospitalization,
elevated healthcare costs, and increased mortality. Accurate early risk
stratification is essential to enable timely intervention and improve clinical
outcomes. We constructed a combined cohort of 19,085 elderly SICU admissions
from the MIMIC-III and MIMIC-IV databases and developed an interpretable
machine learning (ML) framework to predict in-hospital stroke using clinical
data from the first 24 hours of Intensive Care Unit (ICU) stay. The
preprocessing pipeline included removal of high-missingness features, iterative
Singular Value Decomposition (SVD) imputation, z-score normalization, one-hot
encoding, and class imbalance correction via the Adaptive Synthetic Sampling
(ADASYN) algorithm. A two-stage feature selection process-combining Recursive
Feature Elimination with Cross-Validation (RFECV) and SHapley Additive
exPlanations (SHAP)-reduced the initial 80 variables to 20 clinically
informative predictors. Among eight ML models evaluated, CatBoost achieved the
best performance with an AUROC of 0.8868 (95% CI: 0.8802--0.8937). SHAP
analysis and ablation studies identified prior cerebrovascular disease, serum
creatinine, and systolic blood pressure as the most influential risk factors.
Our results highlight the potential of interpretable ML approaches to support
early detection of postoperative stroke and inform decision-making in
perioperative critical care.},
 author = {Tinghuan Li and Shuheng Chen and Junyi Fan and Elham Pishgar and Kamiar Alaei and Greg Placencia and Maryam Pishgar},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.03209v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Predicting Postoperative Stroke in Elderly SICU Patients: An Interpretable Machine Learning Model Using MIMIC Data},
 url = {http://arxiv.org/abs/2506.03209v1},
 year = {2025}
}

@article{2506.03546v1,
 abstract = {Advancements in generative models have enabled multi-agent systems (MAS) to
perform complex virtual tasks such as writing and code generation, which do not
generalize well to physical multi-agent robotic teams. Current frameworks often
treat agents as conceptual task executors rather than physically embodied
entities, and overlook critical real-world constraints such as spatial context,
robotic capabilities (e.g., sensing and navigation). To probe this gap, we
reconfigure and stress-test a hierarchical multi-agent robotic team built on
the CrewAI framework in a simulated emergency department onboarding scenario.
We identify five persistent failure modes: role misalignment; tool access
violations; lack of in-time handling of failure reports; noncompliance with
prescribed workflows; bypassing or false reporting of task completion. Based on
this analysis, we propose three design guidelines emphasizing process
transparency, proactive failure recovery, and contextual grounding. Our work
informs the development of more resilient and robust multi-agent robotic
systems (MARS), including opportunities to extend virtual multi-agent
frameworks to the real world.},
 author = {Yuanchen Bai and Zijian Ding and Angelique Taylor},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.03546v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {From Virtual Agents to Robot Teams: A Multi-Robot Framework Evaluation in High-Stakes Healthcare Context},
 url = {http://arxiv.org/abs/2506.03546v1},
 year = {2025}
}

@article{2506.04831v1,
 abstract = {Healthcare systems face significant challenges in managing and interpreting
vast, heterogeneous patient data for personalized care. Existing approaches
often focus on narrow use cases with a limited feature space, overlooking the
complex, longitudinal interactions needed for a holistic understanding of
patient health. In this work, we propose a novel approach to patient pathway
modeling by transforming diverse electronic health record (EHR) data into a
structured representation and designing a holistic pathway prediction model,
EHR2Path, optimized to predict future health trajectories. Further, we
introduce a novel summary mechanism that embeds long-term temporal context into
topic-specific summary tokens, improving performance over text-only models,
while being much more token-efficient. EHR2Path demonstrates strong performance
in both next time-step prediction and longitudinal simulation, outperforming
competitive baselines. It enables detailed simulations of patient trajectories,
inherently targeting diverse evaluation tasks, such as forecasting vital signs,
lab test results, or length-of-stay, opening a path towards predictive and
personalized healthcare.},
 author = {Chantal Pellegrini and Ege Özsoy and David Bani-Harouni and Matthias Keicher and Nassir Navab},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.04831v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health Trajectories with LLMs},
 url = {http://arxiv.org/abs/2506.04831v1},
 year = {2025}
}

@article{2506.05286v1,
 abstract = {Transparency is a paramount concern in the medical field, prompting
researchers to delve into the realm of explainable AI (XAI). Among these XAI
methods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent
space to human-understandable high-level concepts by generating a conceptual
layer for extracting conceptual features, which has drawn much attention
recently. However, existing methods rely solely on concept features to
determine the model's predictions, which overlook the intrinsic feature
embeddings within medical images. To address this utility gap between the
original models and concept-based models, we propose Vision Concept Transformer
(VCT). Furthermore, despite their benefits, CBMs have been found to negatively
impact model performance and fail to provide stable explanations when faced
with input perturbations, which limits their application in the medical field.
To address this faithfulness issue, this paper further proposes the Stable
Vision Concept Transformer (SVCT) based on VCT, which leverages the vision
transformer (ViT) as its backbone and incorporates a conceptual layer. SVCT
employs conceptual features to enhance decision-making capabilities by fusing
them with image features and ensures model faithfulness through the integration
of Denoised Diffusion Smoothing. Comprehensive experiments on four medical
datasets demonstrate that our VCT and SVCT maintain accuracy while remaining
interpretable compared to baselines. Furthermore, even when subjected to
perturbations, our SVCT model consistently provides faithful explanations, thus
meeting the needs of the medical field.},
 author = {Lijie Hu and Songning Lai and Yuan Hua and Shu Yang and Jingfeng Zhang and Di Wang},
 citations = {},
 comment = {arXiv admin note: text overlap with arXiv:2304.06129 by other authors},
 doi = {10.1007/978-3-032-06066-2_19},
 eprint = {2506.05286v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Stable Vision Concept Transformers for Medical Diagnosis},
 url = {http://arxiv.org/abs/2506.05286v1},
 year = {2025}
}

@article{2506.05877v1,
 abstract = {Clustering ensemble has emerged as an important research topic in the field
of machine learning. Although numerous methods have been proposed to improve
clustering quality, most existing approaches overlook the need for
interpretability in high-stakes applications. In domains such as medical
diagnosis and financial risk assessment, algorithms must not only be accurate
but also interpretable to ensure transparent and trustworthy decision-making.
Therefore, to fill the gap of lack of interpretable algorithms in the field of
clustering ensemble, we propose the first interpretable clustering ensemble
algorithm in the literature. By treating base partitions as categorical
variables, our method constructs a decision tree in the original feature space
and use the statistical association test to guide the tree building process.
Experimental results demonstrate that our algorithm achieves comparable
performance to state-of-the-art (SOTA) clustering ensemble methods while
maintaining an additional feature of interpretability. To the best of our
knowledge, this is the first interpretable algorithm specifically designed for
clustering ensemble, offering a new perspective for future research in
interpretable clustering.},
 author = {Hang Lv and Lianyu Hu and Mudi Jiang and Xinying Liu and Zengyou He},
 citations = {7},
 comment = {},
 doi = {10.17760/d20294158},
 eprint = {2506.05877v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable Clustering Ensemble},
 url = {http://arxiv.org/abs/2506.05877v1},
 year = {2025}
}

@article{2506.06192v1,
 abstract = {Patient stratification identifying clinically meaningful subgroups is
essential for advancing personalized medicine through improved diagnostics and
treatment strategies. Electronic health records (EHRs), particularly those from
intensive care units (ICUs), contain rich temporal clinical data that can be
leveraged for this purpose. In this work, we introduce ICU-TSB (Temporal
Stratification Benchmark), the first comprehensive benchmark for evaluating
patient stratification based on temporal patient representation learning using
three publicly available ICU EHR datasets. A key contribution of our benchmark
is a novel hierarchical evaluation framework utilizing disease taxonomies to
measure the alignment of discovered clusters with clinically validated disease
groupings. In our experiments with ICU-TSB, we compared statistical methods and
several recurrent neural networks, including LSTM and GRU, for their ability to
generate effective patient representations for subsequent clustering of patient
trajectories. Our results demonstrate that temporal representation learning can
rediscover clinically meaningful patient cohorts; nevertheless, it remains a
challenging task, with v-measuring varying from up to 0.46 at the top level of
the taxonomy to up to 0.40 at the lowest level. To further enhance the
practical utility of our findings, we also evaluate multiple strategies for
assigning interpretable labels to the identified clusters. The experiments and
benchmark are fully reproducible and available at
https://github.com/ds4dh/CBMS2025stratification.},
 author = {Dimitrios Proios and Alban Bornet and Anthony Yazdani and Jose F Rodrigues Jr and Douglas Teodoro},
 citations = {0},
 comment = {6 pages 1 table 6 figures},
 doi = {10.1109/cbms65348.2025.00022},
 eprint = {2506.06192v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ICU-TSB: A Benchmark for Temporal Patient Representation Learning for Unsupervised Stratification into Patient Cohorts},
 url = {http://arxiv.org/abs/2506.06192v1},
 year = {2025}
}

@article{2506.06330v1,
 abstract = {As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.},
 author = {James Afful},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.06330v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications},
 url = {http://arxiv.org/abs/2506.06330v1},
 year = {2025}
}

@article{2506.06574v2,
 abstract = {Multi-agent artificial intelligence systems are increasingly deployed in
clinical settings, yet the relationship between component-level optimization
and system-wide performance remains poorly understood. We evaluated this
relationship using 2,400 real patient cases from the MIMIC-CDM dataset across
four abdominal pathologies (appendicitis, pancreatitis, cholecystitis,
diverticulitis), decomposing clinical diagnosis into information gathering,
interpretation, and differential diagnosis. We evaluated single agent systems
(one model performing all tasks) against multi-agent systems (specialized
models for each task) using comprehensive metrics spanning diagnostic outcomes,
process adherence, and cost efficiency. Our results reveal a paradox: while
multi-agent systems generally outperformed single agents, the
component-optimized or Best of Breed system with superior components and
excellent process metrics (85.5% information accuracy) significantly
underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent
system). This finding underscores that successful integration of AI in
healthcare requires not just component level optimization but also attention to
information flow and compatibility between agents. Our findings highlight the
need for end to end system validation rather than relying on component metrics
alone.},
 author = {Suhana Bedi and Iddah Mlauzi and Daniel Shin and Sanmi Koyejo and Nigam H. Shah},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2506.06574v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {The Optimization Paradox in Clinical AI Multi-Agent Systems},
 url = {http://arxiv.org/abs/2506.06574v2},
 year = {2025}
}

@article{2506.07049v1,
 abstract = {Machine learning (ML) systems are utilized in critical sectors, such as
healthcare, law enforcement, and finance. However, these systems are often
trained on historical data that contains demographic biases, leading to ML
decisions that perpetuate or exacerbate existing social inequalities. Causal
fairness provides a transparent, human-in-the-loop framework to mitigate
algorithmic discrimination, aligning closely with legal doctrines of direct and
indirect discrimination. However, current causal fairness frameworks hold a key
limitation in that they assume prior knowledge of the correct causal model,
restricting their applicability in complex fairness scenarios where causal
models are unknown or difficult to identify. To bridge this gap, we propose
FairPFN, a tabular foundation model pre-trained on synthetic causal fairness
data to identify and mitigate the causal effects of protected attributes in its
predictions. FairPFN's key contribution is that it requires no knowledge of the
causal model and still demonstrates strong performance in identifying and
removing protected causal effects across a diverse set of hand-crafted and
real-world scenarios relative to robust baseline methods. FairPFN paves the way
for promising future research, making causal fairness more accessible to a
wider variety of complex fairness problems.},
 author = {Jake Robertson and Noah Hollmann and Samuel Müller and Noor Awad and Frank Hutter},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.07049v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {FairPFN: A Tabular Foundation Model for Causal Fairness},
 url = {http://arxiv.org/abs/2506.07049v1},
 year = {2025}
}

@article{2506.07191v1,
 abstract = {This study employs a robust analytical framework to uncover patterns in
survival outcomes among breast cancer patients from diverse racial and
geographical backgrounds. This research uses the SEER 2021 dataset to analyze
breast cancer survival outcomes to identify and comprehend dissimilarities. Our
approach integrates exploratory data analysis (EDA), through this we identify
key variables that influence survival rates and employ survival analysis
techniques, including the Kaplan-Meier estimator and log-rank test and the
advanced modeling Cox Proportional Hazards model to determine how survival
rates vary across racial groups and countries. Model validation and
interpretation are undertaken to ensure the reliability of our findings, which
are documented comprehensively to inform policymakers and healthcare
professionals. The outcome of this paper is a detailed version of statistical
analysis that not just highlights disparities in breast cancer treatment and
care but also serves as a foundational tool for developing targeted
interventions to address the inequalities effectively. Through this research,
our aim is to contribute to the global efforts to improve breast cancer
outcomes and reduce treatment disparities.},
 author = {Ramisa Farha and Joshua O. Olukoya},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.07191v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach},
 url = {http://arxiv.org/abs/2506.07191v1},
 year = {2025}
}

@article{2506.07228v1,
 abstract = {Brain tumors, regardless of being benign or malignant, pose considerable
health risks, with malignant tumors being more perilous due to their swift and
uncontrolled proliferation, resulting in malignancy. Timely identification is
crucial for enhancing patient outcomes, particularly in nations such as
Bangladesh, where healthcare infrastructure is constrained. Manual MRI analysis
is arduous and susceptible to inaccuracies, rendering it inefficient for prompt
diagnosis. This research sought to tackle these problems by creating an
automated brain tumor classification system utilizing MRI data obtained from
many hospitals in Bangladesh. Advanced deep learning models, including VGG16,
VGG19, and ResNet50, were utilized to classify glioma, meningioma, and various
brain cancers. Explainable AI (XAI) methodologies, such as Grad-CAM and
Grad-CAM++, were employed to improve model interpretability by emphasizing the
critical areas in MRI scans that influenced the categorization. VGG16 achieved
the most accuracy, attaining 99.17%. The integration of XAI enhanced the
system's transparency and stability, rendering it more appropriate for clinical
application in resource-limited environments such as Bangladesh. This study
highlights the capability of deep learning models, in conjunction with
explainable artificial intelligence (XAI), to enhance brain tumor detection and
identification in areas with restricted access to advanced medical
technologies.},
 author = {Shuvashis Sarker},
 citations = {0},
 comment = {2024 6th International Conference on Sustainable Technologies for
  Industry 5.0 (STI)},
 doi = {10.1109/sti64222.2024.10951092},
 eprint = {2506.07228v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh},
 url = {http://arxiv.org/abs/2506.07228v1},
 year = {2025}
}

@article{2506.08486v1,
 abstract = {The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.},
 author = {Rahatara Ferdousi and M Anwar Hossain},
 citations = {},
 comment = {18 pages, 12 figures, IEEE EMBS JBHI},
 doi = {},
 eprint = {2506.08486v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being},
 url = {http://arxiv.org/abs/2506.08486v1},
 year = {2025}
}

@article{2506.09090v1,
 abstract = {This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.},
 author = {Arthur Oghlukyan and Nuria Gomez Blas},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.09090v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications},
 url = {http://arxiv.org/abs/2506.09090v1},
 year = {2025}
}

@article{2506.09108v1,
 abstract = {We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.},
 author = {Yuwei Zhang and Kumar Ayush and Siyuan Qiao and A. Ali Heydari and Girish Narayanswamy and Maxwell A. Xu and Ahmed A. Metwally and Shawn Xu and Jake Garrison and Xuhai Xu and Tim Althoff and Yun Liu and Pushmeet Kohli and Jiening Zhan and Mark Malhotra and Shwetak Patel and Cecilia Mascolo and Xin Liu and Daniel McDuff and Yuzhe Yang},
 citations = {6},
 comment = {},
 doi = {},
 eprint = {2506.09108v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SensorLM: Learning the Language of Wearable Sensors},
 url = {http://arxiv.org/abs/2506.09108v1},
 year = {2025}
}

@article{2506.09114v1,
 abstract = {The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.},
 author = {Jialin Chen and Ziyu Zhao and Gaukhar Nurbek and Aosong Feng and Ali Maatouk and Leandros Tassiulas and Yifeng Gao and Rex Ying},
 citations = {2},
 comment = {},
 doi = {},
 eprint = {2506.09114v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval},
 url = {http://arxiv.org/abs/2506.09114v1},
 year = {2025}
}

@article{2506.09420v1,
 abstract = {Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.},
 author = {Henry Peng Zou and Wei-Chieh Huang and Yaozu Wu and Chunyu Miao and Dongyuan Li and Aiwei Liu and Yue Zhou and Yankai Chen and Weizhi Zhang and Yangning Li and Liancheng Fang and Renhe Jiang and Philip S. Yu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.09420v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy},
 url = {http://arxiv.org/abs/2506.09420v1},
 year = {2025}
}

@article{2506.11056v2,
 abstract = {Inverse problems are central to a wide range of fields, including healthcare,
climate science, and agriculture. They involve the estimation of inputs,
typically via iterative optimization, to some known forward model so that it
produces a desired outcome. Despite considerable development in the
explainability and interpretability of forward models, the iterative
optimization of inverse problems remains largely cryptic to domain experts. We
propose a methodology to produce explanations, from traces produced by an
optimizer, that are interpretable by humans at the abstraction of the domain.
The central idea in our approach is to instrument a differentiable simulator so
that it emits natural language events during its forward and backward passes.
In a post-process, we use a Language Model to create an explanation from the
list of events. We demonstrate the effectiveness of our approach with an
illustrative optimization problem and an example involving the training of a
neural network.},
 author = {Sean Memery and Kevin Denamganai and Anna Kapron-King and Kartic Subr},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.11056v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {xInv: Explainable Optimization of Inverse Problems},
 url = {http://arxiv.org/abs/2506.11056v2},
 year = {2025}
}

@article{2506.11491v2,
 abstract = {Spatial transcriptomics (ST) technologies enable gene expression profiling
with spatial resolution, offering unprecedented insights into tissue
organization and disease heterogeneity. However, current analysis methods often
struggle with noisy data, limited scalability, and inadequate modelling of
complex cellular relationships. We present SemanticST, a biologically informed,
graph-based deep learning framework that models diverse cellular contexts
through multi-semantic graph construction. SemanticST builds multiple
context-specific graphs capturing spatial proximity, gene expression
similarity, and tissue domain structure, and learns disentangled embeddings for
each. These are fused using an attention-inspired strategy to yield a unified,
biologically meaningful representation. A community-aware min-cut loss improves
robustness over contrastive learning, particularly in sparse ST data.
SemanticST supports mini-batch training, making it the first graph neural
network scalable to large-scale datasets such as Xenium (500,000 cells).
Benchmarking across four platforms (Visium, Slide-seq, Stereo-seq, Xenium) and
multiple human and mouse tissues shows consistent 20 percentage gains in ARI,
NMI, and trajectory fidelity over DeepST, GraphST, and IRIS. In re-analysis of
breast cancer Xenium data, SemanticST revealed rare and clinically significant
niches, including triple receptor-positive clusters, spatially distinct
DCIS-to-IDC transition zones, and FOXC2 tumour-associated myoepithelial cells,
suggesting non-canonical EMT programs with stem-like features. SemanticST thus
provides a scalable, interpretable, and biologically grounded framework for
spatial transcriptomics analysis, enabling robust discovery across tissue types
and diseases, and paving the way for spatially resolved tissue atlases and
next-generation precision medicine.},
 author = {Roxana Zahedi and Ahmadreza Argha and Nona Farbehi and Ivan Bakhshayeshi and Youqiong Ye and Nigel H. Lovell and Hamid Alinejad-Rokny},
 citations = {0},
 comment = {6 Figures},
 doi = {},
 eprint = {2506.11491v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SemanticST: Spatially Informed Semantic Graph Learning for Clustering, Integration, and Scalable Analysis of Spatial Transcriptomics},
 url = {http://arxiv.org/abs/2506.11491v2},
 year = {2025}
}

@article{2506.12156v1,
 abstract = {Interpreting large volumes of high-dimensional, unlabeled data in a manner
that is comprehensible to humans remains a significant challenge across various
domains. In unsupervised healthcare data analysis, interpreting clustered data
can offer meaningful insights into patients' health outcomes, which hold direct
implications for healthcare providers. This paper addresses the problem of
interpreting clustered sensor data collected from older adult patients
recovering from lower-limb fractures in the community. A total of 560 days of
multimodal sensor data, including acceleration, step count, ambient motion, GPS
location, heart rate, and sleep, alongside clinical scores, were remotely
collected from patients at home. Clustering was first carried out separately
for each data modality to assess the impact of feature sets extracted from each
modality on patients' recovery trajectories. Then, using context-aware
prompting, a large language model was employed to infer meaningful cluster
labels for the clusters derived from each modality. The quality of these
clusters and their corresponding labels was validated through rigorous
statistical testing and visualization against clinical scores collected
alongside the multimodal sensor data. The results demonstrated the statistical
significance of most modality-specific cluster labels generated by the large
language model with respect to clinical scores, confirming the efficacy of the
proposed method for interpreting sensor data in an unsupervised manner. This
unsupervised data analysis approach, relying solely on sensor data, enables
clinicians to identify at-risk patients and take timely measures to improve
health outcomes.},
 author = {Shehroz S. Khan and Ali Abedi and Charlene H. Chu},
 citations = {},
 comment = {15 pages, 2 figures, 3 tables},
 doi = {10.1007/978-3-032-02215-8_22},
 eprint = {2506.12156v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explaining Recovery Trajectories of Older Adults Post Lower-Limb Fracture Using Modality-wise Multiview Clustering and Large Language Models},
 url = {http://arxiv.org/abs/2506.12156v1},
 year = {2025}
}

@article{2506.12378v1,
 abstract = {Explainable ML algorithms are designed to provide transparency and insight
into their decision-making process. Explaining how ML models come to their
prediction is critical in fields such as healthcare and finance, as it provides
insight into how models can help detect bias in predictions and help comply
with GDPR compliance in these fields. QML leverages quantum phenomena such as
entanglement and superposition, offering the potential for computational
speedup and greater insights compared to classical ML. However, QML models also
inherit the black-box nature of their classical counterparts, requiring the
development of explainability techniques to be applied to these QML models to
help understand why and how a particular output was generated.
  This paper will explore the idea of creating a modular, explainable QML
framework that splits QML algorithms into their core components, such as
feature maps, variational circuits (ansatz), optimizers, kernels, and
quantum-classical loops. Each component will be analyzed using explainability
techniques, such as ALE and SHAP, which have been adapted to analyse the
different components of these QML algorithms. By combining insights from these
parts, the paper aims to infer explainability to the overall QML model.},
 author = {Barra White and Krishnendu Guha},
 citations = {},
 comment = {11 pages},
 doi = {},
 eprint = {2506.12378v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Component Based Quantum Machine Learning Explainability},
 url = {http://arxiv.org/abs/2506.12378v1},
 year = {2025}
}

@article{2506.12437v1,
 abstract = {This paper explores the growing presence of emotionally responsive artificial
intelligence through a critical and interdisciplinary lens. Bringing together
the voices of early-career researchers from multiple fields, it explores how AI
systems that simulate or interpret human emotions are reshaping our
interactions in areas such as education, healthcare, mental health, caregiving,
and digital life. The analysis is structured around four central themes: the
ethical implications of emotional AI, the cultural dynamics of human-machine
interaction, the risks and opportunities for vulnerable populations, and the
emerging regulatory, design, and technical considerations. The authors
highlight the potential of affective AI to support mental well-being, enhance
learning, and reduce loneliness, as well as the risks of emotional
manipulation, over-reliance, misrepresentation, and cultural bias. Key
challenges include simulating empathy without genuine understanding, encoding
dominant sociocultural norms into AI systems, and insufficient safeguards for
individuals in sensitive or high-risk contexts. Special attention is given to
children, elderly users, and individuals with mental health challenges, who may
interact with AI in emotionally significant ways. However, there remains a lack
of cognitive or legal protections which are necessary to navigate such
engagements safely. The report concludes with ten recommendations, including
the need for transparency, certification frameworks, region-specific
fine-tuning, human oversight, and longitudinal research. A curated
supplementary section provides practical tools, models, and datasets to support
further work in this domain.},
 author = {Vivek Chavan and Arsen Cenaj and Shuyuan Shen and Ariane Bar and Srishti Binwani and Tommaso Del Becaro and Marius Funk and Lynn Greschner and Roberto Hung and Stina Klein and Romina Kleiner and Stefanie Krause and Sylwia Olbrych and Vishvapalsinhji Parmar and Jaleh Sarafraz and Daria Soroko and Daksitha Withanage Don and Chang Zhou and Hoang Thuy Duong Vu and Parastoo Semnani and Daniel Weinhardt and Elisabeth Andre and Jörg Krüger and Xavier Fresquet},
 citations = {},
 comment = {From the Spring School 2025 by AI Grid and SCAI (Sorbonne
  University), 16 pages},
 doi = {},
 eprint = {2506.12437v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Feeling Machines: Ethics, Culture, and the Rise of Emotional AI},
 url = {http://arxiv.org/abs/2506.12437v1},
 year = {2025}
}

@article{2506.13903v1,
 abstract = {In domains where transparency and trustworthiness are crucial, such as
healthcare, rule-based systems are widely used and often preferred over
black-box models for decision support systems due to their inherent
interpretability. However, as rule-based models grow complex, discerning
crucial features, understanding their interactions, and comparing feature
contributions across different rule sets becomes challenging. To address this,
we propose a comprehensive framework for estimating feature contributions in
rule-based systems, introducing a graph-based feature visualisation strategy, a
novel feature importance metric agnostic to rule-based predictors, and a
distance metric for comparing rule sets based on feature contributions. By
experimenting on two clinical datasets and four rule-based methods (decision
trees, logic learning machines, association rules, and neural networks with
rule extraction), we showcase our method's capability to uncover novel insights
on the combined predictive value of clinical features, both at the dataset and
class-specific levels. These insights can aid in identifying new risk factors,
signature genes, and potential biomarkers, and determining the subset of
patient information that should be prioritised to enhance diagnostic accuracy.
Comparative analysis of the proposed feature importance score with
state-of-the-art methods on 15 public benchmarks demonstrates competitive
performance and superior robustness. The method implementation is available on
GitHub: https://github.com/ChristelSirocchi/rule-graph.},
 author = {Christel Sirocchi and Damiano Verda},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2506.13903v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Enhancing interpretability of rule-based classifiers through feature graphs},
 url = {http://arxiv.org/abs/2506.13903v1},
 year = {2025}
}

@article{2506.13904v1,
 abstract = {Despite promising developments in Explainable Artificial Intelligence, the
practical value of XAI methods remains under-explored and insufficiently
validated in real-world settings. Robust and context-aware evaluation is
essential, not only to produce understandable explanations but also to ensure
their trustworthiness and usability for intended users, but tends to be
overlooked because of no clear guidelines on how to design an evaluation with
users.
  This study addresses this gap with two main goals: (1) to develop a framework
of well-defined, atomic properties that characterise the user experience of XAI
in healthcare; and (2) to provide clear, context-sensitive guidelines for
defining evaluation strategies based on system characteristics.
  We conducted a systematic review of 82 user studies, sourced from five
databases, all situated within healthcare settings and focused on evaluating
AI-generated explanations. The analysis was guided by a predefined coding
scheme informed by an existing evaluation framework, complemented by inductive
codes developed iteratively.
  The review yields three key contributions: (1) a synthesis of current
evaluation practices, highlighting a growing focus on human-centred approaches
in healthcare XAI; (2) insights into the interrelations among explanation
properties; and (3) an updated framework and a set of actionable guidelines to
support interdisciplinary teams in designing and implementing effective
evaluation strategies for XAI systems tailored to specific application
contexts.},
 author = {Ivania Donoso-Guzmán and Kristýna Sirka Kacafírková and Maxwell Szymanski and An Jacobs and Denis Parra and Katrien Verbert},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2506.13904v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare},
 url = {http://arxiv.org/abs/2506.13904v1},
 year = {2025}
}

@article{2506.13970v1,
 abstract = {This thesis addresses the technical challenges of applying machine learning
to understand and interpret medical audio signals. The sounds of our lungs,
heart, and voice convey vital information about our health. Yet, in
contemporary medicine, these sounds are primarily analyzed through auditory
interpretation by experts using devices like stethoscopes. Automated analysis
offers the potential to standardize the processing of medical sounds, enable
screening in low-resource settings where physicians are scarce, and detect
subtle patterns that may elude human perception, thereby facilitating early
diagnosis and treatment.
  Focusing on the analysis of infant cry sounds to predict medical conditions,
this thesis contributes on four key fronts. First, in low-data settings, we
demonstrate that large databases of adult speech can be harnessed through
neural transfer learning to develop more accurate and robust models for infant
cry analysis. Second, in cost-effective modeling, we introduce an end-to-end
model compression approach for recurrent networks using tensor decomposition.
Our method requires no post-hoc processing, achieves compression rates of
several hundred-fold, and delivers accurate, portable models suitable for
resource-constrained devices. Third, we propose novel domain adaptation
techniques tailored for audio models and adapt existing methods from computer
vision. These approaches address dataset bias and enhance generalization across
domains while maintaining strong performance on the original data. Finally, to
advance research in this domain, we release a unique, open-source dataset of
infant cry sounds, developed in collaboration with clinicians worldwide.
  This work lays the foundation for recognizing the infant cry as a vital sign
and highlights the transformative potential of AI-driven audio monitoring in
shaping the future of accessible and affordable healthcare.},
 author = {Charles C Onu},
 citations = {},
 comment = {PhD Thesis},
 doi = {},
 eprint = {2506.13970v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Making deep neural networks work for medical audio: representation, compression and domain adaptation},
 url = {http://arxiv.org/abs/2506.13970v1},
 year = {2025}
}

@article{2506.14843v1,
 abstract = {Machine Learning (ML) is used to tackle various tasks, such as disease
classification and prediction. The effectiveness of ML models relies heavily on
having large amounts of complete data. However, healthcare data is often
limited or incomplete, which can hinder model performance. Additionally, issues
like the trustworthiness of solutions vary with the datasets used. The lack of
transparency in some ML models further complicates their understanding and use.
In healthcare, particularly in the case of Age-related Macular Degeneration
(AMD), which affects millions of older adults, early diagnosis is crucial due
to the absence of effective treatments for reversing progression. Diagnosing
AMD involves assessing retinal images along with patients' symptom reports.
There is a need for classification approaches that consider genetic, dietary,
clinical, and demographic factors. Recently, we introduced the -Comprehensive
Abstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed
at improving AMD stage classification. CACTUS offers explainability and
flexibility, outperforming standard ML models. It enhances decision-making by
identifying key factors and providing confidence in its results. The important
features identified by CACTUS allow us to compare with existing medical
knowledge. By eliminating less relevant or biased data, we created a clinical
scenario for clinicians to offer feedback and address biases.},
 author = {Luca Gherardini and Imre Lengyel and Tunde Peto and Caroline C. W. Klaverd and Magda A. Meester-Smoord and Johanna Maria Colijnd and EYE-RISK Consortium and E3 Consortium and Jose Sousa},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.14843v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration},
 url = {http://arxiv.org/abs/2506.14843v1},
 year = {2025}
}

@article{2506.15711v1,
 abstract = {Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.},
 author = {Le Jiang and Liyan Ma and Guang Yang},
 citations = {2},
 comment = {},
 doi = {10.1016/j.media.2025.103673},
 eprint = {2506.15711v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Shadow defense against gradient inversion attack in federated learning},
 url = {http://arxiv.org/abs/2506.15711v1},
 year = {2025}
}

@article{2506.15823v1,
 abstract = {"DHEAL-COM - Digital Health Solutions in Community Medicine" is a research
and technology project funded by the Italian Department of Health for the
development of digital solutions of interest in proximity healthcare. The
activity within the DHEAL-COM framework allows scientists to gather a notable
amount of multi-modal data whose interpretation can be performed by means of
machine learning algorithms. The present study illustrates a general automated
pipeline made of numerous unsupervised and supervised methods that can ingest
such data, provide predictive results, and facilitate model interpretations via
feature identification.},
 author = {Chiara Razzetta and Shahryar Noei and Federico Barbarossa and Edoardo Spairani and Monica Roascio and Elisa Barbi and Giulia Ciacci and Sara Sommariva and Sabrina Guastavino and Michele Piana and Matteo Lenge and Gabriele Arnulfo and Giovanni Magenes and Elvira Maranesi and Giulio Amabili and Anna Maria Massone and Federico Benvenuto and Giuseppe Jurman and Diego Sona and Cristina Campi},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2506.15823v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AI-based modular warning machine for risk identification in proximity healthcare},
 url = {http://arxiv.org/abs/2506.15823v1},
 year = {2025}
}

@article{2506.17113v1,
 abstract = {Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.},
 author = {Shoubin Yu and Yue Zhang and Ziyang Wang and Jaehong Yoon and Mohit Bansal},
 citations = {},
 comment = {The first two authors contributed equally; Github link:
  https://github.com/Yui010206/MEXA},
 doi = {},
 eprint = {2506.17113v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation},
 url = {http://arxiv.org/abs/2506.17113v1},
 year = {2025}
}

@article{2506.17329v1,
 abstract = {Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of
Things (IoT), real-time monitoring, and human-centered design toward
personalized medicine and predictive diagnostics. However, the increasing
reliance on interconnected medical technologies exposes them to cyber threats.
Meanwhile, current AI-driven cybersecurity models often neglect biomedical
data, limiting their effectiveness and interpretability. This study addresses
this gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that
integrates network traffic and biomedical sensor data. Classification outputs
indicate that XGBoost achieved 99% F1-score for benign and data alteration, and
81% for spoofing. Explainability findings reveal that network data play a
dominant role in intrusion detection whereas biomedical features contributed to
spoofing detection, with temperature reaching a Shapley values magnitude of
0.37.},
 author = {Pedro H. Lui and Lucas P. Siqueira and Juliano F. Kazienko and Vagner E. Quincozes and Silvio E. Quincozes and Daniel Welfer},
 citations = {0},
 comment = {12 pages, 7 figures, conference},
 doi = {10.5753/sbcas.2025.7182},
 eprint = {2506.17329v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0},
 url = {http://arxiv.org/abs/2506.17329v1},
 year = {2025}
}

@article{2506.17466v1,
 abstract = {Federated learning continues to evolve but faces challenges in
interpretability and explainability. To address these challenges, we introduce
a novel approach that employs Neural Additive Models (NAMs) within a federated
learning framework. This new Federated Neural Additive Models (FedNAMs)
approach merges the advantages of NAMs, where individual networks concentrate
on specific input features, with the decentralized approach of federated
learning, ultimately producing interpretable analysis results. This integration
enhances privacy by training on local data across multiple devices, thereby
minimizing the risks associated with data centralization and improving model
robustness and generalizability. FedNAMs maintain detailed, feature-specific
learning, making them especially valuable in sectors such as finance and
healthcare. They facilitate the training of client-specific models to integrate
local updates, preserve privacy, and mitigate concerns related to
centralization. Our studies on various text and image classification tasks,
using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show
that FedNAMs deliver strong interpretability with minimal accuracy loss
compared to traditional Federated Deep Neural Networks (DNNs). The research
involves notable findings, including the identification of critical predictive
features at both client and global levels. Volatile acidity, sulfates, and
chlorides for wine quality. Chest pain type, maximum heart rate, and number of
vessels for heart disease. Petal length and width for iris classification. This
approach strengthens privacy and model efficiency and improves interpretability
and robustness across diverse datasets. Finally, FedNAMs generate insights on
causes of highly and low interpretable features.},
 author = {Amitash Nanda and Sree Bhargavi Balija and Debashis Sahoo},
 citations = {3},
 comment = {13 pages, 6 figures},
 doi = {},
 eprint = {2506.17466v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {FedNAMs: Performing Interpretability Analysis in Federated Learning Context},
 url = {http://arxiv.org/abs/2506.17466v1},
 year = {2025}
}

@article{2506.17776v2,
 abstract = {Recent advancements in Machine Learning (ML) have yielded powerful models
capable of extracting structured information from diverse and complex data
sources. However, a significant challenge lies in translating these perceptual
or extractive outputs into actionable, reasoned decisions within complex
operational workflows. To address these challenges, this paper introduces a
novel approach that integrates the outputs from various machine learning models
directly with the PyReason framework, an open-world temporal logic programming
reasoning engine. PyReason's foundation in generalized annotated logic allows
for the seamless incorporation of real-valued outputs (e.g., probabilities,
confidence scores) from diverse ML models, treating them as truth intervals
within its logical framework. Crucially, PyReason provides mechanisms,
implemented in Python, to continuously poll ML model outputs, convert them into
logical facts, and dynamically recompute the minimal model, ensuring real-tine
adaptive decision-making. Furthermore, its native support for temporal
reasoning, knowledge graph integration, and fully explainable interface traces
enables sophisticated analysis over time-sensitive process data and existing
organizational knowledge. By combining the strengths of perception and
extraction from ML models with the logical deduction and transparency of
PyReason, we aim to create a powerful system for automating complex processes.
This integration finds utility across numerous domains, including
manufacturing, healthcare, and business operations.},
 author = {Dyuman Aditya and Colton Payne and Mario Leiva and Paulo Shakarian},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2506.17776v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Machine Learning Model Integration with Open World Temporal Logic for Process Automation},
 url = {http://arxiv.org/abs/2506.17776v2},
 year = {2025}
}

@article{2506.18078v1,
 abstract = {We propose a novel nonparametric regression method that models complex
input-output relationships as the sum of convex and concave components. The
method-Identifiable Convex-Concave Nonparametric Least Squares
(ICCNLS)-decomposes the target function into additive shape-constrained
components, each represented via sub-gradient-constrained affine functions. To
address the affine ambiguity inherent in convex-concave decompositions, we
introduce global statistical orthogonality constraints, ensuring that residuals
are uncorrelated with both intercept and input variables. This enforces
decomposition identifiability and improves interpretability. We further
incorporate L1, L2 and elastic net regularisation on sub-gradients to enhance
generalisation and promote structural sparsity. The proposed method is
evaluated on synthetic and real-world datasets, including healthcare pricing
data, and demonstrates improved predictive accuracy and model simplicity
compared to conventional CNLS and difference-of-convex (DC) regression
approaches. Our results show that statistical identifiability, when paired with
convex-concave structure and sub-gradient regularisation, yields interpretable
models suited for forecasting, benchmarking, and policy evaluation.},
 author = {William Chung},
 citations = {0},
 comment = {21 pages, working paper},
 doi = {},
 eprint = {2506.18078v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares},
 url = {http://arxiv.org/abs/2506.18078v1},
 year = {2025}
}

@article{2506.18116v1,
 abstract = {Large Language Models (LLMs) in mental healthcare risk propagating biases
that reinforce stigma and harm marginalized groups. While previous research
identified concerning trends, systematic methods for detecting intersectional
biases remain limited. This work introduces a multi-hop question answering
(MHQA) framework to explore LLM response biases in mental health discourse. We
analyze content from the Interpretable Mental Health Instruction (IMHI) dataset
across symptom presentation, coping mechanisms, and treatment approaches. Using
systematic tagging across age, race, gender, and socioeconomic status, we
investigate bias patterns at demographic intersections. We evaluate four LLMs:
Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic
disparities across sentiment, demographics, and mental health conditions. Our
MHQA approach demonstrates superior detection compared to conventional methods,
identifying amplification points where biases magnify through sequential
reasoning. We implement two debiasing techniques: Roleplay Simulation and
Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot
prompting with BBQ dataset examples. These findings highlight critical areas
where LLMs reproduce mental healthcare biases, providing actionable insights
for equitable AI development.},
 author = {Batool Haider and Atmika Gorti and Aman Chadha and Manas Gaur},
 citations = {1},
 comment = {19 Pages, 7 Figures, 4 Tables (Note: Under Review)},
 doi = {},
 eprint = {2506.18116v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives},
 url = {http://arxiv.org/abs/2506.18116v1},
 year = {2025}
}

@article{2506.18732v1,
 abstract = {The deep integration of foundation models (FM) with federated learning (FL)
enhances personalization and scalability for diverse downstream tasks, making
it crucial in sensitive domains like healthcare. Achieving group fairness has
become an increasingly prominent issue in the era of federated foundation
models (FFMs), since biases in sensitive attributes might lead to inequitable
treatment for under-represented demographic groups. Existing studies mostly
focus on achieving fairness with respect to a single sensitive attribute. This
renders them unable to provide clear interpretability of dependencies among
multiple sensitive attributes which is required to achieve group fairness. Our
paper takes the first attempt towards a causal analysis of the relationship
between group fairness across various sensitive attributes in the FFM. We
extend the FFM structure to trade off multiple sensitive attributes
simultaneously and quantify the causal effect behind the group fairness through
causal discovery and inference. Extensive experiments validate its
effectiveness, offering insights into interpretability towards building
trustworthy and fair FFM systems.},
 author = {Yuning Yang and Han Yu and Tianrun Gao and Xiaodong Xu and Guangyu Wang},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2506.18732v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation Models},
 url = {http://arxiv.org/abs/2506.18732v1},
 year = {2025}
}

@article{2506.19573v1,
 abstract = {Machine learning (ML) techniques play a pivotal role in high-stakes domains
such as healthcare, where accurate predictions can greatly enhance
decision-making. However, most high-performing methods such as neural networks
and ensemble methods are often opaque, limiting trust and broader adoption. In
parallel, symbolic methods like Answer Set Programming (ASP) offer the
possibility of interpretable logical rules but do not always match the
predictive power of ML models. This paper proposes a hybrid approach that
integrates ASP-derived rules from the FOLD-R++ algorithm with black-box ML
classifiers to selectively correct uncertain predictions and provide
human-readable explanations. Experiments on five medical datasets reveal
statistically significant performance gains in accuracy and F1 score. This
study underscores the potential of combining symbolic reasoning with
conventional ML to achieve high interpretability without sacrificing accuracy.},
 author = {Sanne Wielinga and Jesse Heyninck},
 citations = {0},
 comment = {accepted for publication as a Technical Communication at ICLP 2025},
 doi = {},
 eprint = {2506.19573v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable Hybrid Machine Learning Models Using FOLD-R++ and Answer Set Programming},
 url = {http://arxiv.org/abs/2506.19573v1},
 year = {2025}
}

@article{2506.19702v1,
 abstract = {Medical document analysis plays a crucial role in extracting essential
clinical insights from unstructured healthcare records, supporting critical
tasks such as differential diagnosis. Determining the most probable condition
among overlapping symptoms requires precise evaluation and deep medical
expertise. While recent advancements in large language models (LLMs) have
significantly enhanced performance in medical document analysis, privacy
concerns related to sensitive patient data limit the use of online LLMs
services in clinical settings. To address these challenges, we propose a
trustworthy medical document analysis platform that fine-tunes a LLaMA-v3 using
low-rank adaptation, specifically optimized for differential diagnosis tasks.
Our approach utilizes DDXPlus, the largest benchmark dataset for differential
diagnosis, and demonstrates superior performance in pathology prediction and
variable-length differential diagnosis compared to existing methods. The
developed web-based platform allows users to submit their own unstructured
medical documents and receive accurate, explainable diagnostic results. By
incorporating advanced explainability techniques, the system ensures
transparent and reliable predictions, fostering user trust and confidence.
Extensive evaluations confirm that the proposed method surpasses current
state-of-the-art models in predictive accuracy while offering practical utility
in clinical settings. This work addresses the urgent need for reliable,
explainable, and privacy-preserving artificial intelligence solutions,
representing a significant advancement in intelligent medical document analysis
for real-world healthcare applications. The code can be found at
\href{https://github.com/leitro/Differential-Diagnosis-LoRA}{https://github.com/leitro/Differential-Diagnosis-LoRA}.},
 author = {Lei Kang and Xuanshuo Fu and Oriol Ramos Terrades and Javier Vazquez-Corral and Ernest Valveny and Dimosthenis Karatzas},
 citations = {},
 comment = {Accepted at ICDAR 2025},
 doi = {10.1007/978-3-032-04624-6_36},
 eprint = {2506.19702v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis},
 url = {http://arxiv.org/abs/2506.19702v1},
 year = {2025}
}

@article{2506.20425v2,
 abstract = {Linear mixed models (LMMs), which incorporate fixed and random effects, are
key tools for analyzing heterogeneous data, such as in personalized medicine.
Nowadays, this type of data is increasingly wide, sometimes containing
thousands of candidate predictors, necessitating sparsity for prediction and
interpretation. However, existing sparse learning methods for LMMs do not scale
well beyond tens or hundreds of predictors, leaving a large gap compared with
sparse methods for linear models, which ignore random effects. This paper
closes the gap with a new $\ell_0$ regularized method for LMM subset selection
that can run on datasets containing thousands of predictors in seconds to
minutes. On the computational front, we develop a coordinate descent algorithm
as our main workhorse and provide a guarantee of its convergence. We also
develop a local search algorithm to help traverse the nonconvex optimization
surface. Both algorithms readily extend to subset selection in generalized LMMs
via a penalized quasi-likelihood approximation. On the statistical front, we
provide a finite-sample bound on the Kullback-Leibler divergence of the new
method. We then demonstrate its excellent performance in experiments involving
synthetic and real datasets.},
 author = {Ryan Thompson and Matt P. Wand and Joanna J. J. Wang},
 citations = {4},
 comment = {},
 doi = {10.1111/biom.13707},
 eprint = {2506.20425v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Scalable Subset Selection in Linear Mixed Models},
 url = {http://arxiv.org/abs/2506.20425v2},
 year = {2025}
}

@article{2506.20494v1,
 abstract = {Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.},
 author = {Qihang Jin and Enze Ge and Yuhang Xie and Hongying Luo and Junhao Song and Ziqian Bi and Chia Xin Liang and Jibin Guan and Joe Yeong and Junfeng Hao},
 citations = {0},
 comment = {},
 doi = {10.32657/10356/182226},
 eprint = {2506.20494v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Multimodal Representation Learning and Fusion},
 url = {http://arxiv.org/abs/2506.20494v1},
 year = {2025}
}

@article{2506.20589v3,
 abstract = {Recent developments in the Internet of Bio-Nano Things (IoBNT) are laying the
groundwork for innovative applications across the healthcare sector.
Nanodevices designed to operate within the body, managed remotely via the
internet, are envisioned to promptly detect and actuate on potential diseases.
In this vision, an inherent challenge arises due to the limited capabilities of
individual nanosensors; specifically, nanosensors must communicate with one
another to collaborate as a cluster. Aiming to research the boundaries of the
clustering capabilities, this survey emphasizes data-driven communication
strategies in molecular communication (MC) channels as a means of linking
nanosensors. Relying on the flexibility and robustness of machine learning (ML)
methods to tackle the dynamic nature of MC channels, the MC research community
frequently refers to neural network (NN) architectures. This interdisciplinary
research field encompasses various aspects, including the use of NNs to
facilitate communication in MC environments, their implementation at the
nanoscale, explainable approaches for NNs, and dataset generation for training.
Within this survey, we provide a comprehensive analysis of fundamental
perspectives on recent trends in NN architectures for MC, the feasibility of
their implementation at the nanoscale, applied explainable artificial
intelligence (XAI) techniques, and the accessibility of datasets along with
best practices for their generation. Additionally, we offer open-source code
repositories that illustrate NN-based methods to support reproducible research
for key MC scenarios. Finally, we identify emerging research challenges, such
as robust NN architectures, biologically integrated NN modules, and scalable
training strategies.},
 author = {Jorge Torres Gómez and Pit Hofmann and Lisa Y. Debus and Osman Tugay Başaran and Sebastian Lotter and Roya Khanzadeh and Stefan Angerbauer and Bige Deniz Unluturk and Sergi Abadal and Werner Haselmayr and Frank H. P. Fitzek and Robert Schober and Falko Dressler},
 citations = {},
 comment = {Paper submitted to IEEE Communications Surveys & Tutorials},
 doi = {},
 eprint = {2506.20589v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Communicating Smartly in Molecular Communication Environments: Neural Networks in the Internet of Bio-Nano Things},
 url = {http://arxiv.org/abs/2506.20589v3},
 year = {2025}
}

@article{2506.22358v1,
 abstract = {The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.},
 author = {Varvara Kalokyri and Nikolaos S. Tachos and Charalampos N. Kalantzopoulos and Stelios Sfakianakis and Haridimos Kondylakis and Dimitrios I. Zaridis and Sara Colantonio and Daniele Regge and Nikolaos Papanikolaou and The ProCAncer-I consortium and Konstantinos Marias and Dimitrios I. Fotiadis and Manolis Tsiknakis},
 citations = {0},
 comment = {},
 doi = {10.1016/j.csbj.2025.09.041},
 eprint = {2506.22358v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AI Model Passport: Data and System Traceability Framework for Transparent AI in Health},
 url = {http://arxiv.org/abs/2506.22358v1},
 year = {2025}
}

@article{2506.22446v1,
 abstract = {Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.},
 author = {Aakash Tripathi and Asim Waqas and Matthew B. Schabath and Yasin Yilmaz and Ghulam Rasool},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2506.22446v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis},
 url = {http://arxiv.org/abs/2506.22446v1},
 year = {2025}
}

@article{2507.00050v2,
 abstract = {Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.},
 author = {Devin Y. De Silva and Sandareka Wickramanayake and Dulani Meedeniya and Sanka Rasnayaka},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2507.00050v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network},
 url = {http://arxiv.org/abs/2507.00050v2},
 year = {2025}
}

@article{2507.00234v1,
 abstract = {In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.},
 author = {Jiztom Kavalakkatt Francis and Matthew J Darr},
 citations = {},
 comment = {13 pages},
 doi = {},
 eprint = {2507.00234v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations},
 url = {http://arxiv.org/abs/2507.00234v1},
 year = {2025}
}

@article{2507.01017v1,
 abstract = {Human error remains a dominant risk driver in safety-critical sectors such as
nuclear power, aviation, and healthcare, where seemingly minor mistakes can
cascade into catastrophic outcomes. Although decades of research have produced
a rich repertoire of mitigation techniques, persistent limitations: scarce
high-quality data, algorithmic opacity, and residual reliance on expert
judgment, continue to constrain progress. This review synthesizes recent
advances at the intersection of risk-informed decision making, human
reliability assessment (HRA), artificial intelligence (AI), and cognitive
science to clarify how their convergence can curb human-error risk. We first
categorize the principal forms of human error observed in complex
sociotechnical environments and outline their quantitative impact on system
reliability. Next, we examine risk-informed frameworks that embed HRA within
probabilistic and data-driven methodologies, highlighting successes and gaps.
We then survey cognitive and human-performance models, detailing how
mechanistic accounts of perception, memory, and decision-making enrich error
prediction and complement HRA metrics. Building on these foundations, we
critically assess AI-enabled techniques for real-time error detection,
operator-state estimation, and AI-augmented HRA workflows. Across these
strands, a recurring insight emerges: integrating cognitive models with
AI-based analytics inside risk-informed HRA pipelines markedly enhances
predictive fidelity, yet doing so demands richer datasets, transparent
algorithms, and rigorous validation. Finally, we identify promising research
directions, coupling resilience engineering concepts with grounded theory,
operationalizing the iceberg model of incident causation, and establishing
cross-domain data consortia, to foster a multidisciplinary paradigm that
elevates human reliability in high-stakes systems.},
 author = {Xingyu Xiao and Hongxu Zhu and Jingang Liang and Jiejuan Tong and Haitao Wang},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2507.01017v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models},
 url = {http://arxiv.org/abs/2507.01017v1},
 year = {2025}
}

@article{2507.01282v1,
 abstract = {The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.},
 author = {Matthew JY Kang and Wenli Yang and Monica R Roberts and Byeong Ho Kang and Charles B Malpas},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2507.01282v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care},
 url = {http://arxiv.org/abs/2507.01282v1},
 year = {2025}
}

@article{2507.02773v2,
 abstract = {Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.},
 author = {Yuzhang Xie and Hejie Cui and Ziyang Zhang and Jiaying Lu and Kai Shu and Fadi Nahab and Xiao Hu and Carl Yang},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.02773v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs},
 url = {http://arxiv.org/abs/2507.02773v2},
 year = {2025}
}

@article{2507.03310v1,
 abstract = {This paper studies causal discovery in irregularly sampled time series-a
pivotal challenge in high-stakes domains like finance, healthcare, and climate
science, where missing data and inconsistent sampling frequencies distort
causal mechanisms. Traditional methods (e.g., Granger causality, PCMCI) fail to
reconcile multi-scale interactions (e.g., hourly storms vs. decadal climate
shifts), while neural approaches (e.g., CUTS+) lack interpretability, stemming
from a critical gap: existing frameworks either rigidly assume temporal
regularity or aggregate dynamics into opaque representations, neglecting
real-world granularity and auditable logic. To bridge this gap, we propose
ReTimeCausal, a novel integration of Additive Noise Models (ANM) and
Expectation-Maximization (EM) that unifies physics-guided data imputation with
sparse causal inference. Through kernelized sparse regression and structural
constraints, ReTimeCausal iteratively refines missing values (E-step) and
causal graphs (M-step), resolving cross-frequency dependencies and missing data
issues. Extensive experiments on synthetic and real-world datasets demonstrate
that ReTimeCausal outperforms existing state-of-the-art methods under
challenging irregular sampling and missing data conditions.},
 author = {Weihong Li and Anpeng Wu and Kun Kuang and Keting Yin},
 citations = {},
 comment = {12 pages, 2 figures},
 doi = {},
 eprint = {2507.03310v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ReTimeCausal: EM-Augmented Additive Noise Models for Interpretable Causal Discovery in Irregular Time Series},
 url = {http://arxiv.org/abs/2507.03310v1},
 year = {2025}
}

@article{2507.03843v2,
 abstract = {Cost models in healthcare research must balance interpretability, accuracy,
and parameter consistency. However, interpretable models often struggle to
achieve both accuracy and consistency. Ordinary least squares (OLS) models for
high-dimensional regression can be accurate but fail to produce stable
regression coefficients over time when using highly granular ICD-10 diagnostic
codes as predictors. This instability arises because many ICD-10 codes are
infrequent in healthcare datasets. While regularization methods such as Ridge
can address this issue, they risk discarding important predictors. Here, we
demonstrate that reducing the granularity of ICD-10 codes is an effective
regularization strategy within OLS while preserving the representation of all
diagnostic code categories. By truncating ICD-10 codes from seven characters to
six or fewer, we reduce the dimensionality of the regression problem while
maintaining model interpretability and consistency. Mathematically, the merging
of predictors in OLS leads to increased trace of the Hessian matrix, which
reduces the variance of coefficient estimation. Our findings explain why
broader diagnostic groupings like DRGs and HCC codes are favored over highly
granular ICD-10 codes in real-world risk adjustment and cost models.},
 author = {Chi-Ken Lu and David Alonge and Nicole Richardson and Bruno Richard},
 citations = {},
 comment = {Submitted to MLHC 2025},
 doi = {},
 eprint = {2507.03843v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Log-Linear Analytics Approach to Cost Model Regularization for Inpatient Stays through Diagnostic Code Merging},
 url = {http://arxiv.org/abs/2507.03843v2},
 year = {2025}
}

@article{2507.04249v1,
 abstract = {The integration of artificial intelligence (AI) in medical imaging raises
crucial ethical concerns at every stage of its development, from data
collection to deployment. Addressing these concerns is essential for ensuring
that AI systems are developed and implemented in a manner that respects patient
rights and promotes fairness. This study aims to explore the ethical
implications of AI in medical imaging, focusing on five key stages: data
collection, data processing, model training, model evaluation, and deployment.
The goal is to evaluate how these stages adhere to fundamental ethical
principles, including data privacy, fairness, transparency, accountability, and
autonomy. An analytical approach was employed to examine the ethical challenges
associated with each stage of AI development. We reviewed existing literature,
guidelines, and regulations concerning AI ethics in healthcare and identified
critical ethical issues at each stage. The study outlines specific inquiries
and principles for each phase of AI development. The findings highlight key
ethical issues: ensuring patient consent and anonymization during data
collection, addressing biases in model training, ensuring transparency and
fairness during model evaluation, and the importance of continuous ethical
assessments during deployment. The analysis also emphasizes the impact of
accessibility issues on different stakeholders, including private, public, and
third-party entities. The study concludes that ethical considerations must be
systematically integrated into each stage of AI development in medical imaging.
By adhering to these ethical principles, AI systems can be made more robust,
transparent, and aligned with patient care and data control. We propose
tailored ethical inquiries and strategies to support the creation of ethically
sound AI systems in medical imaging.},
 author = {Umer Sadiq Khan and Saif Ur Rehman Khan},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.04249v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Ethics by Design: A Lifecycle Framework for Trustworthy AI in Medical Imaging From Transparent Data Governance to Clinically Validated Deployment},
 url = {http://arxiv.org/abs/2507.04249v1},
 year = {2025}
}

@article{2507.04490v1,
 abstract = {Contextual anomaly detection (CAD) aims to identify anomalies in a target
(behavioral) variable conditioned on a set of contextual variables that
influence the normalcy of the target variable but are not themselves indicators
of anomaly. In many anomaly detection tasks, there exist contextual variables
that influence the normalcy of the target variable but are not themselves
indicators of anomaly. In this work, we propose a novel framework for CAD,
normalcy score (NS), that explicitly models both the aleatoric and epistemic
uncertainties. Built on heteroscedastic Gaussian process regression, our method
regards the Z-score as a random variable, providing confidence intervals that
reflect the reliability of the anomaly assessment. Through experiments on
benchmark datasets and a real-world application in cardiology, we demonstrate
that NS outperforms state-of-the-art CAD methods in both detection accuracy and
interpretability. Moreover, confidence intervals enable an adaptive,
uncertainty-driven decision-making process, which may be very important in
domains such as healthcare.},
 author = {Luca Bindini and Lorenzo Perini and Stefano Nistri and Jesse Davis and Paolo Frasconi},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.04490v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Dealing with Uncertainty in Contextual Anomaly Detection},
 url = {http://arxiv.org/abs/2507.04490v1},
 year = {2025}
}

@article{2507.04881v2,
 abstract = {Brain tumor resection is a highly complex procedure with profound
implications for survival and quality of life. Predicting patient outcomes is
crucial to guide clinicians in balancing oncological control with preservation
of neurological function. However, building reliable prediction models is
severely limited by the rarity of curated datasets that include both pre- and
post-surgery imaging, given the clinical, logistical and ethical challenges of
collecting such data. In this study, we develop a novel framework that
integrates explainable artificial intelligence (XAI) with neuroimaging-based
feature engineering for survival assessment in brain tumor patients. We curated
structural MRI data from 49 patients scanned pre- and post-surgery, providing a
rare resource for identifying survival-related biomarkers. A key methodological
contribution is the development of a global explanation optimizer, which
refines survival-related feature attribution in deep learning models, thereby
improving both the interpretability and reliability of predictions. From a
clinical perspective, our findings provide important evidence that survival
after oncological surgery is influenced by alterations in regions related to
cognitive and sensory functions. These results highlight the importance of
preserving areas involved in decision-making and emotional regulation to
improve long-term outcomes. From a technical perspective, the proposed
optimizer advances beyond state-of-the-art XAI methods by enhancing both the
fidelity and comprehensibility of model explanations, thus reinforcing trust in
the recognition patterns driving survival prediction. This work demonstrates
the utility of XAI-driven neuroimaging analysis in identifying survival-related
variability and underscores its potential to inform precision medicine
strategies in brain tumor treatment.},
 author = {Carmen Jimenez-Mesa and Yizhou Wan and Guilio Sansone and Francisco J. Martinez-Murcia and Javier Ramirez and Pietro Lio and Juan M. Gorriz and Stephen J. Price and John Suckling and Michail Mamalakis},
 citations = {},
 comment = {18 pages, 6 figures},
 doi = {},
 eprint = {2507.04881v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Uncovering Neuroimaging Biomarkers of Brain Tumor Surgery with AI-Driven Methods},
 url = {http://arxiv.org/abs/2507.04881v2},
 year = {2025}
}

@article{2507.05976v1,
 abstract = {The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.},
 author = {Alessandro Umbrico and Guido Bologna and Luca Coraci and Francesca Fracasso and Silvia Gola and Gabriella Cortellessa},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.05976v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Enhancing the Interpretability of Rule-based Explanations through Information Retrieval},
 url = {http://arxiv.org/abs/2507.05976v1},
 year = {2025}
}

@article{2507.06077v1,
 abstract = {This paper tackles the urgent need for efficient energy management in
healthcare facilities, where fluctuating demands challenge operational
efficiency and sustainability. Traditional methods often prove inadequate,
causing inefficiencies and higher costs. To address this, the study presents an
AI-based framework combining Long Short-Term Memory (LSTM), genetic algorithm
(GA), and SHAP (Shapley Additive Explanations), specifically designed for
healthcare energy management. Although LSTM is widely used for time-series
forecasting, its application in healthcare energy prediction remains
underexplored. The results reveal that LSTM significantly outperforms ARIMA and
Prophet models in forecasting complex, non-linear demand patterns. LSTM
achieves a Mean Absolute Error (MAE) of 21.69 and Root Mean Square Error (RMSE)
of 29.96, far better than Prophet (MAE: 59.78, RMSE: 81.22) and ARIMA (MAE:
87.73, RMSE: 125.22), demonstrating superior performance. The genetic algorithm
is applied to optimize model parameters and improve load balancing strategies,
enabling adaptive responses to real-time energy fluctuations. SHAP analysis
further enhances model transparency by explaining the influence of different
features on predictions, fostering trust in decision-making processes. This
integrated LSTM-GA-SHAP approach offers a robust solution for improving
forecasting accuracy, boosting energy efficiency, and advancing sustainability
in healthcare facilities. Future research may explore real-time deployment and
hybridization with reinforcement learning for continuous optimization. Overall,
the study establishes a solid foundation for using AI in healthcare energy
management, highlighting its scalability, efficiency, and resilience potential.},
 author = {Iman Rahimi and Isha Patel},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.06077v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AI-Based Demand Forecasting and Load Balancing for Optimising Energy use in Healthcare Systems: A real case study},
 url = {http://arxiv.org/abs/2507.06077v1},
 year = {2025}
}

@article{2507.07271v2,
 abstract = {The Average Treatment Effect (ATE) is a foundational metric in causal
inference, widely used to assess intervention efficacy in randomized controlled
trials (RCTs). However, in many applications -- particularly in healthcare --
this static summary fails to capture the nuanced dynamics of treatment effects
that vary with both dose and time. We propose a framework for modelling
treatment effect trajectories as smooth surfaces over dose and time, enabling
the extraction of clinically actionable insights such as onset time, peak
effect, and duration of benefit. To ensure interpretability, robustness, and
verifiability -- key requirements in high-stakes domains -- we adapt
SemanticODE, a recent framework for interpretable trajectory modelling, to the
causal setting where treatment effects are never directly observed. Our
approach decouples the estimation of trajectory shape from the specification of
clinically relevant properties (e.g., maxima, inflection points), supporting
domain-informed priors, post-hoc editing, and transparent analysis. We show
that our method yields accurate, interpretable, and editable models of
treatment dynamics, facilitating both rigorous causal analysis and practical
decision-making.},
 author = {Julianna Piskorz and Krzysztof Kacprzyk and Harry Amad and Mihaela van der Schaar},
 citations = {},
 comment = {Presented at the Actionable Interpretability Workshop at ICML 2025},
 doi = {},
 eprint = {2507.07271v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Beyond the ATE: Interpretable Modelling of Treatment Effects over Dose and Time},
 url = {http://arxiv.org/abs/2507.07271v2},
 year = {2025}
}

@article{2507.08330v2,
 abstract = {Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.},
 author = {Nikita Malik and Pratinav Seth and Neeraj Kumar Singh and Chintan Chitroda and Vinay Kumar Sankarapu},
 citations = {},
 comment = {Accepted at The 1st MICCAI Workshop on Efficient Medical AI 2025},
 doi = {},
 eprint = {2507.08330v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretability-Aware Pruning for Efficient Medical Image Analysis},
 url = {http://arxiv.org/abs/2507.08330v2},
 year = {2025}
}

@article{2507.09092v1,
 abstract = {With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27},
 author = {Ram S Iyer and Narayan S Iyer and Rugmini Ammal P},
 citations = {0},
 comment = {12 pages, 10 figures},
 doi = {},
 eprint = {2507.09092v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks},
 url = {http://arxiv.org/abs/2507.09092v1},
 year = {2025}
}

@article{2507.09173v1,
 abstract = {Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.},
 author = {Mengjie Chen and Ming Zhang and Cunquan Qu},
 citations = {0},
 comment = {},
 doi = {10.1145/3711896.3737163},
 eprint = {2507.09173v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations},
 url = {http://arxiv.org/abs/2507.09173v1},
 year = {2025}
}

@article{2507.09470v1,
 abstract = {This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.},
 author = {Mingchuan Yang and Ziyuan Huang},
 citations = {0},
 comment = {29 pages, 5 tables},
 doi = {},
 eprint = {2507.09470v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models},
 url = {http://arxiv.org/abs/2507.09470v1},
 year = {2025}
}

@article{2507.11694v1,
 abstract = {We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.},
 author = {Maximiliano Hormazábal Lagos and Álvaro Bueno Sáez and Pedro Alonso Doval and Jorge Alcalde Vesteiro and Héctor Cerezo-Costas},
 citations = {},
 comment = {This work has been accepted for presentation at the 24nd Portuguese
  Conference on Artificial Intelligence (EPIA 2025) and will be published in
  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)
  series. Please cite the published version when available},
 doi = {10.1007/978-3-032-05179-0_26},
 eprint = {2507.11694v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ExpliCIT-QA: Explainable Code-Based Image Table Question Answering},
 url = {http://arxiv.org/abs/2507.11694v1},
 year = {2025}
}

@article{2507.12192v2,
 abstract = {Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.},
 author = {Victor F. Lopes de Souza and Karima Bakhti and Sofiane Ramdani and Denis Mottet and Abdelhak Imoussaten},
 citations = {},
 comment = {},
 doi = {10.32614/cran.package.evclust},
 eprint = {2507.12192v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable Evidential Clustering},
 url = {http://arxiv.org/abs/2507.12192v2},
 year = {2025}
}

@article{2507.12950v2,
 abstract = {Interpretability can improve the safety, transparency and trust of AI models,
which is especially important in healthcare applications where decisions often
carry significant consequences. Mechanistic interpretability, particularly
through the use of sparse autoencoders (SAEs), offers a promising approach for
uncovering human-interpretable features within large transformer-based models.
In this study, we apply Matryoshka-SAE to the radiology-specialised multimodal
large language model, MAIRA-2, to interpret its internal representations. Using
large-scale automated interpretability of the SAE features, we identify a range
of clinically relevant concepts - including medical devices (e.g., line and
tube placements, pacemaker presence), pathologies such as pleural effusion and
cardiomegaly, longitudinal changes and textual features. We further examine the
influence of these features on model behaviour through steering, demonstrating
directional control over generations with mixed success. Our results reveal
practical and methodological challenges, yet they offer initial insights into
the internal concepts learned by MAIRA-2 - marking a step toward deeper
mechanistic understanding and interpretability of a radiology-adapted
multimodal large language model, and paving the way for improved model
transparency. We release the trained SAEs and interpretations:
https://huggingface.co/microsoft/maira-2-sae.},
 author = {Kenza Bouzid and Shruthi Bannur and Felix Meissen and Daniel Coelho de Castro and Anton Schwaighofer and Javier Alvarez-Valle and Stephanie L. Hyland},
 citations = {0},
 comment = {Actionable Interpretability Workshop at ICML 2025. 24 pages, 7
  figures, 5 tables},
 doi = {},
 eprint = {2507.12950v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Insights into a radiology-specialised multimodal large language model with sparse autoencoders},
 url = {http://arxiv.org/abs/2507.12950v2},
 year = {2025}
}

@article{2507.14176v1,
 abstract = {Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.},
 author = {Andrés Morales-Forero and Lili J. Rueda and Ronald Herrera and Samuel Bassetto and Eric Coatanea},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.14176v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection},
 url = {http://arxiv.org/abs/2507.14176v1},
 year = {2025}
}

@article{2507.14680v1,
 abstract = {Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.},
 author = {Xinheng Lyu and Yuci Liang and Wenting Chen and Meidan Ding and Jiaqi Yang and Guolin Huang and Daokun Zhang and Xiangjian He and Linlin Shen},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.14680v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis},
 url = {http://arxiv.org/abs/2507.14680v1},
 year = {2025}
}

@article{2507.17134v1,
 abstract = {Global health emergencies, such as the COVID-19 pandemic, have exposed
critical weaknesses in traditional medical supply chains, including
inefficiencies in resource allocation, lack of transparency, and poor
adaptability to dynamic disruptions. This paper presents a novel hybrid
framework that integrates blockchain technology with a decentralized, large
language model (LLM) powered multi-agent negotiation system to enhance the
resilience and accountability of medical supply chains during crises. In this
system, autonomous agents-representing manufacturers, distributors, and
healthcare institutions-engage in structured, context-aware negotiation and
decision-making processes facilitated by LLMs, enabling rapid and ethical
allocation of scarce medical resources. The off-chain agent layer supports
adaptive reasoning and local decision-making, while the on-chain blockchain
layer ensures immutable, transparent, and auditable enforcement of decisions
via smart contracts. The framework also incorporates a formal cross-layer
communication protocol to bridge decentralized negotiation with institutional
enforcement. A simulation environment emulating pandemic scenarios evaluates
the system's performance, demonstrating improvements in negotiation efficiency,
fairness of allocation, supply chain responsiveness, and auditability. This
research contributes an innovative approach that synergizes blockchain trust
guarantees with the adaptive intelligence of LLM-driven agents, providing a
robust and scalable solution for critical supply chain coordination under
uncertainty.},
 author = {Mariam ALMutairi and Hyungmin Kim},
 citations = {},
 comment = {11 pages, 6 figure},
 doi = {},
 eprint = {2507.17134v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Resilient Multi-Agent Negotiation for Medical Supply Chains:Integrating LLMs and Blockchain for Transparent Coordination},
 url = {http://arxiv.org/abs/2507.17134v1},
 year = {2025}
}

@article{2507.17539v1,
 abstract = {Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.},
 author = {Xinyao Liu and Diping Song},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2507.17539v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning},
 url = {http://arxiv.org/abs/2507.17539v1},
 year = {2025}
}

@article{2507.17717v1,
 abstract = {AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.},
 author = {Karen Zhou and John Giorgi and Pranav Mani and Peng Xu and Davis Liang and Chenhao Tan},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.17717v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes},
 url = {http://arxiv.org/abs/2507.17717v1},
 year = {2025}
}

@article{2507.17799v1,
 abstract = {Voice disorders affect a significant portion of the population, and the
ability to diagnose them using automated, non-invasive techniques would
represent a substantial advancement in healthcare, improving the quality of
life of patients. Recent studies have demonstrated that artificial intelligence
models, particularly Deep Neural Networks (DNNs), can effectively address this
task. However, due to their complexity, the decision-making process of such
models often remain opaque, limiting their trustworthiness in clinical
contexts. This paper investigates an alternative approach based on Explainable
AI (XAI), a field that aims to improve the interpretability of DNNs by
providing different forms of explanations. Specifically, this works focuses on
concept-based models such as Concept Bottleneck Model (CBM) and Concept
Embedding Model (CEM) and how they can achieve performance comparable to
traditional deep learning methods, while offering a more transparent and
interpretable decision framework.},
 author = {Davide Ghia and Gabriele Ciravegna and Alkis Koudounas and Marco Fantini and Erika Crosetti and Giovanni Succo and Tania Cerquitelli},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2507.17799v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Concept-based approach to Voice Disorder Detection},
 url = {http://arxiv.org/abs/2507.17799v1},
 year = {2025}
}

@article{2507.17979v1,
 abstract = {Identifying the factors driving data shifts in tabular datasets is a
significant challenge for analysis and decision support systems, especially
those focusing on healthcare. Privacy rules restrict data access, and noise
from complex processes hinders analysis. To address this challenge, we propose
SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular
Learning) that (i) extracts privacy-compliant data summary statistics, (ii)
employs twin XGBoost models to disentangle intervention signals from noise with
assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted
decision tree to identify interpretable segments responsible for the shift.
Unlike existing analyses which may ignore noise or require full data access for
LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe
summary statistics. Demonstrating its real-world efficacy, for a MEPS panel
dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of
0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and
statistical tests (F1=0.20) in identifying the segment receiving the subsidy.
Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM,
SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with
injected observational noise, whereas baseline average F1 scores range from
0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,
privacy-conscious workflow that is empirically robust to observational noise.},
 author = {Shubham Mohole and Sainyam Galhotra},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.17979v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning},
 url = {http://arxiv.org/abs/2507.17979v1},
 year = {2025}
}

@article{2507.18115v1,
 abstract = {Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.},
 author = {Soorya Ram Shimgekar and Shayan Vassef and Abhay Goyal and Navin Kumar and Koustuv Saha},
 citations = {5},
 comment = {10 pages, 5 figures, 2 tables, BIBM conference},
 doi = {},
 eprint = {2507.18115v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Agentic AI framework for End-to-End Medical Data Inference},
 url = {http://arxiv.org/abs/2507.18115v1},
 year = {2025}
}

@article{2507.19489v1,
 abstract = {The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.},
 author = {Simone Bendazzoli and Sanna Persson and Mehdi Astaraki and Sebastian Pettersson and Vitali Grozman and Rodrigo Moreno},
 citations = {},
 comment = {26 pages, 12 figures},
 doi = {},
 eprint = {2507.19489v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation},
 url = {http://arxiv.org/abs/2507.19489v1},
 year = {2025}
}

@article{2507.19498v1,
 abstract = {Large language models (LLMs) show promise for tailored healthcare
communication but face challenges in interpretability and multi-task
integration particularly for domain-specific needs like myopia, and their
real-world effectiveness as patient education tools has yet to be demonstrated.
Here, we introduce ChatMyopia, an LLM-based AI agent designed to address text
and image-based inquiries related to myopia. To achieve this, ChatMyopia
integrates an image classification tool and a retrieval-augmented knowledge
base built from literature, expert consensus, and clinical guidelines. Myopic
maculopathy grading task, single question examination and human evaluations
validated its ability to deliver personalized, accurate, and safe responses to
myopia-related inquiries with high scalability and interpretability. In a
randomized controlled trial (n=70, NCT06607822), ChatMyopia significantly
improved patient satisfaction compared to traditional leaflets, enhancing
patient education in accuracy, empathy, disease awareness, and patient-eyecare
practitioner communication. These findings highlight ChatMyopia's potential as
a valuable supplement to enhance patient education and improve satisfaction
with medical services in primary eye care settings.},
 author = {Yue Wu and Xiaolan Chen and Weiyi Zhang and Shunming Liu and Wing Man Rita Sum and Xinyuan Wu and Xianwen Shang and Chea-su Kee and Mingguang He and Danli Shi},
 citations = {},
 comment = {35 pages, 4 figures, 1 table},
 doi = {},
 eprint = {2507.19498v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings},
 url = {http://arxiv.org/abs/2507.19498v1},
 year = {2025}
}

@article{2507.19885v1,
 abstract = {Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.},
 author = {Cesar Augusto Madid Truyts and Amanda Gomes Rabelo and Gabriel Mesquita de Souza and Daniel Scaldaferri Lages and Adriano Jose Pereira and Uri Adrian Prync Flato and Eduardo Pontes dos Reis and Joaquim Edson Vieira and Paulo Sergio Panse Silveira and Edson Amaro Junior},
 citations = {},
 comment = {},
 doi = {10.2139/ssrn.5316224},
 eprint = {2507.19885v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam},
 url = {http://arxiv.org/abs/2507.19885v1},
 year = {2025}
}

@article{2507.20491v1,
 abstract = {Recent advances in large language models (LLMs) have significantly enhanced
question-answering (QA) capabilities, particularly in open-domain contexts.
However, in closed-domain scenarios such as education, healthcare, and law,
users demand not only accurate answers but also transparent reasoning and
explainable decision-making processes. While neural-symbolic (NeSy) frameworks
have emerged as a promising solution, leveraging LLMs for natural language
understanding and symbolic systems for formal reasoning, existing approaches
often rely on large-scale models and exhibit inefficiencies in translating
natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based
Joint-Embedding Predictive Architecture), a lightweight yet effective framework
for converting natural language into first-order logic (NL2FOL). Drawing
inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by
efficiently generating logic representations, while the Z3 solver operates as
System 2, enabling robust logical inference. To rigorously evaluate the
NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework
comprising three custom metrics: conversion score, reasoning score, and
Spearman rho score, which collectively capture the quality of logical
translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA
achieves competitive performance with significantly lower computational
overhead compared to larger LLM-based systems. Our findings highlight the
potential of structured, interpretable reasoning frameworks for building
efficient and explainable QA systems in specialized domains.},
 author = {Tuan Bui and Trong Le and Phat Thai and Sang Nguyen and Minh Hua and Ngan Pham and Thang Bui and Tho Quan},
 citations = {},
 comment = {8 pages, 3 figures. Accepted at the International Joint Conference on
  Neural Networks (IJCNN) 2025, Workshop on Trustworthiness and Reliability in
  Neuro-Symbolic AI. https://2025.ijcnn.org},
 doi = {},
 eprint = {2507.20491v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems},
 url = {http://arxiv.org/abs/2507.20491v1},
 year = {2025}
}

@article{2507.20840v1,
 abstract = {Deep clustering uncovers hidden patterns and groups in complex time series
data, yet its opaque decision-making limits use in safety-critical settings.
This survey offers a structured overview of explainable deep clustering for
time series, collecting current methods and their real-world applications. We
thoroughly discuss and compare peer-reviewed and preprint papers through
application domains across healthcare, finance, IoT, and climate science. Our
analysis reveals that most work relies on autoencoder and attention
architectures, with limited support for streaming, irregularly sampled, or
privacy-preserved series, and interpretability is still primarily treated as an
add-on. To push the field forward, we outline six research opportunities: (1)
combining complex networks with built-in interpretability; (2) setting up
clear, faithfulness-focused evaluation metrics for unsupervised explanations;
(3) building explainers that adapt to live data streams; (4) crafting
explanations tailored to specific domains; (5) adding human-in-the-loop methods
that refine clusters and explanations together; and (6) improving our
understanding of how time series clustering models work internally. By making
interpretability a primary design goal rather than an afterthought, we propose
the groundwork for the next generation of trustworthy deep clustering time
series analytics.},
 author = {Udo Schlegel and Gabriel Marques Tavares and Thomas Seidl},
 citations = {},
 comment = {14 pages, accepted at TempXAI Workshop at ECML-PKDD 2025},
 doi = {},
 eprint = {2507.20840v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards Explainable Deep Clustering for Time Series Data},
 url = {http://arxiv.org/abs/2507.20840v1},
 year = {2025}
}

@article{2507.20872v1,
 abstract = {Alzheimer's disease affects over 55 million people worldwide and is projected
to more than double by 2050, necessitating rapid, accurate, and scalable
diagnostics. However, existing approaches are limited because they cannot
achieve clinically acceptable accuracy, generalization across datasets,
robustness to missing modalities, and explainability all at the same time. This
inability to satisfy all these requirements simultaneously undermines their
reliability in clinical settings. We propose OmniBrain, a multimodal framework
that integrates brain MRI, radiomics, gene expression, and clinical data using
a unified model with cross-attention and modality dropout. OmniBrain achieves
$92.2 \pm 2.4\%$accuracy on the ANMerge dataset and generalizes to the MRI-only
ADNI dataset with $70.4 \pm 2.7\%$ accuracy, outperforming unimodal and prior
multimodal approaches. Explainability analyses highlight neuropathologically
relevant brain regions and genes, enhancing clinical trust. OmniBrain offers a
robust, interpretable, and practical solution for real-world Alzheimer's
diagnosis.},
 author = {Ahmed Sharshar and Yasser Ashraf and Tameem Bakr and Salma Hassan and Hosam Elgendy and Mohammad Yaqub and Mohsen Guizani},
 citations = {0},
 comment = {Published in Third Workshop on Computer Vision for Automated Medical
  Diagnosis CVAMD 2025 in ICCV 2025},
 doi = {},
 eprint = {2507.20872v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of Alzheimer's Disease},
 url = {http://arxiv.org/abs/2507.20872v1},
 year = {2025}
}

@article{2507.21706v1,
 abstract = {Distinguishing pathogenic mutations from benign polymorphisms remains a
critical challenge in precision medicine. EnTao-GPM, developed by Fudan
University and BioMap, addresses this through three innovations: (1)
Cross-species targeted pre-training on disease-relevant mammalian genomes
(human, pig, mouse), leveraging evolutionary conservation to enhance
interpretation of pathogenic motifs, particularly in non-coding regions; (2)
Germline mutation specialization via fine-tuning on ClinVar and HGMD, improving
accuracy for both SNVs and non-SNVs; (3) Interpretable clinical framework
integrating DNA sequence embeddings with LLM-based statistical explanations to
provide actionable insights. Validated against ClinVar, EnTao-GPM demonstrates
superior accuracy in mutation classification. It revolutionizes genetic testing
by enabling faster, more accurate, and accessible interpretation for clinical
diagnostics (e.g., variant assessment, risk identification, personalized
treatment) and research, advancing personalized medicine.},
 author = {Zekai Lin and Haoran Sun and Yucheng Guo and Yujie Yang and Yanwen Wang and Bozhen Hu and Chonghang Ye and Qirong Yang and Fan Zhong and Xiaoming Zhang and Lei Liu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2507.21706v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {EnTao-GPM: DNA Foundation Model for Predicting the Germline Pathogenic Mutations},
 url = {http://arxiv.org/abs/2507.21706v1},
 year = {2025}
}

@article{2507.21882v1,
 abstract = {As Artificial Intelligence (AI) becomes increasingly embedded in healthcare
technologies, understanding the maturity of AI in patient-centric applications
is critical for evaluating its trustworthiness, transparency, and real-world
impact. In this study, we investigate the integration and maturity of AI
feature integration in 116 patient-centric healthcare applications. Using Large
Language Models (LLMs), we extracted key functional features, which are then
categorized into different stages of the Gartner AI maturity model. Our results
show that over 86.21\% of applications remain at the early stages of AI
integration, while only 13.79% demonstrate advanced AI integration.},
 author = {Elmira Onagh and Alireza Davoodi and Maleknaz Nayebi},
 citations = {2},
 comment = {Paper published in COMPSAC 2025},
 doi = {10.1109/compsac65507.2025.00281},
 eprint = {2507.21882v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {The Impact of Foundational Models on Patient-Centric e-Health Systems},
 url = {http://arxiv.org/abs/2507.21882v1},
 year = {2025}
}

@article{2507.22464v1,
 abstract = {Accurate and interpretable prediction of estimated glomerular filtration rate
(eGFR) is essential for managing chronic kidney disease (CKD) and supporting
clinical decisions. Recent advances in Large Multimodal Models (LMMs) have
shown strong potential in clinical prediction tasks due to their ability to
process visual and textual information. However, challenges related to
deployment cost, data privacy, and model reliability hinder their adoption. In
this study, we propose a collaborative framework that enhances the performance
of open-source LMMs for eGFR forecasting while generating clinically meaningful
explanations. The framework incorporates visual knowledge transfer, abductive
reasoning, and a short-term memory mechanism to enhance prediction accuracy and
interpretability. Experimental results show that the proposed framework
achieves predictive performance and interpretability comparable to proprietary
models. It also provides plausible clinical reasoning processes behind each
prediction. Our method sheds new light on building AI systems for healthcare
that combine predictive accuracy with clinically grounded interpretability.},
 author = {Peng-Yi Wu and Pei-Cing Huang and Ting-Yu Chen and Chantung Ku and Ming-Yen Lin and Yihuang Kang},
 citations = {0},
 comment = {},
 doi = {10.1109/iri66576.2025.00059},
 eprint = {2507.22464v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards Interpretable Renal Health Decline Forecasting via Multi-LMM Collaborative Reasoning Framework},
 url = {http://arxiv.org/abs/2507.22464v1},
 year = {2025}
}

@article{2507.22940v2,
 abstract = {We present a novel framework addressing a critical vulnerability in Large
Language Models (LLMs): the prevalence of factual inaccuracies within
intermediate reasoning steps despite correct final answers. This phenomenon
poses substantial risks in high-stakes domains including healthcare, legal
analysis, and scientific research, where erroneous yet confidently presented
reasoning can mislead users into dangerous decisions. Our framework integrates
three core components: (1) a specialized fact-checking classifier trained on
counterfactually augmented data to detect subtle factual inconsistencies within
reasoning chains; (2) an enhanced Group Relative Policy Optimization (GRPO)
reinforcement learning approach that balances factuality, coherence, and
structural correctness through multi-dimensional rewards; and (3) a mechanistic
interpretability method examining how factuality improvements manifest in model
activations during reasoning processes. Extensive evaluation across multi
state-of-the-art models reveals concerning patterns: even leading models like
Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of only 81.93% and
82.57% respectively. Our approach significantly enhances factual robustness (up
to 49.90% improvement) while maintaining or improving performance on
challenging benchmarks including Math-500, AIME-2024, and GPQA. Furthermore,
our neural activation-level analysis provides actionable insights into how
factual enhancements reshape reasoning trajectories within model architectures,
establishing foundations for future training methodologies that explicitly
target factual robustness through activation-guided optimization.},
 author = {Rui Jiao and Yue Zhang and Jinku Li},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2507.22940v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes},
 url = {http://arxiv.org/abs/2507.22940v2},
 year = {2025}
}

@article{2508.01316v1,
 abstract = {Distal myopathy represents a genetically heterogeneous group of skeletal
muscle disorders with broad clinical manifestations, posing diagnostic
challenges in radiology. To address this, we propose a novel multimodal
attention-aware fusion architecture that combines features extracted from two
distinct deep learning models, one capturing global contextual information and
the other focusing on local details, representing complementary aspects of the
input data. Uniquely, our approach integrates these features through an
attention gate mechanism, enhancing both predictive performance and
interpretability. Our method achieves a high classification accuracy on the
BUSI benchmark and a proprietary distal myopathy dataset, while also generating
clinically relevant saliency maps that support transparent decision-making in
medical diagnosis. We rigorously evaluated interpretability through (1)
functionally grounded metrics, coherence scoring against reference masks and
incremental deletion analysis, and (2) application-grounded validation with
seven expert radiologists. While our fusion strategy boosts predictive
performance relative to single-stream and alternative fusion strategies, both
quantitative and qualitative evaluations reveal persistent gaps in anatomical
specificity and clinical usefulness of the interpretability. These findings
highlight the need for richer, context-aware interpretability methods and
human-in-the-loop feedback to meet clinicians' expectations in real-world
diagnostic settings.},
 author = {Mohsen Abbaspour Onari and Lucie Charlotte Magister and Yaoxin Wu and Amalia Lupi and Dario Creazzo and Mattia Tordin and Luigi Di Donatantonio and Emilio Quaia and Chao Zhang and Isel Grau and Marco S. Nobile and Yingqian Zhang and Pietro Liò},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.01316v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Multimodal Attention-Aware Fusion for Diagnosing Distal Myopathy: Evaluating Model Interpretability and Clinician Trust},
 url = {http://arxiv.org/abs/2508.01316v1},
 year = {2025}
}

@article{2508.01388v1,
 abstract = {Explainability remains a critical challenge in artificial intelligence (AI)
systems, particularly in high stakes domains such as healthcare, finance, and
decision support, where users must understand and trust automated reasoning.
Traditional explainability methods such as feature importance and post-hoc
justifications often fail to capture the cognitive processes that underlie
human decision making, leading to either too technical or insufficiently
meaningful explanations. We propose a novel appraisal based framework inspired
by the Component Process Model (CPM) for explainability to address this gap.
While CPM has traditionally been applied to emotion research, we use its
appraisal component as a cognitive model for generating human aligned
explanations. By structuring explanations around key appraisal dimensions such
as relevance, implications, coping potential, and normative significance our
framework provides context sensitive, cognitively meaningful justifications for
AI decisions. This work introduces a new paradigm for generating intuitive,
human-centred explanations in AI driven systems by bridging cognitive science
and explainable AI.},
 author = {Rukshani Somarathna and Madhawa Perera and Tom Gedeon and Matt Adcock},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.01388v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {An Appraisal-Based Approach to Human-Centred Explanations},
 url = {http://arxiv.org/abs/2508.01388v1},
 year = {2025}
}

@article{2508.01956v1,
 abstract = {Electronic health records (EHRs) contain rich unstructured clinical notes
that could enhance predictive modeling, yet extracting meaningful features from
these notes remains challenging. Current approaches range from labor-intensive
manual clinician feature generation (CFG) to fully automated representational
feature generation (RFG) that lack interpretability and clinical relevance.
Here we introduce SNOW (Scalable Note-to-Outcome Workflow), a modular
multi-agent system powered by large language models (LLMs) that autonomously
generates structured clinical features from unstructured notes without human
intervention. We evaluated SNOW against manual CFG, clinician-guided LLM
approaches, and RFG methods for predicting 5-year prostate cancer recurrence in
147 patients from Stanford Healthcare. While manual CFG achieved the highest
performance (AUC-ROC: 0.771), SNOW matched this performance (0.761) without
requiring any clinical expertise, significantly outperforming both baseline
features alone (0.691) and all RFG approaches. The clinician-guided LLM method
also performed well (0.732) but still required expert input. SNOW's specialized
agents handle feature discovery, extraction, validation, post-processing, and
aggregation, creating interpretable features that capture complex clinical
information typically accessible only through manual review. Our findings
demonstrate that autonomous LLM systems can replicate expert-level feature
engineering at scale, potentially transforming how clinical ML models leverage
unstructured EHR data while maintaining the interpretability essential for
clinical deployment.},
 author = {Jiayi Wang and Jacqueline Jil Vallon and Neil Panjwani and Xi Ling and Sushmita Vij and Sandy Srinivas and John Leppert and Mark K. Buyyounouski and Mohsen Bayati},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.01956v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Agent-Based Feature Generation from Clinical Notes for Outcome Prediction},
 url = {http://arxiv.org/abs/2508.01956v1},
 year = {2025}
}

@article{2508.02574v1,
 abstract = {Arabic-language patient feedback remains under-analysed because dialect
diversity and scarce aspect-level sentiment labels hinder automated assessment.
To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that
merges ChatGPT pseudo-labelling with targeted human review to build the first
explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence
is annotated with an aspect and sentiment label (positive, negative, or
neutral), forming a pioneering Arabic dataset aligned with healthcare themes,
with ChatGPT-generated rationales provided for each label to enhance
transparency. To evaluate the impact of annotation quality on model
performance, we created three versions of the training data: a fully supervised
set with all labels reviewed by humans, a semi-supervised set with 50% human
review, and an unsupervised set with only machine-generated labels. We
fine-tuned two transformer models on these datasets for both aspect and
sentiment classification. Experimental results show that our Arabic-specific
model achieved high accuracy even with minimal human supervision, reflecting
only a minor performance drop when using ChatGPT-only labels. Reducing the
number of aspect classes notably improved classification metrics across the
board. These findings demonstrate an effective, scalable approach to Arabic
aspect-based sentiment analysis (SA) in healthcare, combining large language
model annotation with human expertise to produce a robust and explainable
dataset. Future directions include generalisation across hospitals, prompt
refinement, and interpretable data-driven modelling.},
 author = {Eman Alamoudi and Ellis Solaiman},
 citations = {2},
 comment = {},
 doi = {},
 eprint = {2508.02574v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare},
 url = {http://arxiv.org/abs/2508.02574v1},
 year = {2025}
}

@article{2508.03278v1,
 abstract = {High throughput experimentation tools, machine learning (ML) methods, and
open material databases are radically changing the way new materials are
discovered. From the experimentally driven approach in the past, we are moving
quickly towards the artificial intelligence (AI) driven approach, realizing the
'inverse design' capabilities that allow the discovery of new materials given
the desired properties. This review aims to discuss different principles of
AI-driven generative models that are applicable for materials discovery,
including different materials representations available for this purpose. We
will also highlight specific applications of generative models in designing new
catalysts, semiconductors, polymers, or crystals while addressing challenges
such as data scarcity, computational cost, interpretability, synthesizability,
and dataset biases. Emerging approaches to overcome limitations and integrate
AI with experimental workflows will be discussed, including multimodal models,
physics informed architectures, and closed-loop discovery systems. This review
aims to provide insights for researchers aiming to harness AI's transformative
potential in accelerating materials discovery for sustainability, healthcare,
and energy innovation.},
 author = {Albertus Denny Handoko and Riko I Made},
 citations = {},
 comment = {Review Article in the Thematic Issue on Artificial Intelligence for
  Materials Discovery in World Scientific Annual Review of Functional Materials},
 doi = {10.1142/s2810922825400018},
 eprint = {2508.03278v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Artificial Intelligence and Generative Models for Materials Discovery -- A Review},
 url = {http://arxiv.org/abs/2508.03278v1},
 year = {2025}
}

@article{2508.03436v1,
 abstract = {We introduce AI on the Pulse, a real-world-ready anomaly detection system
that continuously monitors patients using a fusion of wearable sensors, ambient
intelligence, and advanced AI models. Powered by UniTS, a state-of-the-art
(SoTA) universal time-series model, our framework autonomously learns each
patient's unique physiological and behavioral patterns, detecting subtle
deviations that signal potential health risks. Unlike classification methods
that require impractical, continuous labeling in real-world scenarios, our
approach uses anomaly detection to provide real-time, personalized alerts for
reactive home-care interventions. Our approach outperforms 12 SoTA anomaly
detection methods, demonstrating robustness across both high-fidelity medical
devices (ECG) and consumer wearables, with a ~ 22% improvement in F1 score.
However, the true impact of AI on the Pulse lies in @HOME, where it has been
successfully deployed for continuous, real-world patient monitoring. By
operating with non-invasive, lightweight devices like smartwatches, our system
proves that high-quality health monitoring is possible without clinical-grade
equipment. Beyond detection, we enhance interpretability by integrating LLMs,
translating anomaly scores into clinically meaningful insights for healthcare
professionals.},
 author = {Davide Gabrielli and Bardh Prenkaj and Paola Velardi and Stefano Faralli},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2508.03436v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AI on the Pulse: Real-Time Health Anomaly Detection with Wearable and Ambient Intelligence},
 url = {http://arxiv.org/abs/2508.03436v1},
 year = {2025}
}

@article{2508.03718v1,
 abstract = {U.S. health insurance is complex, and inadequate understanding and limited
access to justice have dire implications for the most vulnerable. Advances in
natural language processing present an opportunity to support efficient,
case-specific understanding, and to improve access to justice and healthcare.
Yet existing corpora lack context necessary for assessing even simple cases. We
collect and release a corpus of reputable legal and medical text related to
U.S. health insurance. We also introduce an outcome prediction task for health
insurance appeals designed to support regulatory and patient self-help
applications, and release a labeled benchmark for our task, and models trained
on it.},
 author = {Mike Gartner},
 citations = {0},
 comment = {22 pages, 7 figures},
 doi = {},
 eprint = {2508.03718v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding},
 url = {http://arxiv.org/abs/2508.03718v1},
 year = {2025}
}

@article{2508.03739v1,
 abstract = {Early and accurate detection of the bone fracture is paramount to initiating
treatment as early as possible and avoiding any delay in patient treatment and
outcomes. Interpretation of X-ray image is a time consuming and error prone
task, especially when resources for such interpretation are limited by lack of
radiology expertise. Additionally, deep learning approaches used currently,
typically suffer from misclassifications and lack interpretable explanations to
clinical use. In order to overcome these challenges, we propose an automated
framework of bone fracture detection using a VGG-19 model modified to our
needs. It incorporates sophisticated preprocessing techniques that include
Contrast Limited Adaptive Histogram Equalization (CLAHE), Otsu's thresholding,
and Canny edge detection, among others, to enhance image clarity as well as to
facilitate the feature extraction. Therefore, we use Grad-CAM, an Explainable
AI method that can generate visual heatmaps of the model's decision making
process, as a type of model interpretability, for clinicians to understand the
model's decision making process. It encourages trust and helps in further
clinical validation. It is deployed in a real time web application, where
healthcare professionals can upload X-ray images and get the diagnostic
feedback within 0.5 seconds. The performance of our modified VGG-19 model
attains 99.78\% classification accuracy and AUC score of 1.00, making it
exceptionally good. The framework provides a reliable, fast, and interpretable
solution for bone fracture detection that reasons more efficiently for
diagnoses and better patient care.},
 author = {Md. Ehsanul Haque and Abrar Fahim and Shamik Dey and Syoda Anamika Jahan and S. M. Jahidul Islam and Sakib Rokoni and Md Sakib Morshed},
 citations = {},
 comment = {Accepted and presented at THE 16th INTERNATIONAL IEEE CONFERENCE ON
  COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT), held at IIT
  Indore, Madhya Pradesh, India},
 doi = {},
 eprint = {2508.03739v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Modified VGG19-Based Framework for Accurate and Interpretable Real-Time Bone Fracture Detection},
 url = {http://arxiv.org/abs/2508.03739v1},
 year = {2025}
}

@article{2508.04325v1,
 abstract = {Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.},
 author = {Zizhan Ma and Wenxuan Wang and Guo Yu and Yiu-Fai Cheung and Meidan Ding and Jie Liu and Wenting Chen and Linlin Shen},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.04325v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models},
 url = {http://arxiv.org/abs/2508.04325v1},
 year = {2025}
}

@article{2508.04735v1,
 abstract = {Retinal detachment (RD) is a vision-threatening condition that requires
timely intervention to preserve vision. Macular involvement -- whether the
macula is still intact (macula-intact) or detached (macula-detached) -- is the
key determinant of visual outcomes and treatment urgency. Point-of-care
ultrasound (POCUS) offers a fast, non-invasive, cost-effective, and accessible
imaging modality widely used in diverse clinical settings to detect RD.
However, ultrasound image interpretation is limited by a lack of expertise
among healthcare providers, especially in resource-limited settings. Deep
learning offers the potential to automate ultrasound-based assessment of RD.
However, there are no ML ultrasound algorithms currently available for clinical
use to detect RD and no prior research has been done on assessing macular
status using ultrasound in RD cases -- an essential distinction for surgical
prioritization. Moreover, no public dataset currently supports macular-based RD
classification using ultrasound video clips. We introduce Eye Retinal
DEtachment ultraSound, ERDES, the first open-access dataset of ocular
ultrasound clips labeled for (i) presence of retinal detachment and (ii)
macula-intact versus macula-detached status. The dataset is intended to
facilitate the development and evaluation of machine learning models for
detecting retinal detachment. We also provide baseline benchmarks using
multiple spatiotemporal convolutional neural network (CNN) architectures. All
clips, labels, and training code are publicly available at
https://osupcvlab.github.io/ERDES/.},
 author = {Pouyan Navard and Yasemin Ozkut and Srikar Adhikari and Elaine Situ-LaCasse and Josie Acuña and Adrienne Yarnish and Alper Yilmaz},
 citations = {},
 comment = {Under Review, https://github.com/OSUPCVLab/ERDES},
 doi = {},
 eprint = {2508.04735v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ERDES: A Benchmark Video Dataset for Retinal Detachment and Macular Status Classification in Ocular Ultrasound},
 url = {http://arxiv.org/abs/2508.04735v1},
 year = {2025}
}

@article{2508.05581v1,
 abstract = {Large language models (LLMs) have demonstrated remarkable capabilities for
medical question answering and programming, but their potential for generating
interpretable computable phenotypes (CPs) is under-explored. In this work, we
investigate whether LLMs can generate accurate and concise CPs for six clinical
phenotypes of varying complexity, which could be leveraged to enable scalable
clinical decision support to improve care for patients with hypertension. In
addition to evaluating zero-short performance, we propose and test a
synthesize, execute, debug, instruct strategy that uses LLMs to generate and
iteratively refine CPs using data-driven feedback. Our results show that LLMs,
coupled with iterative learning, can generate interpretable and reasonably
accurate programs that approach the performance of state-of-the-art ML methods
while requiring significantly fewer training examples.},
 author = {Guilherme Seidyo Imai Aldeia and Daniel S. Herman and William G. La Cava},
 citations = {},
 comment = {To appear in PMLR, Volume 298, Machine Learning for Healthcare, 2025},
 doi = {},
 eprint = {2508.05581v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models},
 url = {http://arxiv.org/abs/2508.05581v1},
 year = {2025}
}

@article{2508.05753v1,
 abstract = {Explainable Artificial Intelligence (AI) methods are designed to provide
information about how AI-based models make predictions. In healthcare, there is
a widespread expectation that these methods will provide relevant and accurate
information about a model's inner-workings to different stakeholders (ranging
from patients and healthcare providers to AI and medical guideline developers).
This is a challenging endeavour since what qualifies as relevant information
may differ greatly depending on the stakeholder. For many stakeholders,
relevant explanations are causal in nature, yet, explainable AI methods are
often not able to deliver this information. Using the Describe-Predict-Explain
framework, we argue that Explainable AI methods are good descriptive tools, as
they may help to describe how a model works but are limited in their ability to
explain why a model works in terms of true underlying biological mechanisms and
cause-and-effect relations. This limits the suitability of explainable AI
methods to provide actionable advice to patients or to judge the face validity
of AI-based models.},
 author = {Alex Carriero and Anne de Hond and Bram Cappers and Fernando Paulovich and Sanne Abeln and Karel GM Moons and Maarten van Smeden},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2508.05753v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable AI in Healthcare: to Explain, to Predict, or to Describe?},
 url = {http://arxiv.org/abs/2508.05753v1},
 year = {2025}
}

@article{2508.06352v1,
 abstract = {Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.},
 author = {Christian Meske and Justin Brenne and Erdi Uenal and Sabahat Oelcer and Ayseguel Doganguen},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.06352v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI},
 url = {http://arxiv.org/abs/2508.06352v1},
 year = {2025}
}

@article{2508.06406v4,
 abstract = {Blockchain-enabled federated learning (BCFL) addresses fundamental challenges
of trust, privacy, and coordination in collaborative AI systems. This chapter
provides comprehensive architectural analysis of BCFL systems through a
systematic four-dimensional taxonomy examining coordination structures,
consensus mechanisms, storage architectures, and trust models. We analyze
design patterns from blockchain-verified centralized coordination to fully
decentralized peer-to-peer networks, evaluating trade-offs in scalability,
security, and performance. Through detailed examination of consensus mechanisms
designed for federated learning contexts, including Proof of Quality and Proof
of Federated Learning, we demonstrate how computational work can be repurposed
from arbitrary cryptographic puzzles to productive machine learning tasks. The
chapter addresses critical storage challenges by examining multi-tier
architectures that balance blockchain's transaction constraints with neural
networks' large parameter requirements while maintaining cryptographic
integrity. A technical case study of the TrustMesh framework illustrates
practical implementation considerations in BCFL systems through distributed
image classification training, demonstrating effective collaborative learning
across IoT devices with highly non-IID data distributions while maintaining
complete transparency and fault tolerance. Analysis of real-world deployments
across healthcare consortiums, financial services, and IoT security
applications validates the practical viability of BCFL systems, achieving
performance comparable to centralized approaches while providing enhanced
security guarantees and enabling new models of trustless collaborative
intelligence.},
 author = {Murtaza Rangwala and KR Venugopal and Rajkumar Buyya},
 citations = {},
 comment = {32 pages, 6 figures, chapter for edited book (Federated Learning:
  Foundations and Applications)},
 doi = {},
 eprint = {2508.06406v4},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Blockchain-Enabled Federated Learning},
 url = {http://arxiv.org/abs/2508.06406v4},
 year = {2025}
}

@article{2508.07308v1,
 abstract = {HealthBranches is a novel benchmark dataset for medical Question-Answering
(Q&A), specifically designed to evaluate complex reasoning in Large Language
Models (LLMs). This dataset is generated through a semi-automated pipeline that
transforms explicit decision pathways from medical source into realistic
patient cases with associated questions and answers. Covering 4,063 case
studies across 17 healthcare topics, each data point is based on clinically
validated reasoning chains. HealthBranches supports both open-ended and
multiple-choice question formats and uniquely includes the full reasoning path
for each Q&A. Its structured design enables robust evaluation of LLMs'
multi-step inference capabilities, including their performance in structured
Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a
foundation for the development of more trustworthy, interpretable, and
clinically reliable LLMs in high-stakes domains while also serving as a
valuable resource for educational purposes.},
 author = {Cristian Cosentino and Annamaria Defilippo and Marco Dossena and Christopher Irwin and Sara Joubbi and Pietro Liò},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.07308v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways},
 url = {http://arxiv.org/abs/2508.07308v1},
 year = {2025}
}

@article{2508.09187v1,
 abstract = {Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.},
 author = {Almustapha A. Wakili and Babajide J. Asaju and Woosub Jung},
 citations = {},
 comment = {},
 doi = {10.2139/ssrn.5107785},
 eprint = {2508.09187v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring},
 url = {http://arxiv.org/abs/2508.09187v1},
 year = {2025}
}

@article{2508.09362v1,
 abstract = {Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.},
 author = {Md. Milon Islam and Md Rezwanul Haque and S M Taslim Uddin Raju and Fakhri Karray},
 citations = {},
 comment = {Accepted for the IEEE/CVF International Conference on Computer Vision
  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025},
 doi = {},
 eprint = {2508.09362v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition},
 url = {http://arxiv.org/abs/2508.09362v1},
 year = {2025}
}

@article{2508.09468v1,
 abstract = {Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.},
 author = {Muhammad Sakib Khan Inan and Kewen Liao},
 citations = {},
 comment = {Accepted for publication at IJCAI 2025},
 doi = {10.24963/ijcai.2024/1025},
 eprint = {2508.09468v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries},
 url = {http://arxiv.org/abs/2508.09468v1},
 year = {2025}
}

@article{2508.09639v1,
 abstract = {Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.},
 author = {Akshat Dubey and Aleksandar Anžel and Bahar İlgen and Georges Hattab},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2508.09639v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles},
 url = {http://arxiv.org/abs/2508.09639v1},
 year = {2025}
}

@article{2508.09719v1,
 abstract = {Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.},
 author = {Anish Narain and Ritam Majumdar and Nikita Narayanan and Dominic Marshall and Sonali Parbhoo},
 citations = {},
 comment = {32 pages, 7 figures, accepted at Machine Learning for Healthcare
  Conference (MLHC) 2025},
 doi = {},
 eprint = {2508.09719v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models},
 url = {http://arxiv.org/abs/2508.09719v1},
 year = {2025}
}

@article{2508.10020v1,
 abstract = {Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.},
 author = {Chuan Li and Qianyi Zhao and Fengran Mo and Cen Chen},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.10020v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models},
 url = {http://arxiv.org/abs/2508.10020v1},
 year = {2025}
}

@article{2508.10311v1,
 abstract = {Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.},
 author = {Xuan Li and Jialiang Dong and Raymond Wong},
 citations = {},
 comment = {8 pages, 5 figures, 28th European Conference on Artificial
  Intelligence (ECAI-2025)},
 doi = {},
 eprint = {2508.10311v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis},
 url = {http://arxiv.org/abs/2508.10311v1},
 year = {2025}
}

@article{2508.10501v2,
 abstract = {Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.},
 author = {Yushi Feng and Junye Du and Yingying Hong and Qifan Wang and Lequan Yu},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.10501v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning},
 url = {http://arxiv.org/abs/2508.10501v2},
 year = {2025}
}

@article{2508.11181v1,
 abstract = {Accurate and scalable cancer diagnosis remains a critical challenge in modern
pathology, particularly for malignancies such as breast, prostate, bone, and
cervical, which exhibit complex histological variability. In this study, we
propose a transformer-based deep learning framework for multi-class tumor
classification in histopathological images. Leveraging a fine-tuned Vision
Transformer (ViT) architecture, our method addresses key limitations of
conventional convolutional neural networks, offering improved performance,
reduced preprocessing requirements, and enhanced scalability across tissue
types. To adapt the model for histopathological cancer images, we implement a
streamlined preprocessing pipeline that converts tiled whole-slide images into
PyTorch tensors and standardizes them through data normalization. This ensures
compatibility with the ViT architecture and enhances both convergence stability
and overall classification performance. We evaluate our model on four benchmark
datasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and
SipakMed (cervical) dataset -- demonstrating consistent outperformance over
existing deep learning methods. Our approach achieves classification accuracies
of 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical
cancers respectively, with area under the ROC curve (AUC) scores exceeding 99%
across all datasets. These results confirm the robustness, generalizability,
and clinical potential of transformer-based architectures in digital pathology.
Our work represents a significant advancement toward reliable, automated, and
interpretable cancer diagnosis systems that can alleviate diagnostic burdens
and improve healthcare outcomes.},
 author = {Faisal Ahmed},
 citations = {},
 comment = {13 pages, 3 Figures},
 doi = {},
 eprint = {2508.11181v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis},
 url = {http://arxiv.org/abs/2508.11181v1},
 year = {2025}
}

@article{2508.11186v2,
 abstract = {Human Activity Recognition (HAR) plays a critical role in numerous
applications, including healthcare monitoring, fitness tracking, and smart
environments. Traditional deep learning (DL) approaches, while effective, often
require extensive parameter tuning and may lack interpretability. In this work,
we investigate the use of a single three-axis accelerometer and the
Kolmogorov--Arnold Network (KAN) for HAR tasks, leveraging its ability to model
complex nonlinear relationships with improved interpretability and parameter
efficiency. The MotionSense dataset, containing smartphone-based motion sensor
signals across various physical activities, is employed to evaluate the
proposed approach. Our methodology involves preprocessing and normalization of
accelerometer and gyroscope data, followed by KAN-based feature learning and
classification. Experimental results demonstrate that the KAN achieves
competitive or superior classification performance compared to conventional
deep neural networks, while maintaining a significantly reduced parameter
count. This highlights the potential of KAN architectures as an efficient and
interpretable alternative for real-world HAR systems. The open-source
implementation of the proposed framework is available at the Project's GitHub
Repository.},
 author = {Mohammad Alikhani},
 citations = {0},
 comment = {},
 doi = {10.1109/i-coste63786.2024.11025066},
 eprint = {2508.11186v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {KAN-HAR: A Human activity recognition based on Kolmogorov-Arnold Network},
 url = {http://arxiv.org/abs/2508.11186v2},
 year = {2025}
}

@article{2508.13256v1,
 abstract = {Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.},
 author = {Yuting Zhang and Karina V. Bunting and Asgher Champsi and Xiaoxia Wang and Wenqi Lu and Alexander Thorley and Sandeep S Hothi and Zhaowen Qiu and Dipak Kotecha and Jinming Duan},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.13256v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support},
 url = {http://arxiv.org/abs/2508.13256v1},
 year = {2025}
}

@article{2508.13548v1,
 abstract = {Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public
health threat within hospitals as well as long-term care facilities. Better
understanding of MRSA risks, evaluation of interventions and forecasting MRSA
rates are important public health problems. Existing forecasting models rely on
statistical or neural network approaches, which lack epidemiological
interpretability, and have limited performance. Mechanistic epidemic models are
difficult to calibrate and limited in incorporating diverse datasets. We
present CALYPSO, a hybrid framework that integrates neural networks with
mechanistic metapopulation models to capture the spread dynamics of infectious
diseases (i.e., MRSA) across healthcare and community settings. Our model
leverages patient-level insurance claims, commuting data, and healthcare
transfer patterns to learn region- and time-specific parameters governing MRSA
spread. This enables accurate, interpretable forecasts at multiple spatial
resolutions (county, healthcare facility, region, state) and supports
counterfactual analyses of infection control policies and outbreak risks. We
also show that CALYPSO improves statewide forecasting performance by over 4.5%
compared to machine learning baselines, while also identifying high-risk
regions and cost-effective strategies for allocating infection prevention
resources.},
 author = {Rituparna Datta and Jiaming Cui and Gregory R. Madden and Anil Vullikanti},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.13548v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics},
 url = {http://arxiv.org/abs/2508.13548v1},
 year = {2025}
}

@article{2508.15569v1,
 abstract = {Understanding the nuanced performance of machine learning models is essential
for responsible deployment, especially in high-stakes domains like healthcare
and finance. This paper introduces a novel framework, Conformalized Exceptional
Model Mining, which combines the rigor of Conformal Prediction with the
explanatory power of Exceptional Model Mining (EMM). The proposed framework
identifies cohesive subgroups within data where model performance deviates
exceptionally, highlighting regions of both high confidence and high
uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model
Performance Evaluation), which quantifies uncertainty through conformal
prediction's rigorous coverage guarantees. By defining a new quality measure,
Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with
exceptional performance patterns in multi-class classification and regression
tasks. Experimental results across diverse datasets demonstrate the framework's
effectiveness in uncovering interpretable subgroups that provide critical
insights into model behavior. This work lays the groundwork for enhancing model
interpretability and reliability, advancing the state-of-the-art in explainable
AI and uncertainty quantification.},
 author = {Xin Du and Sikun Yang and Wouter Duivesteijn and Mykola Pechenizkiy},
 citations = {},
 comment = {Accepted by ECML-PKDD},
 doi = {10.1007/978-3-032-06066-2_31},
 eprint = {2508.15569v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well},
 url = {http://arxiv.org/abs/2508.15569v1},
 year = {2025}
}

@article{2508.15746v1,
 abstract = {Accurate diagnosis with medical large language models is hindered by
knowledge gaps and hallucinations. Retrieval and tool-augmented methods help,
but their impact is limited by weak use of external knowledge and poor
feedback-reasoning traceability. To address these challenges, We introduce
Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement
learning (RL) that enables steer tracebale retrieval-augmented reasoning for
medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical
retrieval corpus comprising patient records and reliable medical knowledge
sources to support retrieval-aware reasoning across diagnostic scenarios. More
crutially, we frame the LLM as the core agent and the retrieval corpus as its
environment, using tailored rewards on format, retrieval, reasoning structure,
and diagnostic accuracy, thereby evolving the agentic RAG policy from
large-scale data through RL.
  Experiments demonstrate that our end-to-end agentic RL training framework
consistently outperforms prompt-engineering and training-free RAG approaches
across multiple data centers. After training, Deep-DxSearch achieves
substantial gains in diagnostic accuracy, surpassing strong diagnostic
baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks
for both common and rare disease diagnosis under in-distribution and
out-of-distribution settings. Moreover, ablation studies on reward design and
retrieval corpus components confirm their critical roles, underscoring the
uniqueness and effectiveness of our approach compared with traditional
implementations. Finally, case studies and interpretability analyses highlight
improvements in Deep-DxSearch's diagnostic policy, providing deeper insight
into its performance gains and supporting clinicians in delivering more
reliable and precise preliminary diagnoses. See
https://github.com/MAGIC-AI4Med/Deep-DxSearch.},
 author = {Qiaoyu Zheng and Yuze Sun and Chaoyi Wu and Weike Zhao and Pengcheng Qiu and Yongguo Yu and Kun Sun and Yanfeng Wang and Ya Zhang and Weidi Xie},
 citations = {},
 comment = {35 pages, 5 figures, 3 tables},
 doi = {},
 eprint = {2508.15746v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning},
 url = {http://arxiv.org/abs/2508.15746v1},
 year = {2025}
}

@article{2508.16674v1,
 abstract = {Medical report interpretation plays a crucial role in healthcare, enabling
both patient-facing explanations and effective information flow across clinical
systems. While recent vision-language models (VLMs) and large language models
(LLMs) have demonstrated general document understanding capabilities, there
remains a lack of standardized benchmarks to assess structured interpretation
quality in medical reports. We introduce MedRepBench, a comprehensive benchmark
built from 1,900 de-identified real-world Chinese medical reports spanning
diverse departments, patient demographics, and acquisition formats. The
benchmark is designed primarily to evaluate end-to-end VLMs for structured
medical report understanding. To enable controlled comparisons, we also include
a text-only evaluation setting using high-quality OCR outputs combined with
LLMs, allowing us to estimate the upper-bound performance when character
recognition errors are minimized. Our evaluation framework supports two
complementary protocols: (1) an objective evaluation measuring field-level
recall of structured clinical items, and (2) an automated subjective evaluation
using a powerful LLM as a scoring agent to assess factuality, interpretability,
and reasoning quality. Based on the objective metric, we further design a
reward function and apply Group Relative Policy Optimization (GRPO) to improve
a mid-scale VLM, achieving up to 6% recall gain. We also observe that the
OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and
latency issues, motivating further progress toward robust, fully vision-based
report understanding.},
 author = {Fangxin Shang and Yuan Xia and Dalu Yang and Yahui Wang and Binglin Yang},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.16674v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation},
 url = {http://arxiv.org/abs/2508.16674v1},
 year = {2025}
}

@article{2508.16839v3,
 abstract = {Clinical workflows are fragmented as a patchwork of scripts and task-specific
networks that often handle triage, task selection, and model deployment. These
pipelines are rarely streamlined for data science pipeline, reducing efficiency
and raising operational costs. Workflows also lack data-driven model
identification (from imaging/tabular inputs) and standardized delivery of model
outputs. In response, we present a practical, healthcare-first framework that
uses a single vision-language model (VLM) in two complementary roles. First
(Solution 1), the VLM acts as an aware model-card matcher that routes an
incoming image to the appropriate specialist model via a three-stage workflow
(modality -> primary abnormality -> model-card id). Checks are provided by (i)
stagewise prompts that allow early exit via None/Normal/Other and (ii) a
stagewise answer selector that arbitrates between the top-2 candidates at each
stage, reducing the chance of an incorrect selection and aligning the workflow
with clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on
specialty-specific datasets ensuring a single model covers multiple downstream
tasks within each specialty, maintaining performance while simplifying
deployment. Across gastroenterology, hematology, ophthalmology, and pathology,
our single-model deployment matches or approaches specialized baselines.
  Compared with pipelines composed of many task-specific agents, this approach
shows that one VLM can both decide and do. It may reduce effort by data
scientists, shorten monitoring, increase the transparency of model selection
(with per-stage justifications), and lower integration overhead.},
 author = {Shayan Vassef and Soorya Ram Shimegekar and Abhay Goyal and Koustuv Saha and Pi Zonooz and Navin Kumar},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.16839v3},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Route-and-Execute: Auditable Model-Card Matching and Specialty-Level Deployment},
 url = {http://arxiv.org/abs/2508.16839v3},
 year = {2025}
}

@article{2508.17244v1,
 abstract = {Recent developments in Artificial Intelligence (AI) and their applications in
critical industries such as healthcare, fin-tech and cybersecurity have led to
a surge in research in explainability in AI. Innovative research methods are
being explored to extract meaningful insight from blackbox AI systems to make
the decision-making technology transparent and interpretable. Explainability
becomes all the more critical when AI is used in decision making in domains
like fintech, healthcare and safety critical systems such as cybersecurity and
autonomous vehicles. However, there is still ambiguity lingering on the
reliable evaluations for the users and nature of transparency in the
explanations provided for the decisions made by black-boxed AI. To solve the
blackbox nature of Machine Learning based Intrusion Detection Systems, a
framework is proposed in this paper to give an explanation for IDSs decision
making. This framework uses Local Interpretable Model-Agnostic Explanations
(LIME) coupled with Explain Like I'm five (ELI5) and Decision Tree algorithms
to provide local and global explanations and improve the interpretation of
IDSs. The local explanations provide the justification for the decision made on
a specific input. Whereas, the global explanations provides the list of
significant features and their relationship with attack traffic. In addition,
this framework brings transparency in the field of ML driven IDS that might be
highly significant for wide scale adoption of eXplainable AI in cyber-critical
systems. Our framework is able to achieve 85 percent accuracy in classifying
attack behaviour on UNSW-NB15 dataset, while at the same time displaying the
feature significance ranking of the top 10 features used in the classification.},
 author = {Aoun E Muhammad and Kin-Choong Yow and Nebojsa Bacanin-Dzakula and Muhammad Attique Khan},
 citations = {0},
 comment = {This is the authors accepted manuscript of an article accepted for
  publication in Cluster Computing. The final published version is available
  at: 10.1007/s10586-025-05326-9},
 doi = {10.1007/s10586-025-05326-9},
 eprint = {2508.17244v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems},
 url = {http://arxiv.org/abs/2508.17244v1},
 year = {2025}
}

@article{2508.18313v1,
 abstract = {Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.},
 author = {Zi Cai and Yu Liu and Zhiyao Luo and Tingting Zhu},
 citations = {0},
 comment = {CIKM 2025 Full Paper},
 doi = {},
 eprint = {2508.18313v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions},
 url = {http://arxiv.org/abs/2508.18313v1},
 year = {2025}
}

@article{2508.19096v1,
 abstract = {Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.},
 author = {Yongwoo Song and Minbyul Jeong and Mujeen Sung},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2508.19096v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Trustworthy Agents for Electronic Health Records through Confidence Estimation},
 url = {http://arxiv.org/abs/2508.19096v1},
 year = {2025}
}

@article{2508.19097v1,
 abstract = {The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.},
 author = {Armin Berger and Sarthak Khanna and David Berghaus and Rafet Sifa},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2508.19097v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Reasoning LLMs in the Medical Domain: A Literature Survey},
 url = {http://arxiv.org/abs/2508.19097v1},
 year = {2025}
}

@article{2508.19467v1,
 abstract = {Nonmedical opioid use is an urgent public health challenge, with far-reaching
clinical and social consequences that are often underreported in traditional
healthcare settings. Social media platforms, where individuals candidly share
first-person experiences, offer a valuable yet underutilized source of insight
into these impacts. In this study, we present a named entity recognition (NER)
framework to extract two categories of self-reported consequences from social
media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,
depression) and SocialImpacts (e.g., job loss). To support this task, we
introduce RedditImpacts 2.0, a high-quality dataset with refined annotation
guidelines and a focus on first-person disclosures, addressing key limitations
of prior work. We evaluate both fine-tuned encoder-based models and
state-of-the-art large language models (LLMs) under zero- and few-shot
in-context learning settings. Our fine-tuned DeBERTa-large model achieves a
relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming
LLMs in precision, span accuracy, and adherence to task-specific guidelines.
Furthermore, we show that strong NER performance can be achieved with
substantially less labeled data, emphasizing the feasibility of deploying
robust models in resource-limited settings. Our findings underscore the value
of domain-specific fine-tuning for clinical NLP tasks and contribute to the
responsible development of AI tools that may enhance addiction surveillance,
improve interpretability, and support real-world healthcare decision-making.
The best performing model, however, still significantly underperforms compared
to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap
persists between expert intelligence and current state-of-the-art NER/AI
capabilities for tasks requiring deep domain knowledge.},
 author = {Sumon Kanti Dey and Jeanne M. Powell and Azra Ismail and Jeanmarie Perrone and Abeed Sarker},
 citations = {0},
 comment = {Dataset and code: https://github.com/SumonKantiDey/Reddit_Impacts_NER},
 doi = {},
 eprint = {2508.19467v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset},
 url = {http://arxiv.org/abs/2508.19467v1},
 year = {2025}
}

@article{2508.21540v1,
 abstract = {Process mining has emerged as a powerful analytical technique for
understanding complex healthcare workflows. However, its application faces
significant barriers, including technical complexity, a lack of standardized
approaches, and limited access to practical training resources. We introduce
HealthProcessAI, a GenAI framework designed to simplify process mining
applications in healthcare and epidemiology by providing a comprehensive
wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address
unfamiliarity and improve accessibility, the framework integrates multiple
Large Language Models (LLMs) for automated process map interpretation and
report generation, helping translate technical analyses into outputs that
diverse users can readily understand. We validated the framework using sepsis
progression data as a proof-of-concept example and compared the outputs of five
state-of-the-art LLM models through the OpenRouter platform. To test its
functionality, the framework successfully processed sepsis data across four
proof-of-concept scenarios, demonstrating robust technical performance and its
capability to generate reports through automated LLM analysis. LLM evaluation
using five independent LLMs as automated evaluators revealed distinct model
strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency
scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By
integrating multiple Large Language Models (LLMs) for automated interpretation
and report generation, the framework addresses widespread unfamiliarity with
process mining outputs, making them more accessible to clinicians, data
scientists, and researchers. This structured analytics and AI-driven
interpretation combination represents a novel methodological advance in
translating complex process mining results into potentially actionable insights
for healthcare applications.},
 author = {Eduardo Illueca-Fernandez and Kaile Chen and Fernando Seoane and Farhad Abtahi},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2508.21540v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining},
 url = {http://arxiv.org/abs/2508.21540v1},
 year = {2025}
}

@article{2509.00073v1,
 abstract = {Generative Artificial Intelligence (GenAI), particularly Large Language
Models (LLMs), offer powerful capabilities for interpreting the complex data
landscape in healthcare. In this paper, we present a comprehensive overview of
the capabilities, requirements and applications of GenAI for deriving clinical
insights and improving clinical efficiency. We first provide some background on
the forms and sources of patient data, namely real-time Remote Patient
Monitoring (RPM) streams and traditional Electronic Health Records (EHRs). The
sheer volume and heterogeneity of this combined data present significant
challenges to clinicians and contribute to information overload. In addition,
we explore the potential of LLM-powered applications for improving clinical
efficiency. These applications can enhance navigation of longitudinal patient
data and provide actionable clinical decision support through natural language
dialogue. We discuss the opportunities this presents for streamlining clinician
workflows and personalizing care, alongside critical challenges such as data
integration complexity, ensuring data quality and RPM data reliability,
maintaining patient privacy, validating AI outputs for clinical safety,
mitigating bias, and ensuring clinical acceptance. We believe this work
represents the first summarization of GenAI techniques for managing clinician
data overload due to combined RPM / EHR data complexities.},
 author = {Ankit Shetgaonkar and Dipen Pradhan and Lakshit Arora and Sanjay Surendranath Girija and Shashank Kapoor and Aman Raj},
 citations = {},
 comment = {Accepted at IEEE COMPSAC 2025},
 doi = {10.36227/techrxiv.174953189.97963312/v1},
 eprint = {2509.00073v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis},
 url = {http://arxiv.org/abs/2509.00073v1},
 year = {2025}
}

@article{2509.00744v1,
 abstract = {Distinguishing correlation from causation is a fundamental challenge in
machine intelligence, often representing a critical barrier to building robust
and trustworthy systems. While Pearl's $\mathcal{DO}$-calculus provides a
rigorous framework for causal inference, a parallel challenge lies in its
physical implementation. Here, we apply and experimentally validate a quantum
algorithmic framework for performing causal interventions. Our approach maps
causal networks onto quantum circuits where probabilistic links are encoded by
controlled-rotation gates, and interventions are realized by a structural
remodeling of the circuit -- a physical analogue to Pearl's ``graph surgery''.
We demonstrate the method's efficacy by resolving Simpson's Paradox in a
3-qubit model, and show its scalability by quantifying confounding bias in a
10-qubit healthcare simulation. Critically, we provide a proof-of-principle
experimental validation on an IonQ Aria quantum computer, successfully
reproducing the paradox and its resolution in the presence of real-world noise.
This work establishes a practical pathway for quantum causal inference,
offering a new computational tool to address deep-rooted challenges in
algorithmic fairness and explainable AI (XAI).},
 author = {Pilsung Kang},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.00744v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Quantum Causality: Resolving Simpson's Paradox with $\mathcal{DO}$-Calculus},
 url = {http://arxiv.org/abs/2509.00744v1},
 year = {2025}
}

@article{2509.00846v1,
 abstract = {Explaining machine learning (ML) predictions has become crucial as ML models
are increasingly deployed in high-stakes domains such as healthcare. While
SHapley Additive exPlanations (SHAP) is widely used for model interpretability,
it fails to differentiate between causality and correlation, often
misattributing feature importance when features are highly correlated. We
propose Causal SHAP, a novel framework that integrates causal relationships
into feature attribution while preserving many desirable properties of SHAP. By
combining the Peter-Clark (PC) algorithm for causal discovery and the
Intervention Calculus when the DAG is Absent (IDA) algorithm for causal
strength quantification, our approach addresses the weakness of SHAP.
Specifically, Causal SHAP reduces attribution scores for features that are
merely correlated with the target, as validated through experiments on both
synthetic and real-world datasets. This study contributes to the field of
Explainable AI (XAI) by providing a practical framework for causal-aware model
explanations. Our approach is particularly valuable in domains such as
healthcare, where understanding true causal relationships is critical for
informed decision-making.},
 author = {Woon Yee Ng and Li Rong Wang and Siyuan Liu and Xiuyi Fan},
 citations = {},
 comment = {Published in 2025 International Joint Conference on Neural Networks
  (IJCNN). IEEE, 2025},
 doi = {},
 eprint = {2509.00846v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery},
 url = {http://arxiv.org/abs/2509.00846v1},
 year = {2025}
}

@article{2509.01319v2,
 abstract = {Vital signs, such as heart rate and blood pressure, are critical indicators
of patient health and are widely used in clinical monitoring and
decision-making. While deep learning models have shown promise in forecasting
these signals, their deployment in healthcare remains limited in part because
clinicians must be able to trust and interpret model outputs. Without reliable
uncertainty quantification -- particularly calibrated prediction intervals
(PIs) -- it is unclear whether a forecasted abnormality constitutes a
meaningful warning or merely reflects model noise, hindering clinical
decision-making. To address this, we present two methods for deriving PIs from
the Reconstruction Uncertainty Estimate (RUE), an uncertainty measure
well-suited to vital-sign forecasting due to its sensitivity to data shifts and
support for label-free calibration. Our parametric approach assumes that
prediction errors and uncertainty estimates follow a Gaussian copula
distribution, enabling closed-form PI computation. Our non-parametric approach,
based on k-nearest neighbours (KNN), empirically estimates the conditional
error distribution using similar validation instances. We evaluate these
methods on two large public datasets with minute- and hour-level sampling,
representing high- and low-frequency health signals. Experiments demonstrate
that the Gaussian copula method consistently outperforms conformal prediction
baselines on low-frequency data, while the KNN approach performs best on
high-frequency data. These results underscore the clinical promise of
RUE-derived PIs for delivering interpretable, uncertainty-aware vital sign
forecasts.},
 author = {Li Rong Wang and Thomas C. Henderson and Yew Soon Ong and Yih Yng Ng and Xiuyi Fan},
 citations = {},
 comment = {Accepted at the 25th IEEE International Conference on Data Mining
  (ICDM)},
 doi = {},
 eprint = {2509.01319v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards Trustworthy Vital Sign Forecasting: Leveraging Uncertainty for Prediction Intervals},
 url = {http://arxiv.org/abs/2509.01319v2},
 year = {2025}
}

@article{2509.01885v1,
 abstract = {The extraction of critical patient information from Electronic Health Records
(EHRs) poses significant challenges due to the complexity and unstructured
nature of the data. Traditional machine learning approaches often fail to
capture pertinent details efficiently, making it difficult for clinicians to
utilize these tools effectively in patient care. This paper introduces a novel
approach to extracting the OPQRST assessment from EHRs by leveraging the
capabilities of Large Language Models (LLMs). We propose to reframe the task
from sequence labeling to text generation, enabling the models to provide
reasoning steps that mimic a physician's cognitive processes. This approach
enhances interpretability and adapts to the limited availability of labeled
data in healthcare settings. Furthermore, we address the challenge of
evaluating the accuracy of machine-generated text in clinical contexts by
proposing a modification to traditional Named Entity Recognition (NER) metrics.
This includes the integration of semantic similarity measures, such as the BERT
Score, to assess the alignment between generated text and the clinical intent
of the original records. Our contributions demonstrate a significant
advancement in the use of AI in healthcare, offering a scalable solution that
improves the accuracy and usability of information extraction from EHRs,
thereby aiding clinicians in making more informed decisions and enhancing
patient care outcomes.},
 author = {Zhimeng Luo and Abhibha Gupta and Adam Frisch and Daqing He},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.01885v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning},
 url = {http://arxiv.org/abs/2509.01885v1},
 year = {2025}
}

@article{2509.02034v1,
 abstract = {Appointment scheduling is a great challenge in healthcare operations
management. Appointment rules (AR) provide medical practitioners with a simple
yet effective tool to determine patient appointment times. Genetic programming
(GP) can be used to evolve ARs. However, directly applying GP to design ARs may
lead to rules that are difficult for end-users to interpret and trust. A key
reason is that GP is unaware of the dimensional consistency, which ensures that
the evolved rules align with users' domain knowledge and intuitive
understanding. In this paper, we develop a new dimensionally aware GP algorithm
with dimension repair to evolve ARs with dimensional consistency and high
performance. A key innovation of our method is the dimension repair procedure,
which optimizes the dimensional consistency of an expression tree while
minimizing structural changes and ensuring that its output dimension meets the
problem's requirements. We formulate the task as a mixed-integer linear
programming model that can be efficiently solved using common mathematical
programming methods. With the support of the dimension repair procedure, our
method can explore a wider range of AR structures by temporarily breaking the
dimensional consistency of individuals, and then restoring it without altering
their overall structure, thereby identifying individuals with greater potential
advantages. We evaluated the proposed method in a comprehensive set of
simulated clinics. The experimental results demonstrate that our approach
managed to evolve high-quality ARs that significantly outperform not only the
manually designed ARs but also existing state-of-the-art dimensionally aware GP
methods in terms of both objective values and dimensional consistency. In
addition, we analyzed the semantics of the evolved ARs, providing insight into
the design of more effective and interpretable ARs.},
 author = {Huan Zhang and Yang Wang and Ya-Hui Jia and Yi Mei},
 citations = {0},
 comment = {This work has been submitted to the IEEE for possible publication},
 doi = {},
 eprint = {2509.02034v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Genetic Programming with Model Driven Dimension Repair for Learning Interpretable Appointment Scheduling Rules},
 url = {http://arxiv.org/abs/2509.02034v1},
 year = {2025}
}

@article{2509.02036v1,
 abstract = {Large language models (LLMs) hold transformative potential in healthcare, yet
their capacity to interpret longitudinal patient narratives remains
inadequately explored. Dentistry, with its rich repository of structured
clinical data, presents a unique opportunity to rigorously assess LLMs'
reasoning abilities. While several commercial LLMs already exist, DeepSeek, a
model that gained significant attention earlier this year, has also joined the
competition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini
2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal
dental case vignettes through open-ended clinical tasks. Using 34 standardized
longitudinal periodontal cases (comprising 258 question-answer pairs), we
assessed model performance via automated metrics and blinded evaluations by
licensed dentists. DeepSeek emerged as the top performer, demonstrating
superior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert
ratings (median = 4.5/5 vs. 4.0/5), without significantly compromising
readability. Our study positions DeepSeek as the leading LLM for case analysis,
endorses its integration as an adjunct tool in both medical education and
research, and highlights its potential as a domain-specific agent.},
 author = {Hexian Zhang and Xinyu Yan and Yanqi Yang and Lijian Jin and Ping Yang and Junwen Wang},
 citations = {0},
 comment = {Abstract word count: 171; Total word count: 3130; Total number of
  tables: 2; Total number of figures: 3; Number of references: 32},
 doi = {},
 eprint = {2509.02036v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DeepSeek performs better than other Large Language Models in Dental Cases},
 url = {http://arxiv.org/abs/2509.02036v1},
 year = {2025}
}

@article{2509.03240v1,
 abstract = {Accurate evaluation of event detection in time series is essential for
applications such as stress monitoring with wearable devices, where ground
truth is typically annotated as single-point events, even though the underlying
phenomena are gradual and temporally diffused. Standard metrics like F1 and
point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such
real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$)
that incorporates temporal tolerance, enabling a more robust assessment of
event detection when exact alignment is unrealistic. Empirical analysis in
three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one
experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance
patterns invisible to conventional metrics, while its window size can be
adapted to domain knowledge to avoid overestimation. We show that the choice of
evaluation metric strongly influences the interpretation of model performance:
using predictions from TimesFM, only our temporally tolerant metrics reveal
statistically significant improvements over random and null baselines in the
two in-the-wild use cases. This work addresses key gaps in time series
evaluation and provides practical guidance for healthcare applications where
requirements for temporal precision vary by context.},
 author = {Harald Vilhelm Skat-Rørdam and Sneha Das and Kathrine Sofie Rasmussen and Nicole Nadine Lønfeldt and Line Clemmensen},
 citations = {0},
 comment = {15 pages, 6 figures},
 doi = {},
 eprint = {2509.03240v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric},
 url = {http://arxiv.org/abs/2509.03240v1},
 year = {2025}
}

@article{2509.03294v2,
 abstract = {The increasing availability of personal data has enabled significant advances
in fields such as machine learning, healthcare, and cybersecurity. However,
this data abundance also raises serious privacy concerns, especially in light
of powerful re-identification attacks and growing legal and ethical demands for
responsible data use. Differential privacy (DP) has emerged as a principled,
mathematically grounded framework for mitigating these risks. This review
provides a comprehensive survey of DP, covering its theoretical foundations,
practical mechanisms, and real-world applications. It explores key algorithmic
tools and domain-specific challenges - particularly in privacy-preserving
machine learning and synthetic data generation. The report also highlights
usability issues and the need for improved communication and transparency in DP
systems. Overall, the goal is to support informed adoption of DP by researchers
and practitioners navigating the evolving landscape of data privacy.},
 author = {Napsu Karmitsa and Antti Airola and Tapio Pahikkala and Tinja Pitkämäki},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.03294v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Comprehensive Guide to Differential Privacy: From Theory to User Expectations},
 url = {http://arxiv.org/abs/2509.03294v2},
 year = {2025}
}

@article{2509.03626v1,
 abstract = {Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.},
 author = {Zahra Zehtabi Sabeti Moghaddam and Zeinab Dehghani and Maneeha Rani and Koorosh Aslansefat and Bhupesh Kumar Mishra and Rameez Raja Kureshi and Dhavalkumar Thakker},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.03626v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE},
 url = {http://arxiv.org/abs/2509.03626v1},
 year = {2025}
}

@article{2509.04053v1,
 abstract = {Over the past decade, the use of machine learning (ML) models in healthcare
applications has rapidly increased. Despite high performance, modern ML models
do not always capture patterns the end user requires. For example, a model may
predict a non-monotonically decreasing relationship between cancer stage and
survival, keeping all other features fixed. In this paper, we present a
reproducible framework for investigating this misalignment between model
behavior and clinical experiential learning, focusing on the effects of
underspecification of modern ML pipelines. In a prostate cancer outcome
prediction case study, we first identify and address these inconsistencies by
incorporating clinical knowledge, collected by a survey, via constraints into
the ML model, and subsequently analyze the impact on model performance and
behavior across degrees of underspecification. The approach shows that aligning
the ML model with clinical experiential learning is possible without
compromising performance. Motivated by recent literature in generative AI, we
further examine the feasibility of a feedback-driven alignment approach in
non-generative AI clinical risk prediction models through a randomized
experiment with clinicians. Our findings illustrate that, by eliciting
clinicians' model preferences using our proposed methodology, the larger the
difference in how the constrained and unconstrained models make predictions for
a patient, the more apparent the difference is in clinical interpretation.},
 author = {Jacqueline J. Vallon and William Overman and Wanqiao Xu and Neil Panjwani and Xi Ling and Sushmita Vij and Hilary P. Bagshaw and John T. Leppert and Sumit Shah and Geoffrey Sonn and Sandy Srinivas and Erqi Pollom and Mark K. Buyyounouski and Mohsen Bayati},
 citations = {1},
 comment = {},
 doi = {},
 eprint = {2509.04053v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {On Aligning Prediction Models with Clinical Experiential Learning: A Prostate Cancer Case Study},
 url = {http://arxiv.org/abs/2509.04053v1},
 year = {2025}
}

@article{2509.04482v2,
 abstract = {Reliable abstention is critical for retrieval-augmented generation (RAG)
systems, particularly in safety-critical domains such as women's health, where
incorrect answers can lead to harm. We present an energy-based model (EBM) that
learns a smooth energy landscape over a dense semantic corpus of 2.6M
guideline-derived questions, enabling the system to decide when to generate or
abstain. We benchmark the EBM against a calibrated softmax baseline and a
k-nearest neighbour (kNN) density heuristic across both easy and hard
abstention splits, where hard cases are semantically challenging
near-distribution queries. The EBM achieves superior abstention performance
abstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for
softmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives,
performance is comparable across methods, but the EBM's advantage becomes most
pronounced in safety-critical hard distributions. A comprehensive ablation with
controlled negative sampling and fair data exposure shows that robustness stems
primarily from the energy scoring head, while the inclusion or exclusion of
specific negative types (hard, easy, mixed) sharpens decision boundaries but is
not essential for generalisation to hard cases. These results demonstrate that
energy-based abstention scoring offers a more reliable confidence signal than
probability-based softmax confidence, providing a scalable and interpretable
foundation for safe RAG systems.},
 author = {Ravi Shankar and Sheng Wong and Lin Li and Magdalena Bachmann and Alex Silverthorne and Beth Albert and Gabriel Davis Jones},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.04482v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare},
 url = {http://arxiv.org/abs/2509.04482v2},
 year = {2025}
}

@article{2509.05691v1,
 abstract = {Text embedding models are widely used in natural language processing
applications. However, their capability is often benchmarked on tasks that do
not require understanding nuanced numerical information in text. As a result,
it remains unclear whether current embedding models can precisely encode
numerical content, such as numbers, into embeddings. This question is critical
because embedding models are increasingly applied in domains where numbers
matter, such as finance and healthcare. For example, Company X's market share
grew by 2\% should be interpreted very differently from Company X's market
share grew by 20\%, even though both indicate growth in market share. This
study aims to examine whether text embedding models can capture such nuances.
Using synthetic data in a financial context, we evaluate 13 widely used text
embedding models and find that they generally struggle to capture numerical
details accurately. Our further analyses provide deeper insights into embedding
numeracy, informing future research to strengthen embedding model-based NLP
systems with improved capacity for handling numerical content.},
 author = {Ningyuan Deng and Hanyu Duan and Yixuan Tang and Yi Yang},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.05691v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models},
 url = {http://arxiv.org/abs/2509.05691v1},
 year = {2025}
}

@article{2509.06974v1,
 abstract = {Sleep quality significantly impacts well-being. Therefore, healthcare
providers and individuals need accessible and reliable forecasting tools for
preventive interventions. This paper introduces an interpretable,
individualized two-stage adaptive spatial-temporal model for predicting sleep
quality scores. Our proposed framework combines multi-scale convolutional
layers to model spatial interactions across multiple input variables, recurrent
layers and attention mechanisms to capture long-term temporal dependencies, and
a two-stage domain adaptation strategy to enhance generalization. The first
adaptation stage is applied during training to mitigate overfitting on the
training set. In the second stage, a source-free test-time adaptation mechanism
is employed to adapt the model to new users without requiring labels. We
conducted various experiments with five input window sizes (3, 5, 7, 9, and 11
days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model
consistently outperformed time series forecasting baseline approaches,
including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The
best performance was achieved with a three-day input window and a one-day
prediction window, yielding a root mean square error (RMSE) of 0.216.
Furthermore, the model demonstrated good predictive performance even for longer
forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction
window), highlighting its practical utility for real-world applications. We
also conducted an explainability analysis to examine how different features
influence sleep quality. These findings proved that the proposed framework
offers a robust, adaptive, and explainable solution for personalized sleep
forecasting using sparse data from commercial wearable devices.},
 author = {Xueyi Wang and Elisabeth Wilhelm},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.06974v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model},
 url = {http://arxiv.org/abs/2509.06974v1},
 year = {2025}
}

@article{2509.07108v1,
 abstract = {Survival analysis is a fundamental tool for modeling time-to-event outcomes
in healthcare. Recent advances have introduced flexible neural network
approaches for improved predictive performance. However, most of these models
do not provide interpretable insights into the association between exposures
and the modeled outcomes, a critical requirement for decision-making in
clinical practice. To address this limitation, we propose Additive Deep Hazard
Analysis Mixtures (ADHAM), an interpretable additive survival model. ADHAM
assumes a conditional latent structure that defines subgroups, each
characterized by a combination of covariate-specific hazard functions. To
select the number of subgroups, we introduce a post-training refinement that
reduces the number of equivalent latent subgroups by merging similar groups. We
perform comprehensive studies to demonstrate ADHAM's interpretability at the
population, subgroup, and individual levels. Extensive experiments on
real-world datasets show that ADHAM provides novel insights into the
association between exposures and outcomes. Further, ADHAM remains on par with
existing state-of-the-art survival baselines in terms of predictive
performance, offering a scalable and interpretable approach to time-to-event
prediction in healthcare.},
 author = {Mert Ketenci and Vincent Jeanselme and Harry Reyes Nieva and Shalmali Joshi and Noémie Elhadad},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.07108v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ADHAM: Additive Deep Hazard Analysis Mixtures for Interpretable Survival Regression},
 url = {http://arxiv.org/abs/2509.07108v1},
 year = {2025}
}

@article{2509.07432v1,
 abstract = {Preterm birth (PTB), defined as delivery before 37 weeks of gestation, is a
leading cause of neonatal mortality and long term health complications. Early
detection is essential for enabling timely medical interventions.
Electrohysterography (EHG) and tocography (TOCO) are promising non invasive
tools for PTB prediction, but prior studies often suffer from class imbalance,
improper oversampling, and reliance on features with limited physiological
relevance. This work presents a machine learning pipeline incorporating robust
preprocessing, physiologically grounded feature extraction, and rigorous
evaluation. Features were extracted from EHG (and TOCO) signals using Mel
frequency cepstral coefficients, statistical descriptors of wavelet
coefficients, and peaks of the normalized power spectrum. Signal quality was
enhanced via Karhunen Lo\`eve Transform (KLT) denoising through eigenvalue
based subspace decomposition. Multiple classifiers, including Logistic
Regression, Support Vector Machines, Random Forest, Gradient Boosting,
Multilayer Perceptron, and CatBoost, were evaluated on the TPEHGT dataset. The
CatBoost classifier with KLT denoising achieved the highest performance on
fixed interval segments of the TPEHGT dataset, reaching 97.28% accuracy and an
AUC of 0.9988. Ablation studies confirmed the critical role of both KLT
denoising and physiologically informed features. Comparative analysis showed
that including TOCO signals did not substantially improve prediction over EHG
alone, highlighting the sufficiency of EHG for PTB detection. These results
demonstrate that combining denoising with domain relevant features can yield
highly accurate, robust, and clinically interpretable models, supporting the
development of cost effective and accessible PTB prediction tools, particularly
in low resource healthcare settings.},
 author = {Senith Jayakody and Kalana Jayasooriya and Sashini Liyanage and Roshan Godaliyadda and Parakrama Ekanayake and Chathura Rathnayake},
 citations = {0},
 comment = {12 pages, 4 figures, 5 tables, manuscript under review},
 doi = {},
 eprint = {2509.07432v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Spectrotemporal Feature Extraction in EHG Signals and Tocograms for Enhanced Preterm Birth Prediction},
 url = {http://arxiv.org/abs/2509.07432v1},
 year = {2025}
}

@article{2509.08402v1,
 abstract = {The integration of the Internet of Things (IoT) in healthcare has
revolutionized patient monitoring and data collection, allowing real-time
tracking of vital signs, remote diagnostics, and automated medical responses.
However, the transmission and storage of sensitive medical data introduce
significant security and privacy challenges. To address these concerns,
blockchain technology provides a decentralized and immutable ledger that
ensures data integrity, , and transparency. Unlike public blockchains, private
blockchains are permissioned; the access is granted only to authorized
participants; they are more suitable for handling confidential healthcare data.
Although blockchain ensures security and trust, it lacks built-in mechanisms to
support flexible and controlled data sharing; This is where Proxy Re-Encryption
(PRE) comes into play. PRE is a cryptographic technique that allows encrypted
data to be re-encrypted for a new recipient without exposing it to
intermediaries. We propose an architecture integrating private blockchain and
PRE to enable secure, traceable, and privacy-preserving data sharing in
IoT-based healthcare systems. Blockchain guarantees tamper proof
record-keeping, while PRE enables fine-grained access control, allowing medical
professionals to securely share patient data without compromising
confidentiality. This combination creates a robust security framework that
enhances trust and efficiency in digital healthcare ecosystems.},
 author = {Abdou-Essamad Jabri and C. Drocourt and Mostafa Azizi and Gil Utard},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.08402v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Leveraging Blockchain and Proxy Re-Encryption to secure Medical IoT Records},
 url = {http://arxiv.org/abs/2509.08402v1},
 year = {2025}
}

@article{2509.08422v2,
 abstract = {Video-based AI systems are increasingly adopted in safety-critical domains
such as autonomous driving and healthcare. However, interpreting their
decisions remains challenging due to the inherent spatiotemporal complexity of
video data and the opacity of deep learning models. Existing explanation
techniques often suffer from limited temporal coherence, insufficient
robustness, and a lack of actionable causal insights. Current counterfactual
explanation methods typically do not incorporate guidance from the target
model, reducing semantic fidelity and practical utility. We introduce Latent
Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework
designed to explain the behavior of video-based AI models. Compared to previous
approaches, LD-ViCE reduces the computational costs of generating explanations
by operating in latent space using a state-of-the-art diffusion model, while
producing realistic and interpretable counterfactuals through an additional
refinement step. Our experiments demonstrate the effectiveness of LD-ViCE
across three diverse video datasets, including EchoNet-Dynamic (cardiac
ultrasound), FERV39k (facial expression), and Something-Something V2 (action
recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving
an increase in R2 score of up to 68% while reducing inference time by half.
Qualitative analysis confirms that LD-ViCE generates semantically meaningful
and temporally coherent explanations, offering valuable insights into the
target model behavior. LD-ViCE represents a valuable step toward the
trustworthy deployment of AI in safety-critical domains.},
 author = {Payal Varshney and Adriano Lucieri and Christoph Balada and Sheraz Ahmed and Andreas Dengel},
 citations = {},
 comment = {30 pages},
 doi = {10.2139/ssrn.4768660},
 eprint = {2509.08422v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations},
 url = {http://arxiv.org/abs/2509.08422v2},
 year = {2025}
}

@article{2509.08617v1,
 abstract = {Tabular data is the foundation of many applications in fields such as finance
and healthcare. Although DNNs tailored for tabular data achieve competitive
predictive performance, they are blackboxes with little interpretability. We
introduce XNNTab, a neural architecture that uses a sparse autoencoder (SAE) to
learn a dictionary of monosemantic features within the latent space used for
prediction. Using an automated method, we assign human-interpretable semantics
to these features. This allows us to represent predictions as linear
combinations of semantically meaningful components. Empirical evaluations
demonstrate that XNNTab attains performance on par with or exceeding that of
state-of-the-art, black-box neural models and classical machine learning
approaches while being fully interpretable.},
 author = {Khawla Elhadri and Jörg Schlötterer and Christin Seifert},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.08617v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards Interpretable Deep Neural Networks for Tabular Data},
 url = {http://arxiv.org/abs/2509.08617v1},
 year = {2025}
}

@article{2509.08679v1,
 abstract = {\textbf{Background:} Machine learning models trained on electronic health
records (EHRs) often degrade across healthcare systems due to distributional
shift. A fundamental but underexplored factor is diagnostic signal decay:
variability in diagnostic quality and consistency across institutions, which
affects the reliability of codes used for training and prediction.
  \textbf{Objective:} To develop a Signal Fidelity Index (SFI) quantifying
diagnostic data quality at the patient level in dementia, and to test SFI-aware
calibration for improving model performance across heterogeneous datasets
without outcome labels.
  \textbf{Methods:} We built a simulation framework generating 2,500 synthetic
datasets, each with 1,000 patients and realistic demographics, encounters, and
coding patterns based on dementia risk factors. The SFI was derived from six
interpretable components: diagnostic specificity, temporal consistency,
entropy, contextual concordance, medication alignment, and trajectory
stability. SFI-aware calibration applied a multiplicative adjustment, optimized
across 50 simulation batches.
  \textbf{Results:} At the optimal parameter ($\alpha$ = 2.0), SFI-aware
calibration significantly improved all metrics (p $<$ 0.001). Gains ranged from
10.3\% for Balanced Accuracy to 32.5\% for Recall, with notable increases in
Precision (31.9\%) and F1-score (26.1\%). Performance approached reference
standards, with F1-score and Recall within 1\% and Balanced Accuracy and
Detection Rate improved by 52.3\% and 41.1\%, respectively.
  \textbf{Conclusions:} Diagnostic signal decay is a tractable barrier to model
generalization. SFI-aware calibration provides a practical, label-free strategy
to enhance prediction across healthcare contexts, particularly for large-scale
administrative datasets lacking outcome labels.},
 author = {Jingya Cheng and Jiazi Tian and Federica Spoto and Alaleh Azhir and Daniel Mork and Hossein Estiri},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.08679v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Signal Fidelity Index-Aware Calibration for Dementia Predictions Across Heterogeneous Real-World Data},
 url = {http://arxiv.org/abs/2509.08679v1},
 year = {2025}
}

@article{2509.08779v1,
 abstract = {Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in
children that can persist into adulthood, affecting social, academic, and
career life. Early diagnosis is crucial for managing these impacts on patients
and the healthcare system but is often labor-intensive and time-consuming. This
paper presents a novel method to improve ADHD diagnosis precision and
timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram
(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive
temporal-spatial characterization, attention modules, and explainability
techniques optimized for EEG signals. ADHDeepNet integrates feature extraction
and refinement processes to enhance ADHD diagnosis. The model was trained and
validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),
employing nested cross-validation for robust performance. The proposed
two-stage methodology uses a 10-fold cross-subject validation strategy.
Initially, each iteration optimizes the model's hyper-parameters with inner
2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various
standard deviations and magnification levels is applied for data augmentation.
ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC
subjects. To clarify model explainability and identify key brain regions and
frequency bands for ADHD diagnosis, we analyzed the learned weights and
activation patterns of the model's primary layers. Additionally, t-distributed
Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding
in interpreting the model's decisions. This study highlights the potential of
DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.},
 author = {Ali Amini and Mohammad Alijanpour and Behnam Latifi and Ali Motie Nasrabadi},
 citations = {},
 comment = {29 pages, 7 figures. Preprint. Correspondence: alijanpour@ucf.edu},
 doi = {},
 eprint = {2509.08779v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals},
 url = {http://arxiv.org/abs/2509.08779v1},
 year = {2025}
}

@article{2509.08961v1,
 abstract = {Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.},
 author = {Md. Sajeebul Islam Sk. and Md Jobayer and Md Mehedi Hasan Shawon and Md. Golam Raibul Alam},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.08961v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis},
 url = {http://arxiv.org/abs/2509.08961v1},
 year = {2025}
}

@article{2509.10073v1,
 abstract = {Survival analysis is a statistical framework for modeling time-to-event data,
particularly valuable in healthcare for predicting outcomes like patient
discharge or recurrence. This study implements and compares several survival
models - including Weibull, Weibull AFT, Weibull AFT with Gamma Frailty, Cox
Proportional Hazards (CoxPH), Random Survival Forest (RSF), and DeepSurv -
using a publicly available breast cancer dataset. This study aims to benchmark
classical, machine learning, and Bayesian survival models in terms of their
predictive performance, interpretability, and suitability for clinical
deployment. The models are evaluated using performance metrics such as the
Concordance Index (C-index) and the Root Mean Squared Error (RMSE). DeepSurv
showed the highest predictive performance, while interpretable models like RSF
and Weibull AFT with Gamma Frailty offered competitive results. We also
explored the implementation of statistical models from a Bayesian perspective,
including frailty models, due to their ability to properly quantify
uncertainty. Notably, frailty models are not readily available in standard
survival analysis libraries, necessitating custom implementation. Our results
demonstrate that interpretable statistical models, when correctly implemented
using parameters that are effectively estimated using a Bayesian approach, can
perform competitively with modern black-box models. These findings illustrate
the trade-offs between model complexity, interpretability, and predictive
power, highlighting the potential of Bayesian survival models in clinical
decision-making settings.},
 author = {Irving Gómez-Méndez and Sivakorn Phromsiri and Ittiphat Kijpaisansak and Settawut Chaithurdthum},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.10073v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Benchmarking Classical, Machine Learning, and Bayesian Survival Models for Clinical Prediction},
 url = {http://arxiv.org/abs/2509.10073v1},
 year = {2025}
}

@article{2509.11507v1,
 abstract = {Decades' advances in digital health technologies, such as electronic health
records, have largely streamlined routine clinical processes. Yet, most these
systems are still hard to learn and use: Clinicians often face the burden of
managing multiple tools, repeating manual actions for each patient, navigating
complicated UI trees to locate functions, and spending significant time on
administration instead of caring for patients. The recent rise of large
language model (LLM) based agents demonstrates exceptional capability in coding
and computer operation, revealing the potential for humans to interact with
operating systems and software not by direct manipulation, but by instructing
agents through natural language. This shift highlights the need for an
abstraction layer, an agent-computer interface, that translates human language
into machine-executable commands. In digital healthcare, however, requires a
more domain-specific abstractions that strictly follow trusted clinical
guidelines and procedural standards to ensure safety, transparency, and
compliance. To address this need, we present \textbf{MedicalOS}, a unified
agent-based operational system designed as such a domain-specific abstract
layer for healthcare. It translates human instructions into pre-defined digital
healthcare commands, such as patient inquiry, history retrieval, exam
management, report generation, referrals, treatment planning, that we wrapped
as off-the-shelf tools using machine languages (e.g., Python, APIs, MCP,
Linux). We empirically validate MedicalOS on 214 patient cases across 22
specialties, demonstrating high diagnostic accuracy and confidence, clinically
sound examination requests, and consistent generation of structured reports and
medication recommendations. These results highlight MedicalOS as a trustworthy
and scalable foundation for advancing workflow automation in clinical practice.},
 author = {Jared Zhu and Junde Wu},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.11507v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MedicalOS: An LLM Agent based Operating System for Digital Healthcare},
 url = {http://arxiv.org/abs/2509.11507v1},
 year = {2025}
}

@article{2509.12227v2,
 abstract = {We propose a unified framework for adaptive routing in multitask, multimodal
prediction settings where data heterogeneity and task interactions vary across
samples. Motivated by applications in psychotherapy where structured
assessments and unstructured clinician notes coexist with partially missing
data and correlated outcomes, we introduce a routing-based architecture that
dynamically selects modality processing pathways and task-sharing strategies on
a per-sample basis. Our model defines multiple modality paths, including raw
and fused representations of text and numeric features and learns to route each
input through the most informative expert combination. Task-specific
predictions are produced by shared or independent heads depending on the
routing decision, and the entire system is trained end-to-end. We evaluate the
model on both synthetic data and real-world psychotherapy notes predicting
depression and anxiety outcomes. Our experiments show that our method
consistently outperforms fixed multitask or single-task baselines, and that the
learned routing policy provides interpretable insights into modality relevance
and task structure. This addresses critical challenges in personalized
healthcare by enabling per-subject adaptive information processing that
accounts for data heterogeneity and task correlations. Applied to
psychotherapy, this framework could improve mental health outcomes, enhance
treatment assignment precision, and increase clinical cost-effectiveness
through personalized intervention strategies.},
 author = {Marzieh Ajirak and Oded Bein and Ellen Rose Bowen and Dora Kanellopoulos and Avital Falk and Faith M. Gunning and Nili Solomonov and Logan Grosenick},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.12227v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction},
 url = {http://arxiv.org/abs/2509.12227v2},
 year = {2025}
}

@article{2509.12534v1,
 abstract = {The increasing prevalence of retinal diseases poses a significant challenge
to the healthcare system, as the demand for ophthalmologists surpasses the
available workforce. This imbalance creates a bottleneck in diagnosis and
treatment, potentially delaying critical care. Traditional methods of
generating medical reports from retinal images rely on manual interpretation,
which is time-consuming and prone to errors, further straining
ophthalmologists' limited resources. This thesis investigates the potential of
Artificial Intelligence (AI) to automate medical report generation for retinal
images. AI can quickly analyze large volumes of image data, identifying subtle
patterns essential for accurate diagnosis. By automating this process, AI
systems can greatly enhance the efficiency of retinal disease diagnosis,
reducing doctors' workloads and enabling them to focus on more complex cases.
The proposed AI-based methods address key challenges in automated report
generation: (1) A multi-modal deep learning approach captures interactions
between textual keywords and retinal images, resulting in more comprehensive
medical reports; (2) Improved methods for medical keyword representation
enhance the system's ability to capture nuances in medical terminology; (3)
Strategies to overcome RNN-based models' limitations, particularly in capturing
long-range dependencies within medical descriptions; (4) Techniques to enhance
the interpretability of the AI-based report generation system, fostering trust
and acceptance in clinical practice. These methods are rigorously evaluated
using various metrics and achieve state-of-the-art performance. This thesis
demonstrates AI's potential to revolutionize retinal disease diagnosis by
automating medical report generation, ultimately improving clinical efficiency,
diagnostic accuracy, and patient care.},
 author = {Jia-Hong Huang},
 citations = {1},
 comment = {The paper is accepted by the Conference on Information and Knowledge
  Management (CIKM), 2025},
 doi = {},
 eprint = {2509.12534v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DeepEyeNet: Generating Medical Report for Retinal Images},
 url = {http://arxiv.org/abs/2509.12534v1},
 year = {2025}
}

@article{2509.13590v1,
 abstract = {The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.},
 author = {Samer Al-Hamadani},
 citations = {},
 comment = {32 pages, 14 figures, 6 tables},
 doi = {},
 eprint = {2509.13590v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation},
 url = {http://arxiv.org/abs/2509.13590v1},
 year = {2025}
}

@article{2509.13879v1,
 abstract = {Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https: //github.com/PRAISELab-PicusLab/CER.},
 author = {Mariano Barone and Antonio Romano and Giuseppe Riccio and Marco Postiglione and Vincenzo Moscato},
 citations = {0},
 comment = {Proceedings of the 48th International ACM SIGIR Conference on
  Research and Development in Information Retrieval, 2025},
 doi = {10.1145/3726302.3729931},
 eprint = {2509.13879v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Combining Evidence and Reasoning for Biomedical Fact-Checking},
 url = {http://arxiv.org/abs/2509.13879v1},
 year = {2025}
}

@article{2509.13888v1,
 abstract = {Misinformation in healthcare, from vaccine hesitancy to unproven treatments,
poses risks to public health and trust in medical systems. While machine
learning and natural language processing have advanced automated fact-checking,
validating biomedical claims remains uniquely challenging due to complex
terminology, the need for domain expertise, and the critical importance of
grounding in scientific evidence. We introduce CER (Combining Evidence and
Reasoning), a novel framework for biomedical fact-checking that integrates
scientific evidence retrieval, reasoning via large language models, and
supervised veracity prediction. By integrating the text-generation capabilities
of large language models with advanced retrieval techniques for high-quality
biomedical scientific evidence, CER effectively mitigates the risk of
hallucinations, ensuring that generated outputs are grounded in verifiable,
evidence-based sources. Evaluations on expert-annotated datasets (HealthFC,
BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising
cross-dataset generalization. Code and data are released for transparency and
reproducibility: https://github.com/PRAISELab-PicusLab/CER},
 author = {Mariano Barone and Antonio Romano and Giuseppe Riccio and Marco Postiglione and Vincenzo Moscato},
 citations = {},
 comment = {},
 doi = {10.1145/3726302.3730155},
 eprint = {2509.13888v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification},
 url = {http://arxiv.org/abs/2509.13888v1},
 year = {2025}
}

@article{2509.14037v1,
 abstract = {Understanding disease similarity is critical for advancing diagnostics, drug
discovery, and personalized treatment strategies. We present PhenoGnet, a novel
graph-based contrastive learning framework designed to predict disease
similarity by integrating gene functional interaction networks with the Human
Phenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view
model that separately encodes gene and phenotype graphs using Graph
Convolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross
view model implemented as a shared weight multilayer perceptron (MLP) that
aligns gene and phenotype embeddings through contrastive learning. The model is
trained using known gene phenotype associations as positive pairs and randomly
sampled unrelated pairs as negatives. Diseases are represented by the mean
embeddings of their associated genes and/or phenotypes, and pairwise similarity
is computed via cosine similarity. Evaluation on a curated benchmark of 1,100
similar and 866 dissimilar disease pairs demonstrates strong performance, with
gene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764,
outperforming existing state of the art methods. Notably, PhenoGnet captures
latent biological relationships beyond direct overlap, offering a scalable and
interpretable solution for disease similarity prediction. These results
underscore its potential for enabling downstream applications in rare disease
research and precision medicine.},
 author = {Ranga Baminiwatte and Kazi Jewel Rana and Aaron J. Masino},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.14037v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction},
 url = {http://arxiv.org/abs/2509.14037v1},
 year = {2025}
}

@article{2509.14304v1,
 abstract = {Stuttered and dysfluent speech detection systems have traditionally suffered
from the trade-off between accuracy and clinical interpretability. While
end-to-end deep learning models achieve high performance, their black-box
nature limits clinical adoption. This paper looks at the Unconstrained
Dysfluency Modeling (UDM) series-the current state-of-the-art framework
developed by Berkeley that combines modular architecture, explicit phoneme
alignment, and interpretable outputs for real-world clinical deployment.
Through extensive experiments involving patients and certified speech-language
pathologists (SLPs), we demonstrate that UDM achieves state-of-the-art
performance (F1: 0.89+-0.04) while providing clinically meaningful
interpretability scores (4.2/5.0). Our deployment study shows 87% clinician
acceptance rate and 34% reduction in diagnostic time. The results provide
strong evidence that UDM represents a practical pathway toward AI-assisted
speech therapy in clinical environments.},
 author = {Eric Zhang and Li Wei and Sarah Chen and Michael Wang},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.14304v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework},
 url = {http://arxiv.org/abs/2509.14304v1},
 year = {2025}
}

@article{2509.14987v1,
 abstract = {This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)
for healthcare systems to tackle two essential challenges confronting health
information networks: safe data exchange and comprehensible AI-driven clinical
decision-making. Our architecture incorporates blockchain, ensuring patient
records are immutable, auditable, and tamper-proof, alongside Explainable AI
(XAI) methodologies that yield transparent and clinically relevant model
predictions. By incorporating security assurances and interpretability
requirements into a unified optimization pipeline, BXHF ensures both data-level
trust (by verified and encrypted record sharing) and decision-level trust (with
auditable and clinically aligned explanations). Its hybrid edge-cloud
architecture allows for federated computation across different institutions,
enabling collaborative analytics while protecting patient privacy. We
demonstrate the framework's applicability through use cases such as
cross-border clinical research networks, uncommon illness detection and
high-risk intervention decision support. By ensuring transparency,
auditability, and regulatory compliance, BXHF improves the credibility, uptake,
and effectiveness of AI in healthcare, laying the groundwork for safer and more
reliable clinical decision-making.},
 author = {Md Talha Mohsin},
 citations = {0},
 comment = {6 Pages, 4 Figures},
 doi = {},
 eprint = {2509.14987v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Blockchain-Enabled Explainable AI for Trusted Healthcare Systems},
 url = {http://arxiv.org/abs/2509.14987v1},
 year = {2025}
}

@article{2509.15105v1,
 abstract = {Time series forecasting (TSF) is critical in domains like energy, finance,
healthcare, and logistics, requiring models that generalize across diverse
datasets. Large pre-trained models such as Chronos and Time-MoE show strong
zero-shot (ZS) performance but suffer from high computational costs. In this
work, We introduce Super-Linear, a lightweight and scalable mixture-of-experts
(MoE) model for general forecasting. It replaces deep architectures with simple
frequency-specialized linear experts, trained on resampled data across multiple
frequency regimes. A lightweight spectral gating mechanism dynamically selects
relevant experts, enabling efficient, accurate forecasting. Despite its
simplicity, Super-Linear matches state-of-the-art performance while offering
superior efficiency, robustness to various sampling rates, and enhanced
interpretability. The implementation of Super-Linear is available at
\href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}},
 author = {Liran Nochumsohn and Raz Marshanski and Hedi Zisling and Omri Azencot},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.15105v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting},
 url = {http://arxiv.org/abs/2509.15105v1},
 year = {2025}
}

@article{2509.15592v1,
 abstract = {In machine learning applications, predictive models are trained to serve
future queries across the entire data distribution. Real-world data often
demands excessively complex models to achieve competitive performance, however,
sacrificing interpretability. Hence, the growing deployment of machine learning
models in high-stakes applications, such as healthcare, motivates the search
for methods for accurate and explainable predictions. This work proposes a
Personalized Prediction scheme, where an easy-to-interpret predictor is learned
per query. In particular, we wish to produce a "sparse linear" classifier with
competitive performance specifically on some sub-population that includes the
query point. The goal of this work is to study the PAC-learnability of this
prediction model for sub-populations represented by "halfspaces" in a
label-agnostic setting. We first give a distribution-specific PAC-learning
algorithm for learning reference classes for personalized prediction. By
leveraging both the reference-class learning algorithm and a list learner of
sparse linear representations, we prove the first upper bound,
$O(\mathrm{opt}^{1/4} )$, for personalized prediction with sparse linear
classifiers and homogeneous halfspace subsets. We also evaluate our algorithms
on a variety of standard benchmark data sets.},
 author = {Jizhou Huang and Brendan Juba},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.15592v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution},
 url = {http://arxiv.org/abs/2509.15592v1},
 year = {2025}
}

@article{2509.16250v1,
 abstract = {Early and accurate detection through Pap smear analysis is critical to
improving patient outcomes and reducing mortality of Cervical cancer.
State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) require
substantial computational resources, extended training time, and large
datasets. In this study, a lightweight CNN model, S-Net (Simple Net), is
developed specifically for cervical cancer detection and classification using
Pap smear images to address these limitations. Alongside S-Net, six SOTA CNNs
were evaluated using transfer learning, including multi-path (DenseNet201,
ResNet152), depth-based (Serasnet152), width-based multi-connection (Xception),
depth-wise separable convolutions (MobileNetV2), and spatial exploitation-based
(VGG19). All models, including S-Net, achieved comparable accuracy, with S-Net
reaching 99.99%. However, S-Net significantly outperforms the SOTA CNNs in
terms of computational efficiency and inference time, making it a more
practical choice for real-time and resource-constrained applications. A major
limitation in CNN-based medical diagnosis remains the lack of transparency in
the decision-making process. To address this, Explainable AI (XAI) techniques,
such as SHAP, LIME, and Grad-CAM, were employed to visualize and interpret the
key image regions influencing model predictions. The novelty of this study lies
in the development of a highly accurate yet computationally lightweight model
(S-Net) caPable of rapid inference while maintaining interpretability through
XAI integration. Furthermore, this work analyzes the behavior of SOTA CNNs,
investigates the effects of negative transfer learning on Pap smear images, and
examines pixel intensity patterns in correctly and incorrectly classified
samples.},
 author = {Saifuddin Sagor and Md Taimur Ahad and Faruk Ahmed and Rokonozzaman Ayon and Sanzida Parvin},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.16250v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A study on Deep Convolutional Neural Networks, transfer learning, and Mnet model for Cervical Cancer Detection},
 url = {http://arxiv.org/abs/2509.16250v1},
 year = {2025}
}

@article{2509.16685v1,
 abstract = {The integration of artificial intelligence (AI) into medicine is remarkable,
offering advanced diagnostic and therapeutic possibilities. However, the
inherent opacity of complex AI models presents significant challenges to their
clinical practicality. This paper focuses primarily on investigating the
application of explainable artificial intelligence (XAI) methods, with the aim
of making AI decisions transparent and interpretable. Our research focuses on
implementing simulations using various medical datasets to elucidate the
internal workings of the XAI model. These dataset-driven simulations
demonstrate how XAI effectively interprets AI predictions, thus improving the
decision-making process for healthcare professionals. In addition to a survey
of the main XAI methods and simulations, ongoing challenges in the XAI field
are discussed. The study highlights the need for the continuous development and
exploration of XAI, particularly from the perspective of diverse medical
datasets, to promote its adoption and effectiveness in the healthcare domain.},
 author = {Binbin Wen and Yihang Wu and Tareef Daqqaq and Ahmad Chaddad},
 citations = {0},
 comment = {Published in Cognitive Neurodynamics},
 doi = {10.1007/s11571-025-10343-w},
 eprint = {2509.16685v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Towards a Transparent and Interpretable AI Model for Medical Image Classifications},
 url = {http://arxiv.org/abs/2509.16685v1},
 year = {2025}
}

@article{2509.16958v1,
 abstract = {Abductive reasoning - the search for plausible explanations - has long been
central to human inquiry, from forensics to medicine and scientific discovery.
Yet formal approaches in AI have largely reduced abduction to eliminative
search: hypotheses are treated as mutually exclusive, evaluated against
consistency constraints or probability updates, and pruned until a single
"best" explanation remains. This reductionist framing overlooks the way human
reasoners sustain multiple explanatory lines in suspension, navigate
contradictions, and generate novel syntheses. This paper introduces quantum
abduction, a non-classical paradigm that models hypotheses in superposition,
allows them to interfere constructively or destructively, and collapses only
when coherence with evidence is reached. Grounded in quantum cognition and
implemented with modern NLP embeddings and generative AI, the framework
supports dynamic synthesis rather than premature elimination. Case studies span
historical mysteries (Ludwig II of Bavaria, the "Monster of Florence"),
literary demonstrations ("Murder on the Orient Express"), medical diagnosis,
and scientific theory change. Across these domains, quantum abduction proves
more faithful to the constructive and multifaceted nature of human reasoning,
while offering a pathway toward expressive and transparent AI reasoning
systems.},
 author = {Remo Pareschi},
 citations = {0},
 comment = {23 pages, 8 figures, 3 tables; submitted to Sci, MDPI},
 doi = {},
 eprint = {2509.16958v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Quantum Abduction: A New Paradigm for Reasoning under Uncertainty},
 url = {http://arxiv.org/abs/2509.16958v1},
 year = {2025}
}

@article{2509.17991v1,
 abstract = {Almost 50% depression patients face the risk of going into relapse. The risk
increases to 80% after the second episode of depression. Although, depression
detection from social media has attained considerable attention, depression
relapse detection has remained largely unexplored due to the lack of curated
datasets and the difficulty of distinguishing relapse and non-relapse users. In
this work, we present ReDepress, the first clinically validated social media
dataset focused on relapse, comprising 204 Reddit users annotated by mental
health professionals. Unlike prior approaches, our framework draws on cognitive
theories of depression, incorporating constructs such as attention bias,
interpretation bias, memory bias and rumination into both annotation and
modeling. Through statistical analyses and machine learning experiments, we
demonstrate that cognitive markers significantly differentiate relapse and
non-relapse groups, and that models enriched with these features achieve
competitive performance, with transformer-based temporal models attaining an F1
of 0.86. Our findings validate psychological theories in real-world textual
data and underscore the potential of cognitive-informed computational methods
for early relapse detection, paving the way for scalable, low-cost
interventions in mental healthcare.},
 author = {Aakash Kumar Agarwal and Saprativa Bhattacharjee and Mauli Rastogi and Jemima S. Jacob and Biplab Banerjee and Rashmi Gupta and Pushpak Bhattacharyya},
 citations = {},
 comment = {Accepted to EMNLP 2025 Main Conference},
 doi = {},
 eprint = {2509.17991v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media},
 url = {http://arxiv.org/abs/2509.17991v1},
 year = {2025}
}

@article{2509.18112v1,
 abstract = {Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.},
 author = {Sheng Wong and Ravi Shankar and Beth Albert and Gabriel Davis Jones},
 citations = {0},
 comment = {Preparing for journal},
 doi = {},
 eprint = {2509.18112v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis},
 url = {http://arxiv.org/abs/2509.18112v1},
 year = {2025}
}

@article{2509.18140v1,
 abstract = {Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.},
 author = {Iram Wajahat and Amritpal Singh and Fazel Keshtkar and Syed Ahmad Chan Bukhari},
 citations = {0},
 comment = {6 pages, 6 figures},
 doi = {},
 eprint = {2509.18140v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders},
 url = {http://arxiv.org/abs/2509.18140v1},
 year = {2025}
}

@article{2509.19362v1,
 abstract = {Feature attribution is essential for interpreting deep learning models,
particularly in time-series domains such as healthcare, biometrics, and
human-AI interaction. However, standard attribution methods, such as Integrated
Gradients or SHAP, are computationally intensive and not well-suited for
real-time applications. We present DeepACTIF, a lightweight and
architecture-aware feature attribution method that leverages internal
activations of sequence models to estimate feature importance efficiently.
Focusing on LSTM-based networks, we introduce an inverse-weighted aggregation
scheme that emphasises stability and magnitude of activations across time
steps. Our evaluation across three biometric gaze datasets shows that DeepACTIF
not only preserves predictive performance under severe feature reduction (top
10% of features) but also significantly outperforms established methods,
including SHAP, IG, and DeepLIFT, in terms of both accuracy and statistical
robustness. Using Wilcoxon signed-rank tests and effect size analysis, we
demonstrate that DeepACTIF yields more informative feature rankings with
significantly lower error across all top-k conditions (10 - 40%). Our
experiments demonstrate that DeepACTIF not only reduces computation time and
memory usage by orders of magnitude but also preserves model accuracy when
using only top-ranked features. That makes DeepACTIF a viable solution for
real-time interpretability on edge devices such as mobile XR headsets or
embedded health monitors.},
 author = {Benedikt W. Hosp},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.19362v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models},
 url = {http://arxiv.org/abs/2509.19362v1},
 year = {2025}
}

@article{2509.19671v1,
 abstract = {Public healthcare datasets of Chest X-Rays (CXRs) have long been a popular
benchmark for developing computer vision models in healthcare. However, strong
average-case performance of machine learning (ML) models on these datasets is
insufficient to certify their clinical utility. In this paper, we use clinical
context, as captured by prior discharge summaries, to provide a more holistic
evaluation of current ``state-of-the-art'' models for the task of CXR
diagnosis. Using discharge summaries recorded prior to each CXR, we derive a
``prior'' or ``pre-test'' probability of each CXR label, as a proxy for
existing contextual knowledge available to clinicians when interpreting CXRs.
Using this measure, we demonstrate two key findings: First, for several
diagnostic labels, CXR models tend to perform best on cases where the pre-test
probability is very low, and substantially worse on cases where the pre-test
probability is higher. Second, we use pre-test probability to assess whether
strong average-case performance reflects true diagnostic signal, rather than an
ability to infer the pre-test probability as a shortcut. We find that
performance drops sharply on a balanced test set where this shortcut does not
exist, which may indicate that much of the apparent diagnostic power derives
from inferring this clinical context. We argue that this style of analysis,
using context derived from clinical notes, is a promising direction for more
rigorous and fine-grained evaluation of clinical vision models.},
 author = {Andrew Wang and Jiashuo Zhang and Michael Oberst},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.19671v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Revisiting Performance Claims for Chest X-Ray Models Using Clinical Context},
 url = {http://arxiv.org/abs/2509.19671v1},
 year = {2025}
}

@article{2509.20153v2,
 abstract = {This paper examines the integration of emotional intelligence into artificial
intelligence systems, with a focus on affective computing and the growing
capabilities of Large Language Models (LLMs), such as ChatGPT and Claude, to
recognize and respond to human emotions. Drawing on interdisciplinary research
that combines computer science, psychology, and neuroscience, the study
analyzes foundational neural architectures - CNNs for processing facial
expressions and RNNs for sequential data, such as speech and text - that enable
emotion recognition. It examines the transformation of human emotional
experiences into structured emotional data, addressing the distinction between
explicit emotional data collected with informed consent in research settings
and implicit data gathered passively through everyday digital interactions.
That raises critical concerns about lawful processing, AI transparency, and
individual autonomy over emotional expressions in digital environments. The
paper explores implications across various domains, including healthcare,
education, and customer service, while addressing challenges of cultural
variations in emotional expression and potential biases in emotion recognition
systems across different demographic groups. From a regulatory perspective, the
paper examines emotional data in the context of the GDPR and the EU AI Act
frameworks, highlighting how emotional data may be considered sensitive
personal data that requires robust safeguards, including purpose limitation,
data minimization, and meaningful consent mechanisms.},
 author = {Nicola Fabiano},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.20153v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Affective Computing and Emotional Data: Challenges and Implications in Privacy Regulations, The AI Act, and Ethics in Large Language Models},
 url = {http://arxiv.org/abs/2509.20153v2},
 year = {2025}
}

@article{2509.20935v1,
 abstract = {In precision medicine, quantitative multi-omic features, topological context,
and textual biological knowledge play vital roles in identifying
disease-critical signaling pathways and targets. Existing pipelines capture
only part of these-numerical omics ignore topological context, text-centric
LLMs lack quantitative grounded reasoning, and graph-only models underuse node
semantics and the generalization of LLMs-limiting mechanistic interpretability.
Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they
remain limited by unreliable intermediate evaluation, and vulnerability to
reward hacking with computational cost. These gaps motivate integrating
quantitative multi-omic signals, topological structure with node annotations,
and literature-scale text via LLMs, using subgraph reasoning as the principle
bridge linking numeric evidence, topological knowledge and language context.
Therefore, we propose GALAX (Graph Augmented LAnguage model with
eXplainability), an innovative framework that integrates pretrained Graph
Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement
guided by a Graph Process Reward Model (GPRM), which generates disease-relevant
subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated
by a pretrained GNN, enabling process-level supervision without explicit
intermediate reasoning annotations. As an application, we also introduced
Target-QA, a benchmark combining CRISPR-identified targets, multi-omic
profiles, and biomedical graph knowledge across diverse cancer cell lines,
which enables GNN pretraining for supervising step-wise graph construction and
supports long-context reasoning over text-numeric graphs (TNGs), providing a
scalable and biologically grounded framework for explainable,
reinforcement-guided subgraph reasoning toward reliable and interpretable
target and pathway discovery in precision medicine.},
 author = {Heming Zhang and Di Huang and Wenyu Li and Michael Province and Yixin Chen and Philip Payne and Fuhai Li},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.20935v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine},
 url = {http://arxiv.org/abs/2509.20935v1},
 year = {2025}
}

@article{2509.21266v1,
 abstract = {Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.},
 author = {Zijian Shao and Haiyang Shen and Mugeng Liu and Gecheng Fu and Yaoqi Guo and Yanfeng Wang and Yun Ma},
 citations = {},
 comment = {under review},
 doi = {},
 eprint = {2509.21266v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support},
 url = {http://arxiv.org/abs/2509.21266v1},
 year = {2025}
}

@article{2509.21794v1,
 abstract = {Fatigue detection using physiological signals is critical in domains such as
transportation, healthcare, and performance monitoring. While most studies
focus on single modalities, this work examines statistical relationships
between signal pairs to improve classification robustness. Using the DROZY
dataset, we extracted features from ECG, EMG, EOG, and EEG across 15 signal
combinations and evaluated them with Decision Tree, Random Forest, Logistic
Regression, and XGBoost. Results show that XGBoost with the EMG EEG combination
achieved the best performance. SHAP analysis highlighted ECG EOG correlation as
a key feature, and multi signal models consistently outperformed single signal
ones. These findings demonstrate that feature level fusion of physiological
signals enhances accuracy, interpretability, and practical applicability of
fatigue monitoring systems.},
 author = {Kourosh Kakhi and Abbas Khosravi and Roohallah Alizadehsani and U. Rajendra Acharyab},
 citations = {},
 comment = {14 Pages, 12 Figures, 3 Tables},
 doi = {},
 eprint = {2509.21794v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Exploring the Relationships Between Physiological Signals During Automated Fatigue Detection},
 url = {http://arxiv.org/abs/2509.21794v1},
 year = {2025}
}

@article{2509.21923v1,
 abstract = {Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.},
 author = {Fumin Wang},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.21923v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects},
 url = {http://arxiv.org/abs/2509.21923v1},
 year = {2025}
}

@article{2509.22014v1,
 abstract = {Healthcare robotics requires robust multimodal perception and reasoning to
ensure safety in dynamic clinical environments. Current Vision-Language Models
(VLMs) demonstrate strong general-purpose capabilities but remain limited in
temporal reasoning, uncertainty estimation, and structured outputs needed for
robotic planning. We present a lightweight agentic multimodal framework for
video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model
with a SmolAgent-based orchestration layer, it supports chain-of-thought
reasoning, speech-vision fusion, and dynamic tool invocation. The framework
generates structured scene graphs and leverages a hybrid retrieval module for
interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark
and a custom clinical dataset show competitive accuracy and improved robustness
compared to state-of-the-art VLMs, demonstrating its potential for applications
in robot-assisted surgery, patient monitoring, and decision support.},
 author = {Saurav Jha and Stefan K. Ehrlich},
 citations = {},
 comment = {11 pages, 3 figures},
 doi = {},
 eprint = {2509.22014v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics},
 url = {http://arxiv.org/abs/2509.22014v1},
 year = {2025}
}

@article{2509.22329v1,
 abstract = {The pervasive integration of artificial intelligence (AI) across domains such
as healthcare, governance, finance, and education has intensified scrutiny of
its ethical implications, including algorithmic bias, privacy risks,
accountability, and societal impact. While ethics has received growing
attention in computer science (CS) education more broadly, the specific
pedagogical treatment of {AI ethics} remains under-examined. This study
addresses that gap through a large-scale analysis of 3,395 publicly accessible
syllabi from CS and allied areas at leading Indian institutions. Among them,
only 75 syllabi (2.21%) included any substantive AI ethics content. Three key
findings emerged: (1) AI ethics is typically integrated as a minor module
within broader technical courses rather than as a standalone course; (2) ethics
coverage is often limited to just one or two instructional sessions; and (3)
recurring topics include algorithmic fairness, privacy and data governance,
transparency, and societal impact. While these themes reflect growing
awareness, current curricular practices reveal limited depth and consistency.
This work highlights both the progress and the gaps in preparing future
technologists to engage meaningfully with the ethical dimensions of AI, and it
offers suggestions to strengthen the integration of AI ethics within computing
curricula.},
 author = {Anshu M Mittal and P D Parthasarathy and Swaroop Joshi},
 citations = {},
 comment = {Accepted for publication at ACM Compute-2025
  https://isigcse.acm.org/compute/2025/},
 doi = {},
 eprint = {2509.22329v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {AI Ethics Education in India: A Syllabus-Level Review of Computing Courses},
 url = {http://arxiv.org/abs/2509.22329v1},
 year = {2025}
}

@article{2509.22673v1,
 abstract = {Survival analysis is central to clinical research, informing patient
prognoses, guiding treatment decisions, and optimising resource allocation.
Accurate time-to-event predictions not only improve quality of life but also
reveal risk factors that shape clinical practice. For these models to be
relevant in healthcare, interpretability is critical: predictions must be
traceable to patient-specific characteristics, and risk factors should be
identifiable to generate actionable insights for both clinicians and
researchers. Traditional survival models often fail to capture non-linear
interactions, while modern deep learning approaches, though powerful, are
limited by poor interpretability.
  We propose a Pipeline for Interpretable Survival Analysis (PISA) - a pipeline
that provides multiple survival analysis models that trade off complexity and
performance. Using multiple-feature, multi-objective feature engineering, PISA
transforms patient characteristics and time-to-event data into multiple
survival analysis models, providing valuable insights into the survival
prediction task. Crucially, every model is converted into simple patient
stratification flowcharts supported by Kaplan-Meier curves, whilst not
compromising on performance. While PISA is model-agnostic, we illustrate its
flexibility through applications of Cox regression and shallow survival trees,
the latter avoiding proportional hazards assumptions.
  Applied to two clinical benchmark datasets, PISA produced interpretable
survival models and intuitive stratification flowcharts whilst achieving
state-of-the-art performances. Revisiting a prior departmental study further
demonstrated its capacity to automate survival analysis workflows in real-world
clinical research.},
 author = {Thalea Schlender and Catharina J. A. Romme and Yvette M. van der Linden and Luc R. C. W. van Lonkhuijzen and Peter A. N. Bosman and Tanja Alderliesten},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.22673v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {PISA: An AI Pipeline for Interpretable-by-design Survival Analysis Providing Multiple Complexity-Accuracy Trade-off Models},
 url = {http://arxiv.org/abs/2509.22673v1},
 year = {2025}
}

@article{2509.22853v1,
 abstract = {Proteomics data is essential to pathogenic understanding of a disease
phenotype. In cancer, analysis of molecular signatures enables precision
medicine through the identification of biological processes that drive
individualized tumor progression, therapeutic resistance, and clinical
heterogeneity. Recent advances in multimodal large language models (LLMs) have
shown remarkable capacity to integrate and reason across heterogeneous data
modalities. However, performing multi-modal language modeling for molecular
understanding of patient-specific proteomics remains a significant challenge
due to two barriers: (1) the lack of instruction-tuning datasets that enable
clinical interpretation from proteomics data, and (2) the absence of language
modeling architectures designed to capture the rich heterogeneity of molecular
data. In this work, we introduce CPTAC-PROTSTRUCT, the first instruction tuning
dataset for molecular understanding of oncology, comprising over 400k
open-ended examples derived from individualized proteomic profiles curated from
the largest national proteomics cancer study (CPTAC). Additionally, we propose
KRONOS (Knowledge Representation of patient Omics Networks in Oncology via
Structured tuning), a novel graph-LLM framework that leverages molecular
interaction topology with proteomics to learn patient-specific graph
representations for enhanced clinical reasoning. We show that KRONOS achieves
competitive performance across benchmark clinical tasks, including molecular
classification, temporal trajectory modeling, and tumor stage prediction from
proteomics data. Ultimately, this approach empowers LLMs to understand
patient-level pathogenesis, advancing precision medicine through more accurate
diagnosis, prognosis, and treatment stratification.},
 author = {Irsyad Adam and Zekai Chen and David Laub and Shaun Porwal and Arda Pekis and Kevin Brown},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2509.22853v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Patient-specific Biomolecular Instruction Tuning},
 url = {http://arxiv.org/abs/2509.22853v1},
 year = {2025}
}

@article{2509.23213v1,
 abstract = {Understanding causality in event sequences with thousands of sparse event
types is critical in domains such as healthcare, cybersecurity, or vehicle
diagnostics, yet current methods fail to scale. We present OSCAR, a one-shot
causal autoregressive method that infers per-sequence Markov Boundaries using
two pretrained Transformers as density estimators. This enables efficient,
parallel causal discovery without costly global CI testing. On a real-world
automotive dataset with 29,100 events and 474 labels, OSCAR recovers
interpretable causal structures in minutes, while classical methods fail to
scale, enabling practical scientific diagnostics at production scale.},
 author = {Hugo Math and Robin Schön and Rainer Lienhart},
 citations = {0},
 comment = {Accepted at NeuRIPS2025 Workshop CauScien: Discovering Causality in
  Science. arXiv admin note: substantial text overlap with arXiv:2509.19112},
 doi = {},
 eprint = {2509.23213v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences},
 url = {http://arxiv.org/abs/2509.23213v1},
 year = {2025}
}

@article{2509.23255v1,
 abstract = {Human Activity Recognition supports applications in healthcare,
manufacturing, and human-machine interaction. LiDAR point clouds offer a
privacy-preserving alternative to cameras and are robust to illumination. We
propose a HAR method based on graph spectral analysis. Each LiDAR frame is
mapped to a proximity graph (epsilon-graph) and the Laplacian spectrum is
computed. Eigenvalues and statistics of eigenvectors form pose descriptors, and
temporal statistics over sliding windows yield fixed vectors for classification
with support vector machines and random forests. On the MM-Fi dataset with 40
subjects and 27 activities, under a strict subject-independent protocol, the
method reaches 94.4% accuracy on a 13-class rehabilitation set and 90.3% on all
27 activities. It also surpasses the skeleton-based baselines reported for
MM-Fi. The contribution is a compact and interpretable feature set derived
directly from point cloud geometry that provides an accurate and efficient
alternative to end-to-end deep learning.},
 author = {Sasan Sharifipour and Constantino Álvarez Casado and Le Nguyen and Tharindu Ekanayake and Manuel Lage Cañellas and Nhi Nguyen and Miguel Bordallo López},
 citations = {},
 comment = {9 pages, 5 figures, 4 tables, 22 references, conference; Code
  available at https://github.com/Arritmic/oulu-pointcloud-har},
 doi = {},
 eprint = {2509.23255v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {LiDAR-based Human Activity Recognition through Laplacian Spectral Analysis},
 url = {http://arxiv.org/abs/2509.23255v1},
 year = {2025}
}

@article{2509.24118v1,
 abstract = {Electronic health Records (EHRs) have become a cornerstone in modern-day
healthcare. They are a crucial part for analyzing the progression of patient
health; however, their complexity, characterized by long, multivariate
sequences, sparsity, and missing values poses significant challenges in
traditional deep learning modeling. While Transformer-based models have
demonstrated success in modeling EHR data and predicting clinical outcomes,
their quadratic computational complexity and limited context length hinder
their efficiency and practical applications. On the other hand, State Space
Models (SSMs) like Mamba present a promising alternative offering linear-time
sequence modeling and improved efficiency for handling long sequences, but
focus mostly on mixing sequence-level information rather than channel-level
data. To overcome these challenges, we propose HyMaTE (A Hybrid Mamba and
Transformer Model for EHR Representation Learning), a novel hybrid model
tailored for representing longitudinal data, combining the strengths of SSMs
with advanced attention mechanisms. By testing the model on predictive tasks on
multiple clinical datasets, we demonstrate HyMaTE's ability to capture an
effective, richer, and more nuanced unified representation of EHR data.
Additionally, the interpretability of the outcomes achieved by self-attention
illustrates the effectiveness of our model as a scalable and generalizable
solution for real-world healthcare applications. Codes are available at:
https://github.com/healthylaife/HyMaTE.},
 author = {Md Mozaharul Mottalib and Thao-Ly T. Phan and Rahmatollah Beheshti},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.24118v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning},
 url = {http://arxiv.org/abs/2509.24118v1},
 year = {2025}
}

@article{2509.26150v1,
 abstract = {"Double, double toil and trouble; Fire burn and cauldron bubble." As
Shakespeare's witches foretold chaos through cryptic prophecies, modern capital
markets grapple with systemic risks concealed by opaque AI systems. According
to IMF, the August 5, 2024, plunge in Japanese and U.S. equities can be linked
to algorithmic trading yet ab-sent from existing AI incidents database
exemplifies this transparency crisis. Current AI incident databases, reliant on
crowdsourcing or news scraping, systematically over-look capital market
anomalies, particularly in algorithmic and high-frequency trading. We address
this critical gap by proposing a regulatory-grade global database that
elegantly synthesises post-trade reporting frameworks with proven incident
documentation models from healthcare and aviation. Our framework's temporal
data omission technique masking timestamps while preserving percent-age-based
metrics enables sophisticated cross-jurisdictional analysis of emerging risks
while safeguarding confidential business information. Synthetic data validation
(modelled after real life published incidents , sentiments, data) reveals
compelling pat-terns: systemic risks transcending geographical boundaries,
market manipulation clusters distinctly identifiable via K-means algorithms,
and AI system typology exerting significantly greater influence on trading
behaviour than geographical location, This tripartite solution empowers
regulators with unprecedented cross-jurisdictional oversight, financial
institutions with seamless compliance integration, and investors with critical
visibility into previously obscured AI-driven vulnerabilities. We call for
immediate action to strengthen risk management and foster resilience in
AI-driven financial markets against the volatile "cauldron" of AI-driven
systemic risks., promoting global financial stability through enhanced
transparency and coordinated oversight.},
 author = {Anchal Gupta and Gleb Pappyshev and James T Kwok},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2509.26150v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Bubble, Bubble, AI's Rumble: Why Global Financial Regulatory Incident Reporting is Our Shield Against Systemic Stumbles},
 url = {http://arxiv.org/abs/2509.26150v1},
 year = {2025}
}

@article{2510.00035v1,
 abstract = {Deep learning integration into medical imaging systems has transformed
disease detection and diagnosis processes with a focus on pneumonia
identification. The study introduces an intricate deep learning system using
Convolutional Neural Networks for automated pneumonia detection from chest Xray
images which boosts diagnostic precision and speed. The proposed CNN
architecture integrates sophisticated methods including separable convolutions
along with batch normalization and dropout regularization to enhance feature
extraction while reducing overfitting. Through the application of data
augmentation techniques and adaptive learning rate strategies the model
underwent training on an extensive collection of chest Xray images to enhance
its generalization capabilities. A convoluted array of evaluation metrics such
as accuracy, precision, recall, and F1 score collectively verify the model
exceptional performance by recording an accuracy rate of 91. This study tackles
critical clinical implementation obstacles such as data privacy protection,
model interpretability, and integration with current healthcare systems beyond
just model performance. This approach introduces a critical advancement by
integrating medical ontologies with semantic technology to improve diagnostic
accuracy. The study enhances AI diagnostic reliability by integrating machine
learning outputs with structured medical knowledge frameworks to boost
interpretability. The findings demonstrate AI powered healthcare tools as a
scalable efficient pneumonia detection solution. This study advances AI
integration into clinical settings by developing more precise automated
diagnostic methods that deliver consistent medical imaging results.},
 author = {P K Dutta and Anushri Chowdhury and Anouska Bhattacharyya and Shakya Chakraborty and Sujatra Dey},
 citations = {},
 comment = {8 pages, 2 figures},
 doi = {},
 eprint = {2510.00035v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Deep Learning-Based Pneumonia Detection from Chest X-ray Images: A CNN Approach with Performance Analysis and Clinical Implications},
 url = {http://arxiv.org/abs/2510.00035v1},
 year = {2025}
}

@article{2510.00411v2,
 abstract = {The accurate interpretation of chest radiographs using automated methods is a
critical task in medical imaging. This paper presents a comparative analysis
between a supervised lightweight Convolutional Neural Network (CNN) and a
state-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP,
across two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST
benchmark and tuberculosis detection on the Shenzhen TB dataset. Our
experiments show that supervised CNNs serve as highly competitive baselines in
both cases. While the default zero-shot performance of the VLM is lower, we
demonstrate that its potential can be unlocked via a simple yet crucial remedy:
decision threshold calibration. By optimizing the classification threshold on a
validation set, the performance of BiomedCLIP is significantly boosted across
both datasets. For pneumonia detection, calibration enables the zero-shot VLM
to achieve a superior F1-score of 0.8841, surpassing the supervised CNN's
0.8803. For tuberculosis detection, calibration dramatically improves the
F1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's
0.7834. This work highlights a key insight: proper calibration is essential for
leveraging the full diagnostic power of zero-shot VLMs, enabling them to match
or even outperform efficient, task-specific supervised models.},
 author = {Ran Tong and Jiaqi Liu and Su Liu and Jiexi Xu and Lanruo Wang and Tong Wang},
 citations = {},
 comment = {6pages,3 figures.Uunder review of International Conference on
  Artificial Intelligence, Computer, Data Sciences and Applications},
 doi = {},
 eprint = {2510.00411v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Does Bigger Mean Better? Comparitive Analysis of CNNs and Biomedical Vision Language Modles in Medical Diagnosis},
 url = {http://arxiv.org/abs/2510.00411v2},
 year = {2025}
}

@article{2510.00542v1,
 abstract = {Life expectancy is a fundamental indicator of population health and
socio-economic well-being, yet accurately forecasting it remains challenging
due to the interplay of demographic, environmental, and healthcare factors.
This study evaluates three machine learning models -- Linear Regression (LR),
Regression Decision Tree (RDT), and Random Forest (RF), using a real-world
dataset drawn from World Health Organization (WHO) and United Nations (UN)
sources. After extensive preprocessing to address missing values and
inconsistencies, each model's performance was assessed with $R^2$, Mean
Absolute Error (MAE), and Root Mean Squared Error (RMSE). Results show that RF
achieves the highest predictive accuracy ($R^2 = 0.9423$), significantly
outperforming LR and RDT. Interpretability was prioritized through p-values for
LR and feature importance metrics for the tree-based models, revealing
immunization rates (diphtheria, measles) and demographic attributes (HIV/AIDS,
adult mortality) as critical drivers of life-expectancy predictions. These
insights underscore the synergy between ensemble methods and transparency in
addressing public-health challenges. Future research should explore advanced
imputation strategies, alternative algorithms (e.g., neural networks), and
updated data to further refine predictive accuracy and support evidence-based
policymaking in global health contexts.},
 author = {Roman Dolgopolyi and Ioanna Amaslidou and Agrippina Margaritou},
 citations = {},
 comment = {20 pages, 15 figures, 3 tables},
 doi = {10.21203/rs.3.rs-6968809/v1},
 eprint = {2510.00542v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Interpretable Machine Learning for Life Expectancy Prediction: A Comparative Study of Linear Regression, Decision Tree, and Random Forest},
 url = {http://arxiv.org/abs/2510.00542v1},
 year = {2025}
}

@article{2510.00664v1,
 abstract = {Understanding the inner workings of deep learning models is crucial for
advancing artificial intelligence, particularly in high-stakes fields such as
healthcare, where accurate explanations are as vital as precision. This paper
introduces Batch-CAM, a novel training paradigm that fuses a batch
implementation of the Grad-CAM algorithm with a prototypical reconstruction
loss. This combination guides the model to focus on salient image features,
thereby enhancing its performance across classification tasks. Our results
demonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and
image reconstruction quality while reducing training and inference times. By
ensuring models learn from evidence-relevant information,this approach makes a
relevant contribution to building more transparent, explainable, and
trustworthy AI systems.},
 author = {Giacomo Ignesti and Davide Moroni and Massimo Martinelli},
 citations = {0},
 comment = {18 pages, 7 figures, submitted to SN Computer Science Springer Nature},
 doi = {},
 eprint = {2510.00664v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Batch-CAM: Introduction to better reasoning in convolutional deep learning models},
 url = {http://arxiv.org/abs/2510.00664v1},
 year = {2025}
}

@article{2510.01194v1,
 abstract = {Access to obstetric ultrasound is often limited in low-resource settings,
particularly in rural areas of low- and middle-income countries. This work
proposes a human-in-the-loop artificial intelligence (AI) system designed to
assist midwives in acquiring diagnostically relevant fetal images using blind
sweep protocols. The system incorporates a classification model along with a
web-based platform for asynchronous specialist reviews. By identifying key
frames in blind sweep studies, the AI system allows specialists to concentrate
on interpretation rather than having to review entire videos. To evaluate its
performance, blind sweep videos captured by a small group of soft-trained
midwives using a low-cost Point-of-Care Ultrasound (POCUS) device were
analyzed. The system demonstrated promising results in identifying standard
fetal planes from sweeps made by non-experts. A field evaluation indicated good
usability and a low cognitive workload, suggesting that it has the potential to
expand access to prenatal imaging in underserved regions.},
 author = {Juan Barrientos and Michaelle Pérez and Douglas González and Favio Reyna and Julio Fajardo and Andrea Lara},
 citations = {0},
 comment = {Accepted at MICCAI 2025 MIRASOL Workshop, 10 pages, 5 figures},
 doi = {},
 eprint = {2510.01194v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Development and Evaluation of an AI-Driven Telemedicine System for Prenatal Healthcare},
 url = {http://arxiv.org/abs/2510.01194v1},
 year = {2025}
}

@article{2510.01899v1,
 abstract = {Healthcare generates diverse streams of data, including electronic health
records (EHR), medical imaging, genetics, and ongoing monitoring from wearable
devices. Traditional diagnostic models frequently analyze these sources in
isolation, which constrains their capacity to identify cross-modal correlations
essential for early disease diagnosis. Our research presents a multimodal
foundation model that consolidates diverse patient data through an
attention-based transformer framework. At first, dedicated encoders put each
modality into a shared latent space. Then, they combine them using multi-head
attention and residual normalization. The architecture is made for pretraining
on many tasks, which makes it easy to adapt to new diseases and datasets with
little extra work. We provide an experimental strategy that uses benchmark
datasets in oncology, cardiology, and neurology, with the goal of testing early
detection tasks. The framework includes data governance and model management
tools in addition to technological performance to improve transparency,
reliability, and clinical interpretability. The suggested method works toward a
single foundation model for precision diagnostics, which could improve the
accuracy of predictions and help doctors make decisions.},
 author = {Md Talha Mohsin and Ismail Abdulrashid},
 citations = {0},
 comment = {6 pages},
 doi = {},
 eprint = {2510.01899v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Multimodal Foundation Models for Early Disease Detection},
 url = {http://arxiv.org/abs/2510.01899v1},
 year = {2025}
}

@article{2510.02120v2,
 abstract = {Accounting for inter-individual variability in brain function is key to
precision medicine. Here, by considering functional inter-individual
variability as meaningful data rather than noise, we introduce VarCoNet, an
enhanced self-supervised framework for robust functional connectome (FC)
extraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs
self-supervised contrastive learning to exploit inherent functional
inter-individual variability, serving as a brain function encoder that
generates FC embeddings readily applicable to downstream tasks even in the
absence of labeled data. Contrastive learning is facilitated by a novel
augmentation strategy based on segmenting rs-fMRI signals. At its core,
VarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series
processing, enhanced with a robust Bayesian hyperparameter optimization. Our
VarCoNet framework is evaluated on two downstream tasks: (i) subject
fingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)
autism spectrum disorder (ASD) classification, using rs-fMRI data from the
ABIDE I and ABIDE II datasets. Using different brain parcellations, our
extensive testing against state-of-the-art methods, including 13 deep learning
methods, demonstrates VarCoNet's superiority, robustness, interpretability, and
generalizability. Overall, VarCoNet provides a versatile and robust framework
for FC analysis in rs-fMRI.},
 author = {Charalampos Lamprou and Aamna Alshehhi and Leontios J. Hadjileontiadis and Mohamed L. Seghier},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2510.02120v2},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {VarCoNet: A variability-aware self-supervised framework for functional connectome extraction from resting-state fMRI},
 url = {http://arxiv.org/abs/2510.02120v2},
 year = {2025}
}

@article{2510.02325v1,
 abstract = {This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,
and explainable research prototype developed as a single-investigator project.
The system leverages the emerging Model Context Protocol (MCP) to orchestrate
multiple intelligent agents for patient interaction, including symptom
checking, medication suggestions, and appointment scheduling. The platform
integrates a dedicated Privacy and Compliance Layer that applies role-based
access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit
logging, aligning with major healthcare data protection standards such as HIPAA
(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate
multilingual patient-doctor interaction (English, French, Arabic) and
transparent diagnostic reasoning powered by large language models. As an
applied AI contribution, this work highlights the feasibility of combining
agentic orchestration, multilingual accessibility, and compliance-aware
architecture in healthcare applications. This platform is presented as a
research prototype and is not a certified medical device.},
 author = {Mohammed A. Shehab},
 citations = {0},
 comment = {6 pages, 1 figure. Submitted as a system/vision paper},
 doi = {},
 eprint = {2510.02325v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents},
 url = {http://arxiv.org/abs/2510.02325v1},
 year = {2025}
}

@article{2510.02781v1,
 abstract = {Age Related Macular Degeneration(AMD) has been one of the most leading causes
of permanent vision impairment in ophthalmology. Though treatments, such as
anti VEGF drugs or photodynamic therapies, were developed to slow down the
degenerative process of AMD, there is still no specific cure to reverse vision
loss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD
or AMD itself within the patient retina in early stages is a crucial task to
reduce the possibility of vision impairment. Apart from traditional approaches,
deep learning based methods, especially attention mechanism based CNNs and
GradCAM based XAI analysis on OCT scans, exhibited successful performance in
distinguishing AMD retina from normal retinas, making it possible to use AI
driven models to aid medical diagnosis and analysis by ophthalmologists
regarding AMD. However, though having significant success, previous works
mostly focused on prediction performance itself, not pathologies or underlying
causal mechanisms of AMD, which can prohibit intervention analysis on specific
factors or even lead to less reliable decisions. Thus, this paper introduces a
novel causal AMD analysis model: GCVAMD, which incorporates a modified
CausalVAE approach that can extract latent causal factors from only raw OCT
images. By considering causality in AMD detection, GCVAMD enables causal
inference such as treatment simulation or intervention analysis regarding major
risk factors: drusen and neovascularization, while returning informative latent
causal features that can enhance downstream tasks. Results show that through
GCVAMD, drusen status and neovascularization status can be identified with AMD
causal mechanisms in GCVAMD latent spaces, which can in turn be used for
various tasks from AMD detection(classification) to intervention analysis.},
 author = {Daeyoung Kim},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2510.02781v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular Degeneration Risk Factor Detection and Prediction},
 url = {http://arxiv.org/abs/2510.02781v1},
 year = {2025}
}

@article{2510.03066v1,
 abstract = {Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.},
 author = {Ahsan Farabi and Israt Khandaker and Ibrahim Khalil Shanto and Md Abdul Ahad Minhaz and Tanisha Zaman},
 citations = {},
 comment = {},
 doi = {},
 eprint = {2510.03066v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition},
 url = {http://arxiv.org/abs/2510.03066v1},
 year = {2025}
}

@article{2510.03265v1,
 abstract = {Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.},
 author = {Bowei Tian and Yexiao He and Wanghao Ye and Ziyao Wang and Meng Liu and Ang Li},
 citations = {0},
 comment = {},
 doi = {},
 eprint = {2510.03265v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {MindCraft: How Concept Trees Take Shape In Deep Models},
 url = {http://arxiv.org/abs/2510.03265v1},
 year = {2025}
}

@article{2510.03306v1,
 abstract = {Current atlas-based approaches to brain network analysis rely heavily on
standardized anatomical or connectivity-driven brain atlases. However, these
fixed atlases often introduce significant limitations, such as spatial
misalignment across individuals, functional heterogeneity within predefined
regions, and atlas-selection biases, collectively undermining the reliability
and interpretability of the derived brain networks. To address these
challenges, we propose a novel atlas-free brain network transformer (atlas-free
BNT) that leverages individualized brain parcellations derived directly from
subject-specific resting-state fMRI data. Our approach computes ROI-to-voxel
connectivity features in a standardized voxel-based feature space, which are
subsequently processed using the BNT architecture to produce comparable
subject-level embeddings. Experimental evaluations on sex classification and
brain-connectome age prediction tasks demonstrate that our atlas-free BNT
consistently outperforms state-of-the-art atlas-based methods, including
elastic net, BrainGNN, Graphormer and the original BNT. Our atlas-free approach
significantly improves the precision, robustness, and generalizability of brain
network analyses. This advancement holds great potential to enhance
neuroimaging biomarkers and clinical diagnostic tools for personalized
precision medicine.},
 author = {Shuai Huang and Xuan Kan and James J. Lah and Deqiang Qiu},
 citations = {3},
 comment = {},
 doi = {},
 eprint = {2510.03306v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Atlas-free Brain Network Transformer},
 url = {http://arxiv.org/abs/2510.03306v1},
 year = {2025}
}

@article{2510.03859v1,
 abstract = {Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT},
 author = {Raghav Sharma and Manan Mehta},
 citations = {},
 comment = {22 pages},
 doi = {},
 eprint = {2510.03859v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning},
 url = {http://arxiv.org/abs/2510.03859v1},
 year = {2025}
}

@article{2510.04033v1,
 abstract = {Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.},
 author = {Ayush Noori and Adam Rodman and Alan Karthikesalingam and Bilal A. Mateen and Christopher A. Longhurst and Daniel Yang and Dave deBronkart and Gauden Galea and Harold F. Wolf III and Jacob Waxman and Joshua C. Mandel and Juliana Rotich and Kenneth D. Mandl and Maryam Mustafa and Melissa Miles and Nigam H. Shah and Peter Lee and Robert Korom and Scott Mahoney and Seth Hain and Tien Yin Wong and Trevor Mundel and Vivek Natarajan and Noa Dagan and David A. Clifton and Ran D. Balicer and Isaac S. Kohane and Marinka Zitnik},
 citations = {28},
 comment = {},
 doi = {},
 eprint = {2510.04033v1},
 journal = {arXiv preprint},
 source = {arXiv},
 title = {A global log for medical AI},
 url = {http://arxiv.org/abs/2510.04033v1},
 year = {2025}
}
