TY  - CONF
TI  - Role of Explainable AI in Medical Diagnostics and Healthcare: A Pilot Study on Parkinson's Speech Detection
T2  - 2024 10th International Conference on Control, Automation and Robotics (ICCAR)
SP  - 289
EP  - 294
AU  - S. S
AU  - A. S
AU  - S. S. N
AU  - D. S. S. S
PY  - 2024
KW  - Voice activity detection
KW  - Deep learning
KW  - Explainable AI
KW  - Reviews
KW  - Parkinson's disease
KW  - Hospitals
KW  - Decision making
KW  - AI
KW  - ML
KW  - Explainable AI
KW  - Explainability
KW  - Interpretability
KW  - Healthcare
DO  - 10.1109/ICCAR61844.2024.10569414
JO  - 2024 10th International Conference on Control, Automation and Robotics (ICCAR)
IS  - 
SN  - 2251-2454
VO  - 
VL  - 
JA  - 2024 10th International Conference on Control, Automation and Robotics (ICCAR)
Y1  - 27-29 April 2024
AB  - Decision making is one of the most important steps in healthcare applications. There are lot of Artificial Intelligence (AI) based tools have been developed to provide better solutions for diagnostic problems in last two or three decades. In hospitals environment, the trust on the results provided by these AI models is a question mark as they are simply a black box to healthcare professionals. This is due to the lack of explainability and interpretability of the results provided by those models. Explainable AI is a rapidly growing technology to address this issue by incorporating various methods to bring explainability and interpretability. This paper aims at providing a comprehensive review on explainable AI methods addressed for health care applications. Furthermore, a pilot study on the use of Explainable AI algorithms using Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) algorithms is experimented on Parkinson's disease detection.
ER  - 

TY  - CONF
TI  - AI in Precision Medicine
T2  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
SP  - 1
EP  - 5
AU  - S. P. Sarode
AU  - S. Khobragade
PY  - 2024
KW  - Industries
KW  - Ethics
KW  - Image recognition
KW  - Precision medicine
KW  - Hematology
KW  - Neural networks
KW  - Graphics processing units
KW  - Transforms
KW  - Electronic healthcare
KW  - Artificial intelligence
KW  - Digital Health. Data Integration
KW  - Deep Neural Networks (DNN). Virtual Biomarkers Screens
KW  - Single-Cell Research
KW  - Digital Image Recognition
DO  - 10.1109/IDICAIEI61867.2024.10842767
JO  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
Y1  - 29-30 Nov. 2024
AB  - The advancement of artificial intelligent (AI) and artificial intelligence (ML) to be technologies in the discipline of personalised medicine is currently causing a significant upheaval in the healthcare sector. This transforms the finding of solutions for the information that is on the patients to get better treatment outcomes. This study explores new achievements in these areas concentrating on AI and ML used within precision medicine, for instance, digital photograph analysis, single-cell investigations, and virtual screening. The introduction of AI and ML technologies leading to an enhancement of deep learning has been a significant result of large-scale datasets and computational technological breakthroughs in the research area, especially in the places of confinement, i.e., in the graphic processing units (GPUs) is the study that explores AI in precision medicine, encouraging the initiative to adopt the considerations in the ethical uses of AI and the diversified usage of data.
ER  - 

TY  - CONF
TI  - Review and Explore the Transformative Impact of Artificial Intelligence (AI) in Smart Healthcare Systems
T2  - 2024 International Conference on Advances in Computing Research on Science Engineering and Technology (ACROSET)
SP  - 1
EP  - 5
AU  - S. Nagar
AU  - M. Patidar
AU  - R. Sheikh
AU  - S. Singh
AU  - A. Kashiv
AU  - A. Jain
AU  - S. K. Namdeo
AU  - P. Paranjpe
AU  - D. S. Mandloi
PY  - 2024
KW  - Technological innovation
KW  - Reviews
KW  - Navigation
KW  - Telemedicine
KW  - Medical services
KW  - Real-time systems
KW  - Planning
KW  - Artificial intelligence
KW  - Prognostics and health management
KW  - Medical diagnostic imaging
KW  - Artificial Intelligence (AI)
KW  - Explainable AI (XAI)
KW  - Ethical Considerations
KW  - Healthcare Systems
KW  - Healthcare Resource Allocation
KW  - Smart Treatment Planning
DO  - 10.1109/ACROSET62108.2024.10743527
JO  - 2024 International Conference on Advances in Computing Research on Science Engineering and Technology (ACROSET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Advances in Computing Research on Science Engineering and Technology (ACROSET)
Y1  - 27-28 Sept. 2024
AB  - Because of its high level of complexity and data collection capabilities, the healthcare industry employs artificial intelligence (AI) more than other sectors. Artificial intelligence is becoming an increasingly important part of our daily routines. AI's features include extremely low error rates, constant accessibility, quick analysis, and real-time data dissemination. This paper examines the crucial role of artificial intelligence in intelligent healthcare systems, with a particular emphasis on enhancing healthcare delivery. Artificial intelligence in healthcare is rapidly evolving, with the potential to significantly enhance patient care, diagnosis, treatment, and cost distribution. It also presents issues of transparency, honesty, and availability. This study will look into the benefits, drawbacks, and other aspects of artificial intelligence in intelligent healthcare systems.
ER  - 

TY  - CONF
TI  - Evolution of Artificial Intelligence and Machine Learning in Betterment of Healthcare Services
T2  - 2024 International Conference on Healthcare Innovations, Software and Engineering Technologies (HISET)
SP  - 367
EP  - 369
AU  - A. I. Abu Eid
AU  - S. Maula
AU  - A. Wagh
AU  - V. Vyas
AU  - P. Patil
AU  - P. Sridhar
PY  - 2024
KW  - Industries
KW  - Technological innovation
KW  - Ethics
KW  - Medical services
KW  - Machine learning
KW  - Learning (artificial intelligence)
KW  - Software
KW  - Hardware
KW  - Artificial intelligence
KW  - Electronic medical records
KW  - artificial intelligence (AI)
KW  - healthcare
KW  - machine learning (ML)
KW  - patients
KW  - technology
DO  - 10.1109/HISET61796.2024.00107
JO  - 2024 International Conference on Healthcare Innovations, Software and Engineering Technologies (HISET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Healthcare Innovations, Software and Engineering Technologies (HISET)
Y1  - 18-19 Jan. 2024
AB  - Artificial Intelligence usually comprises a system that is composed of hardware and software. Machine learning is a subset of artificial intelligence (AI). The study discovered that the environment and contemporary technology on the current healthcare services have created new demands, which has led to the formation of these five categories of research. By matching eligible patients with clinical trials, AI technologies in the healthcare industry may also be beneficial for medical research. There are numerous other fascinating uses for AI in the biomedical field. It is evident that AI is becoming more and more relevant in the field of biomedicine. AI technologies may be able to assist in addressing significant health issues, but its potential may be constrained by the caliber of the health data that is now available and by the fact that AI cannot exhibit certain human traits, like empathy. Numerous ethical and societal concerns are brought up by the use of AI, many of which are also brought up by the use of data and healthcare technologies more generally.
ER  - 

TY  - CONF
TI  - AI-Driven Personalized Healthcare: Leveraging Multimodal Data for Precision Medicine
T2  - 2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)
SP  - 693
EP  - 697
AU  - R. Lin
PY  - 2024
KW  - Explainable AI
KW  - Genomics
KW  - Data integration
KW  - Medical services
KW  - Mental health
KW  - Real-time systems
KW  - Artificial intelligence
KW  - Bioinformatics
KW  - Electronic medical records
KW  - Medical diagnostic imaging
KW  - Artificial Intelligence
KW  - Multimodel Learning
KW  - Healthcare System
KW  - Personalized Healthcare
DO  - 10.1109/WI-IAT62293.2024.00112
JO  - 2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)
Y1  - 9-12 Dec. 2024
AB  - The integration of artificial intelligence (AI) into personalized healthcare is transforming medical practice by enabling precise, individualized treatments through multimodal data fusion. This paper explores how AI can combine diverse data sources-including genomic profiles, medical imaging, electronic health records (EHRs), and real-time data from wearable devices-to create comprehensive, patient-specific health insights. By leveraging AI-driven analysis, healthcare providers can offer more accurate diagnoses, personalized treatment plans, and proactive interventions that improve patient outcomes. Key use cases, such as personalized cancer treatment, cardiovascular disease management, and mental health monitoring, illustrate AI's potential to tailor treatments to individual profiles. Nonetheless, challenges including data integration, model interpretability, patient privacy, and the need for representative datasets pose obstacles to widespread adoption. Addressing these challenges will be essential to ensuring the effectiveness and trustworthiness of AI systems in clinical practice. Looking forward, the future of personalized healthcare will involve a fully integrated AI ecosystem capable of providing real-time, predictive insights through continuous multimodal data analysis. Advances in areas like AI-driven genomics, remote patient monitoring, and explainable AI will further enhance the precision, accessibility, and proactivity of healthcare, ushering in a new era of patient-centered care.
ER  - 

TY  - CONF
TI  - Artificial Intelligence in Healthcare Systems
T2  - 2024 IEEE 17th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)
SP  - 54
EP  - 57
AU  - T. Siradanai
AU  - C. L. Kok
AU  - C. K. Ho
AU  - Y. Y. Koh
AU  - T. H. Teo
PY  - 2024
KW  - Industries
KW  - Data privacy
KW  - Precision medicine
KW  - Decision making
KW  - Surgery
KW  - Medical services
KW  - Streaming media
KW  - Drug discovery
KW  - Artificial intelligence
KW  - Biomedical imaging
KW  - Artificial Intelligence (AI)
KW  - Healthcare
DO  - 10.1109/MCSoC64144.2024.00019
JO  - 2024 IEEE 17th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)
IS  - 
SN  - 2771-3075
VO  - 
VL  - 
JA  - 2024 IEEE 17th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)
Y1  - 16-19 Dec. 2024
AB  - Artificial intelligence (AI) is transforming the healthcare systems by introducing cutting-edge solutions that enhance diagnostic accuracy, optimize clinical workflows, and personalize patient care. This report explores the current and potential applications of AI in healthcare, with a focus on its role in medical imaging, personalized medicine, surgery, drug discovery, workflow management, and virtual healthcare. Using AI's ability to process vast datasets, detect patterns, and assist in decision-making, healthcare systems can achieve improved efficiency and patient outcomes. Despite its benefits, the integration of AI into healthcare faces challenges related to data privacy, algorithmic transparency, biasness, and ethical concerns. Additionally, this report discusses on the future of AI, highlighting its potential in healthcare industries.
ER  - 

TY  - CONF
TI  - Explainable AI for Chest Diagnosis Prediction
T2  - 2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)
SP  - 1
EP  - 6
AU  - M. Gangam
AU  - V. Baghel
AU  - M. M. Ali
AU  - M. Raj
AU  - A. pranav
AU  - V. ranjan
PY  - 2024
KW  - COVID-19
KW  - Explainable AI
KW  - Transfer learning
KW  - Sociology
KW  - Neural networks
KW  - Market research
KW  - Medical diagnosis
KW  - Artificial Intelligence
KW  - Explainable AI (XAI)
KW  - Convolutional Neural Networks (CNN)
KW  - COVID-19 Diagnosis
KW  - Deep Learning
DO  - 10.1109/ic-ETITE58242.2024.10493633
JO  - 2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)
Y1  - 22-23 Feb. 2024
AB  - Significant advancement has been achieved in medical reasoning (Artificial intelligence). However, the interpretability and confidence of many deep learning model discovery ideas face difficulties in fundamental applications like COVID-19 identification. The compatibility of Explainable AI (XAI) techniques with COVID-19 results according to chest X-ray pictures is examined in this work. Our model achieves uncompromising exactness and provides interpretable experiences in its dynamic interaction by combining Local Interpretable Model Agnostic Explanations (LIME) and the VGG16 architecture in conjunction with a transfer learning technique. Upgrading transparency, confidence, and comprehension in artificial intelligence-driven clinical diagnostics is the goal of the research.
ER  - 

TY  - CONF
TI  - Explainable AI (XAI) and Machine Learning Based Alert System for Lung Cancer Detection in Healthcare 4.0
T2  - 2024 IEEE Globecom Workshops (GC Wkshps)
SP  - 1
EP  - 6
AU  - A. Gaurav
AU  - B. B. Gupta
AU  - J. Wu
AU  - V. Arya
AU  - K. T. Chui
PY  - 2024
KW  - Support vector machines
KW  - Logistic regression
KW  - Accuracy
KW  - Explainable AI
KW  - Lung cancer
KW  - Medical services
KW  - Predictive models
KW  - Real-time systems
KW  - Planning
KW  - Diseases
KW  - Machine Learning
KW  - Explainable AI (XAI)
KW  - Lung Cancer Detection
KW  - Healthcare 4.0
DO  - 10.1109/GCWkshp64532.2024.11101118
JO  - 2024 IEEE Globecom Workshops (GC Wkshps)
IS  - 
SN  - 2166-0077
VO  - 
VL  - 
JA  - 2024 IEEE Globecom Workshops (GC Wkshps)
Y1  - 8-12 Dec. 2024
AB  - In the world more then 10% people are suffering from lung cancer, due to this there is a need for frameworks that can detects the lung cancer at preliminary stage. However, most of the machine learning models are act like a black-box, and their results are not easily understandable by the normal person and they also not provide any type of alert system. In this context, we proposed an early detection framework for lung cancer using machine learning and explainable AI, that will alert the patient at early stage. As lung cancer prediction is a binary classification problem, we used support vector machine (SVM) in our proposed framework for the detection of lung cancer. Our proposed framework achieves an accuracy of 95.39% in the detection of lung cancer. We also compared the proposed approach result with other traditional machine learning techniques.
ER  - 

TY  - CONF
TI  - A Comprehensive Review of Explainable AI Applications in Healthcare
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
SP  - 1
EP  - 8
AU  - A. Dhar
AU  - S. Gupta
AU  - E. S. Kumar
PY  - 2024
KW  - Visualization
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Reviews
KW  - Hospitals
KW  - Decision making
KW  - Skin
KW  - Medical diagnosis
KW  - Security
KW  - Lesions
KW  - Explainable Artificial Intelligence (XAI)
KW  - Machine learning
KW  - Deep learning
KW  - Healthcare
KW  - AI Transparency
KW  - Ethical AI
KW  - Model-Agnostic Methods
DO  - 10.1109/ICCCNT61001.2024.10725578
JO  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
IS  - 
SN  - 2473-7674
VO  - 
VL  - 
JA  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
Y1  - 24-28 June 2024
AB  - Explainable Artificial Intelligence (XAI) has emerged as a pivotal solution to address the opacity of conventional AI models, particularly in high-stakes applications like healthcare, finance, and security. While machine learning and deep learning excel at predictive tasks, their black box nature hinders interpretability and accountability. XAI aims to rectify this by enabling AI systems to provide understandable explanations for their decisions, fostering trust and transparency between humans and machines. This paper delves into the foundational principles and applications of XAI, emphasizing its role in enhancing collaboration and mitigating risks in AI deployment. Through techniques such as model-agnostic methods and interactive visualization, XAI empowers users to interrogate AI models and uncover biases or errors. Moreover, by enlightening decision-making processes, XAI facilitates comprehension and evaluation, vital for ensuring responsible and ethical AI usage. We examined recently published articles detailing the development and utilization of XAI in the healthcare sector. Our investigation involved searching scholarly databases including Scopus, Web of Science, IEEE Xplore, and PubMed for relevant publications spanning from 2018 to 2022. This review not only contributes to the existing body of literature on XAI but also serves as a roadmap for guiding future research endeavors in this burgeoning field.This comprehensive analysis emphasizes the critical role of Explainable Artificial Intelligence (XAI) in improving trust, transparency, and accountability in healthcare AI applications, establishing the path for further breakthroughs and wider implementation.
ER  - 

TY  - CONF
TI  - Explainable AI in Healthcare: Enhancing Transparency and Trust in Predictive Models
T2  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
SP  - 1660
EP  - 1664
AU  - R. K. Yekollu
AU  - T. B. Ghuge
AU  - S. S. Biradar
AU  - S. V. Haldikar
AU  - O. F. Mohideen Abdul Kader
PY  - 2024
KW  - Ethics
KW  - Explainable AI
KW  - Communication systems
KW  - Decision making
KW  - Lung cancer
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Sepsis
KW  - Medical diagnostic imaging
KW  - Artificial Intelligent
KW  - Healthcare
KW  - Trust
KW  - Predictive Models
DO  - 10.1109/ICESC60852.2024.10690121
JO  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
IS  - 
SN  - 2996-5357
VO  - 
VL  - 
JA  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
Y1  - 7-9 Aug. 2024
AB  - Explainable AI in healthcare brings transparency and trust to predictive models, empowering doctors and patients to understand decision-making processes. Validating models against clinical expertise improves accuracy and addresses ethical concerns. Real-life examples include detecting lung cancer nodules and predicting sepsis. The future holds enhanced diagnosis and treatment strategies, improved patient engagement, and deeper insights into algorithmic decision-making.
ER  - 

TY  - CONF
TI  - Towards Explainable AI: Interpretable Models for Complex Decision-making
T2  - 2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
SP  - 1
EP  - 5
AU  - J. Singh
AU  - S. Rani
AU  - G. Srilakshmi
PY  - 2024
KW  - Training
KW  - Analytical models
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Closed box
KW  - Predictive models
KW  - Explainable AI (XAI)
KW  - Interpretable Models
KW  - Complex Decision-making
KW  - Transparency in AI
KW  - Machine Learning Algorithms
DO  - 10.1109/ICKECS61492.2024.10616500
JO  - 2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
Y1  - 18-19 April 2024
AB  - In the rapidly evolving landscape of artificial intelligence (AI), the integration of explainable AI (XAI) models into complex decision-making processes has become paramount. This paper addresses the critical need for transparency and interpretability in AI systems, particularly those involved in high-stakes decisions in sectors such as healthcare, finance, and autonomous systems. Traditional AI models, while powerful, often operate as “black boxes,” offering little to no insight into their decision-making processes. This opacity can lead to trust issues, ethical concerns, and regulatory challenges. To bridge this gap, we propose a novel framework of interpretable models designed to enhance the explainability of AI without compromising on performance. Our approach leverages state-of-the-art techniques in machine learning, including feature importance analysis and model-agnostic methods, to develop models that are both accurate and interpretable. We demonstrate the efficacy of our proposed models through rigorous testing on complex datasets, showcasing significant improvements in transparency and user trust. Furthermore, our findings reveal that our interpretable models can achieve comparable, if not superior, performance to traditional “black box” models, thereby challenging the notion that explainability necessarily comes at the cost of accuracy. This work contributes to the burgeoning field of XAI by providing a viable pathway towards the development of AI systems that are not only powerful but also comprehensible and trustworthy.
ER  - 

TY  - CONF
TI  - AI-Enabled Data-Driven Approaches for Personalized Medicine and Healthcare Analytics
T2  - 2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM)
SP  - 1
EP  - 5
AU  - D. Mendhe
AU  - A. Dogra
AU  - P. S. Nair
AU  - S. Punitha
AU  - K. S. Preetha
AU  - S. B. G. T. Babu
PY  - 2024
KW  - Technological innovation
KW  - Ethics
KW  - Accuracy
KW  - Precision medicine
KW  - Ecosystems
KW  - Artificial neural networks
KW  - Mathematics
KW  - AI-Enabled Approaches
KW  - Disease Outcome Prediction
KW  - Personalized Medicine
KW  - Healthcare Analytics
KW  - Data-Driven Healthcare
KW  - Decision Support Systems
KW  - Healthcare Innovation
KW  - SHAP Values
KW  - Model Interpretability
KW  - Deep Neural Networks
DO  - 10.1109/ICONSTEM60960.2024.10568722
JO  - 2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM)
Y1  - 4-5 April 2024
AB  - In the context of personalized medicine and healthcare analytics, this study digs into the potentially game-changing area of AI-enabled data-driven Approaches. Our research demonstrates the possibility of using a deep neural network for illness outcome prediction, with interpretability ensured by SHAP values. Both the importance of AI in facilitating personalized therapy, data-driven insights, and ethical compliance and the need for robust model performance are emphasized in the study. The study's results provide an appealing picture of a future in which healthcare is more accurate, efficient, and patient-centered as the healthcare environment continues to change. This study sets the groundwork for an AI-driven healthcare ecosystem where innovations improve the quality and delivery of care, as well as patient outcomes, treatment, and medical research.
ER  - 

TY  - CONF
TI  - Deep Learning Models in Healthcare
T2  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
SP  - 1
EP  - 6
AU  - S. Thakur
AU  - S. Narad
PY  - 2024
KW  - Deep learning
KW  - Biological system modeling
KW  - Precision medicine
KW  - Medical services
KW  - Computer architecture
KW  - Predictive models
KW  - Feature extraction
KW  - Data models
KW  - Complexity theory
KW  - Medical diagnostic imaging
KW  - deep learning
KW  - healthcar
KW  - biomedical informatics
KW  - precision medicine
KW  - electronic health records
KW  - medical imaging
DO  - 10.1109/IDICAIEI61867.2024.10842703
JO  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
Y1  - 29-30 Nov. 2024
AB  - Modern society has placed more emphasis on well-being, focusing on better healthcare services at whatever cost. In the same vein, the healthcare system is working towards extending population health, improving treatment efficiency, and incrementally enhancing the experiences of patients. However, gaining and making sense of various complex biomedical data has been hard to get in the process of reforming healthcare. Modern biomedical research combines all types of information, from imaging to electronic health records, text, and sensor data, characterized by complexity, heterogeneity, and often ambiguity. Conventional statistical learning and data mining methods need a tremendous amount of feature engineering to extract meaningful insights and then build predictive or clustering models. However, these approaches are badly impeded due to the complex nature of the data and the lack of domain expertise. Deep learning is a disruptive technology that sidesteps conventional feature engineering and enables end-to-end learning directly from complex clinical data. Advanced architectures formulate a paramount data analysis at unprecedented scales and complexities, foretelling rapid, efficient, and precise insights. Deep learning integrated into health care holds several advantages in decision-making, mimicking human cognition. The multiple-layer architectures enable superior computational capabilities and refine vast amounts of previously untapped healthcare data. A capability like this would democratize expertise and let all health practitioners work right on the front lines while performing at a top specialist level. The sharing of deep learning models across different healthcare institutions could be done without the fear of leaking patient data, and this will open the way for a new generation of personalized medicine. However, the interpretability of models and other ethical concerns remain pertinent challenges. This review underlines deep learning as having a transformative role in health care by underscoring that it will help harness the vast biomedical data for the betterment of human health.
ER  - 

TY  - CONF
TI  - A Comprehensive Review of Supervised Learning Algorithms in Healthcare Applications
T2  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
SP  - 1
EP  - 6
AU  - A. Barhate
AU  - A. Tale
AU  - N. Jikar
AU  - P. Verma
AU  - P. Kumar
AU  - P. Yesankar
PY  - 2024
KW  - Industries
KW  - Deep learning
KW  - Ethics
KW  - Systematics
KW  - Reviews
KW  - Precision medicine
KW  - Supervised learning
KW  - Medical services
KW  - Classification algorithms
KW  - Tuning
KW  - Supervised Learning
KW  - Machine Learning
KW  - Healthcare Applications
KW  - Deep Learning
DO  - 10.1109/IDICAIEI61867.2024.10842845
JO  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
Y1  - 29-30 Nov. 2024
AB  - Supervised learning has revolutionized the concept of personalization in treatment with the development of Precision Medicine. This review aims to provide a systematic analysis of the utilization of supervised learning in healthcare and its functions in diagnostics and prognoses, recommendations for treatment, and patient surveillance. Classification algorithms such as Logistic Regression, Support Vector Machines, and Decision Trees. Regression techniques such as Linear Regression, Polynomial Regression, and Neural Networks. Ensemble methods such as Bagging, Boosting, and Stacking. Deep learning techniques such as Convolutional Neural Networks, Recurrent Neural Networks, and Long Short-Term Memory are discussed for their efficiency and applicability in various healthcare applications. There are strengths highlighted by the review including enhanced diagnostic capabilities and patient-targeted therapy alongside weaknesses such as quality of data, interpretability as well as compatibility with existing healthcare systems. This review gives a more generalized approach to how supervised learning algorithms help to enhance healthcare results and identifies the likely topic that requires more research in the future.
ER  - 

TY  - CONF
TI  - Examining the Influence of Explainable Artificial Intelligence on Healthcare Diagnosis and Decision Making
T2  - 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT)
SP  - 136
EP  - 141
AU  - V. Jain
AU  - A. Dhruv
PY  - 2024
KW  - Visualization
KW  - Additives
KW  - Explainable AI
KW  - Computational modeling
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Explainable AI (XAI)
KW  - Local Interpretable Model-Agnostic Explanations (LIME)
KW  - SHapley Additive exPlanations (SHAP)
DO  - 10.1109/InCACCT61598.2024.10551183
JO  - 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT)
Y1  - 2-3 May 2024
AB  - Artificial Intelligence (AI) has made significant strides in revolutionizing healthcare, offering unparalleled opportunities for improved diagnostics and decision-making. The use of AI in healthcare sector is becoming more popular in today’s era. In recent times, eXplainable AI (XAI) has shown significant improvement in medical diagnosis. Doctors are also validating patient test reports via XAI predictions. However, these intelligent systems pose challenges with regards to the underlying understanding and interpretations of AI models. In other words, it is essential to investigate the reasons when these models make certain predictions. The proposed work aims to improve the interpretability of various AI models by employing 2 techniques, Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP). The study uses the Random Forest Classifier on the Breast Cancer Wisconsin (Diagnostic) dataset. The findings of this study are expected not only to advance AI technologies in healthcare but also build trust in doctors and other healthcare experts.
ER  - 

TY  - CONF
TI  - The Future of Patient Care: Revolutionizing Treatment Plans through Deep Learning and Precision Medicine
T2  - 2024 IEEE Conference on Engineering Informatics (ICEI)
SP  - 1
EP  - 10
AU  - Riyaz
PY  - 2024
KW  - Deep learning
KW  - Ethics
KW  - Recurrent neural networks
KW  - Precision medicine
KW  - Genomics
KW  - Transforms
KW  - Proteomics
KW  - Predictive models
KW  - Data models
KW  - Artificial intelligence
KW  - Deep Learning
KW  - Precision Medicine
KW  - Health Care
KW  - Artificial Intelligence
DO  - 10.1109/ICEI64305.2024.10912362
JO  - 2024 IEEE Conference on Engineering Informatics (ICEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE Conference on Engineering Informatics (ICEI)
Y1  - 20-28 Nov. 2024
AB  - The evolution of patient care is increasingly influenced by precision medicine, which seeks to customize medical treatments based on the unique characteristics of each individual. However, the integration of advanced technologies, particularly deep learning, poses significant challenges and opportunities in this domain. This study investigates the application of deep learning algorithms in crafting precision medicine strategies, addressing critical research problems such as the analysis of intricate datasets that encompass genomic, proteomic, and clinical information. We propose a robust framework that leverages convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to effectively identify biomarkers and forecast patient-specific responses to various therapies, with a focus on diseases like cancer and cardiovascular conditions. Our results indicate that deep learning models markedly improve the accuracy of disease prediction and the personalization of treatment plans compared to conventional approaches. Furthermore, we examine the hurdles related to data variability, the interpretability of models, and ethical implications tied to the implementation of these technologies in clinical practice. Through detailed case studies, we highlight how deep learning can transform patient care by facilitating more precise and individualized treatment strategies. This research emphasizes the necessity for interdisciplinary collaboration in advancing precision medicine and outlines potential pathways for integrating artificial intelligence into healthcare systems.
ER  - 

TY  - CONF
TI  - An Overview for Trustworthy and Explainable Artificial Intelligence in Healthcare
T2  - 2024 28th International Conference on Information Technology (IT)
SP  - 1
EP  - 5
AU  - K. Arslanoğlu
AU  - M. Karaköse
PY  - 2024
KW  - Data privacy
KW  - Analytical models
KW  - Systematics
KW  - Explainable AI
KW  - Reviews
KW  - Medical services
KW  - Data models
KW  - XAI
KW  - trustworthy
KW  - blockchain
KW  - health
DO  - 10.1109/IT61232.2024.10475733
JO  - 2024 28th International Conference on Information Technology (IT)
IS  - 
SN  - 2836-3744
VO  - 
VL  - 
JA  - 2024 28th International Conference on Information Technology (IT)
Y1  - 21-24 Feb. 2024
AB  - Recently, the increased use of artificial intelligence in healthcare has significantly changed the developments in the field of medicine. Medical centres have adopted AI applications and used it in many applications to predict disease diagnosis and reduce health risks in a predetermined way. In addition to Artificial Intelligence (AI) techniques for processing data and understanding the results of this data, Explainable Artificial Intelligence (XAI) techniques have also gained an important place in the healthcare sector. In this study, reliable and explainable artificial intelligence studies in the field of healthcare were investigated and the blockchain framework, one of the latest technologies in the field of reliability, was examined. Many researchers have used blockchain technology in the healthcare industry to exchange information between laboratories, hospitals, pharmacies, and doctors and to protect patient data. In our study, firstly, the studies whose keywords were XAI and Trustworthy Artificial Intelligence were examined, and then, among these studies, priority was given to current articles using Blockchain technology. Combining the existing methods and results of previous studies and organizing these studies, our study presented a general framework obtained from the reviewed articles. Obtaining this framework from current studies will be beneficial for future studies of both academics and scientists.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence in Healthcare -A Review
T2  - 2024 International Conference on Inventive Computation Technologies (ICICT)
SP  - 1059
EP  - 1064
AU  - I. S
AU  - A. S. Pillai
PY  - 2024
KW  - Explainable AI
KW  - Reviews
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Stroke (medical condition)
KW  - Reliability
KW  - Healthcare
KW  - Prediction
KW  - Brain Stroke
KW  - Decision-Making
KW  - Artificial Intelligence
KW  - Explainable Artificial Intelligence
DO  - 10.1109/ICICT60155.2024.10544745
JO  - 2024 International Conference on Inventive Computation Technologies (ICICT)
IS  - 
SN  - 2767-7788
VO  - 
VL  - 
JA  - 2024 International Conference on Inventive Computation Technologies (ICICT)
Y1  - 24-26 April 2024
AB  - This study focuses on the application of Explainable Artificial Intelligence (XAI) in Stroke, heart attack and cancer prediction, highlighting the importance of XAI. This technology helps users to clearly understand the reason behind detection, prediction and classification. XAI further enhances the capabilities of these models ensuring that their predictions are accurate but also transparent, interpretable, and actionable. This paper explores recent research and developments in XAI Applications for three healthcare domains. It also examines the algorithm, datasets, and accuracy achieved, shedding light on how XAI has improved the accuracy and trustworthiness of predictive models. This study thoroughly analyses Explainable AI’s effect on healthcare, delivering insightful information about how XAI is changing the field of predictive medicine and enhancing patient outcomes and healthcare decision-making.
ER  - 

TY  - JOUR
TI  - A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?
T2  - IEEE Transactions on Artificial Intelligence
SP  - 1429
EP  - 1442
AU  - S. Bharati
AU  - M. R. H. Mondal
AU  - P. Podder
PY  - 2024
KW  - Medical services
KW  - Medical diagnostic imaging
KW  - Deep learning
KW  - Explainable AI
KW  - Artificial intelligence
KW  - Biomedical imaging
KW  - Deep learning (DL)
KW  - explainable artificial intelligence (XAI)
KW  - healthcare
KW  - medical care
KW  - medical imaging
KW  - medicine
DO  - 10.1109/TAI.2023.3266418
JO  - IEEE Transactions on Artificial Intelligence
IS  - 4
SN  - 2691-4581
VO  - 5
VL  - 5
JA  - IEEE Transactions on Artificial Intelligence
Y1  - April 2024
AB  - Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses standards for relevant work published from 1 January 2012 to 2 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will contribute to the formalization of the XAI field.
ER  - 

TY  - CONF
TI  - Advanced Machine Learning Algorithms for 1Predictive Analytics in Healthcare to Enhance Patient Outcomes with Data-Driven Insights
T2  - 2024 International Conference on Recent Advances in Science and Engineering Technology (ICRASET)
SP  - 1
EP  - 5
AU  - R. Pandiarajan
AU  - J. Jagannathan
AU  - P. S. Ramesh
AU  - A. Ponmalar
AU  - Sudha
PY  - 2024
KW  - Data privacy
KW  - Machine learning algorithms
KW  - Hospitals
KW  - Data integrity
KW  - Machine learning
KW  - Reinforcement learning
KW  - Predictive models
KW  - Data models
KW  - Predictive analytics
KW  - Diseases
KW  - Machine learning
KW  - predictive analytics
KW  - healthcare
KW  - patient outcomes
KW  - data-driven insights
KW  - disease prediction
KW  - chronic disease management
KW  - hospital operations
KW  - data quality
KW  - privacy
KW  - model interpretability
DO  - 10.1109/ICRASET63057.2024.10895597
JO  - 2024 International Conference on Recent Advances in Science and Engineering Technology (ICRASET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Recent Advances in Science and Engineering Technology (ICRASET)
Y1  - 21-22 Nov. 2024
AB  - This research aims to explore the use of modern complex defensive machine learning algorithms in the provision of predictive analytics for health improvement. Incorporating electronic health records, medical image information, and genomic data the work proposes and assesses multiple machine learning models including deep learning, ensemble learning, and reinforcement learning. The main findings shown very high performance in disease prediction, in chronic illness and in the application of efficiency improvements in hospitals as a whole. Overall, such threats as data quality, data privacy, and model interpretability have been discussed in the research and emphasize the importance of the machine learning approach in healthcare. It is highlighted that further development and integration of the interdisciplinary approaches as well as the use of the transparent and interpretable artificial intelligence and machine learning models can be urgent to increase the acceptance among the healthcare staff. This paper enhances the literature on applying machine learning in healthcare systems adding to the repertoire of knowledge for improved patient care.
ER  - 

TY  - STD
TI  - IEEE Guide for an Architectural Framework for Explainable Artificial Intelligence
T2  - IEEE Std 2894-2024
SP  - 1
EP  - 55
PY  - 2024
KW  - IEEE Standards
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Artificial intelligence
KW  - Machine learning
KW  - AI
KW  - architectural framework
KW  - artificial intelligence
KW  - explainable AI
KW  - explainable artificial intelligence
KW  - IEEE 2894™
KW  - machine learning
KW  - XAI
DO  - 10.1109/IEEESTD.2024.10659410
JO  - IEEE Std 2894-2024
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE Std 2894-2024
Y1  - 30 Aug. 2024
AB  - A new wave of artificial intelligence applications that offer extensive benefits to our daily lives has been led to by dramatic success in machine learning. The loss of explainability during this transition, however, means vulnerability to vicious data, poor model structure design, and suspicion of stakeholders and the general public--all with a range of legal implications. The study of explainable AI (XAI), which is an active research field that aims to make AI systems results more understandable to humans, has been called for by this dilemma. This is a field with great hopes for improving the trust and transparency of AI-based systems and is considered a necessary route for AI to move forward. A technological blueprint for building, deploying, and managing machine learning models, while meeting the requirements of transparent and trustworthy AI by adopting a variety of XAI methodologies, is provided by this guide. It defines the architectural framework and application guidelines for explainable AI, including: description and definition of XAI; the types of XAI methods and the application scenarios to which each type applies; and performance evaluation of XAI.
ER  - 

TY  - CONF
TI  - Empowering Decision Making in Healthcare - A Comparative Analysis of eXplainable AI Techniques Using SHAP and LIME for Cancer Patients Data
T2  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
SP  - 1
EP  - 5
AU  - S. Mujawar
AU  - M. Sayed
AU  - A. Chaware
PY  - 2024
KW  - Machine learning algorithms
KW  - Accuracy
KW  - Explainable AI
KW  - Impurities
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Bagging
KW  - Cancer
KW  - Machine learning
KW  - XAI
KW  - SHAP
KW  - LIME
KW  - Bagging
KW  - Gini Importance Plot
KW  - Permutation Importance Plot
KW  - PDP
DO  - 10.1109/DELCON64804.2024.10867049
JO  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
Y1  - 21-23 Nov. 2024
AB  - The present study undertook a comparative analysis of eXplainable AI techniques using SHAP and LIME for cancer patient data. The purpose of the present study is to highlight the lack of trustworthiness on machine learning algorithms. It also focused on the effectiveness of using an advanced technique as eXplainable Artificial Intelligence (XAI) to empower decision making in health care. The present study attempted to understand the predictive models when applied to cancer patient data from the National Cancer Institute's. The methodology used was SHAP (SHapley Additive Explanations) and LIME (Local Interpretable Model-Agnostic Explanations) to shed light on machine learning model. Initially, four machine algorithms were trained to get the best predictive model. The performance metrics were studied on different algorithm and accordingly all the result were compared. The Gini Importance Plot and Permutation Importance Plot were applied to assess how much each feature reduces impurity (like Gini impurity) across all trees. Further to get a deeper insights XAI technique SHAP and Lime along with a case study on a patient was performed through prediction formula that gave an accurate and trustworthy result. Partial Dependence Plot (PDP) was used to show the overall relationship between a feature and the predicted outcome, averaged over the dataset. The finding of the study showed Bagging algorithm performed the best in all aspect. It can be concluded that using eXplainable AI techniques makes the result more reliable and trustworthy. It was interpreted that the present study gave a promising AI-powered healthcare solutions for better and easier understanding of machine learning algorithm in health care domain. Based on the findings and result of the present study it can be suggested that eXplainable AI can be further used in other domains like finance, cyber security and law enforcement.
ER  - 

TY  - CONF
TI  - Fostering Trust in AI-Driven Healthcare: A Brief Review of Ethical and Practical Considerations
T2  - 2024 International Symposium on Electronics and Telecommunications (ISETC)
SP  - 1
EP  - 4
AU  - C. L. Sîrbu
AU  - M. A. Mercioni
PY  - 2024
KW  - Ethics
KW  - Data privacy
KW  - Reviews
KW  - System performance
KW  - Buildings
KW  - Medical services
KW  - Telecommunications
KW  - Artificial intelligence
KW  - Monitoring
KW  - Standards
KW  - algorithmic bias
KW  - artificial intelligence
KW  - data privacy
KW  - ethical standards
KW  - explainable AI
KW  - informed consent
KW  - healthcare
KW  - transparency
DO  - 10.1109/ISETC63109.2024.10797264
JO  - 2024 International Symposium on Electronics and Telecommunications (ISETC)
IS  - 
SN  - 2475-7861
VO  - 
VL  - 
JA  - 2024 International Symposium on Electronics and Telecommunications (ISETC)
Y1  - 7-8 Nov. 2024
AB  - As artificial intelligence (AI) continues to become more integrated into healthcare systems, establishing trust is crucial for its successful implementation and widespread use. This paper examines the ethical and practical considerations necessary for cultivating trust in AI-driven healthcare. It begins by exploring the ethical challenges that can erode patient and provider confidence in AI, such as concerns about data privacy, informed consent, and algorithmic bias. The discussion then moves to the importance of transparency in AI systems, emphasizing the need for explainable AI models that enable healthcare professionals to understand and interpret AI-driven recommendations effectively. Additionally, the paper highlights the vital role of accountability and governance frameworks to ensure that AI applications comply with ethical standards and regulations. It also considers practical aspects, including the implementation of strong data management practices, ongoing monitoring of AI system performance, and the participation of multidisciplinary teams in the development and deployment of AI solutions. By addressing these ethical and practical issues, this paper aims to offer a strategic approach for building trust in AI-driven healthcare, ultimately leading to better patient outcomes, improved care delivery, and a more ethical application of advanced technologies within the health sector.
ER  - 

TY  - CONF
TI  - Considering the Clinical Significance of Artificial Intelligence and Biosensors in the Healthcare Sector: A Review
T2  - 2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)
SP  - 1
EP  - 5
AU  - B. Haque
AU  - E. A. Siddiqui
AU  - S. K. Jha
PY  - 2024
KW  - Technological innovation
KW  - Machine learning algorithms
KW  - Reviews
KW  - Precision medicine
KW  - Point of care
KW  - Biosensors
KW  - Artificial intelligence
KW  - Biosensor
KW  - Healthcare
KW  - Computer-aided diagnosis
KW  - Artificial intelligence
KW  - Clinical datasets
DO  - 10.1109/SCEECS61402.2024.10482041
JO  - 2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)
IS  - 
SN  - 2688-0288
VO  - 
VL  - 
JA  - 2024 IEEE International Students' Conference on Electrical, Electronics and Computer Science (SCEECS)
Y1  - 24-25 Feb. 2024
AB  - Over the last decade, medical imaging, wearable sensors, personal health records, and public health groups have increased medical research data. This data may be used by cloud computing, GPUs, FPGAs, and TPUs. Several powerful AI algorithms have been created to analyze healthcare's massive databases. Health, biology, biosensors, and AI advances are covered. We address precision medicine, medical imaging, and IoT biosensors with machine learning. We study the newest wearable biosensing tech. Modern gadgets detect ailments using AI to examine electrochemical and electrophysiological data. These innovations show customized medicine by offering accurate, efficient, and economical point-of-care therapy. In healthcare data, edge computing, quick AI, and federated learning are examined. We finish with data-driven AI, IoT healthcare and biosensors, and data modality distribution adjustments. My last sentence is future thinking.
ER  - 

TY  - CONF
TI  - “Explainable AI” Disease Detection with Reasoning
T2  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
SP  - 222
EP  - 227
AU  - M. Kumar
AU  - K. Ramrakhiyani
AU  - H. Garg
PY  - 2024
KW  - Adaptation models
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Transforms
KW  - Predictive models
KW  - Prediction algorithms
KW  - SHAP
KW  - explainable AI
KW  - Large language models
KW  - Neural networks
KW  - shapley scores
KW  - Prompt development
KW  - Acceptance Score
KW  - Agreement Score
KW  - Weightage
DO  - 10.1109/ICAAIC60222.2024.10575153
JO  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
Y1  - 5-7 June 2024
AB  - Explainable AI (XAI) has gained significant attention globally due to its capability to provide transparency and understanding of complex machine learning models. We explore the potential benefits of XAI and integrate them into the domain of disease detection. Our objective is to leverage the power of large language models coupled with advanced explainability algorithms to enhance the accuracy and interpretability of disease predictions based on input symptoms. By doing so, we aim to not only achieve precise disease identification but also offer clear and comprehensible explanations for the predictions made. This approach holds the promise of bridging the gap between the intricate nature of AI-driven disease detection systems and the need for transparency and trustworthiness in medical diagnoses.
ER  - 

TY  - CONF
TI  - Smart Healthcare: Harnessing the Power of Machine Learning for Predictive Analysis
T2  - 2024 Parul International Conference on Engineering and Technology (PICET)
SP  - 1
EP  - 7
AU  - A. Barhate
AU  - P. Kumar
AU  - P. Verma
AU  - N. Jikar
AU  - A. Tale
AU  - V. Hikre
PY  - 2024
KW  - Ethics
KW  - Solid modeling
KW  - Medical services
KW  - Machine learning
KW  - Radiology
KW  - Supercomputers
KW  - Real-time systems
KW  - Internet of Things
KW  - Predictive analytics
KW  - Biomedical imaging
KW  - Machine Learning
KW  - Predictive Analytics
KW  - Smart Healthcare
KW  - Disease Detection
KW  - Medical Imaging
DO  - 10.1109/PICET60765.2024.10716168
JO  - 2024 Parul International Conference on Engineering and Technology (PICET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Parul International Conference on Engineering and Technology (PICET)
Y1  - 3-4 May 2024
AB  - Smart healthcare employs cloud computing, the Internet of Things (IoT), and supercomputers to manage the healthcare system. Medical data storage volume is increasing, necessitating the adoption of Machine Learning (ML) models for predicting outcomes, particularly in the treatment of chronic illnesses. The trend of care personalization and technological integration is highlighted by the development of ML applications for clinical decision support systems, including real-time predictive analytics. Predictive analytics applications employ ML and Artificial Intelligence (AI) to forecast patient risks, outcomes, and treatment responses. Predictive analytics is more important in healthcare when it comes to detecting Alzheimer's disease, cardiovascular events, radiology, and other conditions. Therefore, the current study highlights the change in illness identification, medical imaging, and personalized medication brought about by ML and Predictive Analytics in Precision Medicine (PM). However, ethical concerns and the deployment of transparent AI models still need more research. This study anticipates a time when healthcare will be quicker, better informed, and more patient-centered by using the insights provided by intelligent healthcare data. By effectively saving patient lives, the use of ML in healthcare ushers in a new era for the industry.
ER  - 

TY  - CONF
TI  - Explainable Artificial intelligence based framework for orthopedic patient classification using biomechanical features
T2  - 2024 International Conference on Emerging Systems and Intelligent Computing (ESIC)
SP  - 637
EP  - 642
AU  - V. Aelgani
AU  - S. K. Gupta
AU  - V. A. Narayana
PY  - 2024
KW  - Biomechanics
KW  - Deep learning
KW  - Osteoporosis
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Biological system modeling
KW  - Predictive models
KW  - Orthopedic
KW  - Biomechanical Features
KW  - Machine Learning
KW  - Ensemble learning
KW  - deep learning
KW  - and explainable AI
DO  - 10.1109/ESIC60604.2024.10481671
JO  - 2024 International Conference on Emerging Systems and Intelligent Computing (ESIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Emerging Systems and Intelligent Computing (ESIC)
Y1  - 9-10 Feb. 2024
AB  - The benefit of using explainable AI for classifying patients based on biomechanical features is that it makes AI-based diagnoses and predictions more transparent, accountable, and clinically valuable. This results in improved patient care, improved decision-making, and a deeper comprehension of conditions. The medical field has widely embraced the application of predictive models for medical diagnosis. Various AI models are used for patient classification and illness detection in the medical diagnostic sector. This study’s main objective is to help medical practitioners anticipate osteoporosis. In our study, we have utilised machine learning algorithms to evaluate the effectiveness of each algorithm in identifying and categorizing orthopedic patients. We grouped machine learning algorithms into three cohorts, viz., solitary learning, ensemble learning, and deep learning. Each cohort is evaluated on a dataset containing 310 patients’ biomechanical features. Our experimental results indicate that ensemble AI models are superior to solitary and deep learning models. The best ensemble classifier, Extra-Tree, achieved 89% accuracy and a 96% AUC score in classifying patients suffering from orthopedics. Finally, we explained the extra-tree ensemble model predictions using the LIME and SHAP explainable artificial intelligence frameworks.
ER  - 

TY  - CONF
TI  - A Comparative Study of Artificial Intelligence and eXplainable AI Techniques for Pulmonary Disease Detection and Its Severity Classification
T2  - 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)
SP  - 1
EP  - 7
AU  - L. Pawar
AU  - S. Patil
AU  - S. Dhotre
AU  - S. Sonawane
PY  - 2024
KW  - Surveys
KW  - Deep learning
KW  - Technological innovation
KW  - Pneumonia
KW  - Explainable AI
KW  - Tuberculosis
KW  - Pulmonary diseases
KW  - Taxonomy
KW  - Classification algorithms
KW  - Artificial intelligence
KW  - Chest Radiograph
KW  - Feature Extraction
KW  - Deep Learning
KW  - Severity Check
KW  - eXplainable AI
DO  - 10.1109/IDAP64064.2024.10710903
JO  - 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP)
Y1  - 21-22 Sept. 2024
AB  - As respiratory diseases like tuberculosis, pneumonia, COVID-19, and cancer become more common, the need for accurate and efficient diagnostic tools is growing. This paper provides an overview of the use of artificial intelligence (AI) and explainable AI techniques for pulmonary disease detection and severity classification. Using AI, especially deep learning (DL) algorithms, has shown promise in automating the process of detecting pulmonary diseases from chest radiographs. This study looks at different AI methodologies, such as deep learning architectures and preprocessing techniques, and uses explainable AI (XAI) to improve pulmonary disease detection and severity classification mechanisms. It also addresses the restrictions and challenges associated with AI-based disease detection, offering insights into possible future directions. To group cutting-edge techniques according to image types, DL algorithms, XAI techniques, and targeted pulmonary diseases, a taxonomy is put out. By helping researchers organize their work and contributions, this taxonomy eventually improves the efficiency of AI-assisted pulmonary disease detection and severity classification systems.
ER  - 

TY  - CONF
TI  - Revolutionizing Medical Diagnostics with Transparent AI-Driven Decision Support Systems
T2  - 2024 4th International Conference on Mobile Networks and Wireless Communications (ICMNWC)
SP  - 1
EP  - 7
AU  - A. M
AU  - T. Mittal
AU  - K. C. Sunkara
AU  - H. Pandey
AU  - G. Jadhav
AU  - V. Nemane
PY  - 2024
KW  - Decision support systems
KW  - Deep learning
KW  - Wireless communication
KW  - Surveys
KW  - Visualization
KW  - Accuracy
KW  - Oncology
KW  - Medical diagnosis
KW  - Usability
KW  - Medical diagnostic imaging
KW  - explainable AI
KW  - decision support systems
KW  - healthcare diagnostics
KW  - SHAP
KW  - LIME
KW  - clinical decision-making
KW  - diagnostic accuracy
KW  - transparency
KW  - interpretability
KW  - patient management
DO  - 10.1109/ICMNWC63764.2024.10872336
JO  - 2024 4th International Conference on Mobile Networks and Wireless Communications (ICMNWC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Mobile Networks and Wireless Communications (ICMNWC)
Y1  - 4-5 Dec. 2024
AB  - This study introduces an innovative AI-Driven Decision Support System (DSS) for revolutionizing healthcare diagnostics, emphasizing the use of Explainable AI (XAI) to enhance transparency and trust in medical decision-making. The proposed system integrates machine learning and deep learning algorithms to analyze and interpret complex medical data, providing clinicians with clear, understandable insights that support diagnostic accuracy and patient outcomes. The system was evaluated on a dataset of 25,000 patient records across multiple diagnostic domains, including cardiology, oncology, and radiology. Key performance metrics showed a significant improvement in diagnostic precision and explain ability. The model achieved an accuracy of 94.8% in diagnosing cardiac conditions, with an explanation accuracy of 92.1%, measured by how well the XAI techniques correlated with expert evaluations. For oncology cases, the system demonstrated an 87.5% reduction in false positives, while maintaining an overall accuracy of 93.2%. In terms of clinician trust, surveys indicated a 35% increase in confidence when using the XAI-based system compared to traditional black-box models. Additionally, the time to generate diagnostic explanations was reduced by 40%, improving clinical workflow and decision-making efficiency. These results underscore the transformative potential of integrating XAI into healthcare decision support systems, providing not only accurate predictions but also interpretable insights that enhance trust and usability in medical diagnostics.
ER  - 

TY  - CONF
TI  - Leveraging Explainable Artificial Intelligence (XAI) Methods Supporting Local and Global Explainability for Smart Grids
T2  - 2024 Global Energy Conference (GEC)
SP  - 164
EP  - 169
AU  - G. Ozdemir
AU  - U. Ozdemir
AU  - M. Kuzlu
AU  - F. O. Catak
PY  - 2024
KW  - Photovoltaic systems
KW  - Explainable AI
KW  - Finance
KW  - Estimation
KW  - Medical services
KW  - Learning (artificial intelligence)
KW  - Predictive models
KW  - Smart grids
KW  - Reliability
KW  - Forecasting
KW  - Explainable AI (XAI)
KW  - forecasting
KW  - solar PV power generation
KW  - smart grids
DO  - 10.1109/GEC61857.2024.10881108
JO  - 2024 Global Energy Conference (GEC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Global Energy Conference (GEC)
Y1  - 4-6 Dec. 2024
AB  - In recent decades, Artificial Intelligence/Machine Learning (AI/ML) methods have been applied in a variety of fields, from healthcare to finance, retail, energy, and many more, with remarkable improvements. However, AI-based solutions are still questionable due to concerns regarding their trustworthiness. Explainable AI (XAI) has become an emerging research field that addresses those concerns about trustworthiness, particularly for explainability and transparency. In this study, three XAI methods supporting local and global explainability, i.e. SHAP, PFI, and LIME, are utilized to investigate the key features and their impact on the model's outputs for solar photovoltaic (PV) power generation forecasting.
ER  - 

TY  - CONF
TI  - The Role of AI Techniques in Diagnosing Health Conditions with Integration of AI
T2  - 2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)
SP  - 167
EP  - 172
AU  - M. R. Viignesh
AU  - A. Josphin Selsia
AU  - M. Bhuvaneswari
AU  - S. Pathak
AU  - K. Yadav
AU  - G. Amirthayogam
PY  - 2024
KW  - Deep learning
KW  - Analytical models
KW  - Explainable AI
KW  - Reviews
KW  - Computational modeling
KW  - Plagiarism
KW  - Medical services
KW  - Explainable AI
KW  - Medical Imaging
KW  - Deep Learning Models
KW  - Interpretability
KW  - XAI Methods
KW  - Clinical Applications
KW  - Saliency
KW  - CAM Models
KW  - Radiomics
KW  - Human-Centered XAI
DO  - 10.1109/ICACITE60783.2024.10617043
JO  - 2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)
Y1  - 14-15 May 2024
AB  - Deep learning models for artificial intelligence (AI) have found wide-ranging applications in a variety of fields, most notably healthcare imaging and healthcare chores. Because medical decision-making is so vital, artificial intelligence must be able to mimic human judgment and interpretation abilities. Explainable AI (XAI) becomes an important element, with the goal of revealing how deep learning models function behind the scenes and how decisions are made. The latest XAI methods in healthcare and related medical imaging applications are examined in this study. In order to improve understanding in medical imaging domains, the study highlights techniques that are used to systematically classify different forms of XAI. A particular focus is on tackling difficult XAI problems in medical applications, including recommendations for the creation of better interpretable models using deep learning in medical text and picture analytics. Furthermore, the study delineates possible avenues for exploration in clinical themes, with a particular emphasis on applications related to medical imaging, offering developers and researchers valuable information to consider.
ER  - 

TY  - CONF
TI  - Consistency of XAI Models against Medical Expertise: An Assessment Protocol
T2  - 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)
SP  - 732
EP  - 736
AU  - E. Arnaud
AU  - M. Elbattah
AU  - A. Pitteman
AU  - G. Dequen
AU  - D. A. Ghazali
AU  - P. A. Moreno-Sánchez
PY  - 2024
KW  - Measurement
KW  - Protocols
KW  - Systematics
KW  - Explainable AI
KW  - Refining
KW  - Neural networks
KW  - Predictive models
KW  - Explainable AI
KW  - Interpretability
KW  - Human in the Loop
KW  - Trustworthy AI
DO  - 10.1109/ICHI61247.2024.00116
JO  - 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)
Y1  - 3-6 June 2024
AB  - Despite the significant advances made by Artificial Intelligence (AI) models in enhancing medical diagnostics and prognostics, their opacity poses a hurdle to widespread clinical adoption. In this regard, Explainable AI (XAI) aims to demystify these complex models, such as neural networks, by revealing the reasoning behind predictions. However, a notable gap exists in enabling non-experts to verify these explanations, necessitating human-in-the-loop evaluation. This paper introduces a systematic protocol, including a novel “consistency” metric, to evaluate the SHAP-based explanations of XAI, comparing them against the clinical knowledge of expert clinicians. We demonstrate how this metric could facilitate both global and feature-specific analyses, operating at the level of individual instances, and thus enhancing AI transparency. It is conceived that the implications of this work may extend beyond the medical context, offering a standardized methodology that could potentially improve the interpretability and acceptance of AI systems in diverse domains.
ER  - 

TY  - CONF
TI  - Trust, Ethics, and User-Centric Design in AI-Integrated Genomics
T2  - 2024 2nd International Conference on Cyber Resilience (ICCR)
SP  - 1
EP  - 6
AU  - F. Al-Akayleh
AU  - A. S. A. Ali Agha
PY  - 2024
KW  - Ethics
KW  - Precision medicine
KW  - Genomics
KW  - Medical services
KW  - Big Data
KW  - Prediction algorithms
KW  - Bioinformatics
KW  - Artificial Intelligence in Genomics
KW  - Ethical Considerations
KW  - Algorithmic Transparency
KW  - Regulatory Feedback Mechanisms
KW  - Data Privacy and Security
DO  - 10.1109/ICCR61006.2024.10532890
JO  - 2024 2nd International Conference on Cyber Resilience (ICCR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Cyber Resilience (ICCR)
Y1  - 26-28 Feb. 2024
AB  - This study examines the integration of genomics and artificial intelligence (AI) in the healthcare industry, focusing on the ethical and trust-related issues that arise from this integration. This integration highlights the significance of protecting genomic data by employing homomorphic encryption. This study emphasizes the significance of algorithmic transparency. It suggests employing interpretative frameworks like Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) to enhance the comprehensibility of AI algorithms. The article discusses two key regulatory measures: the Trustworthy Artificial Intelligence Initiative and the Genomic Data Sharing (GDS) Policy. This study examines the effectiveness of current practices in adapting to rapid technological advancements while maintaining ethical standards. This emphasizes the significance of attaining a balance between the benefits of predictive analytics in healthcare and the ethical considerations, such as informed consent and data integrity, as we transition from big data to big mechanisms. The importance of integration lies in its ability to revolutionize the healthcare sector. The study highlights the importance of robust governance frameworks to ensure technological advancements adhere to ethical standards and earn public trust. In summary, it is imperative to acknowledge and address the ethical considerations associated with integrating genomics and AI in healthcare to ensure effective and responsible implementation. This area is a major focus in contemporary medical research and practice.
ER  - 

TY  - JOUR
TI  - Unraveling the Black Box: A Review of Explainable Deep Learning Healthcare Techniques
T2  - IEEE Access
SP  - 66556
EP  - 66568
AU  - N. Y. Murad
AU  - M. H. Hasan
AU  - M. H. Azam
AU  - N. Yousuf
AU  - J. S. Yalli
PY  - 2024
KW  - Deep learning
KW  - Medical services
KW  - Explainable AI
KW  - Fuzzy logic
KW  - Surveys
KW  - Taxonomy
KW  - Ethics
KW  - Artificial intelligence
KW  - Artificial intelligence
KW  - deep learning
KW  - explainability
KW  - XAI
DO  - 10.1109/ACCESS.2024.3398203
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - The integration of deep learning in healthcare has propelled advancements in diagnostics and decision support. However, the inherent opacity of deep neural networks (DNNs) poses challenges to their acceptance and trust in clinical settings. This survey paper delves into the landscape of explainable deep learning techniques within the healthcare domain, offering a thorough examination of deep learning explainability techniques. Recognizing the pressing need for nuanced interpretability, we extend our focus to include the integration of fuzzy logic as a novel and vital category. The survey begins by categorizing and critically analyzing existing intrinsic, visualization, and distillation techniques, shedding light on their strengths and limitations in healthcare applications. Building upon this foundation, we introduce fuzzy logic as a distinct category, emphasizing its capacity to address uncertainties inherent in medical data, thus contributing to the interpretability of DNNs. Fuzzy logic, traditionally applied in decision-making contexts, offers a unique perspective on unraveling the black box of DNNs, providing a structured framework for capturing and explaining complex decision processes. Through a comprehensive exploration of techniques, we showcase the effectiveness of fuzzy logic as an additional layer of interpretability, complementing intrinsic, visualization, and distillation methods. Our survey contributes to a holistic understanding of explainable deep learning in healthcare, facilitating the seamless integration of DNNs into clinical workflows. By combining traditional methods with the novel inclusion of fuzzy logic, we aim to provide a nuanced and comprehensive view of interpretability techniques, advancing the transparency and trustworthiness of deep learning models in the healthcare landscape.
ER  - 

TY  - CONF
TI  - Process Quality Assurance of Artificial Intelligence in Medical Diagnosis
T2  - 2024 International Conference on Intelligent Systems and Computer Vision (ISCV)
SP  - 1
EP  - 8
AU  - N. Moghadasi
AU  - M. Piran
AU  - R. S. Valdez
AU  - S. Baek
AU  - N. Moghaddasi
AU  - T. L. Polmateer
AU  - J. H. Lambert
PY  - 2024
KW  - Computer vision
KW  - Quality assurance
KW  - Additives
KW  - Explainable AI
KW  - Systems modeling
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Risks of AI
KW  - Systems Engineering
KW  - SHAP
KW  - LIME
KW  - Anchors
KW  - Multi-Criteria Decision Analysis
KW  - Ethical AI
DO  - 10.1109/ISCV60512.2024.10620154
JO  - 2024 International Conference on Intelligent Systems and Computer Vision (ISCV)
IS  - 
SN  - 2768-0754
VO  - 
VL  - 
JA  - 2024 International Conference on Intelligent Systems and Computer Vision (ISCV)
Y1  - 8-10 May 2024
AB  - While artificial Intelligence (AI) has shown promising results in the healthcare field, it is undeniable that there are risks associated with AI in healthcare that society must acknowledge. This paper presents a comprehensive systems modeling framework aimed at evaluating trust and being responsive to reasons for mistrust and distrust in AI-assisted medical diagnosis, with a specific focus on the diagnosis of cardiac sarcoidosis, utilizing Explainable Artificial Intelligence (XAI) techniques. The design includes two primary sections: 1. Identifying the most and least disruptive scenarios to the system, as well as the most important initiatives for the system. 2. Utilizing XAI techniques such as SHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic Explanations (LIME), and Anchors models to provide explanations on how machine learning models justify their outcomes. The findings indicate the significance of employing explainable AI in critical domains like healthcare, where lives of patients are at risk. XAI can be employed to analyze the outcomes for AI users, determining the significance of features, improving comprehension of AI outputs, enhancing transparency, explainability, and interpretability of AI outputs, and facilitating data assessment.
ER  - 

TY  - CONF
TI  - Opening the Black Box: Explainable AI Insights into Obesity Prediction Using SMOTE-Balanced Stacking Ensembles
T2  - 2024 IEEE 16th International Conference on Computational Intelligence and Communication Networks (CICN)
SP  - 1127
EP  - 1131
AU  - R. G. Tiwari
AU  - V. Vimal
AU  - A. K. Agarwal
PY  - 2024
KW  - Training
KW  - Support vector machines
KW  - Obesity
KW  - Machine learning algorithms
KW  - Accuracy
KW  - Explainable AI
KW  - Stacking
KW  - Predictive models
KW  - Boosting
KW  - Public healthcare
KW  - Imbalanced Dataset
KW  - SMOTE
KW  - Obesity Prediction
KW  - Traditional Machine Learning
KW  - Boosting Algorithms
KW  - Stacking Ensemble
KW  - Explainable AI
KW  - Model Interpretability
KW  - Public Health
KW  - Machine Learning
DO  - 10.1109/CICN63059.2024.10847434
JO  - 2024 IEEE 16th International Conference on Computational Intelligence and Communication Networks (CICN)
IS  - 
SN  - 2472-7555
VO  - 
VL  - 
JA  - 2024 IEEE 16th International Conference on Computational Intelligence and Communication Networks (CICN)
Y1  - 22-23 Dec. 2024
AB  - Obesity, caused by an energy imbalance, is a major public health issue. Machine learning algorithms can be hampered by class imbalance in obesity dataset analysis. This research makes an attempt tobalance an unbalanced obesity dataset using the Synthetic Minority Over-sampling Technique (SMOTE) and uses this upgraded dataset for training Decision Trees, Support Vector Machines, Logistic Regression, and sophisticated boosting techniques like XGBoost and AdaBoost. Through thorough experimentation, the best three machine learning approaches are determined and a stacking ensemble model is created using these models. The proposed model achieves 92.5% accuracy on the test dataset. Explainable machine learning methods like SHAP is also used to improve forecasts' interpretability. These approaches reveal the feature contributions affecting the model's predictions, helping us understand obesity's causes. Findings show that SMOTE oversampling can alleviate class imbalance and that stacking ensemble models can improve predictive performance. Explainable AI also makes obesity research machine learning applications more transparent, enabling better public health interventions and policy decisions.
ER  - 

TY  - CONF
TI  - Enhancing Cardiac AI Explainability Through Ontology-Based Evaluation
T2  - 2024 15th International Conference on Information, Intelligence, Systems & Applications (IISA)
SP  - 1
EP  - 4
AU  - N. Tsolakis
AU  - C. Maga-Nteve
AU  - S. Vrochidis
AU  - N. Bassiliades
AU  - G. Meditskos
PY  - 2024
KW  - Deep learning
KW  - Explainable AI
KW  - Prevention and mitigation
KW  - Data preprocessing
KW  - Focusing
KW  - Medical services
KW  - Ontologies
KW  - Data models
KW  - Complexity theory
KW  - Cardiology
KW  - ontologies
KW  - artificial intelligence
KW  - cardiac XAI
KW  - healthcare AI
KW  - XAI evaluation
DO  - 10.1109/IISA62523.2024.10786663
JO  - 2024 15th International Conference on Information, Intelligence, Systems & Applications (IISA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 15th International Conference on Information, Intelligence, Systems & Applications (IISA)
Y1  - 17-19 July 2024
AB  - The increasing utilization of Deep Learning models in healthcare and cardiology necessitates robust explainability to ensure trust, understandability and bias mitigation. Especially in cardiology, where life-critical decisions should be made, explainable Artificial Intelligence (XAI) solutions play a pivotal role. As of now, the utility of the Artificial Intelligence (AI) advancements in explainable AI is limited by the complexity of the explanations provided and the lack of applicability of explanations in real-world clinical settings. To bridge this gap, we propose a novel ontology-based methodology to evaluate XAI techniques, focusing on cardiac “black-box” models. More specifically, we use ontologies to assess and ensure the clarity and relevance of AI explanations, fostering the development of explainable and trustworthy systems for clinical use. Our solution unfolds in sequential phases, beginning with rigorous data preprocessing, feature engineering and continuing with model architecture design and implementation. The next stage includes the deployment of various XAI methods to produce meaningful explanations, which being evaluated by leveraging an ontology scheme.
ER  - 

TY  - CONF
TI  - Breast Cancer Diagnosis with XAI-Integrated Deep Learning Approach
T2  - 2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
SP  - 659
EP  - 665
AU  - S. A. Tanim
AU  - G. M. Imdadul Alam
AU  - T. E. Shrestha
AU  - M. Islam
AU  - F. Jahan
AU  - K. Nur
PY  - 2024
KW  - Deep learning
KW  - Training
KW  - Technological innovation
KW  - Accuracy
KW  - Explainable AI
KW  - Neural networks
KW  - Breast cancer
KW  - Cleaning
KW  - Prognostics and health management
KW  - Informatics
KW  - breast cancer
KW  - deep learning
KW  - eXplainable artificial intelligence
KW  - model interpretability
KW  - healthcare
DO  - 10.1109/3ict64318.2024.10824574
JO  - 2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
IS  - 
SN  - 2770-7466
VO  - 
VL  - 
JA  - 2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
Y1  - 17-19 Nov. 2024
AB  - Breast cancer is one of the world's significant health challenges. There is a need to have better techniques in the early diagnosis of this disease to increase patients' survival rates. This paper introduces a robust approach for breast cancer detection by integrating deep-learning and classical machine-learning techniques into a custom lightweight neural network model. This approach is based on integrating conventional machine-learning and deep-learning paradigms using a range of data pre-processing steps, including data cleaning, label converting, scaling, and feature selecting to improve the given dataset's readiness for training. The proposed model demonstrated impressive accuracy in breast cancer detection compared to individual classifiers by achieving an overall accuracy of 97.54 %. Additionally, integrating eXplainable Artificial Intelligence (XAI) techniques gives the application interpretability and transparency for clinicians to make sense of the feature's importance and individual prognosis. It is a more accurate and easy-to-understand tool for clinicians, making it a better use of faulty or confusing reference values. This study presents the need to learn more about making deep learning and eXplainable Artificial Intelligence (XAI) complementary approaches to breast cancer diagnosis and treatment research.
ER  - 

TY  - CONF
TI  - A Comprehensive Review of Interpretability in AI and Its Implications for Trust in Critical Applications
T2  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
SP  - 1683
EP  - 1693
AU  - U. S. Kathait
AU  - A. Rana
AU  - R. Chauhan
AU  - R. Rawat
PY  - 2024
KW  - Measurement
KW  - Industries
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Storms
KW  - Reviews
KW  - Medical services
KW  - Learning (artificial intelligence)
KW  - Inference algorithms
KW  - Artificial intelligence
KW  - Artificial intelligence
KW  - deep learning
KW  - black box
KW  - interpretability
DO  - 10.1109/ICSES63445.2024.10763115
JO  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
Y1  - 15-17 Oct. 2024
AB  - With the advent of deep learning (DL) and other advancements in machine learning, artificial intelligence has recently taken the globe by storm. The requirement for openness and interpretability has arisen as computing power has reached superhuman capability; its absence is a serious problem in industries like healthcare and finance, where trust is crucial because of its enormous impact on human lives. This necessitates more interpretability, which often means understanding the algorithmic system. Unfortunately, there are still a lot of unanswered questions regarding the DL. Our knowledge of machine decision-making is still incomplete. We provide an overview and categorize the interpretabilities provided by different studies. However, there is a significant disadvantage with AI: people would see it as a “black box,” which would erode trust in its trustworthiness. In a field where decisions can literally mean the difference between life and death, this is a severe issue. Considering different application domains and activities, this study offers a thorough overview of the literature on the latest developments in XAI approaches and assessment metrics.
ER  - 

TY  - CONF
TI  - A Survey on Skin Lesion Detection and Classification using Machine Learning
T2  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
SP  - 1
EP  - 5
AU  - R. Yadav
AU  - A. Bhat
PY  - 2024
KW  - Machine learning algorithms
KW  - Dermatology
KW  - Transfer learning
KW  - Collaboration
KW  - Machine learning
KW  - Genetics
KW  - Skin
KW  - skin lesion
KW  - machine learning
KW  - survey
KW  - deep learning
KW  - CNN
DO  - 10.1109/AIMLA59606.2024.10531571
JO  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
Y1  - 15-16 March 2024
AB  - The research explores how dermatologists use machine learning to quickly and accurately identify and classify skin injury. Conventional diagnosis techniques depend on visual examination, but are subjective and have different interpretations. The ability to analyze data and recognize patterns is a possible remedy for ML. The paper examines the current technology of machine learning, focusing on validation in the real world, and deals with data set variability. Although the research acknowledges the fruitfulness of deep learning (DL), the research emphasizes the benefits of traditional ML techniques regarding interpretation and processing performance. Methods for automating the analysis of skin lesions, such as feature engineering, rule-based techniques and traditional ML algorithms, have been studied. The study suggests using advanced transfer learning techniques, integrating genetic and clinical data, and refining the way artificial intelligence (AI) is explained in order to get over barriers. Intending to enhance the accessibility and correctness of skin lesion identification, the future requires collaboration between dermatology and machine learning to develop real-time diagnostic tools. By offering scalable solutions for rapid diagnosis of lesions and improved patient outcomes, this combination of medical expertise and ML capabilities has the power to revolutionize dermatology. The future of automated dermatological diagnosis is expected to be shaped by collaboration between machine learning experts and dermatologists, enabling more personalized treatment for patients.
ER  - 

TY  - CONF
TI  - Transforming Healthcare: Advancements, Applications, and Future Directions of Machine Learning
T2  - 2024 10th International Conference on Smart Computing and Communication (ICSCC)
SP  - 502
EP  - 506
AU  - S. K. Puli
AU  - P. Usha
PY  - 2024
KW  - Data privacy
KW  - Machine learning algorithms
KW  - Surgery
KW  - Medical services
KW  - Prediction algorithms
KW  - Data models
KW  - Real-time systems
KW  - Planning
KW  - Predictive analytics
KW  - Standards
KW  - Machine Learning
KW  - Healthcare
KW  - Diagnostics
KW  - Treatment
KW  - Patient Care
KW  - Predictive Analytics
KW  - Personalized Medicine
DO  - 10.1109/ICSCC62041.2024.10690530
JO  - 2024 10th International Conference on Smart Computing and Communication (ICSCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 10th International Conference on Smart Computing and Communication (ICSCC)
Y1  - 25-27 July 2024
AB  - Machine learning (ML) has become a transformative technology with widespread applications across numerous industries, particularly in healthcare. This paper provides an overview of recent advancements and applications of ML in health-care, highlighting its potential to revolutionize diagnostics, treatment, and patient care. By leveraging large datasets and sophisticated algorithms, ML algorithms can assist healthcare providers in making more accurate diagnoses, predicting patient outcomes, personalizing treatment plans, and optimizing health-care operations. However, to ensure the ethical and responsible use of ML in healthcare, it is crucial to address significant challenges, including safeguarding data privacy, improving model interpretability, and adhering to regulatory compliance standards.
ER  - 

TY  - JOUR
TI  - Empowering Glioma Prognosis With Transparent Machine Learning and Interpretative Insights Using Explainable AI
T2  - IEEE Access
SP  - 31697
EP  - 31718
AU  - A. Palkar
AU  - C. C. Dias
AU  - K. Chadaga
AU  - N. Sampathila
PY  - 2024
KW  - Tumors
KW  - Explainable AI
KW  - Machine learning algorithms
KW  - Prediction algorithms
KW  - Medical services
KW  - Support vector machines
KW  - Medical diagnostic imaging
KW  - Machine learning
KW  - Deep learning
KW  - Medical treatment
KW  - Random forests
KW  - Brain tumors
KW  - Diagnostic expert systems
KW  - Logistic regression
KW  - Decision trees
KW  - Glioma
KW  - molecular makeup
KW  - explainable artificial intelligence (XAI)
KW  - SHAP
KW  - LIME
KW  - QLattice
KW  - Eli5
KW  - machine learning
DO  - 10.1109/ACCESS.2024.3370238
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - The primary objective of this research is to create a reliable technique to determine whether a patient has glioma, a specific kind of brain tumour, by examining various diagnostic markers, using a variety of machine learning as well as deep learning approaches, and involving XAI (explainable artificial intelligence) methods. Through the integration of patient data, including medical records, genetic profiles, algorithms using machine learning have the ability to predict how each individual will react to different medical interventions. To guarantee regulatory compliance and inspire confidence in AI-driven healthcare solutions, XAI is incorporated. Machine learning methods employed in this study includes Random Forest, decision trees, logistic regression, KNN, Adaboost, SVM, Catboost, LGBM classifier, and Xgboost whereas the deep learning methods include ANN and CNN. Four alternative XAI strategies, including SHAP, Eli5, LIME, and QLattice algorithm, are employed to comprehend the predictions of the model. The Xgboost, a ML model achieved accuracy, precision, recall, f1 score, and AUC of 88%, 82%, 94%, 88%, and 92%, respectively. The best characteristics according to XAI techniques are IDH1, Age at diagnosis, PIK3CA, ATRX, PTEN, CIC, EGFR and TP53. By applying data analytic techniques, the objective is to provide healthcare professionals with practical tool that enhances their capacity for decision-making, enhances resource management, and ultimately raises the bar for patient care. Medical experts can customise treatments and improve patient outcomes by taking into account patient’s particular characteristics. XAI provides justifications to foster faith amongst patients and medical professionals who must rely on AI-assisted diagnosis and treatment recommendations.
ER  - 

TY  - CONF
TI  - Explainable AI-Based ECG Heartbeat Classification Using Deep Learning Models
T2  - 2024 4th International Conference on Artificial Intelligence and Signal Processing (AISP)
SP  - 1
EP  - 5
AU  - R. Moningi
AU  - S. Mahakur
AU  - S. Mundada
AU  - A. Kumar Tripathy
PY  - 2024
KW  - Deep learning
KW  - Accuracy
KW  - Additives
KW  - Heart beat
KW  - Signal processing algorithms
KW  - Electrocardiography
KW  - Signal processing
KW  - Reliability
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - ECG
KW  - Heartbeat classification
KW  - CNN
KW  - LSTM
KW  - Explainable AI
DO  - 10.1109/AISP61711.2024.10870845
JO  - 2024 4th International Conference on Artificial Intelligence and Signal Processing (AISP)
IS  - 
SN  - 2640-5768
VO  - 
VL  - 
JA  - 2024 4th International Conference on Artificial Intelligence and Signal Processing (AISP)
Y1  - 26-28 Oct. 2024
AB  - The ECG is fundamental in detecting cardiovascular ailments, and its thorough analysis is imperative. The analysis of conventional ECG depends on interpretation based on the expertise of the professional, which is slow and prone to mistakes. Deep learning models, which include CNNs, LSTMs, and Attention techniques introduced recently, have demonstrated promising results in the automation of ECG classification with high accuracy. However, a major drawback regarding these models is the ‘black box’ nature, which goes against clinical usage and comparison. This study adopts SHapley Additive exPlanations (SHAP) values and Local Interpretable Model-agnostic Explanations (LIME) techniques under deep learning algorithms for identifying ECG heartbeats. The model is trained and tested on the MIT-BIH Arrhythmia Database, and the performance seems to be accurate with an overall accuracy of 98.25%. Local Interpretable Model-Agnostic Explanations involving the construction of a local model, such as LIME and Shapely Additive explanations, or SHAP, help to provide more explanation regarding what the model is doing, thus increasing its reliability for clinical use. In this paper, the focus is presented on the experiment's method and outcomes, with a discussion of the interpretability of the final model as a crucial factor in its application in medical diagnostics.
ER  - 

TY  - CONF
TI  - Bayesian XAI Methods Towards a Robustness-Centric Approach to Deep Learning: An ABIDE I Study
T2  - 2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA)
SP  - 1
EP  - 5
AU  - F. Bargagna
AU  - L. Anita De Santi
AU  - M. F. Santarelli
AU  - V. Positano
AU  - N. Vanello
PY  - 2024
KW  - Deep learning
KW  - Autism
KW  - Uncertainty
KW  - Biological system modeling
KW  - Predictive models
KW  - Biomarkers
KW  - Bayes methods
KW  - Bayesian
KW  - xAI
KW  - Deep Learning
KW  - ABIDE
KW  - Autism
DO  - 10.1109/MeMeA60663.2024.10596826
JO  - 2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA)
IS  - 
SN  - 2837-5882
VO  - 
VL  - 
JA  - 2024 IEEE International Symposium on Medical Measurements and Applications (MeMeA)
Y1  - 26-28 June 2024
AB  - Developing reliable and explainable models is a crucial point to effectively integrate and exploit the potentials offered by Deep Learning (DL) architectures in high-stakes scenarios like healthcare. There are several applications that exploit DL to support Autism Spectrum Disorder (ASD) diagnosis, eventually augmented by explainable AI (XAI) tools to provide hints on the decision-making process implemented. On the other hand, Bayesian Neural Networks (BNNs) can provide, together with their prediction, epistemic uncertainty (uncertainty of the model), a key component to asserting the model's reliability. To date, there are no applications which exploit the advantages offered by BNNs and XAI to support the research of biomarkers in ASD. In the present work, authors first developed a BNN which classifies ASD subjects from resting state functional Magnetic Resonance Imaging (rs-fMRI) data obtained from the Autism Brain Imaging Data Exchange (ABIDE) dataset. A Layerwise Relevance Propagation (LRP) algorithm was then used to estimate the importance of cross-correlation connectivity coefficients in the returned predictions. Finally, a group analysis was performed to highlight functional brain connections that report the highest impact on the model's correct classification of ASD subjects. This work ended up producing a framework which combines a bayesian neural network with a XAI methodology, towards a robustness-centric deep learning approach, applied to the case study of ASD diagnosis.
ER  - 

TY  - CONF
TI  - XAI approach to drug dosage optimization and review-based prediction of side effects
T2  - 2024 International Conference on Computing, Internet of Things and Microwave Systems (ICCIMS)
SP  - 1
EP  - 6
AU  - M. Oumlaz
AU  - I. Kamdar
AU  - Y. Oumlaz
AU  - A. Oukaira
AU  - A. Z. Benelhaouare
AU  - A. Lakhssassi
PY  - 2024
KW  - Drugs
KW  - Adaptation models
KW  - Explainable AI
KW  - Precision medicine
KW  - Decision making
KW  - Predictive models
KW  - Prediction algorithms
KW  - Decision trees
KW  - Optimization
KW  - Random forests
KW  - Explainable Artificial Intelligence
KW  - Drug Dosage Optimization
KW  - Personalized Medicine
KW  - Decision Trees
KW  - Random Forests
DO  - 10.1109/ICCIMS61672.2024.10690415
JO  - 2024 International Conference on Computing, Internet of Things and Microwave Systems (ICCIMS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Computing, Internet of Things and Microwave Systems (ICCIMS)
Y1  - 29-31 July 2024
AB  - One of the major advancements in medicine is “personalized medicine”: a new approach with a promising future that takes into account the unique makeup of each individual patient instead of broad populations. One of the challenges of personalized medicine is the complexity: the more variables to account for, the more complex the adaptation of a certain treatment to a certain patient becomes; thus, the need for artificial intelligence: a tool that can be used to account for the various variables, and perform the computational and mathematical analyses required to succeed in the task of personalizing a certain treatment modality. This study explores explainable artificial intelligence (XAI) for drug dosage optimization based on individual patient data, utilizing advanced machine learning techniques: Decision Trees and Random Forests to be specific. Our approach applies XAI using ML models to provide clear and interpretable dosage recommendations, as well as predicting potential side effects. The methodology proves a high adaptability to diverse patient profiles, achieving an accuracy of 70% in dosage optimization. These results showcase the efficacy of XAI and the utilized algorithms in improving personalized treatment modalities, providing clinicians with a clearer vision, and understandable dosage recommendations for tailored treatment plans.
ER  - 

TY  - CONF
TI  - Exploring the Role of Gut Microbiota in Hypertension: Insights from Machine Learning and eXplainable AI
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
SP  - 1
EP  - 5
AU  - K. Yadav
AU  - N. Tanwar
AU  - Y. Hasija
PY  - 2024
KW  - Hypertension
KW  - Deep learning
KW  - Microorganisms
KW  - Explainable AI
KW  - Databases
KW  - Forestry
KW  - Regulation
KW  - Ensemble learning
KW  - Public healthcare
KW  - Random forests
KW  - gutMDisorder
KW  - Random Forest
KW  - XAI
KW  - SHAP
KW  - Hypertension
DO  - 10.1109/ICCCNT61001.2024.10725123
JO  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
IS  - 
SN  - 2473-7674
VO  - 
VL  - 
JA  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
Y1  - 24-28 June 2024
AB  - Recent research has found a growing connection between the gut microbiome and the regulation of hypertension, moving from observing associations to investigating direct cause-and-effect relationships. More than 1.13 billion people worldwide suffer from hypertension, a serious public health issue that has been related to an imbalance in gut microbiota. Machine Learning (ML) and Deep Learning (DL) methods offer advanced tools for data analysis, with the Random Forest (RF) classifier chosen for its ensemble learning approach. The model, trained with data from the gutMDisorder database, achieved an 85% accuracy by utilizing the Synthetic Minority Over-sampling Technique (SMOTE), showcasing the efficacy of SMOTE in addressing dataset imbalance. Utilizing Shapley Additive Explanations (SHAP) analysis in eXplainable AI (XAI) found significant gut microbial taxa, such as Holdemania in the Erysipelotrichaceae family, that differentiate persons with hypertension from healthy controls. The findings suggest that manipulating gut microbiota could be a treatment option for controlling hypertension. They also emphasize the importance of clear and comprehensible AI models in medical research.
ER  - 

TY  - CONF
TI  - Explainable AI for Biometrics
T2  - 2024 International Conference on Applied Mathematics & Computer Science (ICAMCS)
SP  - 184
EP  - 190
AU  - S. Benziane
AU  - K. Labed
PY  - 2024
KW  - Biometrics
KW  - Visualization
KW  - Explainable AI
KW  - Reviews
KW  - Biological system modeling
KW  - Computational modeling
KW  - Focusing
KW  - Finance
KW  - Medical services
KW  - Usability
KW  - Explainable AI (XAI)
KW  - Biometrics
KW  - Interpretability
KW  - Transparency
KW  - Feature Importance Analysis
DO  - 10.1109/ICAMCS62774.2024.00030
JO  - 2024 International Conference on Applied Mathematics & Computer Science (ICAMCS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Applied Mathematics & Computer Science (ICAMCS)
Y1  - 28-30 Sept. 2024
AB  - Explainable AI has a cascading effect, causing increased transparency in decision-making processes, which fosters greater user trust and acceptance. This trust leads to wider adoption of AI technologies in sensitive applications like healthcare and finance, where understanding and accountability are essential. In this paper, we propose an explainable AI survey-based biometrics models. It consists first of a comprehensive review of current explainable AI techniques applied to biometrics, focusing on methods such as feature importance analysis, saliency maps, and rule-based models. We then assess these methods' effectiveness in enhancing transparency and interpretability within biometric systems. This paper proposes a comprehensive framework for integrating explainable AI techniques into biometric models to enhance transparency, trust, and usability. The framework leverages a combination of explainable AI techniques, including feature attribution methods, interpretable model architectures, and visualization tools, to enhance the transparency and accountability of biometric models.
ER  - 

TY  - CONF
TI  - Applications of AI in Biomedical Genomics and Pharmaceuticals
T2  - 2024 2nd International Conference on Cyber Resilience (ICCR)
SP  - 1
EP  - 5
AU  - M. Al-Remawi
AU  - R. A. A. Rahem
PY  - 2024
KW  - Metalearning
KW  - Solid modeling
KW  - Translational research
KW  - Machine learning algorithms
KW  - Precision medicine
KW  - Genomics
KW  - Predictive models
KW  - AI-driven genomic Analysis
KW  - Predictive Models in Healthcare
KW  - Diagnostic Assistance in Genomics
KW  - Therapeutic Applications of AI
KW  - Advanced Computational Strategies in Genomics
DO  - 10.1109/ICCR61006.2024.10532792
JO  - 2024 2nd International Conference on Cyber Resilience (ICCR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Cyber Resilience (ICCR)
Y1  - 26-28 Feb. 2024
AB  - The integration of artificial intelligence (AI) and genomics has resulted in a new era in precision medicine, significantly revolutionizing diagnostics, therapeutics, and biomedical research. This article examines various applications of an integrated AI algorithms/genomics system to enhance predictive models, enhance diagnostic accuracy, and enable personalized therapies. Advancements in machine learning, deep learning, and data analytics have facilitated an enhanced understanding of intricate genomic data. Additionally, the applications of AI in diverse domains, such as polygenic risk scores, drug discovery, gene editing, and translational research, is presented. AI also explores some advanced computational methods such as few-shot learning, meta-learning, and federated learning. The limited availability of data and privacy concerns are still preventing the proper usage of AI in the fields of biomedical genomics and pharmaceuticals. This study explores integrating user-centric design elements, augmented reality, and virtual reality into genomic systems for improved user experience. The progress of AI in genomics represents a significant change towards developing adaptable and learning-focused knowledge bases.
ER  - 

TY  - CONF
TI  - Emerging Frontiers of Artificial Intelligence in Epilepsy Care: A Comprehensive Overview
T2  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
SP  - 1
EP  - 5
AU  - P. S. Moon
AU  - N. I. Chaudhary
AU  - P. A. Bainalwar
AU  - V. K. Moon
AU  - S. S. Shambharkar
PY  - 2024
KW  - Deep learning
KW  - Ethics
KW  - Accuracy
KW  - Reviews
KW  - Epilepsy
KW  - Brain modeling
KW  - Electroencephalography
KW  - Reliability
KW  - Artificial intelligence
KW  - Long short term memory
KW  - Epilepsy
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - CNN
KW  - LSTM
KW  - EEG
DO  - 10.1109/IDICAIEI61867.2024.10842843
JO  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
Y1  - 29-30 Nov. 2024
AB  - Epilepsy, a chronic neurological disorder characterized by recurrent seizures, poses significant challenges in diagnosis, treatment, and management. In recent years, Artificial Intelligence (AI) has emerged as a promising tool to revolutionize various aspects of epilepsy care. This comprehensive review explores the advancements and applications of AI in epilepsy, encompassing diagnostic support, seizure prediction, personalized treatment strategies, and therapeutic monitoring. AI techniques such as machine learning, deep learning, and natural language processing are leveraged to analyze diverse datasets including electroencephalogram (EEG) recordings, imaging studies, genetic data, and clinical records. Key applications include automated seizure detection and classification systems, predictive models for personalized treatment plans, and real-time monitoring tools to enhance patient management. Furthermore, AI-driven approaches enable integration of multimodal data to improve accuracy and reliability in epilepsy diagnosis and prognosis. Challenges such as data heterogeneity, interpretability of AI models, ethical considerations, and regulatory issues are also discussed. This review highlights the transformative potential of AI in advancing epilepsy research and clinical practice, offering insights into future directions for leveraging AI to optimize care and improve outcomes for individuals living with epilepsy.
ER  - 

TY  - CONF
TI  - CardioNetFusion: A Deep Learning and Explainable AI Integrated System for Cardiovascular Disease Detection
T2  - 2024 IEEE International Conference on Data Mining Workshops (ICDMW)
SP  - 388
EP  - 395
AU  - G. Raja
AU  - S. Essaky
AU  - S. R. Muthu
AU  - B. S. Suresh
AU  - J. Cho
AU  - S. Yoo
AU  - S. V. Easwaramoorthy
PY  - 2024
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Medical services
KW  - Electrocardiography
KW  - Sensor fusion
KW  - Robustness
KW  - Real-time systems
KW  - Convolutional neural networks
KW  - Application programming interfaces
KW  - Healthcare
KW  - Cardio Vascular Diseases (CVD)
KW  - Electrocardiogram (ECG)
KW  - Deep Learning
KW  - Explainable AI (XAI)
DO  - 10.1109/ICDMW65004.2024.00056
JO  - 2024 IEEE International Conference on Data Mining Workshops (ICDMW)
IS  - 
SN  - 2375-9259
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Data Mining Workshops (ICDMW)
Y1  - 9-9 Dec. 2024
AB  - Early detection of Cardiovascular Diseases (CVD) plays a vital role in effective treatment and management. However, traditional methods for analyzing ECG signals are often limited in both accuracy and interpretability. In this work, we introduce a novel model, CardioNetFusion, designed to address these limitations by incorporating advancements in Deep Learning (DL) and Explainable AI (XAI). The model leverages the strengths of Convolutional Neural Networks (CNN), MobileNetV2, and VGG16 to improve the robustness of ECG signal interpretation. To further enhance transparency and provide healthcare professionals with actionable insights, saliency maps are utilized to visualize model predictions. Additionally, by integrating an Application Programming Interface (API) within an IoT-based sensor fusion framework, the model supports real-time cardiovascular health monitoring. Achieving a classification accuracy of 98.7% in detecting arrhythmias, CardioNetFusion surpasses existing methodologies and offers a practical, reliable solution for early CVD diagnosis and management. Traditional methods of ECG analysis often have limitations in terms of diagnostic accuracy and interpretability. This paper proposes a CardioNetFusion model to address the challenges of conventional methods of ECG data analysis by leveraging advances in Deep Learning (DL) and Explainable AI (XAI). The proposed CardioNetFusion model integrates Convolutional Neural Networks (CNN), MobileNetV2, and VGG16 models to improve the robustness of ECG signal interpretation. Additionally, we utilized saliency maps to enhance model transparency and provide clear, actionable insights for healthcare professionals. Integrating Application Programming Interface (API) in the IoT-based sensor fusion approach to our proposed model facilitates seamless real-time monitoring of cardiovascular health. With an impressive 98.7% accuracy in arrhythmia classification, CardioNetFusion outperformed existing methods and ensured reliable and interpretable diagnostics.
ER  - 

TY  - CONF
TI  - Enhancing Interpretability, Reliability and Trustworthiness: Applications of Explainable Artificial Intelligence in Medical Imaging, Financial Markets, and Sentiment Analysis
T2  - 2024 16th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)
SP  - 1
EP  - 9
AU  - K. Srivastava
AU  - A. Sorathiya
AU  - J. Mehta
AU  - V. Chotaliya
PY  - 2024
KW  - Sentiment analysis
KW  - Ethics
KW  - Pneumonia
KW  - Explainable AI
KW  - Decision making
KW  - Finance
KW  - Entertainment industry
KW  - accountability
KW  - explainable artificial intelligence
KW  - gradient-weighted class activation mapping
KW  - interpretability
KW  - local interpretable model-agnostic explanations
KW  - shapley additive explanations
KW  - transparency
DO  - 10.1109/ECAI61503.2024.10607516
JO  - 2024 16th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 16th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)
Y1  - 27-28 June 2024
AB  - In today’s technological era, as AI systems become more integral to critical decision-making, the importance of Explainable Artificial Intelligence (XAI) has become more pronounced. It addresses the challenge of understanding complex machine learning and deep learning models, ensuring transparency, interpretability, and accountability. This research paper provides a comprehensive analysis of XAI, focusing on its significance, methodologies, challenges, and future prospects. Theoretical foundations of XAI are elucidated, clarifying key concepts such as interpretability, transparency, and accountability. We differentiate between model-agnostic and model-specific XAI methods, outlining their strengths and limitations. A range of recent XAI techniques, including Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-CAM), are scrutinized. Through case studies in Healthcare (Pneumonia Classification), Finance (Stock Price Prediction), and Entertainment (Sentiment Analysis), we demonstrate how XAI enhances the understandability and trustworthiness of AI systems. Additionally, a comparative study of all three methods on all three case studies has been conducted, and the results are compared. Challenges such as scalability issues and ethical considerations, including biases and fairness, are discussed. Looking ahead, we offer insights into future XAI research trajectories, aiming to foster public trust and shape a future where AI systems are both intelligent and comprehensible.
ER  - 

TY  - CONF
TI  - Leveraging XAI for Discovering Crucial Demographic, Clinical and Pathological Diabetes Mellitus Biomarkers
T2  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
SP  - 1
EP  - 6
AU  - R. Ahuja
AU  - G. Indra
PY  - 2024
KW  - Pathology
KW  - Analytical models
KW  - Explainable AI
KW  - Biological system modeling
KW  - Biological processes
KW  - Biomarkers
KW  - Diabetes
KW  - Medical diagnosis
KW  - Biomedical monitoring
KW  - Monitoring
KW  - Diabetes Mellitus
KW  - Explainable Artificial Intelligence
KW  - XAI
KW  - Machine Learning
KW  - SHAP
KW  - Permutation Importance
DO  - 10.1109/DELCON64804.2024.10866967
JO  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
Y1  - 21-23 Nov. 2024
AB  - Diabetes Mellitus (DM) is a chronic metabolic disorder [1] which results in hyperglycemia caused by defects in insulin secretion or action. Prevalence of DM especially Type 2 Diabetes Mellitus(T2DM) is steadily increasing worldwide, making it a daunting health challenge. Multiple factors such as sedentary lifestyles, family history, genetics, unhealthy diets, obesity, and ageing drive its occurrence. Effective management of diabetes requires early detection, accurate diagnosis, and personalized treatment strategies. Biomarkers, quantifiable indicators of biological processes and disease states, play a crucial role in disease diagnosis, prognosis, and treatment monitoring. Unlocking significant biomarkers for DM detection is complex due to its multifactorial nature and intricate interactions. In recent years, Explainable Artificial Intelligence (XAI) has emerged as a promising approach for modelling and analyzing biomarkers, offering interpretability through feature importance techniques. This paper highlights the potential of XAI in early detection of T2DM and its progression. We delved deep into three publicly available diabetes datasets viz PIMA, Mendeley and Vanderbilt to get insights about top-tier clinical, pathological and demographic biomarkers essential for the timely detection of T2DM. We applied nine Machine Learning algorithms to determine the most accurate approach. XAI techniques of Shapley values, Permutation Importance and Decision Trees were utilized for comprehensive global interpretations of results and biomarker discovery. This study can be leveraged for clinical decision support systems to identify which biomarkers warrant regular monitoring for timely therapeutic intervention, thereby preventing progression of T2DM and its complications.
ER  - 

TY  - CONF
TI  - Enhancing Liver Cirrhosis Prognosis: A Machine Learning and Explainable AI Approach
T2  - 2024 Third International Conference on Artificial Intelligence, Computational Electronics and Communication System (AICECS)
SP  - 1
EP  - 5
AU  - J. Jeyabalan
AU  - Karthikeyan
PY  - 2024
KW  - Machine learning algorithms
KW  - Liver diseases
KW  - Explainable AI
KW  - Precision medicine
KW  - Decision making
KW  - Mortality
KW  - Genomics
KW  - Prediction algorithms
KW  - Predictive analytics
KW  - Prognostics and health management
KW  - Liver Cirrhosis
KW  - Predictive Analytics
KW  - Machine Learning
KW  - Survival Prediction
KW  - Gradient Boosting Algorithms
DO  - 10.1109/AICECS63354.2024.10956250
JO  - 2024 Third International Conference on Artificial Intelligence, Computational Electronics and Communication System (AICECS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Third International Conference on Artificial Intelligence, Computational Electronics and Communication System (AICECS)
Y1  - 12-14 Dec. 2024
AB  - Liver cirrhosis is a final stage of many chronic liver diseases, accompanying with higher morbidity and mortality rates reduce quality of life. The aim of this study is to examine the predictive potential of predictive analytics for survival in patients diagnosed with liver cirrhosis using a large clinical dataset. The employ advanced machine learning tools, like XGBoost, LightGBM and CatBoost to predict survival rates and classify patients in different prognostic subgroups. The above data preprocessing strategy and ensemble model with soft voting and weighted XGBoost work well toward the performance. It achieves of 0.4264 Logarithmic loss and an F1 score of 0.826 over 10 cross-validation splits on a Tensorflow dataset compared to the state-of-the-art genome analysis tools, These values of SHapley Additive exPlanations (SHAP) are even more important to understand the features used in model decisions. These results reflect the promise of machine learning to enable personalized medicine and clinical decision-making in hepatology by improving upon current prognostic predictions.
ER  - 

TY  - CONF
TI  - Unveiling the Vision: A Comprehensive Review of Computer Vision in AI and ML
T2  - 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)
SP  - 1
EP  - 6
AU  - M. Laad
AU  - R. Maurya
AU  - N. Saiyed
PY  - 2024
KW  - Deep learning
KW  - Computer vision
KW  - Reviews
KW  - Smart cities
KW  - Surveillance
KW  - Precision medicine
KW  - Urban planning
KW  - Computer vision
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Historical Department
KW  - Methodologies
KW  - Traditional
KW  - Approaches
KW  - Deep Learning
KW  - Interpretability
DO  - 10.1109/ADICS58448.2024.10533631
JO  - 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)
Y1  - 18-19 April 2024
AB  - In the era of rapid technological advancement, Computer Vision has emerged as a transformative force, reshaping the landscape of Artificial Intelligence (AI) and Machine Learning (ML). This comprehensive review paper aims to delve into the intricate evolution, methodologies, applications, challenges, and future trajectories of Computer Vision. Moving beyond a mere exploration of technical intricacies, our objective is to present a holistic narrative that encapsulates the profound impact of computer Vision on AI and ML and its repercussions across society. The journey begins by traversing the philosophical and historical roots of Computer Vision, unraveling the threads that connect human visual perception to the development of artificial vision. By exploring the historical evolution from early image processing to the current era of deep learning, we seek to elucidate the intellectual milestones that have shaped the field. Methodologically, this paper navigates through both traditional approaches and contemporary deep learning paradigms. It dissects traditional methods, emphasizing their enduring relevance and influence on modern Computer Vision applications. In parallel, exploring deep learning delves into established architectures all the nuanced impact of design choices on interpretability and explain ability. Applications form a cornerstone of our review, with an enriched focus on case studies that spotlight the transformative influence of Computer Vision. Beyond the traditional domains of image recognition, we delve into the healthcare renaissance, where Computer Vision contributes to diagnostics, drug discovery, and personalized medicine. Furthermore, we explore its role in smart cities, extending beyond surveillance to urban planning, traffic management, and environmental monitoring.
ER  - 

TY  - JOUR
TI  - Developing a Transparent Diagnosis Model for Diabetic Retinopathy Using Explainable AI
T2  - IEEE Access
SP  - 149700
EP  - 149709
AU  - T. Shahzad
AU  - M. Saleem
AU  - M. S. Farooq
AU  - S. Abbas
AU  - M. A. Khan
AU  - K. Ouahada
PY  - 2024
KW  - Accuracy
KW  - Predictive models
KW  - Diabetic retinopathy
KW  - Medical services
KW  - Explainable AI
KW  - Machine learning algorithms
KW  - Logistic regression
KW  - Deep learning
KW  - Retina
KW  - Blindness
KW  - Artificial intelligence
KW  - Convolutional neural networks
KW  - Diabetic retinopathy (DR)
KW  - artificial intelligence (AI)
KW  - explainable AI (XAI)
KW  - convolutional neural network (CNN)
DO  - 10.1109/ACCESS.2024.3475550
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Diabetic retinopathy is a leading cause of vision complications and partially sighted which pose considerable diagnostic difficulties because of its diverse and varying symptoms. Some of them include the fact that the disease displays a non-uniform pattern, where patients present different symptoms; the requirement of highly qualified specialists to interpret the images of the fundus; the risk of errors in the interpretation of images or their inconsistency; and the absence of clear morphological signs often makes early diagnosis unlikely. Traditional diagnosis mostly rely on the expert interpretation of retinal images, which can lead to bias and inaccuracy; highlighting the need for improved diagnostic methods. Although traditional Artificial Intelligence (AI) methods enhance the diagnostic capabilities remarkably, their black box nature and information opacity restrict healthcare providers to comprehend the reasoning framework of the AI to build trust and optimize its usage in practice. Explainable AI (XAI) is an emerging approach that addresses the black-box problem by improving the interpretability of models, which allows users to understand the logic behind certain decisions. This research proposed a diagnosis model for detecting diabetic retinopathy using XAI approaches that increases the interpretability of the models to help clinicians understand the reasons behind the decisions. The proposed model is used to enhance diagnostic accuracy, offer comprehensible, and concise insights regarding the diagnostics. The convergence history plots of the proposed model validate the learning process to achieve 94% better diagnostic accuracy than traditional methods while improving interpretability and applicability in healthcare settings, indicating improvement in accuracy and loss reduction.
ER  - 

TY  - CONF
TI  - Deep Learning for Forecast, Treatment, and Diagnosis of Cancer
T2  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
SP  - 1790
EP  - 1795
AU  - T. Sarada
AU  - A. Sruthi
PY  - 2024
KW  - Deep learning
KW  - Reviews
KW  - Histopathology
KW  - Transcriptomics
KW  - Cancer treatment
KW  - Oncology
KW  - Biology
KW  - Bioinformatics
KW  - Artificial intelligence
KW  - Cancer
KW  - Deep learning
KW  - Artificial Intelligence
KW  - Machine Learning
DO  - 10.1109/ICESC60852.2024.10689855
JO  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
IS  - 
SN  - 2996-5357
VO  - 
VL  - 
JA  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
Y1  - 7-9 Aug. 2024
AB  - Deep learning, a subset of artificial intelligence, is revolutionizing cancer research by unlocking insights from complex biological data. Leveraging neural networks, researchers can analyze vast datasets to identify patterns and predict disease progression. Applications span genomics, transcriptomics, methylation, and histopathology, enabling the integration of diverse data types for comprehensive cancer understanding. This review explores the potential of deep learning in cancer diagnosis, treatment planning, and prognosis prediction. While promising, challenges such as data accessibility and model interpretability hinder widespread adoption. Addressing these limitations is crucial for realizing the full potential of deep learning in precision oncology.
ER  - 

TY  - CONF
TI  - Impact of Artificial Intelligence on Global Healthcare Sector Performance
T2  - 2024 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)
SP  - 1
EP  - 5
AU  - K. Maran
AU  - P. Priyadarshini
AU  - C. R. Senthilnanthan
AU  - M. Manikandan
AU  - R. G. Kumar
AU  - M. Ramu
PY  - 2024
KW  - Ethics
KW  - Technological innovation
KW  - Accuracy
KW  - Costs
KW  - Generative AI
KW  - Virtual assistants
KW  - Medical services
KW  - Radiology
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - Artificial Intelligence
KW  - Healthcare
KW  - hospital
KW  - diagnostic etc
DO  - 10.1109/ICPECTS62210.2024.10780008
JO  - 2024 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)
Y1  - 8-9 Oct. 2024
AB  - Artificial intelligence (AI) has the potential to revolutionize many aspects of healthcare, including diagnosis, treatment planning, medicine discovery, and healthcare management. This research examines the current applications, benefits, and drawbacks of artificial intelligence (AI) in healthcare on a global scale. Artificial intelligence has considerably enhanced personalized treatment recommendations, hospital management, and diagnostic precision, particularly in the fields of radiology and pathology. Artificial intelligence (AI) is reducing the time and money required to bring new treatments to market by speeding up the search for innovative medicines and enhancing the accuracy of efficacy estimates. The use of AI in healthcare still faces challenges, despite these advancements. The integration of AI into current systems, the protection of user data, and the prevention of algorithmic bias are all significant concerns. Clinical decision-making with AI presents serious ethical concerns, particularly for patients' rights to make their own decisions and for the doctor-patient relationship as a whole. The disparities in AI adoption throughout the world highlight the need for developed and poor nations to work together to provide equitable access to AI. This study investigates the profound influence of artificial intelligence (AI) on the healthcare sector, emphasizing its market expansion, revenue growth, and applications across various medical disciplines. The AI healthcare market is expected to grow from $19.27 billion in 2023 to $613.81 billion by 2034, reflecting a significant annual increase driven by technological advancements and rising demand for high-quality care. The compound annual growth rate (CAGR) of 40.2% projected from 2022 to 2029 illustrates the rapid adoption of AI technologies. Generative AI revenue in healthcare is projected to rise from $1.07 million in 2022 to $21.74 million by 2032, highlighting its expanding role in enhancing medical services. AI is notably prevalent in radiology, comprising 75.2% of its applications, but its impact extends to other fields such as cardiovascular care and neurology, where it enhances diagnostic precision and treatment effectiveness. The study also explores the growing use of AI-powered medical imaging, virtual assistants, and chatbots, marking a shift towards more accurate and efficient healthcare delivery. By examining these trends, the research aims to offer insights into how AI can address current healthcare challenges, optimize costs, and advance medical services, particularly in developing regions. This analysis underscores the need for continued investigation into AI’s potential to revolutionize healthcare and improve patient outcomes.
ER  - 

TY  - CONF
TI  - Unveiling Early Signs: A Deep Learning Approach for Pre-symptomatic Disease Detection in Medical Imaging
T2  - 2024 5th IEEE Global Conference for Advancement in Technology (GCAT)
SP  - 1
EP  - 5
AU  - A. Sharma
AU  - T. Barua
AU  - P. Garg
AU  - P. Goel
AU  - R. Saxena
AU  - K. Mohuddin
PY  - 2024
KW  - Deep learning
KW  - Sensitivity
KW  - Explainable AI
KW  - Computed tomography
KW  - Transfer learning
KW  - X-rays
KW  - Data augmentation
KW  - Data models
KW  - Medical diagnostic imaging
KW  - Diseases
KW  - Pre-symptomatic Disease Detection
KW  - Medical Imaging (X-ray, CT scan
KW  - MRI)
KW  - Early Diagnosis
KW  - Preventative Medicine
KW  - Data Augmentation
KW  - Transfer Learning
KW  - Semi-supervised Learning
KW  - Explainable AI (XAI)
KW  - Performance Metrics (accuracy, sensitivity, specificity)
KW  - Data Scarcity
KW  - Model Interpretability
KW  - Clinical Integration
KW  - Ethical Considerations (data privacy, bias)
KW  - Potential Benefits
KW  - Improved Patient Outcomes
KW  - Earlier Diagnosis
KW  - Enhanced Treatment Efficacy
KW  - Reduced Healthcare Costs
DO  - 10.1109/GCAT62922.2024.10924093
JO  - 2024 5th IEEE Global Conference for Advancement in Technology (GCAT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 5th IEEE Global Conference for Advancement in Technology (GCAT)
Y1  - 4-6 Oct. 2024
AB  - Catching diseases early is a big challenge in healthcare. Traditional methods often wait for symptoms to appear, which can be too late. Medical images, like X-rays and CT scans, can give us a look inside the body to find problems early. This study uses deep learning (DL) to analyze these images and find subtle signs of disease before symptoms show up. The researchers created a new DL model that can work with different types of medical images. This model uses a type of AI called convolutional neural networks (CNNs) to learn complex patterns in the images. These patterns might be too small for humans to see, but they could be signs of an early stage of disease. One big problem with using DL for early disease detection is that we don't have many medical images of people who have the disease before they show symptoms. This study explores ways to get more data, like using existing data in a new way or using less labeled data. Another challenge is understanding how the DL model makes its decisions. This is important in healthcare because we need to know why the model thinks someone might be sick. The study looks into ways to make the model more explainable. The researchers tested their model on many different types of medical images and found that it can accurately identify early signs of disease. They also explored how this model could be used in real-world medical settings to improve patient care. This research shows great promise for preventing diseases and improving patient outcomes.
ER  - 

TY  - CONF
TI  - Machine Learning and Deep Learning Approaches for Automated Diabetic Retinopathy Diagnosis
T2  - 2024 International Conference on Intelligent Systems for Cybersecurity (ISCS)
SP  - 1
EP  - 6
AU  - B. Fulkar
AU  - R. Burle
AU  - P. Patil
AU  - S. Gaurkhade
AU  - S. Gundewar
AU  - U. Pacharaney
PY  - 2024
KW  - Deep learning
KW  - Diabetic retinopathy
KW  - Reviews
KW  - Transfer learning
KW  - Imaging
KW  - Transforms
KW  - Medical services
KW  - Diabetic Retinopathy
KW  - Deep Learning
KW  - Convolutional Neural Networks
KW  - Recurrent Neural Networks
KW  - Transfer Learning
KW  - Ensemble Methods
KW  - Attention Mechanisms
KW  - Explainable AI
KW  - Healthcare
KW  - Medical Imaging
DO  - 10.1109/ISCS61804.2024.10581336
JO  - 2024 International Conference on Intelligent Systems for Cybersecurity (ISCS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Intelligent Systems for Cybersecurity (ISCS)
Y1  - 3-4 May 2024
AB  - Since diabetic retinopathy (DR) is the primary cause of blindness in working-age adults worldwide, vision loss must be avoided by receiving early detection and treatment. In recent years, deep learning techniques have become more and more useful tools for automated drug discovery, possibly offering solutions to overcome the limitations of traditional screening methods. In-depth descriptions of the most recent deep learning methods for DR detection are provided in this review article. We begin by discussing the importance of DR screening, its prevalence, and the challenges faced by healthcare systems in managing this condition. Next, we discuss different deep learning architectures used for DR detection, such as CNNs and RNNs, and their combinations. We investigate how to improve the precision, resilience, and interpretability of DR detection models through the use of explainable AI, ensemble methods, transfer learning, and attention mechanisms. We also examine publicly available datasets, evaluation metrics, and preprocessing techniques that are commonly used to benchmark DR detection algorithms. We discuss the limitations and possible uses of deep learning-based deep readout detection, emphasizing the significance of large-scale, diverse datasets, the interpretability of the model, and real-world deployment concerns. The overall objectives of this review are to advance the field of automated DR detection by highlighting recent developments, compiling existing knowledge, and identifying areas that require further research.
ER  - 

TY  - CONF
TI  - Deep Learning with XAI based Multi-Modal MRI Brain Tumor Image Analysis using Image Fusion Techniques
T2  - 2024 International Conference on Trends in Quantum Computing and Emerging Business Technologies
SP  - 1
EP  - 5
AU  - K. Thiruvenkadam
AU  - V. Ravindran
AU  - A. Thiyagarajan
PY  - 2024
KW  - Deep learning
KW  - Image segmentation
KW  - Image analysis
KW  - Quantum computing
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Computational modeling
KW  - MRI
KW  - Brain tumor
KW  - Image fusion
KW  - Segmentation
KW  - Classification
KW  - Deep learning
KW  - XAI
KW  - BraTS
DO  - 10.1109/TQCEBT59414.2024.10545215
JO  - 2024 International Conference on Trends in Quantum Computing and Emerging Business Technologies
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Trends in Quantum Computing and Emerging Business Technologies
Y1  - 22-23 March 2024
AB  - This proposed work depicts the brain tumor image segmentation, classification and retrieval of magnetic resonance imaging (MRI) of brain images using deep learning with explainable artificial intelligence (XAI). In medical image analysis, deep learning takes the leading role to detect and classify diseases in the image and to help doctors in medical diagnosis. In deep learning based medical imaging analysis, features of the images are learned by neural networks. This process is considered as ‘black-box’ model. To understand the inner performance of this model, XAI approaches are implemented with deep learning algorithms. Multimodal MRI brain tumor images are taken for preprocessing technique. These images are fused using image fusion rule, and then the fused image is carry out with deep learning models to classify tumor and non tumor images from the dataset. The MRI images taken from the BraTS dataset are used for experiment. The performance analyses are done with evaluation parameters such as accuracy, precision and recall.
ER  - 

TY  - CONF
TI  - Transforming Healthcare with Federated Learning-based Artficial Intelligence: Concepts, Classifications, and Challenges
T2  - 2024 3rd International Conference on Sentiment Analysis and Deep Learning (ICSADL)
SP  - 209
EP  - 216
AU  - G. U. Maheswari
AU  - J. G. Jeslin
AU  - M. Rajasuguna
AU  - A. S
PY  - 2024
KW  - Industries
KW  - Training
KW  - Privacy
KW  - Explainable AI
KW  - Federated learning
KW  - Medical services
KW  - Data models
KW  - Federated learning
KW  - Artificial Intelligence
KW  - Explainable Artificial Intelligence
KW  - Healthcare
DO  - 10.1109/ICSADL61749.2024.00040
JO  - 2024 3rd International Conference on Sentiment Analysis and Deep Learning (ICSADL)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Sentiment Analysis and Deep Learning (ICSADL)
Y1  - 13-14 March 2024
AB  - The realm of intelligent healthcare has seen a surge in discussions about transformative technologies, with a focus on federated learning (FL), artificial intelligence (AI), and explainable AI (XAI). Traditionally, healthcare data has been centralized, with a limited number of entities sharing raw data across the industry. However, this approach poses significant risks and challenges. Introducing AI into the healthcare ecosystem opens the door to a multitude of agent collaborators, each capable of working alongside their chosen host. The decentralized nature of FL is a noteworthy aspect, as it ensures that raw data remains within its source while enabling model-based communication within the system. The fusion of FL, AI, and XAI offers the potential to address some of the system's limitations and challenges. This article delves deep into the ways in which FL is revolutionizing healthcare facilities through AI integration. We begin by examining the current landscape of futuristic technologies, including FL, AI, XAI, and their relationship with the healthcare sector. We categorize FL in conjunction with AI, exploring a wide array of applications in healthcare IT. Key issues within the healthcare domain, such as privacy, security, stability, and reliability, are addressed. Furthermore, we showcase practical use cases of FL and AI in medical settings. We conclude by shedding light on the various future directions and avenues of research in the management of healthcare using FL-based AI. In this article, we offer insights into the development and implementation of the Contribution-Aware Federated Learning (CAreFL) system for intelligent healthcare. This system not only optimizes the FL model aggregation process based on evaluation results but also assesses the contributions of FL participants in a fair and transparent manner, ensuring efficiency and confidentiality.
ER  - 

TY  - CONF
TI  - Explainable AI for Healthcare Diagnosis in Renal Cancer
T2  - 2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0
SP  - 1
EP  - 6
AU  - Yukta
AU  - A. P. Biswas
AU  - S. Kashyap
PY  - 2024
KW  - Training
KW  - Accuracy
KW  - Explainable AI
KW  - Medical services
KW  - Feature extraction
KW  - Convolutional neural networks
KW  - Medical diagnostic imaging
KW  - Healthcare diagnosis
KW  - Renal cancers
KW  - Explainable AI (XAI)
KW  - CNN
KW  - LIME
KW  - Grad-CAM
KW  - accuracy
KW  - patient care
DO  - 10.1109/OTCON60325.2024.10687607
JO  - 2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 OPJU International Technology Conference (OTCON) on Smart Computing for Innovation and Advancement in Industry 4.0
Y1  - 5-7 June 2024
AB  - Healthcare diagnosis plays a pivotal role in ensuring the timely and accurate identification of diseases, with renal cancers posing particular challenges due to the intricacies involved in detecting subtle tumor patterns within the renal system. This research explores the application of explainable AI (XAI) in enhancing healthcare diagnostics for renal cancers, aiming to improve early detection and treatment outcomes. Our study examines high-resolution medical imaging data from various sources, such as PACS systems and clinical databases, by integrating sophisticated AI algorithms, such as Convolutional Neural Networks (CNNs), and techniques, such as Gradient Weighted Class Activation Mapping (Grad CAM) and Local Interpretable Model Agnostic Explanations (LIME), to explain AI processes. The proposed model achieves a remarkable 99.75 percent accuracy and 99.74 percent $F 1$ score, demonstrating its efficacy in diagnosing renal diseases and cancers. Visualization tools such as confusion matrices, classification reports, LIME explanations, and Grad-CAM explanations provide insights into the model’s decision-making processes, aiding clinicians in understanding diagnostic predictions and improving patient care. This research underscores the importance of XAI in healthcare, contributing to advancements in renal cancer diagnostics and paving the way for more effective and precise disease management strategies.
ER  - 

TY  - STD
TI  - IEEE Approved Draft Guide for an Architectural Framework for Explainable Artificial Intelligence
T2  - IEEE P2894/D9, August 2023
SP  - 1
EP  - 51
PY  - 2024
KW  - IEEE Standards
KW  - Artificial intelligence
KW  - Computer architecture
KW  - AI
KW  - architectural framework
KW  - artificial intelligence
KW  - explainable AI
KW  - explainable artificial intelligence
KW  - IEEE 2894™
KW  - machine learning
KW  - XAI
DO  - 
JO  - IEEE P2894/D9, August 2023
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE P2894/D9, August 2023
Y1  - 21 Feb. 2024
AB  - A new wave of artificial intelligence applications that offer extensive benefits to our daily lives has been led to by dramatic success in machine learning. The loss of explainability during this transition, however, means vulnerability to vicious data, poor model structure design, and suspicion of stakeholders and the general public--all with a range of legal implications. The study of explainable AI (XAI), which is an active research field that aims to make AI systems results more understandable to humans, has been called for by this dilemma. This is a field with great hopes for improving the trust and transparency of AI-based systems and is considered a necessary route for AI to move forward. A technological blueprint for building, deploying, and managing machine learning models, while meeting the requirements of transparent and trustworthy AI by adopting a variety of XAI methodologies, is provided by this guide. It defines the architectural framework and application guidelines for explainable AI, including: description and definition of XAI; the types of XAI methods and the application scenarios to which each type applies; and performance evaluation of XAI.
ER  - 

TY  - CONF
TI  - From the Perspective of AI Safety: Analyzing the Impact of XAI Performance on Adversarial Attack
T2  - GLOBECOM 2024 - 2024 IEEE Global Communications Conference
SP  - 4982
EP  - 4987
AU  - A. Asiri
AU  - F. Wu
AU  - Z. Tian
AU  - S. Yu
PY  - 2024
KW  - Deep learning
KW  - Explainable AI
KW  - Medical services
KW  - Safety
KW  - Security
KW  - Global communication
KW  - AI Safety
KW  - Adversarial Attack
KW  - FGSM Attack
KW  - Explainable AI
KW  - Saliency Map
DO  - 10.1109/GLOBECOM52923.2024.10901342
JO  - GLOBECOM 2024 - 2024 IEEE Global Communications Conference
IS  - 
SN  - 2576-6813
VO  - 
VL  - 
JA  - GLOBECOM 2024 - 2024 IEEE Global Communications Conference
Y1  - 8-12 Dec. 2024
AB  - The outstanding performance of machine learning models across various fields enables them to be used in sensitive and high-risk activities such as healthcare, automated driving, and security services. However, the explainability of their outputs and the safety of their operation are major concerns. Although Explainable AI (XAI) can enhance the interpretability of AI models, further research is necessary to evaluate its effectiveness in explaining adversarial attacks. The use of XAI techniques and the rise of adversarial attacks are important issues related to the explainability and security of deep learning, respectively. Furthermore, the relation between the explainability and safety of deep learning, such as the vulnerability of the XAI explanation to an adversarial attack, is key to unlocking these concerns. In this paper, we use the Saliency Map as an XAI technique to explain the behavior of the Fast Gradient Sign Method (FGSM) adversarial attack on the ResNet model, and show the vulnerability related to such an explanation with respect to the attack. Extensive experiments show that as the severity of FGSM attack on the ResNet model increases, the Saliency Map gradually fails, exposing its potential vulnerability.
ER  - 

TY  - CONF
TI  - AL-XAI-MERS: Unveiling Alzheimer's Mysteries with Explainable AI
T2  - 2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)
SP  - 1
EP  - 7
AU  - A. Deshmukh
AU  - N. Kallivalappil
AU  - K. D'souza
AU  - C. Kadam
PY  - 2024
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Decision making
KW  - Market research
KW  - Brain modeling
KW  - Data models
KW  - Medical diagnosis
KW  - Alzheimer's disease
KW  - brain MRI scans
KW  - machine learning
KW  - Explainable Artificial Intelligence (XAI)
KW  - medical diagnostics
KW  - second opinion
KW  - transparency
KW  - healthcare
KW  - disease detection
KW  - pattern recognition
DO  - 10.1109/ic-ETITE58242.2024.10493489
JO  - 2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE)
Y1  - 22-23 Feb. 2024
AB  - Alzheimer's disease poses an escalating global health challenge, necessitating accurate and timely diagnosis for effective intervention. This study presents a novel approach to Alzheimer's detection utilising advanced machine learning techniques applied to brain MRI scans. Leveraging Explainable Artificial Intelligence (XAI) methods, the developed model not only detects Alzheimer's disease but also offers transparent insights into the intricate patterns within the MRI data. In an era where Alzheimer's prevalence is rising, our methodology provides a valuable tool for clinicians and patients. By employing XAI, individuals can gain a comprehensive understanding of their MRI results, enabling them to seek second opinions and fostering a deeper comprehension of their condition. This research marks a significant step towards democratising medical diagnostics, empowering individuals with knowledge and promoting informed decision-making in Alzheimer's diagnosis and management.
ER  - 

TY  - CONF
TI  - Unmasking VGG16: LIME Visualizations for Brain Tumor Diagnosis
T2  - 2024 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI)
SP  - 1
EP  - 6
AU  - R. Tiwari
AU  - R. Agrawal
PY  - 2024
KW  - Heating systems
KW  - Deep learning
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Medical diagnosis
KW  - Medical diagnostic imaging
KW  - Machine intelligence
KW  - Image classification
KW  - VGG16
KW  - XAI
KW  - LIME
KW  - HEATMAP
DO  - 10.1109/CVMI61877.2024.10781708
JO  - 2024 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI)
Y1  - 19-20 Oct. 2024
AB  - Brain tumors are complex and potentially life-threatening conditions requiring accurate diagnosis. This study explores using VGG16 CNN architecture to classify brain tumors in MRI scans. CNNs excel in image classification but lack transparency, raising concerns in critical applications like medical diagnosis. To address this, we use Explainable AI (XAI) techniques. We enhance interpretability using Superpixel images, Heatmaps, and Image Masks to visualize important features for predictions. Our approach achieves 98.8% accuracy, with visualizations aiding in understanding the model’s decisions and providing insights for medical professionals. This research contributes to XAI in medical imaging, showing the potential of combining deep learning with interpretable methods for healthcare decision-making.
ER  - 

TY  - CONF
TI  - Artificial Intelligence in Healthcare Supply Chain Management: A Bibliometric Analysis: Subtitle as needed (AI in Healthcare Supply Chain)
T2  - 2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)
SP  - 1265
EP  - 1268
AU  - S. Gupta
AU  - A. Gupta
AU  - N. Virmani
AU  - M. Singh
PY  - 2024
KW  - Supply chain management
KW  - Reviews
KW  - Bibliometrics
KW  - Supply chains
KW  - Decision making
KW  - Transportation
KW  - Demand forecasting
KW  - Medical services
KW  - Artificial intelligence
KW  - Resilience
KW  - Artificial Intelligence (AI)
KW  - Healthcare Supply Chain Management (HSCM)
KW  - Patients Safety
KW  - Improved Healthcare Performance
DO  - 10.1109/ICACCS60874.2024.10717239
JO  - 2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)
IS  - 
SN  - 2575-7288
VO  - 1
VL  - 1
JA  - 2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)
Y1  - 14-15 March 2024
AB  - The presented paper discussed the review of Healthcare Supply Chain Management (HSCM) using Artificial Intelligence (AI). The implementation of artificial intelligence (AI) in HSCM has numerous benefits, including accurate demand forecasting of medical supplies, cost reduction, increased transparency, visibility, data-driven decision-making, enhanced supply chain resilience, streamlined healthcare operations, optimized transportation, and many more. Our approach to using AI in HSCM involved a thorough examination of the literature and bibliometric analysis. Research was started by exploring the Scopus database using suitable keywords. After the inclusion and exclusion criteria have been applied, the relevant papers were gone through full-text readings. Using Vos-viewer, the research papers were further analyzed for bibliometric analysis.
ER  - 

TY  - CONF
TI  - Enhancing Results with ANFIS and XAI for Soil Contamination Monitoring and Public Health Risk Assessment
T2  - 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)
SP  - 1120
EP  - 1123
AU  - Y. Trofimov
AU  - A. Averkin
PY  - 2024
KW  - Accuracy
KW  - Uncertainty
KW  - Explainable AI
KW  - Urban areas
KW  - Data models
KW  - Mathematical models
KW  - Soil pollution
KW  - Risk management
KW  - Public healthcare
KW  - Synthetic data
KW  - neuro-fuzzy system
KW  - ANFIS
KW  - XAI
KW  - interpretability
KW  - environmental monitoring
KW  - fuzzy logic
KW  - health risks
DO  - 10.1109/PIERE62470.2024.10805065
JO  - 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)
Y1  - 15-17 Nov. 2024
AB  - The article presents a study on the application of Explainable Artificial Intelligence (XAI) principles and the neuro-fuzzy system ANFIS to improve the accuracy and interpretability of environmental soil pollution monitoring in the city of Dubna. The work explores the mathematical aspects of integrating fuzzy logic to model uncertainties and nonlinear dependencies in geoecological data, enabling transparent decision-making processes. As part of the research, a synthetic dataset was generated to enhance spatial modeling precision and address gaps in the existing data. This synthetic data, combined with the XAI framework, facilitated improved interpolation, leading to a more accurate and comprehensive understanding of pollutant distribution across different urban zones. The use of ANFIS allowed for the development of interpretable models that account for the spatial variability of pollutants and provide precise assessments of public health risks. The implementation of XAI further enhanced the transparency of inference processes and reduced uncertainty in predicting environmental risks, which is crucial for developing robust and effective environmental management strategies.
ER  - 

TY  - CONF
TI  - Revolutionizing Healthcare Intelligence Multisensory Data Fusion with Cutting-Edge Machine Learning and Deep Learning for Patients’ Cognitive Knowledge
T2  - 2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
SP  - 1
EP  - 7
AU  - S. R. Swamy
AU  - K. S. Nandini Prasad
PY  - 2024
KW  - Deep learning
KW  - Knowledge engineering
KW  - Ethics
KW  - Machine learning algorithms
KW  - Data integration
KW  - Medical services
KW  - Data models
KW  - Healthcare Intelligence
KW  - Multisensor Data Fusion
KW  - Machine Learning
KW  - Deep Learning
KW  - Cognitive Knowledge
DO  - 10.1109/ICKECS61492.2024.10616464
JO  - 2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
Y1  - 18-19 April 2024
AB  - The study explores the recent advancements in healthcare that have witnessed an unprecedented integration of multisensory data, ranging from wearable devices to medical imaging and electronic health records. This paper explores the transformative impact of cutting-edge machine learning algorithms and deep learning algorithms in processing and interpreting this multisensory data, offering unparalleled insights into patients’ cognitive well-being. The review delves into the types and challenges of multisensory data, evaluating performance of supervised and supervised machine learning models for disease detection and pattern recognition respectively. Additionally, it scrutinizes the importance of deep learning applications that are based on convolutional neural networks as well as recurrent neural networks for medical image classification. The paper also mentions the importance of the integration of multisensor data fusion with these advanced algorithms, emphasizing real-time monitoring systems and ethical considerations. Summaries from research data shows effectiveness of remote patient care and predictive analysis and personalized treatment plans. The exploration of future directions and opportunities underscores the prime importance for further advancements in explainable AI, collaborative learning, and edge computing, consolidating the vision of a healthcare paradigm revolutionized by intelligent data fusion and cutting-edge technologies.
ER  - 

TY  - CONF
TI  - Integrating and Interpreting Biomedical Analysis: A Comprehensive Analysis of Machine Learning Algorithms for Precision Medicine
T2  - 2024 International Conference on Advances in Computing, Communication and Materials (ICACCM)
SP  - 1
EP  - 6
AU  - G. Sasikala
AU  - S. B. J
AU  - D. A
AU  - C. L. Annapoorani
AU  - P. M
AU  - V. Mythily
PY  - 2024
KW  - Machine learning algorithms
KW  - Precision medicine
KW  - Biological system modeling
KW  - Supervised learning
KW  - Genomics
KW  - Proteomics
KW  - Prediction algorithms
KW  - Planning
KW  - Bioinformatics
KW  - Unsupervised learning
KW  - Clinical Records
KW  - Disease Diagnosis
KW  - Treatment Response Prediction
KW  - Personalized Treatment Planning
KW  - Supervised Learning
DO  - 10.1109/ICACCM61117.2024.11059016
JO  - 2024 International Conference on Advances in Computing, Communication and Materials (ICACCM)
IS  - 
SN  - 2642-7354
VO  - 
VL  - 
JA  - 2024 International Conference on Advances in Computing, Communication and Materials (ICACCM)
Y1  - 22-23 Nov. 2024
AB  - A new era in healthcare has begun with precision medicine, which tailors medications to the specific characteristics of each individual patient. This shift is being driven by the advent of advanced machine learning (ML) algorithms that can parse and make sense of biological data culled from diverse sources like clinical records, proteomics, and genomes. This study aims to give a thorough evaluation of ML algorithms employed in biomedical analysis for precision medicine, with a particular emphasis on these algorithms' integration, interpretation, and practical uses. In this first step, we take stock of biological data and its current state, drawing attention to issues like data volume, privacy concerns, and data heterogeneity that pose obstacles to its integration. We continue by taking a look at the most recent developments in supervised and unsupervised learning as well as deep learning algorithms used in biological analysis. In the context of precision medicine, we discuss the pros and cons of these algorithms and emphasise their ability to handle complex data and extract valuable insights. Methods for feature selection, data preprocessing, and model validation are among the topics covered in this investigation of how to integrate ML algorithms with biological data. To make sure that healthcare providers and patients can understand and trust the results of machine learning (ML) models, we investigate how explainable AI (XAI) can help interpret these decisions. We also provide case examples that show how ML algorithms have been used for precision medicine to diagnose diseases, predict how treatments will work, and create individualised treatment plans. These examples show how ML has the ability to revolutionise healthcare by enhancing patient outcomes while decreasing expenses. Lastly, we discuss potential future developments and areas for future research in the field, such as improving XAI methods, integrating multi-omics data, and creating more sophisticated ML models. The research highlights the significance of integrating knowledge from several disciplines to progress precision medicine, specifically computer science, data science, and biomedical sciences.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence based ML Models for Heart Disease Prediction
T2  - 2024 3rd International Conference on Computational Modelling, Simulation and Optimization (ICCMSO)
SP  - 160
EP  - 164
AU  - S. Kommineni
AU  - S. Muddana
AU  - R. Senapati
PY  - 2024
KW  - Heart
KW  - Explainable AI
KW  - Computational modeling
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Nearest neighbor methods
KW  - Algorithm
KW  - Heart disease
KW  - Machine Learning
KW  - XAI
DO  - 10.1109/ICCMSO61761.2024.00042
JO  - 2024 3rd International Conference on Computational Modelling, Simulation and Optimization (ICCMSO)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Computational Modelling, Simulation and Optimization (ICCMSO)
Y1  - 14-16 June 2024
AB  - Heart disease prediction is important in healthcare because it enables timely identification and intervention of actual condition of the patient. However, the task of accurately predicting disease remains a challenging task. In this paper, we have proposed a framework for heart disease prediction using explainable artificial intelligence (XAI) based Machine Learning (ML) models such as Decision Tree (DT), Random Forest (RF), k-nearest neighbors (KNN), AdaBoost, Logistic Regression (LR), Naive Bayes (NB), and Neural Network (NN). The efficiency of those models were evaluated using MCC, accuracy, precision, recall, and AUC. Finally, it is observed that, DT emerges as the most effective model offering interpretability. This study underscores the importance of transparent models in healthcare and advocates in order to incorporate XAI to enhance interpretability and medical decision-making.
ER  - 

TY  - CONF
TI  - Roles of AI in Healthcare
T2  - 2024 International Conference on Information Technology and Computing (ICITCOM)
SP  - 289
EP  - 294
AU  - N. P. Apriyanto
AU  - N. Alamsyah
AU  - O. Wijaya
AU  - E. Loniza
AU  - Y. Satriyandari
PY  - 2024
KW  - Accuracy
KW  - Reviews
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Real-time systems
KW  - Data mining
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - Diseases
KW  - Healthcare
KW  - Artificial Intelligence
KW  - Descriptive-AI
KW  - Predictive-AI
KW  - Prescriptive-AI
DO  - 10.1109/ICITCOM62788.2024.10762060
JO  - 2024 International Conference on Information Technology and Computing (ICITCOM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Information Technology and Computing (ICITCOM)
Y1  - 7-8 Aug. 2024
AB  - Artificial Intelligence (AI) has transformed the healthcare landscape, introducing capabilities that span from predictive diagnostics to personalized treatment strategies. This review explores the multifaceted roles of AI within healthcare, emphasizing its application in three primary areas: descriptive, predictive, and prescriptive AI. Descriptive AI focuses on data analysis and the extraction of meaningful information from large datasets, helping in the identification of patterns and trends that enhance diagnostic and operational procedures. Predictive AI forecasts health outcomes and tailors medical interventions by utilizing advanced algorithms to analyze historical and real-time data, thereby improving the accuracy of medical assessments and disease anticipation. Prescriptive AI goes further by not only predicting outcomes but also suggesting actionable interventions tailored to individual patient needs, thus optimizing healthcare delivery and patient management. The integration of machine learning, deep learning, and generative AI technologies is thoroughly examined, highlighting their impact on improving diagnostics, patient care, and the overall efficiency of healthcare services.
ER  - 

TY  - CONF
TI  - Machine Learning Algorithms for Stroke Risk Prediction Leveraging on Explainable Artificial Intelligence Techniques (XAI)
T2  - 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)
SP  - 1
EP  - 6
AU  - O. Ugbomeh
AU  - V. Yiye
AU  - E. Ibeke
AU  - C. P. Ezenkwu
AU  - V. Sharma
AU  - A. Alkhayyat
PY  - 2024
KW  - Measurement
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Prevention and mitigation
KW  - Medical services
KW  - Learning (artificial intelligence)
KW  - Predictive models
KW  - Prediction algorithms
KW  - Informatics
KW  - Random forests
KW  - Explainable Artificial Intelligence (XAI)
KW  - Explain Like I’m Five (ELI5)
KW  - Local Interpretable Model Agnostic Explanation (LIME)
KW  - Machine Learning
DO  - 10.1109/ICEECT61758.2024.10739320
JO  - 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)
Y1  - 29-31 Aug. 2024
AB  - Stroke poses a significant global health challenge, contributing to widespread mortality and disability. Identifying predictors of stroke risk is crucial for enabling timely interventions, thereby reducing the increasing impact of strokes. This research addresses this imperative by employing Explainable Artificial Intelligence (XAI) techniques to pinpoint stroke risk predictors. To bridge existing gaps, six machine learning models were assessed using key performance metrics. Utilising the Synthetic Minority Over-sampling Technique (SMOTE) to minimize the impact of the imbalanced nature of the dataset used in this research, the Random Forest algorithm emerged as the most effective among the algorithms with an accuracy of 94.5%, AUC-ROC of 0.95, recall of 0.96, precision of 0.93, and an F1 score of 0.95. This study explored the interpretation of these algorithms and results using Local Interpretable Model-agnostic Explanations (LIME) and Explain Like I’m Five (ELI5). With the interpretations, healthcare providers can gain insight into patients’ stroke risk predictions. Key stroke risk factors highlighted by the study include Age, Marital Status, Glucose Level, Body Mass Index, Work Type, Heart Disease, and Gender. This research significantly contributes to healthcare and healthcare informatics by providing insights that can enhance strategies for stroke prevention and management, ultimately leading to improved patient care. The identified predictors offer valuable information for healthcare professionals to develop targeted interventions, fostering a proactive approach to mitigating the impact of strokes on individuals and the healthcare system.
ER  - 

TY  - CONF
TI  - Explainable AI for Real-Time Object Detection in Autonomous Driving
T2  - 2024 International Conference on Artificial Intelligence, Metaverse and Cybersecurity (ICAMAC)
SP  - 1
EP  - 5
AU  - T. Sajid
AU  - O. A. Latif
PY  - 2024
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Focusing
KW  - Object detection
KW  - Predictive models
KW  - Real-time systems
KW  - Safety
KW  - Autonomous vehicles
KW  - YOLOv8
KW  - Explainable AI (XAI)
KW  - Object Detection
KW  - Autonomous Driving
KW  - Class Activation Mapping (CAM)
KW  - Layerwise Relevance Propagation (LRP)
DO  - 10.1109/ICAMAC62387.2024.10829286
JO  - 2024 International Conference on Artificial Intelligence, Metaverse and Cybersecurity (ICAMAC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Artificial Intelligence, Metaverse and Cybersecurity (ICAMAC)
Y1  - 25-26 Oct. 2024
AB  - The increasing complexity of Artificial Intelligence (AI) models, especially in applications like autonomous driving, healthcare, and other systems, requires not only high accuracy but also transparency in the decision-making processes. This paper presents an approach to integrating Explainable AI (XAI) methods with YOLOv8, a state-of-the-art object detection model, to provide interpretability of its predictions in real-time autonomous driving environments. We focus on implementing various Class Activation Mapping (CAM) methods, such as Grad-CAM and HiResCAM, as well as Layer-wise Relevance Propagation (LRP) to create an explanation framework. We also combine CAM and LRP to make use of the strengths of both methods, improving the overall explainability. This method achieves good performance, running object detection at 50 to 60 frames per second (FPS) on a standard dedicated GPU, and around 24 FPS when running alongside the explainability models. Furthermore, this approach maintains a high detection accuracy, due to its post hoc nature, making sure that the XAI implementations do not affect the model’s detection performance. Our experimental results shows that the combined XAI approach not only preserves the high detection accuracy of YOLOv8 but also provides more meaningful visual explanations using the XAI methods, showing that this integrated XAI methods could improve the trustworthiness and safety of AI-driven systems in autonomous vehicles and other possible fields.
ER  - 

TY  - CONF
TI  - Preventing Health Records Risk Analysis with Explainable AI
T2  - 2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
SP  - 308
EP  - 312
AU  - V. Pandimurugan
AU  - B. Balakiruthiga
AU  - J. Umamageswaran
AU  - S. A. Angayarkanni
AU  - V. Rajaram
AU  - A. Chinnasamy
PY  - 2024
KW  - Industries
KW  - Privacy
KW  - Ethics
KW  - Explainable AI
KW  - Standards organizations
KW  - Medical services
KW  - Organizations
KW  - Security
KW  - Risk management
KW  - Electronic medical records
KW  - Electronic Health Records
KW  - Security And Privacy
KW  - Explainable AI
KW  - Sensitivity
DO  - 10.1109/ICSSAS64001.2024.10760785
JO  - 2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
Y1  - 23-25 Oct. 2024
AB  - The security of electronic health records plays a crucial concern in the healthcare industry due to the sensitive nature of the information and its ethical aspects. Security analysis of health records involves evaluating, managing, and preserving the integrity, confidentiality, and availability of sensitive healthcare information. Explainable AI plays a significant part in the security analysis of health records by offering transparency, interpretability, and accountability in the decision-making process, thereby enhancing trust, compliance, and proactive risk management in handling sensitive healthcare information. The proper implementing of explainable AI in practice in healthcare organizations can significantly reduce the risks associated with handling sensitivee health records and ensure patient data remains secure and confidential.
ER  - 

TY  - CONF
TI  - Advancing Predictive Modelling in Healthcare A Data Science Approach Utilizing AI-Driven Algorithms
T2  - 2024 OITS International Conference on Information Technology (OCIT)
SP  - 363
EP  - 368
AU  - R. Vadisetty
PY  - 2024
KW  - Analytical models
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Medical services
KW  - Transforms
KW  - Predictive models
KW  - Prediction algorithms
KW  - Predictive analytics
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - Explainable artificial intelligence
KW  - Predictive modelling
KW  - Healthcare informatics
KW  - Human-computer interactions
DO  - 10.1109/OCIT65031.2024.00070
JO  - 2024 OITS International Conference on Information Technology (OCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 OITS International Conference on Information Technology (OCIT)
Y1  - 12-14 Dec. 2024
AB  - In theory, AI should be able to learn from data from a variety of sources and replicate human intellect in order to carry out tasks, identify patterns, or make predictions. Many areas of technology have made extensive use of AI and ML algorithms, including: autonomous vehicles, recommendation systems in e-commerce and social media, financial technology, question answering systems, and natural language processing. Additionally, AI is slowly but surely altering the medical research scene. Since its inception fifty years ago, the rule-based approach to illness diagnosis and clinical decision support has garnered considerable attention. This strategy is focused on curating medical knowledge and building powerful decision rules. Predictive modelling in healthcare has recently demonstrated promise with the use of machine learning techniques like deep learning, which can account for complicated connections between features. The lack of explainability in some of these algorithms makes it difficult for them to be fully embraced in actual clinical situations, even if many of these AI and ML algorithms can reach exceptional performance. New forms of explainable artificial intelligence (XAI) are appearing to aid patients in conveying their inner thoughts, feelings, and behaviours to medical staff. Clinicians put their faith in XAI because it explains the results of its predictions so that they may understand how to put the modelling to use in real-world scenarios rather than just blindly following them. The intricacy of medical knowledge means that there are still many possibilities that need to be explored in order to make XAI useful in clinical settings.
ER  - 

TY  - CONF
TI  - The Performance Analysis of Health Care Automation using Artificial Intelligence Model
T2  - 2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)
SP  - 990
EP  - 994
AU  - R. Sawanni
AU  - M. Mathur
AU  - R. Gupta
AU  - A. Kumar
AU  - Richa
AU  - A. Garg
PY  - 2024
KW  - Automation
KW  - Training data
KW  - Medical services
KW  - Market research
KW  - Performance analysis
KW  - Object recognition
KW  - Medical diagnosis
KW  - Artificial Intelligence
KW  - Healthcare
KW  - Optimization
KW  - Medical Records
KW  - Patient
KW  - Privacy
DO  - 10.1109/IC3SE62002.2024.10593586
JO  - 2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)
Y1  - 9-11 May 2024
AB  - Artificial intelligence (AI) has shown great promise in healthcare, especially in healthcare automation. Without human input, AI systems are able to analyze vast volumes of data and make predictions and judgments based on that data. As a result, patient outcomes may be improved, and several healthcare systems may operate more accurately and efficiently. Processes in administration can be streamlined and optimized using AI. As a result, medical staff members are less burdened and can concentrate on giving patients high-quality treatment. AI's application in healthcare presents difficulties. A few issues to be concerned about are AI algorithms' lack of explainability and transparency, possible bias in the training data, and moral problems with patient privacy and data security. Medical diagnosis is a crucial area in which artificial intelligence has been used to automate healthcare. AI algorithms can produce diagnoses and treatment regimens by examining a patient's symptoms, medical history, and other pertinent information. This has demonstrated encouraging outcomes in raising the accuracy of diagnoses and lowering mistake rates.
ER  - 

TY  - CONF
TI  - A Review on the Integration of Artificial Intelligence in Healthcare
T2  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
SP  - 880
EP  - 884
AU  - A. V. Pargaien
AU  - S. Pargaien
AU  - A. Nawaz
AU  - T. Kumar
PY  - 2024
KW  - Training
KW  - Surveys
KW  - Ethics
KW  - Privacy
KW  - Reviews
KW  - Medical services
KW  - Companies
KW  - Regulation
KW  - Security
KW  - Artificial intelligence
KW  - Healthcare
KW  - Artificial Intelligence
KW  - Technology
DO  - 10.1109/ICESC60852.2024.10689737
JO  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
IS  - 
SN  - 2996-5357
VO  - 
VL  - 
JA  - 2024 5th International Conference on Electronics and Sustainable Communication Systems (ICESC)
Y1  - 7-9 Aug. 2024
AB  - Artificial Intelligence (AI) is increasingly influential in health care, offering potential enhancements in diagnosis, patient care, and treatment therapies. By utilizing large datasets and sophisticated algorithms, AI assists healthcare professionals in making informed decisions, including resource allocation. For effective and safe use, professionals require adequate training and education in these technologies. AI also allows patients by improving their understanding of health conditions and providing customized solutions for diagnosis and treatment. However, concerns about data privacy, security, and ethical use are dominant. Companies must ensure that patient data is protected from breaches and misuse and is not exploited for identification or control purposes. Additionally, there are ongoing debates about the transparency, and accountability of AI systems in healthcare. This study highlights the necessity for enabling stringent regulatory frameworks to ensure these technologies are used ethically and responsibly, increasing the trust and reliability in their application within the healthcare sector.
ER  - 

TY  - CONF
TI  - Machine Learning in Healthcare: Opportunities and Challenges
T2  - 2024 International Conference on Smart Technologies for Sustainable Development Goals (ICSTSDG)
SP  - 1
EP  - 5
AU  - P. M. A. M.A.Y
AU  - K. Sudha
AU  - E. Pooja
AU  - D. Lekha
AU  - M. Ezhilvendan
AU  - G. S
PY  - 2024
KW  - Ethics
KW  - Protocols
KW  - Federated learning
KW  - Medical services
KW  - Predictive models
KW  - Real-time systems
KW  - Safety
KW  - Sustainable development
KW  - Medical diagnostic imaging
KW  - Monitoring
KW  - Artificial intelligence
KW  - health technology
KW  - precision treatment
KW  - imaging
KW  - diagnosis
DO  - 10.1109/ICSTSDG61998.2024.11026363
JO  - 2024 International Conference on Smart Technologies for Sustainable Development Goals (ICSTSDG)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Smart Technologies for Sustainable Development Goals (ICSTSDG)
Y1  - 6-8 Nov. 2024
AB  - Advancements in Machine Learning are revolutionizing healthcare by innovating in diagnostic and healthcare support technologies. The study explores the core ML strategies in healthcare including supervised and unsupervised models as well as their role in predicting diseases and in monitoring real-time. Despite ML's great promise challenges including privacy of data and ethical issues restrict its widespread acceptance. To address these problems, emergent approaches like federated learning and explainable AI are unfolding. To realize the full potential of this technology, ML for health care requires a collaborative effort from the health care professional, AI analytics specialist, and the policy makers. In addressing those challenges this work also assesses the current state of application of ML in healthcare as it looks at the challenges and the way forward to enhance the uptake of healthcare technology such as IoT and Blockchain towards developing a personalized healthcare system.
ER  - 

TY  - CONF
TI  - The Application of Explainable AI (XAI) to Create Understanding and Trust in AI Decision-Making: A Study
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
SP  - 1
EP  - 7
AU  - R. Pradhan
AU  - D. Jain
AU  - K. Mittal
AU  - Nandini
PY  - 2024
KW  - Analytical models
KW  - Explainable AI
KW  - Law
KW  - Decision making
KW  - Natural languages
KW  - Medical services
KW  - Banking
KW  - Automobiles
KW  - Explainable AI (XAI)
KW  - Understanding
KW  - Trust
KW  - Decision-Making
DO  - 10.1109/ICCCNT61001.2024.10724234
JO  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
IS  - 
SN  - 2473-7674
VO  - 
VL  - 
JA  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
Y1  - 24-28 June 2024
AB  - The demand for Explainable AI (XAI) has increased recently to create confidence and comprehension in AI decision-making. The goal of the AI research area known as “XAI” is to create models and algorithms something everyone can understand and utilize with ease. Making AI systems’ internal operations transparent and understandable is the aim of XAI, which can assist humans in making better decisions by enabling them to comprehend the thinking behind the decisions made by AI. This paper will examine the numerous uses of XAI in various fields, including banking, healthcare, autonomous cars, and legal and governmental decision-making. Additionally, we will go over the various XAI methodologies, including natural language explanations, model interpretability, and feature importance analysis. Lastly, we’ll look at the difficulties and potential paths for research on XAI. The purpose of this paper is to give broad examination of the XAI research field today and discuss how it can affect the development of confidence and comprehension in AI decision-making.
ER  - 

TY  - CONF
TI  - Advancements in Machine Learning and Deep Learning Techniques for Sickle Cell Disease Detection using Digital Morphology
T2  - 2024 International Conference on Computer and Applications (ICCA)
SP  - 1
EP  - 8
AU  - R. S. K. Abdul
AU  - E. B. George
AU  - V. K. Anand
AU  - E. S. Al Abri
AU  - M. S. Al Rahbi
AU  - K. N. Nadaf
AU  - A. W. Ahmed Al-Abdali
PY  - 2024
KW  - Deep learning
KW  - Accuracy
KW  - Reviews
KW  - Soft sensors
KW  - Scalability
KW  - Transfer learning
KW  - Morphology
KW  - Machine learning
KW  - Manuals
KW  - Diseases
KW  - Sickle Cell Disease
KW  - Machine Learning
KW  - Deep Learning
KW  - Explainable AI
DO  - 10.1109/ICCA62237.2024.10927806
JO  - 2024 International Conference on Computer and Applications (ICCA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Computer and Applications (ICCA)
Y1  - 17-19 Dec. 2024
AB  - Sickle Cell Disease (SCD) is still a serious public health concern, particularly in areas with inadequate access to healthcare. The management of the condition depends on an early and precise diagnosis; yet, traditional diagnostic techniques, such manual blood smear examinations, are frequently laborious, subjective, and prone to variability. The major developments in deep learning (DL) and machine learning (ML) approaches that are redefining SCD detection especially with regard to the application of digital morphology are examined in this review paper. The sickle-shaped red blood cells from microscopic pictures can effectively categorized by utilizing convolutional neural networks (CNNs), transfer learning, and ensemble learning techniques. This method offers better accuracy, speed, and scalability over conventional approaches. These models allow for more accurate assessments of cell morphology in addition to improving the accuracy of SCD diagnosis, lowering the possibility of human error. Additionally, issues like interpretability and data imbalance, offering answers via creative model designs and pre-processing methods. This research demonstrates the revolutionary potential of machine learning and deep learning in the healthcare industry, where intelligent, automated technologies may supplement clinical knowledge and improve outcomes for SCD patients worldwide. By combining these technologies, scalable and easily available diagnostic tools could be developed in the future, leading to more fair healthcare delivery.
ER  - 

TY  - CONF
TI  - Machine Learning and Big Data Analytics in Smart Cities and Healthcare: Applications, Challenges, and Future Directions
T2  - 2024 International Conference on Computer and Applications (ICCA)
SP  - 01
EP  - 06
AU  - V. Pasupuleti
AU  - B. Thuraka
AU  - C. S. Kodete
PY  - 2024
KW  - Data privacy
KW  - Smart cities
KW  - Federated learning
KW  - Scalability
KW  - Precision medicine
KW  - Decision making
KW  - Medical services
KW  - Big Data
KW  - Data models
KW  - Optimization
KW  - Machine Learning
KW  - Big Data Analytics
KW  - Smart Cities
KW  - Healthcare
KW  - Predictive Analytics
DO  - 10.1109/ICCA62237.2024.10928098
JO  - 2024 International Conference on Computer and Applications (ICCA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Computer and Applications (ICCA)
Y1  - 17-19 Dec. 2024
AB  - Machine learning (ML) and big data analytics are playing a pivotal role in transforming both smart cities and healthcare systems by enhancing decision-making, resource optimization, and predictive modeling. In smart cities, these technologies are applied in areas like traffic management, energy optimization, and public safety. In healthcare, ML contributes to predictive diagnostics, personalized medicine, and remote patient monitoring. Despite their potential, these domains face significant challenges, including data privacy concerns, scalability issues, and the interpretability of complex ML models. This paper provides a comprehensive review of the current applications of ML and big data in smart cities and healthcare, identifying shared challenges and exploring the future impact of emerging technologies such as IoT, 5G, and federated learning. Through case studies and analysis, we highlight the key opportunities for interdisciplinary collaboration and discuss how these technologies are shaping the future of urban management and personalized healthcare delivery.
ER  - 

TY  - CONF
TI  - Explainable AI for Fraud Detection: Enhancing Transparency and Trust in Financial Decision-Making
T2  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
SP  - 1
EP  - 6
AU  - R. Kapale
AU  - P. Deshpande
AU  - S. Shukla
AU  - S. Kediya
AU  - Y. Pethe
AU  - S. Metre
PY  - 2024
KW  - Industries
KW  - Explainable AI
KW  - Decision making
KW  - Information security
KW  - Medical services
KW  - Hazards
KW  - Fraud
KW  - Reliability
KW  - Iterative methods
KW  - Guidelines
KW  - Explainable AI (XAI)
KW  - Fraud detection
KW  - Financial decision-making
KW  - Transparency
KW  - Trust
DO  - 10.1109/IDICAIEI61867.2024.10842874
JO  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
Y1  - 29-30 Nov. 2024
AB  - Within the past few a long time, utilizing Counterfeit Insights (AI) to discover extortion in managing an account frameworks has gotten a parcel of consideration since it seem make things more secure and more effective. But when AI models are utilized, particularly in critical areas like keeping money, there has to be a adjust between exactness and openness. This exposition looks at how Logical AI (XAI) can offer assistance unravel these issues by making trick discovery frameworks less demanding to understand. This will lead to more openness and trust within the ways that monetary choices are made. The most reason of this think about is to see into how XAI strategies can offer assistance us get it how complex AI models utilized for trick spotting truly work. By clarifying how these models make choices, XAI not as it were increments openness but too makes a difference individuals who have a stake within the matter, like budgetary controllers, bookkeepers, and conclusion clients, get it why certain choices are made. This information is exceptionally critical for making beyond any doubt that individuals are held dependable and take after the rules set by the Basel Committee on Managing an account Supervision (BCBS) and the Common Information Security Control (GDPR). The think about moreover looks at diverse XAI strategies and how they can be utilized to find fraud. These incorporate model-agnostic approaches such as LIME (Nearby Interpretable Model-agnostic Clarifications) and SHAP (SHapley Added substance Clarifications). These strategies grant you data around how critical highlights are by appearing you which components have the greatest affect on foreseeing extortion. These sorts of findings not as it were offer assistance to test and progress models, but they moreover allow money related educate more data to assist them make superior choices approximately how to handle hazard and halt tricks. This consider moreover talks about the benefits of XAI that go beyond just being able to be deciphered. For illustration, it talks approximately how it can offer assistance construct believe among accomplices. XAI makes a space where subject specialists can work together to check the exactness of AI-made choices and make models superior by closing the hole between AI's ability to foresee the future and human instinctual.
ER  - 

TY  - CONF
TI  - A Survey of Bias and Fairness in Healthcare AI
T2  - 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)
SP  - 642
EP  - 650
AU  - I. D. Mienye
AU  - G. Obaido
AU  - I. D. Emmanuel
AU  - A. A. Ajani
PY  - 2024
KW  - Surveys
KW  - Ethics
KW  - Reviews
KW  - Medical services
KW  - Organizations
KW  - Oral communication
KW  - Learning (artificial intelligence)
KW  - artificial intelligence
KW  - bias
KW  - ethics
KW  - fairness
KW  - healthcare
KW  - machine learning
DO  - 10.1109/ICHI61247.2024.00103
JO  - 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)
Y1  - 3-6 June 2024
AB  - The integration of artificial intelligence (AI) tech-nologies in healthcare has shown great promise in improving patient outcomes. However, there have been concerns about bias and ethical implications. This review explores the key issues of bias, fairness, and ethical considerations in health care AI. The review provides a foundation for future research and policy development in the field of health care AI. It will be beneficial to researchers, policymakers, healthcare providers, and technology developers who are involved in the development of AI systems in healthcare.
ER  - 

TY  - CONF
TI  - AI Doctors in Healthcare: A Comparative Journey through Diagnosis, Treatment, Care, Drug Development, and Health Analysis
T2  - 2024 Sixth International Conference on Computational Intelligence and Communication Technologies (CCICT)
SP  - 485
EP  - 492
AU  - G. Kaur
AU  - H. Singh
AU  - S. Mehta
AU  - Kirti
PY  - 2024
KW  - Drugs
KW  - Accuracy
KW  - Reviews
KW  - Precision medicine
KW  - Medical services
KW  - Transforms
KW  - Natural language processing
KW  - Artificial Intelligence
KW  - Diagnosis
KW  - Precision Medicine
KW  - Clinical Decision Support
KW  - Drug Development
DO  - 10.1109/CCICT62777.2024.00081
JO  - 2024 Sixth International Conference on Computational Intelligence and Communication Technologies (CCICT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Sixth International Conference on Computational Intelligence and Communication Technologies (CCICT)
Y1  - 19-20 April 2024
AB  - The integration of Artificial Intelligence (AI) in healthcare represents a transformative shift, offering unprecedented opportunities to revolutionize diagnostics, treatment, and patient care. It is a paradigm change in the healthcare industry. This study offers a thorough analysis of current developments in AI applications in a range of healthcare fields. Key findings are revealed through a review of groundbreaking research, demonstrating AI’s capacity to improve clinical decision-making, customize therapies, and improve diagnostic accuracy. From using deep learning for drug development to using explainable AI for leukemia diagnosis, each paper clarifies specific approaches and difficulties. This study highlights the enormous potential of AI to enhance patient outcomes and transform healthcare delivery paradigms by looking at its role in treatment optimization, clinical decision support, and healthcare data analysis.
ER  - 

TY  - JOUR
TI  - XAI Unveiled: Revealing the Potential of Explainable AI in Medicine: A Systematic Review
T2  - IEEE Access
SP  - 191498
EP  - 191516
AU  - N. Scarpato
AU  - P. Ferroni
AU  - F. Guadagni
PY  - 2024
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Medical services
KW  - Mathematical models
KW  - Stakeholders
KW  - Prediction algorithms
KW  - Decision making
KW  - Data models
KW  - Analytical models
KW  - Measurement
KW  - Explainability
KW  - artificial intelligence
KW  - interpretability
KW  - medicine
KW  - explainable artificial intelligence
KW  - interpretable artificial intelligence
DO  - 10.1109/ACCESS.2024.3514197
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Nowadays, artificial intelligence in medicine plays a leading role. This necessitates the need to ensure that artificial intelligence systems are not only high-performing but also comprehensible to all stakeholders involved, including doctors, patients, healthcare providers, etc. As a result, the explainability of artificial intelligence systems has become a widely discussed subject in recent times, leading to the publication of numerous approaches and solutions. In this paper, we aimed to provide a systematic review of these approaches in order to analyze their role in making artificial intelligence interpretable for everyone. The conducted review was carried out in accordance with the PRISMA statement. We conducted a BIAS analysis, identifying 87 scientific papers from those retrieved as having a low risk of BIAS. Subsequently, we defined a classification framework based on the classification taxonomy and applied it to analyze these papers. The results show that, although most AI approaches in medicine currently incorporate explainability methods, the evaluation of these systems is not always performed. When evaluation does occur, it is most often focused on improving the system itself rather than assessing users’ perception of the system’s effectiveness. To address these limitations, we propose a framework for evaluating explainability approaches in medicine, aimed at guiding developers in designing effective human-centered methods.
ER  - 

TY  - JOUR
TI  - Knee Osteoarthritis Analysis Using Deep Learning and XAI on X-Rays
T2  - IEEE Access
SP  - 68870
EP  - 68879
AU  - R. Ahmed
AU  - A. S. Imran
PY  - 2024
KW  - X-ray imaging
KW  - Medical services
KW  - Analytical models
KW  - Predictive models
KW  - Osteoarthritis
KW  - Explainable AI
KW  - Visualization
KW  - Deep learning
KW  - Prosthetics
KW  - Knee
KW  - Medical diagnosis
KW  - Explainable artificial intelligence (XAI)
KW  - deep learning
KW  - knee osteoarthritis
KW  - healthcare
KW  - diagnosis
KW  - classification
DO  - 10.1109/ACCESS.2024.3400987
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Knee osteoarthritis (OA) is a chronic disorder mainly arising from age-related factors affecting the knee joints. Its diagnosis is critically important and is usually done by medical practitioners using X-ray images. Although this process is accurate, it is time-consuming. X-ray images have facilitated the use of deep learning (DL) models for the automation of the diagnosis of knee OA, commonly employing convolutional neural network (CNN) based architectures. However, the lack of models’ interpretability makes the results less trustworthy. This work builds on the existing state-of-the-art (SOTA) pre-trained DL models to understand the model’s behavior in classifying highly complex knee OA cases utilizing a divide-and-conquer approach - from multi-class to a binary class for better results interpretability and explainability using explainable artificial intelligence (XAI). Five SOTA fine-tuned DL models are tested on Kellgren-Lawrence (KL) graded X-ray images. Both multi-class and binary-class (using the multiple subsets derived from the original dataset to examine how the models perform with different data combinations) classification approaches and their interpretability of findings using Gradient-weighted Class Activation Mapping (GradCAM) are undertaken in this study. The GradCAM visualization of EfficientNetb7 demonstrates that when the degree of variance between different classes increases, the model’s efficiency in classifying knee OA also increases. Specifically, it becomes more effective at distinguishing normal and severe cases with 99.13% classification accuracy. However, the model’s efficacy drops to 67% for other cases, indicating that it cannot classify knee OA as effectively as doctors.
ER  - 

TY  - CONF
TI  - AI-Driven Predictive Modeling for Accelerated Drug Discovery and Personalized Medicine Development
T2  - 2024 IEEE 1st International Conference on Green Industrial Electronics and Sustainable Technologies (GIEST)
SP  - 1
EP  - 7
AU  - T. N. Kalyani
AU  - M. A. Lakshmi
AU  - V. Yamuna
AU  - M. Rizvana
AU  - V. Shoba
AU  - A. Athiraja
PY  - 2024
KW  - Drugs
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Precision medicine
KW  - Biological system modeling
KW  - Scalability
KW  - Transforms
KW  - Predictive models
KW  - Real-time systems
KW  - Drug discovery
KW  - AI-driven predictive modeling
KW  - Drug discovery
KW  - Personalized medicine
KW  - Machine learning algorithms
KW  - Biomedical datasets
KW  - Patient stratification
DO  - 10.1109/GIEST62955.2024.10959983
JO  - 2024 IEEE 1st International Conference on Green Industrial Electronics and Sustainable Technologies (GIEST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 1st International Conference on Green Industrial Electronics and Sustainable Technologies (GIEST)
Y1  - 25-26 Oct. 2024
AB  - The research investigates how AI-driven predictive modelling can revolutionize personalized medicine and speed up drug discovery. Through the utilization of sophisticated machine learning algorithms and vast biomedical datasets, the research endeavours to expedite the identification of auspicious medication candidates and customize therapies to specific patient profiles. According to the research, AI models considerably improve predictions of drug-target interactions, anticipate side effects, and pinpoint important biomarkers for patient classification. These AI-driven techniques perform better than conventional methods in terms of processing speed and predicted accuracy, which speeds up the process of finding efficient medications and individualized treatment regimens. This research holds the potential to completely transform the pharmaceutical sector by cutting down on the duration and expenses associated with medication development and enhancing patient outcomes via more focused treatments. AI technologies enable a significant improvement in the accuracy and effectiveness of medical treatments; the model shows a 20% increase in accuracy over conventional approaches.
ER  - 

TY  - CONF
TI  - Healthcare Fraud Detection Using Machine Learning
T2  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
SP  - 1119
EP  - 1123
AU  - N. N. Islam Prova
PY  - 2024
KW  - Adaptation models
KW  - Analytical models
KW  - Accuracy
KW  - Stacking
KW  - Medical services
KW  - Forestry
KW  - Predictive models
KW  - Data models
KW  - Fraud
KW  - Tuning
KW  - Healthcare Fraud Detection
KW  - Machine Learning
KW  - Deep Learning
KW  - Isolation Forest
KW  - Random Forest
KW  - SVM
KW  - XGBoost
KW  - Stacking Ensemble
KW  - Neural Network
KW  - Hyperparameter Tuning and Optimization
KW  - Model Interpretability
KW  - Real-Time Fraud Detection
KW  - Automated Model Retraining
DO  - 10.1109/ICoICI62503.2024.10696476
JO  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
Y1  - 28-30 Aug. 2024
AB  - Healthcare fraud in the United States signifies a considerable illicit financial drain with estimations suggesting annual losses amounting to tens of billions of dollars. Such fraudulent activities encompass a wide array of schemes including but not limited to billing for un rendered services, upcoding to receive higher reimbursements, and engaging in unlawful kickback arrangements. Recognizing the criticality of this issue, this research analyzes the utilization of machine learning techniques for the detection of health care fraud. Through an analysis of the dataset which amalgamates inpatient, and outpatient claims data with beneficiary information across 558,211 records spanning various dimensions, this study illustrates the application of several ML models including Random Forest, XGBoost, SVM, Isolation Forest, a Deep Learning Model,and a Stacking Ensemble approach. The models are evaluated based on their accuracy, precision, recall, Fl score, and ROC AU Cscore with a particular focus on their applicability to healthcare fraud detection. Among the models evaluated, the Stacking Ensemble Model emerged as particularly efficacious, achieving an accuracy of 92.79% and an exceptional ROC AUC score of 96.95%. Incorporating hyperparameter tuning, this study further enhances interpretability and decision-making through SHAP value analysis, offering deep insights into model predictions and feature importances. Additionally, it introduces an innovative real-time health care fraud detection pipeline and an automated model retraining framework, ensuring the system remains effective against evolving fraud tactics by continuously adapting and improving.
ER  - 

TY  - CONF
TI  - Tumor Detection Using Deep Learning and Explainable Artificial Intelligence
T2  - 2024 International Conference on Computational Intelligence and Network Systems (CINS)
SP  - 1
EP  - 7
AU  - S. D. Mehra
AU  - S. Aswani
AU  - S. D. Shetty
PY  - 2024
KW  - Deep learning
KW  - Visualization
KW  - Lungs
KW  - Medical services
KW  - Brain modeling
KW  - Artificial intelligence
KW  - Biomedical imaging
KW  - Residual neural networks
KW  - Network systems
KW  - Image classification
KW  - Brain Tumor
KW  - Lung tumor
KW  - Tumor Detection
KW  - ResNet50
KW  - Saliency Maps
KW  - Integrated Gradients
DO  - 10.1109/CINS63881.2024.10864458
JO  - 2024 International Conference on Computational Intelligence and Network Systems (CINS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Computational Intelligence and Network Systems (CINS)
Y1  - 28-29 Nov. 2024
AB  - Deep learning methods in artificial intelligence (AI) have transformed the automated detection of tumors in vital organs due to their exceptional image classification capabilities. However, their lack of transparency frequently impedes the establishment of clinical trust and the widespread adoption of these models. This study presents a deep learning methodology for automatically classifying and detecting brain and lung tumors using a medical imaging dataset. In order to tackle the inherent opacity of deep learning models and improve its interpretability, two XAI(explainable AI) techniques: Integrated Gradients and Saliency Maps are used in this study. The latest Convolutional Neural Network(CNN) architecture, ResNet50, is used in this study to provide a thorough method for identifying lung and brain tumors from medical imaging data. Additionally saliency maps and integrated gradients provide the visual depiction of the model's focus by highlighting the areas in images that have the highest impact on the model's classification decision. This approach not only attains high accuracy in tumor detection but also provides the interpretability and transparency of black-box AI models, resulting in increased trust in healthcare providers. Future work will investigate the adoption of other categories of XAI techniques and expand this framework to include other forms of medical imaging data, thereby emphasizing the signifi-cance of AI in healthcare.
ER  - 

TY  - CONF
TI  - An Explainability-Enhanced Retrieval System for the Healthcare Sector
T2  - 2024 5th IEEE Global Conference for Advancement in Technology (GCAT)
SP  - 1
EP  - 6
AU  - R. R. K. Menon
AU  - G. Ashok
AU  - L. Reghu
PY  - 2024
KW  - Measurement
KW  - Ethics
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Information retrieval
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - XAI
KW  - XIR
KW  - BEIR
KW  - Learning to Rank
KW  - Knowledge Graph
KW  - LIME
KW  - SHAP
DO  - 10.1109/GCAT62922.2024.10923902
JO  - 2024 5th IEEE Global Conference for Advancement in Technology (GCAT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 5th IEEE Global Conference for Advancement in Technology (GCAT)
Y1  - 4-6 Oct. 2024
AB  - Explainable Artificial Intelligence, or XAI, is gaining traction in the machine learning space because it makes AI systems easier to design and operate, boosts public trust, and makes outcomes easier to analyse. The purpose of the emerging field of XAI is to make AI models more visible and understandable. Even though explainability is being widely used in most of the black-box models these days, research is still in the development phase when it comes to introducing XAI in the retrieval systems. So, Healthcare-based retrieval systems using XAI has been concentrated in this work. XAI may provide users with an explanation of the retrieval of specific documents when they query healthcare-related materials. Patients can more easily comprehend and have faith in the advise provided when they use XAI technologies. To ensure that the logic underlying AI predictions is understandable and transparent, explainable AI models are crucial in the healthcare industry for clarifying the predictions made by medical diagnostic systems. We provide an Information Retrieval model that ranks the search results according to a relevant dimension that takes information truthfulness into consideration. To enhance the interpretability of the recovered findings and give users a justification for the scientific evidence that support the claims mentioned the retrieved documents, the system incorporates a retrieval model that utilises the explainability technique.
ER  - 

TY  - CONF
TI  - XSH-ParK: XAI-based Parkinson Disease Diagnosis Framework For Smart Healthcare Using MRI Images
T2  - GLOBECOM 2024 - 2024 IEEE Global Communications Conference
SP  - 3521
EP  - 3526
AU  - S. Vaghasiya
AU  - F. Ramoliya
AU  - R. Gupta
AU  - S. Tanwar
AU  - J. J. P. C. Rodrigues
AU  - I. Woungang
PY  - 2024
KW  - Neurological diseases
KW  - Measurement
KW  - Accuracy
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Predictive models
KW  - Medical diagnosis
KW  - Global communication
KW  - Medical diagnostic imaging
KW  - Residual neural networks
KW  - XAI
KW  - Parkinson’s Disease
KW  - AI
KW  - Interpretable AI
KW  - Explainable AI
DO  - 10.1109/GLOBECOM52923.2024.10901665
JO  - GLOBECOM 2024 - 2024 IEEE Global Communications Conference
IS  - 
SN  - 2576-6813
VO  - 
VL  - 
JA  - GLOBECOM 2024 - 2024 IEEE Global Communications Conference
Y1  - 8-12 Dec. 2024
AB  - Parkinson’s disease (PD) is a neurodegenerative disease which is the second most common neurological disease. Early diagnosis of PD poses significant challenges as in earlier stages of PD, symptoms can’t be clinically recognized. This paper presents a framework called XSH-ParK with integrated deep learning (DL) models and XAI techniques to assist in early PD diagnosis using MRI scans. Pre-trained models VGG16, InceptionV3, ResNet50 and a custom CNN are used to analyze the NTUA dataset, which consists of MRI scans of 78 individuals. Through rigorous evaluation considering accuracy, precision, recall, and F1-score metrics, it is evident that the fine-tuned VGG16 model achieves the highest efficiency with an accuracy rate of 97.56% in the XSH-ParK framework. Additionally, LIME and integrated gradient are the XAI methods used on the top-performing VGG16 model to provide transparent and interpretable diagnostic insights, Enabling healthcare professionals to understand the reasons behind the models’ decisions.
ER  - 

TY  - CONF
TI  - AI Models for Predicting Drug Efficacy and Toxicity to Accelerate Personalized Medicine and Drug Discovery
T2  - 2024 4th International Conference on Soft Computing for Security Applications (ICSCSA)
SP  - 283
EP  - 290
AU  - N. Arun
AU  - K. Ravichandran
PY  - 2024
KW  - Drugs
KW  - Ethics
KW  - Toxicology
KW  - Precision medicine
KW  - Organoids
KW  - Data integrity
KW  - Predictive models
KW  - Drug discovery
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Organoid Intelligence (OI)
KW  - Brain Organoids
KW  - Artificial Intelligence (AI)
KW  - Neurodevelopment
KW  - Biocomputing
KW  - Neurological Disorders
DO  - 10.1109/ICSCSA64454.2024.00051
JO  - 2024 4th International Conference on Soft Computing for Security Applications (ICSCSA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Soft Computing for Security Applications (ICSCSA)
Y1  - 24-25 Sept. 2024
AB  - Today, Artificial Intelligence (AI) is ushering in early drug discovery and precision medicine due to its abilities to increase the efficiencies of the drug process. Conventional approaches to drug development are time-consuming and time which is often more than ten years is required to get a new drug to patients. The existing landscape of drug discovery has been revolutionized by the application of AI, ML, as well as big data analytics. With help of AI models like the one from the CUNY Graduate Centre providing an opportunity to analyze large data sets in order to identify potential drugs, as well as predict the effectiveness and toxicity of such drugs, absence of high–throughput virtual screening substantially amplifies the costs and time required for the development of new drugs. These models mimic the effects of drugs on biological targets meaning that the general success rates in clinical trials are improved. In addition, AI leads to the enhancement of patients care through the customization of drugs and dosage forms according to the patients’ genes, lifestyle and environment. The complex genetic data are analyzed by the AI systems and biomarkers of patients’ reaction to the medications are determined, so the personalized approach to the treatment can be introduced. AI also anticipates and prevents drug toxicity and interactions to reduce bad effects on the patient’s health. The study highlights the critical factors influencing the integration of AI in drug discovery: Concerning, Data Quality and Quantity, Integration with Existing Systems, Algorithmic Transparency and Interpretability, and Regulation and Ethical Issues, Computational Power, and Interdisciplinary Approach and Collaboration. Results derived from ANOVA, Chi-Square, and One-Sample t-tests accentuate the equal significance of these factors the need to go further with the process of enhancement. Thus, the study states that the importance of these factors are recognized but unique approaches are need for the use of AI in precision medicine.
ER  - 

TY  - CONF
TI  - Risks and Challenges of Using Ai In Healthcare
T2  - 2024 12th International Scientific Conference on Computer Science (COMSCI)
SP  - 1
EP  - 4
AU  - V. Samokisheva
AU  - R. Trifonov
AU  - G. Pavlova
PY  - 2024
KW  - Training
KW  - Ethics
KW  - Electric potential
KW  - Data privacy
KW  - Automation
KW  - Medical services
KW  - Safety
KW  - Stakeholders
KW  - Risk management
KW  - Artificial intelligence
KW  - Artificial Intelligence (AI)
KW  - Healthcare
KW  - Safety Concerns
KW  - Ethical Issues
KW  - Health Information Technology
DO  - 10.1109/COMSCI63166.2024.10778509
JO  - 2024 12th International Scientific Conference on Computer Science (COMSCI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 12th International Scientific Conference on Computer Science (COMSCI)
Y1  - 13-15 Sept. 2024
AB  - Artificial intelligence (AI) is increasingly utilized in analyzing complex and large-scale data for producing results without intervention of human in various healthcare contexts, such as genomics, bioinformatics, and analysis of image. Given technology give opportunities that are significant in diagnosis, treatment, imaging, analysis of the risk, management of the lifestyle, management of health information, and health assistance. AI is an topic that is used increasingly in many fields, more usually in medicine, which leads to the transformation of the diagnostic system, to make better the development of new drugs, to make the quality improvements for medical services and patient care in general and to reduce costs. Every day stories trend in the media about AI applications and their huge potential for medical practice revolutionizing. However, no matter of the big promises for electronic health records in the early 21st century, the excitement around AI has sometimes leads to focused view of its capabilities while ignoring challenges of technology, and safety of humans and ethical concerns which must be really considered.While AI presents significant opportunities, it also introduces challenges and pitfalls, particularly concerning safety and ethical issues. This article reviews AI in healthcare, emphasizing its implications for safety and ethics. Key strategies for ensuring safer AI technology include safest design, safety reserves, mechanisms of safe fail, and procedural safeguards, while recognizing the importance of addressing cost, risk, and uncertainty. Additionally, the article identifies three critical ethical issues: transparency and privacy, dynamic information, consent, ownership, discrimination. The article suggests that protocols should be established and shared with all stakeholders to mitigate these challenges and clear guidance should be given.
ER  - 

TY  - CONF
TI  - Exploiting Deep Convolutional Neural Networks and Gated Recurrent Units for Superior Lung Anomaly Detection and Classification with AI Transparency
T2  - 2024 International Conference on Data Science and Network Security (ICDSNS)
SP  - 1
EP  - 6
AU  - R. Raman
AU  - V. Kumar
AU  - B. G. Pillai
AU  - A. Verma
AU  - S. Rastogi
AU  - R. Meenakshi
PY  - 2024
KW  - Accuracy
KW  - Decision making
KW  - Lung
KW  - Logic gates
KW  - Convolutional neural networks
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - deep convolutional neural networks (cnn)
KW  - gated recurrent units (gru)
KW  - lung anomaly detection
KW  - ai transparency
KW  - medical image classification
DO  - 10.1109/ICDSNS62112.2024.10691165
JO  - 2024 International Conference on Data Science and Network Security (ICDSNS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Data Science and Network Security (ICDSNS)
Y1  - 26-27 July 2024
AB  - The rapid evolution of artificial intelligence (AI) technologies provides unprecedented opportunities for enhancing diagnostic accuracy in medical imaging, particularly in the detection and classification of lung anomalies. This study introduces a novel hybrid approach that integrates Deep Convolutional Neural Networks (CNNs) and Gated Recurrent Units (GRUs) to exploit both spatial and temporal data dimensions, significantly improving the precision of anomaly detection and classification. By emphasizing AI transparency, the methodology also develops a framework to elucidate the decision-making processes of the models. Our comprehensive dataset of lung images, processed and augmented for effective training, forms the basis of our research. Preliminary results reveal a significant improvement in detection and classification accuracy, achieving an accuracy of 94%, precision of 92%, recall of 93%, and an Fl score of 92.5%, compared to traditional methods. These results not only validate the effectiveness of combining CNNs with GRUs but also demonstrate the model's capability to offer more interpretable and trustworthy AI diagnostics. This research underscores the potential of advanced AI methodologies to revolutionize lung anomaly diagnostics and emphasizes the critical need for transparency in AI-driven medical tools, ensuring their reliability and clinical usability.
ER  - 

TY  - CONF
TI  - Addressing the Spread of Infectious Diseases in the Era of Explainable AI
T2  - 2024 World Conference on Complex Systems (WCCS)
SP  - 1
EP  - 6
AU  - B. Addaali
AU  - R. Latif
AU  - A. Saddik
PY  - 2024
KW  - Explainable AI
KW  - Pandemics
KW  - Infectious diseases
KW  - Reviews
KW  - Prediction algorithms
KW  - Vaccines
KW  - History
KW  - Artificial intelligence
KW  - Epidemiology
KW  - Monitoring
KW  - infectious diseases
KW  - epidemics
KW  - AI
KW  - machine learning
KW  - deep learning
KW  - explainable AI
DO  - 10.1109/WCCS62745.2024.10765539
JO  - 2024 World Conference on Complex Systems (WCCS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 World Conference on Complex Systems (WCCS)
Y1  - 11-14 Nov. 2024
AB  - Artificial intelligence technology (AI) has significantly advanced the field of epidemiology. During COVID-19 pandemic, multiple researches have employed machine learning (ML) and deep learning (DL) in the early detection, monitoring, future outbreaks prediction and drugs and vaccines development. But the real implementation of AI in healthcare institutions still very limited except for few applications such as screening body temperature using thermal cameras at the entrances of public places. The reason behind this limitation is the “black-box” nature of AI algorithms. Explainable AI (XAI) addresses this issue by explaining the decisions made by the algorithm. In this review, we introduce mathematical epidemiology and summarize the history and subfields of AI in addition to its applications in fighting pandemics. Furthermore, we highlight the added value of XAI in predicting and managing infectious diseases.
ER  - 

TY  - CONF
TI  - Interpretable Deep Learning Approaches for Reliable GI Image Classification: A Study with the HyperKvasir Dataset
T2  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
SP  - 1666
EP  - 1671
AU  - S. B. Wahid
AU  - Z. Tasnim Rothy
AU  - R. K. News
AU  - S. Anwar Rieyan
PY  - 2024
KW  - Accuracy
KW  - Explainable AI
KW  - Boosting
KW  - Prediction algorithms
KW  - Gastrointestinal tract
KW  - Reliability
KW  - Medical diagnosis
KW  - Medical diagnostic imaging
KW  - Diseases
KW  - Residual neural networks
KW  - Deep Learning
KW  - Boosting Algorithms
KW  - Gastrointestinal Diseases
KW  - Explainable AI
DO  - 10.1109/ICCIT64611.2024.11022387
JO  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 2474-9656
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
Y1  - 20-22 Dec. 2024
AB  - Deep learning has emerged as a promising tool for automating gastrointestinal (GI) disease diagnosis. However, multi-class GI disease classification remains underexplored. This study addresses this gap by presenting a framework that uses advanced models like InceptionNetV3 and ResNet50, combined with boosting algorithms (XGB, LGBM), to classify lower GI abnormalities. InceptionNetV3 with XGB achieved the best recall of 0.81 and an F1 score of 0.90. To assist clinicians in understanding model decisions, the Grad-CAM technique, a form of explainable AI, was employed to highlight the critical regions influencing predictions, fostering trust in these systems. This approach significantly improves both the accuracy and reliability of GI disease diagnosis.
ER  - 

TY  - CONF
TI  - Evaluating the Effectiveness of Feature Selection and Explainable AI in Predicting Acute Myocardial Infarction Using Machine Learning Models
T2  - 2024 5th International Conference on Machine Learning and Computer Application (ICMLCA)
SP  - 6
EP  - 10
AU  - Y. Wang
AU  - J. Shen
AU  - S. Li
AU  - X. Li
AU  - G. U. Nneji
AU  - H. N. Monday
PY  - 2024
KW  - Support vector machines
KW  - Ethics
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Medical services
KW  - Myocardium
KW  - Predictive models
KW  - Feature extraction
KW  - Medical diagnostic imaging
KW  - Myocardial Infarction
KW  - PMIA
KW  - SHAP
KW  - Machine Learning
KW  - Borderline-SMOTE
KW  - explainability
KW  - SHAP value
DO  - 10.1109/ICMLCA63499.2024.10753731
JO  - 2024 5th International Conference on Machine Learning and Computer Application (ICMLCA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 5th International Conference on Machine Learning and Computer Application (ICMLCA)
Y1  - 18-20 Oct. 2024
AB  - Acute myocardial Infarctions (MIs), commonly known as heart attacks, are critical medical emergencies that often lead to severe complications, including post-myocardial infarction angina (PMIA). Early and accurate prediction of PMIA is crucial for timely intervention and improved patient outcomes. This study focuses on predicting PMIA using machine learning classifiers on the UCI myocardial infarction complication datasets, which includes 1700 patient samples with 111 clinical features. The dataset is balanced using Borderline Synthetic Minority Oversampling (Borderline-SMOTE) and standardized. Twelve classifiers are employed and the Shapley Additive exPlanations (SHAP) are used to interpret the best-performing models, revealing the decision-making processes. The SVM classifier achieved the highest cross-validated accuracy of 98.55%, demonstrating superior predictive capability. SHAP values provided insights into the importance of each feature, enhancing model interpretability and transparency. This approach highlights the significance of feature selection and explain ability in building trustworthy AI models for healthcare. The findings contribute to improved diagnosis and management of PMIA, underscoring the ethical adoption of AI in clinical settings.
ER  - 

TY  - CONF
TI  - Deep Learning Algorithms for Enhanced Precision in Automated Medical Imaging Diagnoses to Improve Healthcare Outcomes
T2  - 2024 10th International Conference on Communication and Signal Processing (ICCSP)
SP  - 703
EP  - 707
AU  - P. Neelaveni
AU  - K. Seethalakshmi
AU  - N. V. Rao
AU  - M. Prabhu
AU  - K. V. Kanimozhi
AU  - B. Arunagiri
PY  - 2024
KW  - Deep learning
KW  - Training
KW  - Analytical models
KW  - Technological innovation
KW  - Machine learning algorithms
KW  - Ultrasonic imaging
KW  - Heuristic algorithms
KW  - Deep Learning
KW  - Medical Imaging
KW  - Automated Diagnoses
KW  - Healthcare Outcomes
DO  - 10.1109/ICCSP60870.2024.10543493
JO  - 2024 10th International Conference on Communication and Signal Processing (ICCSP)
IS  - 
SN  - 2836-1873
VO  - 
VL  - 
JA  - 2024 10th International Conference on Communication and Signal Processing (ICCSP)
Y1  - 12-14 April 2024
AB  - Over the last few years of when deep learning systems have been introduced into medical imaging procedures they are uncorrupted. They should be commended as the best ways of revolutionizing diagnosis processes and improving healthcare outcomes. This report focuses on detailed examination of deep learning for application to automated medical diagnostic, which is one of the most cutting-edge technology at present. Diverse input was the is collected by different kinds of modalities like X-ray, CT scan, MRI, and ultrasound. Also, this input modality is preprocessed to train and validate the proposed deep learning model. The network's architecture was appositely chosen, and the training strategy worked sense prep, which was made to improve the achieved success, included to the method of tuning and cross-validity was used. The qualitative analysis of accuracy, sensitivity and specificity was very good, which is a proof for a better performance of the novel method compared to the baseline ones. Furthermore, the qualitative data analysis and interpretability led to fundamental discussions on the model's performance and the decision-making procedure. Moreover, the model is of good generalizability, maintaining high accuracy on different datasets of validation group, therefore, it is predictive for clinical uses. Overall, this paper confirms wide application of deep learning methods in the field of machine learning-based medical diagnosis and highlights need of further research and development in the area of high tech ones which at present are highly dynamic by nature.
ER  - 

TY  - CONF
TI  - Ethics of Artificial Intelligence: challenges, opportunities and future prospects
T2  - 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 5860
EP  - 5867
AU  - F. R. Falvo
AU  - M. Cannataro
PY  - 2024
KW  - Ethics
KW  - Technological innovation
KW  - Explainable AI
KW  - Focusing
KW  - Europe
KW  - Medical services
KW  - Regulation
KW  - Security
KW  - Artificial intelligence
KW  - Guidelines
KW  - ethics
KW  - transparency
KW  - responsibility
KW  - XAI
DO  - 10.1109/BIBM62325.2024.10822112
JO  - 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 2156-1133
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 3-6 Dec. 2024
AB  - Artificial Intelligence (AI) has rapidly transformed numerous sectors, including healthcare, justice, and commerce, providing substantial benefits while also raising complex ethical questions: this article explores the main ethical challenges associated with AI, focusing on issues such as algorithmic bias, data privacy and security, transparency, and accountability. The importance of Explainable Artificial Intelligence (XAI) in enhancing the interpretability of algorithmic decisions is emphasized, particularly in healthcare, where model opacity can have a direct impact on patient outcomes. The paper further examines regulatory frameworks and ethical guidelines, including the European Union’s AI Act, advocating for a multidisciplinary approach that combines innovation and accountability to develop AI systems that respect human rights and foster user trust. In conclusion, the article underscores the need for interdisciplinary collaboration and adaptable regulations to ensure the ethical development of AI, promoting fairness and transparency.
ER  - 

