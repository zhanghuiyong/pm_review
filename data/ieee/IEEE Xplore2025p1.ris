TY  - CONF
TI  - Enhancing Explainability and Transparency in Deep Learning Models for Healthcare Applications
T2  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
SP  - 1
EP  - 5
AU  - V. Indhumathi
AU  - V. Sankaradass
AU  - V. K. M. Manish
AU  - K. Jagadeesh
PY  - 2025
KW  - Deep learning
KW  - Training
KW  - Hands
KW  - Accuracy
KW  - Explainable AI
KW  - Statistical analysis
KW  - MIMICs
KW  - Medical services
KW  - Predictive models
KW  - Medical diagnosis
KW  - Deep Learning
KW  - Explainability
KW  - Transparency
KW  - Healthcare
KW  - Interpretability
KW  - Explainable AI (XAI)
DO  - 10.1109/ICDSAAI65575.2025.11011799
JO  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
Y1  - 28-29 March 2025
AB  - Recently, it has been observed that deep learning models can be very useful in handling healthcare facilities mostly medical diagnosis and management. However, they are not interpretable and transparent enough to be accepted and used in the clinical environments where the matching reasoning is important. This paper proposes a new strategy to improve explainability of deep learning approaches when used in healthcare data set. We introduce an IDNN design with explainable layers and postdoc approaches like SHAP and LIME for visualization and explanation of model predictions. Our framework is applied to multiple healthcare datasets such as MIMICIII and ChestXray14 where we will compare the traditional method with the proposed method by training accuracy and explainability evaluation metrics. Results indicate that this causes minimal compromise in accuracy on the other hand, other explainability measures of model fidelity, and comprehensibility reveal significant gains, signifying the potential of this method in advancing the use of AI solutions in healthcare contexts.
ER  - 

TY  - CONF
TI  - Exploring the Role of XAI in Enhancing Predictive Model Transparency in Healthcare Risk Assessment
T2  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
SP  - 1
EP  - 5
AU  - R. Mandava
AU  - S. S. Vellela
AU  - N. Malathi
AU  - K. Haritha
AU  - S. Gorintla
AU  - L. Dalavai
PY  - 2025
KW  - Ethics
KW  - Explainable AI
KW  - Computational modeling
KW  - Decision making
KW  - Closed box
KW  - Medical services
KW  - Predictive models
KW  - Safety
KW  - Risk management
KW  - Standards
KW  - Explainable AI (XAI)
KW  - healthcare
KW  - predictive modeling
KW  - risk assessment
KW  - clinician trust
KW  - patient safety
KW  - transparency
KW  - interpretability
KW  - bias detection
KW  - regulatory compliance
KW  - ethical AI
KW  - human-AI collaboration
DO  - 10.1109/ICCRTEE64519.2025.11052988
JO  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Computational Robotics, Testing and Engineering Evaluation (ICCRTEE)
Y1  - 28-30 May 2025
AB  - The increasing integration of Artificial Intelligence (AI) in healthcare demands systems that not only predict accurately but also explain their decisions transparently. This study explores the role of Explainable Artificial Intelligence (XAI) in enhancing transparency, interpretability, and trust in AI-driven predictive models used for healthcare risk assessment. It evaluates XAI techniques like SHAP and LIME in explaining black-box models such as Random Forest and Neural Networks, focusing on clinician trust, ethical adoption, and patient safety. Utilizing real-world healthcare datasets, the study investigates how interpretable models can bridge the gap between AI outputs and clinical decision-making. Results demonstrate that XAI significantly improves clinician confidence and regulatory compliance without greatly compromising prediction performance. The research underscores the transformative potential of XAI in fostering responsible and effective AI deployment in clinical environments.
ER  - 

TY  - CONF
TI  - AI-Driven Transcriptomics: Advancing Gene Expression Analysis and Precision Medicine
T2  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
SP  - 1
EP  - 5
AU  - F. Aburub
AU  - F. Al-Akayleh
AU  - R. A. Abdel-Rahem
AU  - M. Al-Remawi
AU  - A. S. A. A. Agha
PY  - 2025
KW  - Deep learning
KW  - Technological innovation
KW  - Translation
KW  - Precision medicine
KW  - RNA
KW  - Transcriptomics
KW  - Biomarkers
KW  - Gene expression
KW  - Artificial intelligence
KW  - Diseases
KW  - Transcriptomics
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Gene Expression
KW  - Precision Medicine
DO  - 10.1109/ICCIAA65327.2025.11013076
JO  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
Y1  - 28-30 April 2025
AB  - Transcriptomics, the comprehensive study of RNA transcripts, has transformed our understanding of cellular functions and disease mechanisms by capturing real-time gene expression profiles. However, the sheer scale and complexity of RNA sequencing (RNA-Seq) data-particularly in single-cell experiments-pose significant computational and interpretive challenges. Artificial intelligence (AI), which encompasses machine learning (ML) and deep learning (DL), has emerged as a key driver in handling these challenges, offering data-driven solutions that enhance expression quantification, functional annotation, and clinical translation. By integrating multi-omics datasets, clinical records, and large-scale transcriptomic outputs, AI-powered approaches can detect subtle expression patterns, identify novel biomarkers, and inform patient-specific therapeutic strategies. This article highlights current AI applications in transcriptomics, discusses technical and ethical considerations, and outlines future directions for applying AI in precision medicine. Ultimately, AI has the potential to streamline transcriptomic analysis, refine disease characterization, and improve patient outcomes through more targeted interventions.
ER  - 

TY  - CONF
TI  - Harnessing AI Algorithms for Accurate Medical Diagnosis from Electronic Health Record
T2  - 2025 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
SP  - 1
EP  - 5
AU  - Jyoti
AU  - Nishant
AU  - T. Aggarwal
AU  - Sumit
PY  - 2025
KW  - Deep learning
KW  - Data privacy
KW  - Accuracy
KW  - Decision making
KW  - Predictive models
KW  - Natural language processing
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - Electronic medical records
KW  - Diseases
KW  - Intelligence
KW  - Electronic Health Records
KW  - Medical Diagnosis
KW  - Machine Learning
KW  - Deep Learning
KW  - Predictive Modeling
KW  - Natural Language Processing
DO  - 10.1109/IATMSI64286.2025.10985432
JO  - 2025 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
IS  - 
SN  - 
VO  - 3
VL  - 3
JA  - 2025 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
Y1  - 6-8 March 2025
AB  - The integration of Artificial Intelligence into health care systems has really transformed the analyses and interpretations that medical professionals conduct with regards to Electronic Health Records. EHRs are full of patient data, which, when effectively used, may help enhance diagnostic accuracy and treatment outcomes. The current paper discusses the role of AI algorithms in processing and analyzing EHR data to enable accurate and timely medical diagnosis. Specifically, we look into various techniques and methods in machine learning and deep learning, including natural language processing for unstructured clinical notes and predictive modeling for disease detection. Our approach ultimately develops towards improving the accuracy of diagnosis by finding patterns or correlations within complex datasets, reducing rates of misdiagnosis, and most importantly ensuring that clinicians’ decision-making abilities would be bettered. We also discuss some challenges surrounding data quality, privacy issues, and the requirement for explainable AI to be operated in clinical environments where transparency is the key. We are going to be able to demonstrate how this system can help revolutionize diagnostics because of its automated nature, really improving patient care and healthcare workflows in the process.
ER  - 

TY  - CONF
TI  - Integrating Fuzzy Logic with Deep Learning: A New Approach to Explainable Artificial Intelligence
T2  - 2025 6th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI)
SP  - 1701
EP  - 1706
AU  - R. Imamguluyev
PY  - 2025
KW  - Deep learning
KW  - Fuzzy logic
KW  - Uncertainty
KW  - Explainable AI
KW  - Autonomous systems
KW  - Decision making
KW  - Finance
KW  - Medical services
KW  - Cognition
KW  - Reliability
KW  - Explainable artificial intelligence (AI)
KW  - Fuzzy logic
KW  - Deep learning
KW  - Interpretability
KW  - Model transparency
KW  - Hybrid approach
DO  - 10.1109/ICMCSI64620.2025.10883618
JO  - 2025 6th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 6th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI)
Y1  - 7-8 Jan. 2025
AB  - The rapid progress in artificial intelligence (AI) has led to the development of robust deep learning models, yet their black box nature presents major challenges regarding interpretability and transparency. This article presents a novel method for explainable artificial intelligence (XAI) that merges Fuzzy Logic with deep learning. Fuzzy logic is renowned for its capability to manage uncertainty and facilitate approximate reasoning, providing a clear framework that enhances the decision-making capabilities of deep learning models. By integrating systems grounded in fuzzy rules, our goal is to boost the interpretability of deep learning models while maintaining their predictive performance. The proposed strategy connects human-understandable logic with the intricate calculations of neural networks, shedding light on the internal mechanisms of artificial intelligence systems. We validate the effectiveness of this combined approach through various case studies and experiments, showcasing enhanced transparency and reliability of the models. This fusion of Fuzzy Logic with deep learning adds to the expanding domain of XAI and holds promise for broader applications where explainability is crucial, such as in healthcare, finance, and autonomous systems.
ER  - 

TY  - CONF
TI  - Applications of Artificial Intelligence in Biomedicine and Healthcare: Improving Efficiency, Decision-Making, and Tailored Patient Care
T2  - 2025 International Conference on Emerging Smart Computing and Informatics (ESCI)
SP  - 1
EP  - 6
AU  - S. R. Kumar
AU  - S. K. Vuppala
AU  - B. S. R
AU  - K. Thilagam
AU  - R. Y P
AU  - S. S
PY  - 2025
KW  - Deep learning
KW  - Accuracy
KW  - Precision medicine
KW  - Decision making
KW  - Medical services
KW  - Natural language processing
KW  - Drug discovery
KW  - Artificial intelligence
KW  - Predictive analytics
KW  - Medical diagnostic imaging
KW  - Medical Diagnostics
KW  - Healthcare Decision-Making
KW  - Predictive Analytics
KW  - Personalized Medicine
KW  - Precision Medicine
KW  - Clinical Decision Support Systems (CDSS)
KW  - Healthcare Efficiency
DO  - 10.1109/ESCI63694.2025.10988342
JO  - 2025 International Conference on Emerging Smart Computing and Informatics (ESCI)
IS  - 
SN  - 2996-1815
VO  - 
VL  - 
JA  - 2025 International Conference on Emerging Smart Computing and Informatics (ESCI)
Y1  - 5-7 March 2025
AB  - Artificial Intelligence (AI) in biology and healthcare could change the business by boosting efficiency, decision-making, and patient care. AI technologies including machine learning, deep learning, and natural language processing are being used in healthcare diagnosis, treatment, drug discovery, and administration. As AI analyzes enormous amounts of complicated medical data, finds patterns, and predicts, diagnostic accuracy and clinical decision-making have increased, enabling early identification of cancer, cardiovascular, and neurological diseases. In addition to diagnostic applications, AI-driven systems are improving predictive analytics, helping healthcare providers forecast patient outcomes, optimize treatment plans, and reduce medical errors. Another area where AI is helping is personalized medicine and tailored patient care. AI systems analyze genetic, environmental, and lifestyle factors to design treatment regimens, increasing patient results and lowering side effects. AI applications in drug research and development have also sped the discovery of new medicinal compounds and optimized clinical trial designs for better results and faster approval. AI in healthcare faces data privacy problems, regulatory difficulties, and the requirement for robeust training datasets to maintain system accuracy, despite significant gains. AI decision-making in medicine requires careful ethical concerns, notably transparency and accountability. AI technology integration into healthcare systems will improve patient experiences, results, efficiency, cost, and equity. This study examines AI's uses in biomedicine and healthcare, including successes, problems, and future directions.
ER  - 

TY  - CONF
TI  - Revolutionizing Healthcare: The Role of AI and BigData Analytics in Disease Prevention and Management
T2  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
SP  - 321
EP  - 324
AU  - V. Gatkine
AU  - S. N. Khode
AU  - P. Gourshettiwar
PY  - 2025
KW  - Deep learning
KW  - Technological innovation
KW  - Data privacy
KW  - Sentiment analysis
KW  - Scalability
KW  - Medical services
KW  - Big Data
KW  - Resource management
KW  - Artificial intelligence
KW  - Diseases
KW  - Machine learning
KW  - Big Data Analytics
KW  - Healthcare Innovations
KW  - Artificial Intelligence
DO  - 10.1109/ICSADL65848.2025.10933003
JO  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
Y1  - 18-20 Feb. 2025
AB  - Artificial Intelligence (AI) and Big Data Analytics are greatly transforming the healthcare scenario by facilitating the early detection of diseases, personalized medicine, and improved public health interventions. AI, by employing machine learning and deep learning, processes big and complex datasets, whereas Big Data detects patterns and predicts risks, enhancing resource utilization and readiness for health emergencies. Yet, the technologies are faced with a range of challenges that include data security concerns, matters of integrating many systems, and the requirement of real-time processing in ever-changing healthcare environments. Although recent technological advancements such as sophisticated deep learning algorithms and decentralized data processing have been promising solutions to some of these challenges, issues of easy integration and privacy remain. This paper suggests an objective-oriented framework to enhance the application of AI and Big Data solutions in healthcare to overcome current limitations and establish a more efficient, personalized, and predictive healthcare system.
ER  - 

TY  - CONF
TI  - A Systematic Review on Incorporation of Artificial Intelligence in Precision Healthcare
T2  - 2025 International Conference on Networks and Cryptology (NETCRYPT)
SP  - 903
EP  - 907
AU  - K. Dwivedi
AU  - B. Chugh
AU  - J. Chaudhary
AU  - D. C. Joshi
AU  - P. Bhatt
PY  - 2025
KW  - Ethics
KW  - Federated learning
KW  - Precision medicine
KW  - Genomics
KW  - Collaboration
KW  - Medical services
KW  - Artificial intelligence
KW  - Bioinformatics
KW  - Medical diagnostic imaging
KW  - Systematic literature review
KW  - Medical Imaging
KW  - Genomic Data
KW  - Multi-Modal AI Systems
KW  - Explainable AI
KW  - Federated Learning
DO  - 10.1109/NETCRYPT65877.2025.11102485
JO  - 2025 International Conference on Networks and Cryptology (NETCRYPT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Networks and Cryptology (NETCRYPT)
Y1  - 29-31 May 2025
AB  - Precision medicine is transforming healthcare in the future owing to artificial intelligence (AI), enabling treatment to be personalized in line with a person's genetic, environmental, and lifestyle characteristics. In this article, AI's potential to revolutionize clinical decision-making, hasten drug discovery, and boost diagnostic accuracy is explored. Advanced AI technologies such as deep learning and natural language processing unlock new insights in genomic data, medical imaging, and patient records to enable more personalized treatment and better outcomes for patients. Among the technologies that build transparency and trust, aside from overcoming challenges related to algorithmic interpretability and bias, are explainable models and multi-modal AI systems. Despite its promise, AI in precision medicine is also hindered by various challenges such as concerns over confidentiality of data, ethical questions, and a lack of standardized regulatory frameworks. The review highlights AI's use in genomics, diagnosis, and treatment alongside challenges in deployment and new developments such as federated learning and use of real-world evidence. Ultimately, the article underscores the importance of high ethical standards and cross-disciplinary collaboration in unleashing AI's potential to deliver personalized medicine to the entire globe. By overcoming challenges that already face it and developing new methods, AI can revolutionize precision medicine and pave the way for a new era of patient-centered healthcare.
ER  - 

TY  - CONF
TI  - Explainable AI in Healthcare: A Stacking-Based Approach for Diabetes Classification
T2  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
SP  - 1
EP  - 6
AU  - D. Acharya
AU  - D. B
AU  - R. P. Nair
PY  - 2025
KW  - Support vector machines
KW  - Accuracy
KW  - Explainable AI
KW  - Stacking
KW  - Medical services
KW  - Predictive models
KW  - Feature extraction
KW  - Boosting
KW  - Diabetes
KW  - Random forests
KW  - Diabetes Classification
KW  - Explainable Artificial Intelligence (XAI)
KW  - Stacking Ensemble Models
KW  - Machine Learning in Healthcare
DO  - 10.1109/ICSSES64899.2025.11009637
JO  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
Y1  - 21-22 March 2025
AB  - Diabetes classification remains a challenge in healthcare, demanding accurate and interpretable machine learning models for timely diagnosis. This study introduces a novel stacking ensemble framework combining Random Forest, Gradient Boosting, XGBoost, and Support Vector Machines to enhance prediction performance. To address class imbalance, the hybrid SMOTEENN technique ensures balanced representation. Feature engineering extracts meaningful patterns from the data, including polynomial interactions, feature ratios, and PCA. SHAP and LIME achieve explainability, providing insights into feature importance and prediction behavior. Experimental results show the framework achieves 97.1 % accuracy and 0.998 ROC-AUC, outperforming standalone models. Its transparency, supported by visual aids, makes it highly practical for clinical use. Future work will explore larger datasets and other medical prediction tasks.
ER  - 

TY  - CONF
TI  - Machine Learning Approaches in Cervical Cancer Research: A Comprehensive Literature Review
T2  - 2025 3rd Cognitive Models and Artificial Intelligence Conference (AICCONF)
SP  - 1
EP  - 4
AU  - M. Yüksel
AU  - T. Ozseven
PY  - 2025
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Feature extraction
KW  - Prediction algorithms
KW  - Distance measurement
KW  - Classification algorithms
KW  - Ensemble learning
KW  - Risk management
KW  - Epidemiology
KW  - Cervical cancer
KW  - cervical cancer
KW  - machine learning
KW  - artificial intelligence
KW  - human papillomavirus (HPV)
KW  - early diagnosis
KW  - Pap smear
KW  - colposcopy
KW  - feature selection
KW  - ensemble learning
KW  - explainable artificial intelligence (XAI)
KW  - medical image processing
KW  - risk prediction
KW  - clinical decision support systems
KW  - epidemiology
KW  - data balancing
DO  - 10.1109/AICCONF64766.2025.11064277
JO  - 2025 3rd Cognitive Models and Artificial Intelligence Conference (AICCONF)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd Cognitive Models and Artificial Intelligence Conference (AICCONF)
Y1  - 13-14 June 2025
AB  - Cervical cancer (CrC) continues to pose a significant global threat to women’s health and accounts for a considerable share of the worldwide cancer burden. Early diagnosis and effective management strategies play a critical role in reducing the morbidity and mortality rates associated with the disease. This review aims to comprehensively evaluate the epidemiology, etiology, traditional screening and diagnostic methods, and the current and potential applications of machine learning (ML) and artificial intelligence (AI) algorithms in cervical cancer. The reviewed studies cover a broad spectrum, ranging from the role of Human Papillomavirus (HPV) infection in disease progression to cancer cell classification, prediction of treatment response, and individual risk assessment. This review highlights the transformative potential of ML and AI techniques in the fight against cervical cancer, demonstrating how advanced methods such as feature selection, ensemble learning, and explainable artificial intelligence (XAI) enhance diagnostic accuracy and clinical applicability.
ER  - 

TY  - CONF
TI  - Enhancing Interpretability: The Role of Explainable AI in Healthcare Diagnostics
T2  - 2025 International Conference on Electronics and Renewable Systems (ICEARS)
SP  - 1
EP  - 6
AU  - N. Zade
AU  - M. Langote
AU  - P. Verma
PY  - 2025
KW  - Renewable energy sources
KW  - Explainable AI
KW  - Scalability
KW  - Closed box
KW  - Medical services
KW  - Predictive models
KW  - Regulation
KW  - Artificial intelligence
KW  - Prognostics and health management
KW  - Cancer
KW  - Explainable AI
KW  - Machine Learning
KW  - Local Interpretable Model-agnostic Explanations
KW  - Shapley Additive Explanations
DO  - 10.1109/ICEARS64219.2025.10940671
JO  - 2025 International Conference on Electronics and Renewable Systems (ICEARS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Electronics and Renewable Systems (ICEARS)
Y1  - 11-13 Feb. 2025
AB  - XAI is now transforming the use of AI in diagnosing diseases by overcoming some of the problems inherent in most black-box approaches. In time-sensitive speciality areas like computer-aided diagnosis, image analysis, and predictive modelling, SHAP, LIME, and attention-based XAI techniques explain how AI decides to help doctors and other healthcare professionals trust AI. Therefore, this paper aims to clarify why XAI is needed in healthcare, introduce techniques, and show their examples in cancer diagnosis and disease prognosis. XAI improves diagnostic performance and satisfies regulatory and ethical requirements for increased usage in clinical practice. Because of XAI, clinicians can use the best of artificial intelligence to complement their knowledge in enhancing the treatment of patients. That is why the outlook of XAI in healthcare includes the next steps in achieving higher scalability, managing bias, and working with diverse regulatory frameworks, which will define the future of AI solutions in healthcare.
ER  - 

TY  - CONF
TI  - Survey on Explainable AI Based Meta-Learning to Enhance Patient-Specific ECG Anomaly Detection
T2  - 2025 International Conference on Innovative Trends in Information Technology (ICITIIT)
SP  - 1
EP  - 5
AU  - K. Pragash
AU  - K. Jeevitha
AU  - R. Pushpa
AU  - B. Sriram
AU  - A. K. Govindaraj
PY  - 2025
KW  - Metalearning
KW  - Surveys
KW  - Accuracy
KW  - Explainable AI
KW  - Training data
KW  - Electrocardiography
KW  - Market research
KW  - Classification algorithms
KW  - Public healthcare
KW  - Anomaly detection
KW  - Electrocardiograms (ECG)
KW  - Explainable AI (XAI)
KW  - Machine-learning
KW  - Meta-learning
KW  - Anomaly detection
DO  - 10.1109/ICITIIT64777.2025.11041623
JO  - 2025 International Conference on Innovative Trends in Information Technology (ICITIIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Innovative Trends in Information Technology (ICITIIT)
Y1  - 21-22 Feb. 2025
AB  - Cardiovascular disorders are becoming increasingly prevalent and represent a significant public health challenge. Today, computerized ECG diagnostic systems often fail to identify rare but important critical heart problems. This is mostly because ECG datasets are not always balanced. Electrocardiograms (ECG) are essential for evaluating cardiac health and detecting irregularities, and their early identification plays a pivotal role in improving patient outcomes. This study examines this field by developing advanced machine-learning algorithms and Explainable AI (XAI) techniques for automated ECG classification. By employing meta-learning approaches, we harness normal ECG data to train models that can effectively identify deviations indicative of rare and critical cardiac function. This method notably enhances Detection effectiveness, but also promotes transparency in decision-making, ultimately leading to better patient care and outcomes in cardiology.
ER  - 

TY  - CONF
TI  - Neuro-Symbolic Self-Explanatory AI (NS-XAI): A Real-Time Framework for Transparent and Adaptive Decision-Making
T2  - 2025 International Conference on Computing Technologies (ICOCT)
SP  - 1
EP  - 5
AU  - E. V
PY  - 2025
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Knowledge graphs
KW  - Real-time systems
KW  - Fraud
KW  - Reliability
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - Next generation networking
KW  - Explainable AI (XAI)
KW  - Real-Time AI
KW  - Neuro-Symbolic AI
KW  - Self-Explanatory AI
KW  - Trustworthy AI
KW  - Knowledge Graphs
KW  - Transparent Decision-Making
KW  - Edge AI
KW  - Bias Reduction
DO  - 10.1109/ICOCT64433.2025.11118512
JO  - 2025 International Conference on Computing Technologies (ICOCT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Computing Technologies (ICOCT)
Y1  - 13-14 June 2025
AB  - Artificial Intelligence (AI) is transforming industries such as healthcare, finance, cybersecurity, and autonomous systems, where real-time and transparent decision-making is crucial. However, most Explainable AI (XAI) techniques provide post-hoc explanations, meaning they explain decisions only after they are made. This delay reduces trust and makes AI difficult to use in time-sensitive applications like fraud detection, medical diagnosis, and autonomous driving. To overcome these challenges, we propose Neuro-Symbolic Self-Explanatory AI (NS-XAI), a novel approach that enables AI to explain its decisions instantly while making them. NS-XAI integrates Neural AI, which learns complex patterns from data, with Symbolic AI, which generates human-readable rules to justify decisions. Additionally, it features a self-explanatory layer that continuously refines explanations based on past decisions, ensuring consistency and improving interpretability. This system enhances fairness by helping users understand not only what decision was made but also why and how it was reached. Unlike traditional methods, NS-XAI operates in real-time, making it ideal for applications where instant and interpretable AI decisions are required. In fraud detection, it can immediately explain why a transaction is flagged as suspicious; and in healthcare, it can justify diagnoses with clear reasoning. By bridging the gap between AI accuracy and explainability, NS-XAI significantly reduces bias, enhances trust, and improves decision reliability Experiments show that NS-XAI is 10 times faster than existing explainability models while maintaining high accuracy. By offering real-time, clear, and adaptive explanations, this system makes AI more transparent, fair, and user-friendly, paving the way for the next generation of responsible and interpretable AI applications.
ER  - 

TY  - CONF
TI  - A Review on AI-Driven Multi-Modal Data Integration in Personalized Medicine: Advancements in Diagnosis, Prognosis, and Treatment Optimization
T2  - 2025 International Conference on Emerging Technologies in Engineering Applications (ICETEA)
SP  - 1
EP  - 6
AU  - M. E
AU  - K. P. J
AU  - B. P
AU  - Y. P. V
AU  - S. R
AU  - P. R
PY  - 2025
KW  - Deep learning
KW  - Protocols
KW  - Reviews
KW  - Precision medicine
KW  - Soft sensors
KW  - Data integration
KW  - Predictive models
KW  - Prognostics and health management
KW  - Medical diagnostic imaging
KW  - Wearable sensors
KW  - Personalized Medicine
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Precision Medicine
KW  - Medical Data Fusion
KW  - Diagnosis
KW  - Prognosis
DO  - 10.1109/ICETEA64585.2025.11099728
JO  - 2025 International Conference on Emerging Technologies in Engineering Applications (ICETEA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Emerging Technologies in Engineering Applications (ICETEA)
Y1  - 5-6 June 2025
AB  - The merging of various data types constitutes an innovative medical strategy which enhances physicians' abilities to detect and predict diseases while creating better therapy protocols. Artificial intelligence (AI) along with deep learning models improve precision healthcare by analyzing various data sources which encompass genomic sequences and electronic health records (EHRs) with medical imaging information and information gathered from wearable sensors. This review discusses essential findings from multi-modal data studies about diagnostic tools developed with AI methods as well as predictive models that forecast therapy results and novel biomarker identification techniques. This paper elaborates on methodological techniques and points out dominant patterns before discussing existing knowledge deficits scientists must resolve. A discussion about data heterogeneity, interpretability issues and privacy challenges emerges during the exploration of future methods to improve AI-driven multi-modal integration in clinical practice.
ER  - 

TY  - CONF
TI  - Explainable AI (XAI) in Predictive Cloud Optimization: Cost, Workload and Performance
T2  - SoutheastCon 2025
SP  - 1424
EP  - 1429
AU  - V. Viradia
AU  - A. Jain
AU  - S. S. Ogety
AU  - A. Donvir
PY  - 2025
KW  - Deep learning
KW  - Cloud computing
KW  - Ethics
KW  - Costs
KW  - Explainable AI
KW  - Biological system modeling
KW  - Predictive models
KW  - Resource management
KW  - Artificial intelligence
KW  - Optimization
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Cloud Computing
KW  - Explainable AI (XAI)
KW  - Cloud Optimization
DO  - 10.1109/SoutheastCon56624.2025.10971567
JO  - SoutheastCon 2025
IS  - 
SN  - 1558-058X
VO  - 
VL  - 
JA  - SoutheastCon 2025
Y1  - 22-30 March 2025
AB  - The increasingly complex nature of cloud computing ecosystems used for highly scalable distributed applications requires advanced methodologies in optimizing resource distribution and workload prediction while achieving the optimal level of performance with reduced expenditure. Traditional approaches often become impractical in addressing the erratic nature of cloud workloads, resulting in inefficiency and potential overspending on costs for the enterprises. This research explores deep learning techniques like RNNs, LSTMs, GRUs and DNNs aimed at enhancing predictive capabilities for optimal cloud optimization regarding cost control, workload forecasting, and performance enhancement. However, the opaque nature of many artificial intelligence models is a significant barrier to their widespread adoption in critical cloud management tasks. The lack of transparency in AI-driven decisions can create concerns about trust, accountability, and compliance with regulatory standards. By using Explainable AI (XAI), cloud providers and users can obtain explanations of how AI models predict workload patterns, delineate potential bottlenecks, and offer the best configurations for resources. This kind of transparency promotes trust and effective oversight of cloud operations. This paper reviews a few of the most prevalent methods: SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), which are used to understand the factors that drive AI-based decisions for cloud optimization.
ER  - 

TY  - CONF
TI  - Predictive Healthcare Machine Learning Models that are Easy to Understand: Finding the Right Balance Between Precision and Openness in Clinical Prognoses
T2  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
SP  - 1
EP  - 6
AU  - S. R. Kumar
AU  - E. S
AU  - S. A
AU  - B. R. Celia
AU  - R. M
AU  - D. Karunkuzhali
PY  - 2025
KW  - Deep learning
KW  - Training
KW  - Ethics
KW  - Accuracy
KW  - Explainable AI
KW  - Closed box
KW  - Medical services
KW  - Machine learning
KW  - Predictive models
KW  - Smart systems
KW  - Model Interpretability
KW  - Clinical Prognoses
KW  - Precision and Openness
KW  - Deep Learning
KW  - Explainable AI (XAI)
KW  - Decision Support Systems
KW  - Model Transparency
KW  - Healthcare AI Adoption
KW  - Black-box Models
DO  - 10.1109/ICSSES64899.2025.11009332
JO  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
Y1  - 21-22 March 2025
AB  - Predicting patient outcomes, optimising treatment plans, and enhancing clinical decision-making are three areas where machine learning (ML) models have grown in prominence in healthcare. But even though these models are very accurate predictors, they are frequently “black boxes,” which makes it hard for doctors to comprehend and rely on their results. For predictive healthcare models to be useful, doctors must be able to understand how the models work and why they are accurate. Only then can they improve patient care. This research seeks to delve into the intricate relationship between healthcare machine learning models' precision and interpretability, with an emphasis on creating methods that are both exceptionally precise and easily comprehensible for doctors. We look at various ML methods-like logistic regression, decision trees, and random forests-that make patient outcome predictions in a more understandable way, and we compare them to complicated models like deep learning, which focus on accuracy but aren't always transparent. To provide light on the decision-making processes of models without sacrificing predictive power, we investigate the potential applications of models such as explainable AI (XAI) and feature significance approaches to clinical datasets. The “sweet spot” where an improved model's predictability remains unaffected by efforts to make it more explainable is an important part of this work.
ER  - 

TY  - JOUR
TI  - Randomized Explainable Machine Learning Models for Efficient Medical Diagnosis
T2  - IEEE Journal of Biomedical and Health Informatics
SP  - 6474
EP  - 6481
AU  - D. Muhammad
AU  - I. Ahmed
AU  - M. O. Ahmad
AU  - M. Bendechache
PY  - 2025
KW  - Computational modeling
KW  - Training
KW  - Biological system modeling
KW  - Data models
KW  - Medical services
KW  - Deep learning
KW  - Medical diagnosis
KW  - Cancer
KW  - Bioinformatics
KW  - Accuracy
KW  - Big Data
KW  - deep learning
KW  - explainable AI
KW  - extreme learning machines
KW  - healthcare
KW  - random vector functional link
KW  - randomized neural networks
DO  - 10.1109/JBHI.2024.3491593
JO  - IEEE Journal of Biomedical and Health Informatics
IS  - 9
SN  - 2168-2208
VO  - 29
VL  - 29
JA  - IEEE Journal of Biomedical and Health Informatics
Y1  - Sept. 2025
AB  - Deep learning-based models have revolutionized medical diagnostics by using Big Data to enhance disease diagnosis and clinical decision-making. However, their significant computational demands and opaque decision-making processes, often characterized as “black-box” systems, pose major challenges in time-critical and resource-constrained healthcare settings. To address these issues, this study explores the application of randomized machine learning models, specifically Extreme Learning Machines (ELMs) and Random Vector Functional Link (RVFL) networks, in medical diagnostics. These models introduce stochasticity into their training processes, reducing computational complexity and training times while maintaining accuracy. Furthermore, we integrate Explainable AI techniques namely Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP) to explain the decision-making rationale of ELMs and RVFL. Performance evaluations on genitourinary cancers and coronary artery disease datasets demonstrate that RVFL outperforms traditional deep learning models, achieving superior accuracy of 88.29% with a computational overhead of 6.22 seconds for genitourinary cancers, and an accuracy of 81.64% with a computational time of 0.0308 seconds for coronary artery disease. This research highlights the potential of randomized models in enhancing efficiency and transparency in medical diagnosis, thereby accelerating better treatment outcomes and advocating for more accessible and interpretable AI solutions in healthcare.
ER  - 

TY  - CONF
TI  - Integrating Explainable Machine Learning (XAI) in Stroke Medicine: Opportunities and Challenges for Early Diagnosis and Prevention
T2  - 2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS)
SP  - 461
EP  - 469
AU  - V. Shobana
AU  - S. Maheshwari
AU  - M. Savithri
AU  - S. S. Ramasamy
AU  - N. Kumar
PY  - 2025
KW  - Ethics
KW  - Data privacy
KW  - Adaptation models
KW  - Explainable AI
KW  - Prevention and mitigation
KW  - Decision making
KW  - Medical services
KW  - Data models
KW  - Planning
KW  - Medical diagnostic imaging
KW  - Brain Stroke
KW  - Explainable Artificial Intelligence (XAI)
KW  - Machine Learning
DO  - 10.1109/ICMLAS64557.2025.10969073
JO  - 2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Machine Learning and Autonomous Systems (ICMLAS)
Y1  - 10-12 March 2025
AB  - Stroke is a leading cause of mortality and disability worldwide, emphasizing the critical need for early diagnosis and prevention. Machine learning (ML) has demonstrated significant potential in improving stroke prediction and management by analysing complex datasets for risk stratification, diagnosis, and treatment planning. However, the adoption of ML in stroke medicine is limited by the opacity of these models, which can hinder clinical trust and decision-making. Explainable Artificial Intelligence (XAI) addresses this challenge by making ML models more interpretable and transparent, enabling healthcare professionals to understand, validate, and trust their outputs. This research work explores the integration of XAI in stroke medicine, highlighting its potential to enhance early diagnosis, personalized prevention strategies, and treatment planning. We discuss the opportunities XAI provides in identifying high-risk patients, uncovering critical predictors, and enabling informed clinical decisions. Furthermore, we examine challenges such as ensuring model reliability, addressing biases in stroke datasets, and navigating ethical considerations related to patient data privacy and algorithmic accountability.
ER  - 

TY  - CONF
TI  - AI-Powered Transformation in Healthcare: Innovations, Implementation Challenges, and Future Perspectives
T2  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
SP  - 1
EP  - 7
AU  - H. V. Koli
AU  - D. J. Thaker
AU  - J. Khengar
PY  - 2025
KW  - Ethics
KW  - Technological innovation
KW  - Explainable AI
KW  - Medical services
KW  - Documentation
KW  - Prediction algorithms
KW  - Market research
KW  - Artificial intelligence
KW  - Predictive analytics
KW  - Interoperability
KW  - Artificial Intelligence (AI)
KW  - Healthcare Applications
KW  - Machine Learning
KW  - Deep Learning
KW  - Medical Diagnostics
KW  - Predictive Analytics
KW  - Clinical Decision Support
KW  - Data Privacy
KW  - Ethics in Healthcare AI
KW  - Algorithmic Bias
KW  - Healthcare Innovation
KW  - Interoperability
KW  - Personalized Medicine
DO  - 10.1109/ICTEST64710.2025.11042869
JO  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
Y1  - 3-5 April 2025
AB  - Artificial Intelligence (AI) is revolutionizing the healthcare industry by enhancing diagnostic accuracy, personalizing treatment plans, and improving patient outcomes. AI-driven applications, such as deep learning-based medical imaging, predictive analytics, and natural language processing for clinical documentation, are transforming traditional healthcare workflows. However, the integration of AI in healthcare is fraught with challenges, including data privacy concerns, algorithmic bias, regulatory constraints, and the interpretability of AI-driven decisions. Ensuring transparency and fairness in AI models remains a key hurdle in their widespread adoption. This paper aims to provide an in-depth analysis of AI applications in healthcare, explore emerging trends such as GPT-based models for medical documentation and federated learning for privacy-preserving AI, and discusses the ethical and technical challenges that must be addressed to harness AI’s full potential. We conclude with recommendations for future research directions, including improving AI explainability, developing robust regulatory frameworks, and enhancing dataset diversity to mitigate bias.
ER  - 

TY  - CONF
TI  - Explainable AI for Gastrointestinal Disease Detection using Ensemble Deep Learning Techniques
T2  - 2025 International Conference on Visual Analytics and Data Visualization (ICVADV)
SP  - 1113
EP  - 1119
AU  - S. G. Gideon
AU  - P. J. B. Princess
PY  - 2025
KW  - Deep learning
KW  - Explainable AI
KW  - Visual analytics
KW  - Medical services
KW  - Predictive models
KW  - Transformers
KW  - Gastrointestinal tract
KW  - Reliability
KW  - Medical diagnostic imaging
KW  - Diseases
KW  - Explainable AI (XAI)
KW  - Gastrointestinal Disease Detection
KW  - Ensemble Deep Learning
KW  - Grad-CAM
KW  - SHAP
KW  - LIME
KW  - Medical Imaging
DO  - 10.1109/ICVADV63329.2025.10961940
JO  - 2025 International Conference on Visual Analytics and Data Visualization (ICVADV)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Visual Analytics and Data Visualization (ICVADV)
Y1  - 4-6 March 2025
AB  - This is a GI disease and a global threat, with treatment that doesn't take long when diseases are rightly identified. This study is introducing a novel approach to use multiple AI deep learning systems, to use for GI diseases identification and to offer explanations. In order to distinguish a few details amid the medical images based on endoscopy images then an identifying framework of such a type of deep learning techniques incorporating CNNs, RNNs, & the transformers. It indicates to doctors which parts of the image are responsible for the most salient results, all while keeping the results in the form of a explainability tools Grad-CAM, SHAP, and LIME. That is all the more true because the system's decision-making process is displayed in a framework and that's how healthcare providers trust AI predictions. That is, clearly predictive properties of the system, confirmed by the test result, accompanied with additional information of interpretative nature that can be utilized by physicians. The system enhances diagnostic processes with minimal error, which indicates that more doctors will trust in artificial intelligence medical solutions.
ER  - 

TY  - CONF
TI  - AI for Predictive Healthcare: Real-Time Insight Generation from Diverse Patient Data
T2  - 2025 International Conference on Data Science and Business Systems (ICDSBS)
SP  - 1
EP  - 7
AU  - D. K. Singh
AU  - K. K. Bharti
AU  - B. Shahi
AU  - A. Larhgotra
PY  - 2025
KW  - Ethics
KW  - Explainable AI
KW  - Federated learning
KW  - Scalability
KW  - Medical services
KW  - Transforms
KW  - Real-time systems
KW  - Medical diagnostic imaging
KW  - Edge computing
KW  - Diseases
KW  - AI-powered healthcare
KW  - real-time medical insights
KW  - disease diagnosis
KW  - risk assessment
KW  - anomaly detection
KW  - machine learning in healthcare
KW  - explainable AI (XAI)
KW  - data privacy
KW  - federated learning
KW  - multimodal data integration
KW  - cloud-based computing
KW  - edge computing
KW  - personalized healthcare
KW  - continuous learning frameworks
KW  - healthcare innovation
KW  - patient-centric AI solutions
DO  - 10.1109/ICDSBS63635.2025.11031750
JO  - 2025 International Conference on Data Science and Business Systems (ICDSBS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Data Science and Business Systems (ICDSBS)
Y1  - 17-18 April 2025
AB  - The research studies an AI-based system that distributes instantaneous predictive medical information between various healthcare platforms used for disease analysis and risk evaluation and anomaly detection. Supervision from intuitive machine learning enables this system to assess enormous medical datasets in order to deliver current accurate data needed by physicians for clinical treatment choices. The system attains strong performance criteria during reporting to display readiness for clinical implementation throughout various healthcare environments. Medical workflows resist full AI integration due to three fundamental challenges which combine data heterogeneity with model scalability limitations and ethical problems. Technology systems establish data privacy protection by using privacy-preserving methods involving differential privacy and federated learning. A new focus in artificial intelligence research develops XAI methods to build understandable systems that enhance user trust and system usability. Evolutionary medical information demands a real-time system with adaptive learning capabilities by implementing cloud-based computing alongside edge capabilities which should also unite multiple data formats in system enhancements. The system demonstrates potential to transform personalized healthcare through expanded applications by addressing existing barriers to improve both patient care and healthcare operational effectiveness in diverse healthcare settings.
ER  - 

TY  - CONF
TI  - Enhancing Transparency in Healthcare Decision-Making: An Explainable AI Approach
T2  - 2025 12th International Conference on Emerging Trends in Engineering & Technology - Signal and Information Processing (ICETET - SIP)
SP  - 1
EP  - 6
AU  - T. Thakur
AU  - S. S. Khurshid
AU  - N. Sharma
PY  - 2025
KW  - Deep learning
KW  - Ethics
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Decision making
KW  - Medical services
KW  - Skin
KW  - Computational efficiency
KW  - Diseases
KW  - Explainable AI (XAI)
KW  - Healthcare Decision-Making
KW  - Model Interpretability
KW  - LIME (Local Interpretable Model-agnostic Explanations)
KW  - SHAP (SHapley Additive exPlanations)
KW  - Skin Disease Classification
KW  - Medical Diagnostics
DO  - 10.1109/ICETETSIP64213.2025.11156742
JO  - 2025 12th International Conference on Emerging Trends in Engineering & Technology - Signal and Information Processing (ICETET - SIP)
IS  - 
SN  - 2157-0485
VO  - 
VL  - 
JA  - 2025 12th International Conference on Emerging Trends in Engineering & Technology - Signal and Information Processing (ICETET - SIP)
Y1  - 1-2 Aug. 2025
AB  - Advances in computational technologies have enabled health systems to make accurate diagnoses and develop precise treatment plans for each patient. However, this capability coexists with a major problem: the opacity of complex algorithmic systems, commonly referred to as the “black box” problem. This work explores the contribution of interpretability toward increasing trust, accountability, and usability of such systems in clinical decision-making. The main focus of this chapter is to outline the applications of healthcare computing, discuss the pros and cons, and provide an indepth introduction to techniques of interpretability aimed at increasing transparency. The discussion covers model-agnostic approaches, such as LIME and SHAP, along with model-specific techniques developed for deep learning and ensemble models, demonstrating the potential to enhance transparency in healthcare environments. Case studies and empirical evidence highlight how these tools can be integrated into clinical workflows to illuminate algorithmic decisions. Furthermore, the exploration extends to how computational models can inform and alleviate potential ethical issues and minimize possible biases while increasing adoption by health professionals. The focus is on critical standardized measurements for interpretability to ensure systems positively influence patient outcomes. Efforts to link the high computational powers of complex systems to applied transparency emphasize the crucial need for interpretability in healthcare, fostering a more reliable, ethical, and efficient health environment.
ER  - 

TY  - CONF
TI  - AXONS-3: An XAI-Augmented Approach for Advancing Trust and Transparency in 3D Brain Tumor Segmentation
T2  - 2025 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)
SP  - 64
EP  - 71
AU  - J. Abyasa
AU  - R. Rahmania
PY  - 2025
KW  - Training
KW  - Solid modeling
KW  - Uncertainty
KW  - Three-dimensional displays
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Brain tumors
KW  - Brain modeling
KW  - Reliability
KW  - Stakeholders
KW  - 3D U-Net
KW  - brain tumor segmentation
KW  - deep learning
KW  - Explainable AI (XAI)
KW  - healthcare AI
KW  - interpretability
KW  - neuroimaging
KW  - post-hoc
KW  - transparency
DO  - 10.1109/CogSIMA64436.2025.11079472
JO  - 2025 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)
IS  - 
SN  - 2379-1675
VO  - 
VL  - 
JA  - 2025 IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)
Y1  - 2-5 June 2025
AB  - Early brain tumor detection remains a critical challenge in medicine due to its impact on patient outcomes. While magnetic resonance imaging (MRI) is a key tool, the challenges accumulated from the grayscale nature of MRI and high volume of data, coupled with human cognitive limitations and time pressures in radiology, create a potentially large margin of diagnostic uncertainty and error. Despite their performance, deep learning solutions face resistance in clinical adoption due to the lack of trust that roots from the opacity of such models. This research introduces the AXONS-3 workflow with the aim of bridging model outputs with the practical needs of clinicians by integrating interpretability and transparency into artificial intelligence (AI) systems for clinical decision-making. First, a 3D U-Net model is trained on the BraTS2020 dataset using T1Gd, T2, and FLAIR MRI sequences to segment brain tumors into sub-regions of NCR/NET, ED, and ET. Then, post-hoc visual Explainable AI (XAI) techniques, including gradient-based methods and uncertainty quantification, are augmented to the workflow to interpret the process of reaching the predicted segmentation. The proposed AXONS-3 workflow provides visually intuitive feedback and justifications to foster greater stakeholder comprehension and trust, contributing to the transparency of AIdriven systems needed for reliable adoption in clinical settings.
ER  - 

TY  - CONF
TI  - Revolutionizing Healthcare: The Synergy of AI and Bioinformatics
T2  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
SP  - 692
EP  - 697
AU  - N. Sharma
AU  - S. Sharma
PY  - 2025
KW  - Accuracy
KW  - Biological system modeling
KW  - Precision medicine
KW  - Medical services
KW  - Genetics
KW  - Numerical models
KW  - Artificial intelligence
KW  - Bioinformatics
KW  - Medical diagnostic imaging
KW  - Diseases
KW  - Artificial Intelligence
KW  - Bioinformatics
KW  - Machine Learning
KW  - Deep Learning
KW  - Precision Medicine
KW  - Phenotypic Data
KW  - Proteomic Data
DO  - 10.1109/ISPCC66872.2025.11039515
JO  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
IS  - 
SN  - 2643-8615
VO  - 
VL  - 
JA  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
Y1  - 6-8 March 2025
AB  - By facilitating accurate diagnosis, individualized treatment, and important scientific discoveries, the combination of artificial intelligence (AI) and bioinformatics is revolutionizing healthcare. In order to analyze huge biological datasets, find patterns, and create models for illness diagnosis and treatment interventions, artificial intelligence (AI) techniques like machine learning (ML) and deep learning (DL) are essential. Precision medicine is fueled by bioinformatics, which uses phenotypic, proteomic, and genetic data to improve treatment outcomes, cut costs, and minimize human error. Together, AI and bioinformatics improve clinical judgment and the early identification of genetic and metabolic diseases, promoting medical research and healthcare provision. The effect, difficulties, and future prospects of the combination of AI and bioinformatics are highlighted in this research.
ER  - 

TY  - CONF
TI  - Quantifying Explainability: Essential Guidelines for Evaluating XAI Methods
T2  - 2025 International Symposium on iNnovative Informatics of Biskra (ISNIB)
SP  - 1
EP  - 5
AU  - S. Djellikh
AU  - I. Youkana
AU  - A. Amari
AU  - R. Saouli
PY  - 2025
KW  - Measurement
KW  - Deep learning
KW  - Visualization
KW  - Explainable AI
KW  - Finance
KW  - Medical services
KW  - Predictive models
KW  - Robustness
KW  - Informatics
KW  - Guidelines
KW  - Deep Learning
KW  - Explainable AI methods
KW  - Qualitative assessment
KW  - Quantitative evaluation
DO  - 10.1109/ISNIB64820.2025.10983231
JO  - 2025 International Symposium on iNnovative Informatics of Biskra (ISNIB)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Symposium on iNnovative Informatics of Biskra (ISNIB)
Y1  - 28-30 Jan. 2025
AB  - Deep Learning (DL) models have demonstrated excellence in executing complex tasks within sensitive fields such as healthcare, finance, and autonomous systems. However, ensuring that their decisions are understandable and trustworthy has become a critical concern. Explainable AI methods (XAI) have emerged to address this challenge by offering diverse techniques to explain how DL models arrive at their predictions. The increasing number of XAI methods, combined with their varied outputs, makes it challenging to assess which method is most suitable for a given task. In the literature, two main approaches have been proposed for evaluating XAI methods: qualitative and quantitative. In this paper, we focus on quantitative metrics and essential properties, such as faithfulness, robustness, and complexity, to effectively evaluate XAI methods. By analyzing these properties, we provide clear insight into the advantageous values, whether higher or lower. Additionally, to ensure consistency across metrics, we propose a new scores' transformation so that higher values consistently indicate better performance. This work serves as a comprehensive guide for practitioners and researchers, enabling them to select, assess, and improve explainability methods tailored to specific use cases.
ER  - 

TY  - CONF
TI  - Enhancing Interpretability in Diabetics Prediction: A Comparative Study of SHAP, LIME and Permutation Feature Importance
T2  - 2025 AI-Driven Smart Healthcare for Society 5.0
SP  - 1
EP  - 6
AU  - U. Mitra
AU  - P. Sarkar
AU  - J. Mondal
AU  - J. Kundu
PY  - 2025
KW  - Pregnancy
KW  - Ethics
KW  - Explainable AI
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Mathematical models
KW  - Diabetes
KW  - Glucose
KW  - Insulin
KW  - Diabetics prediction
KW  - Explainable Artificial Intelligence (XAI) for Disabetics prediction
KW  - Local Interpretable Modelagnostic Explanations (LIME) for Diabetics prediction
KW  - Comparing XAI for Disbetics prediction
DO  - 10.1109/IEEECONF64992.2025.10962890
JO  - 2025 AI-Driven Smart Healthcare for Society 5.0
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 AI-Driven Smart Healthcare for Society 5.0
Y1  - 14-15 Feb. 2025
AB  - Diabetes is a prevalent condition with rising global impacts on morbidity and mortality. This paper presents an in-depth analysis of machine learning (ML) models for diabetes prediction. To improve interpretability, the study incorporates multiple Explainable AI (XAI) techniques, including SHAP, LIME, and Permutation Feature Importance, which provide both global and local insights into model predictions. Using multiple XAI methods allows for a comprehensive understanding of model behavior from different perspectives—SHAP offers consistent, mathematically sound feature attributions; LIME provides localized, instance-specific explanations; and Permutation Feature Importance highlights overall feature relevance. Consistently across these XAI methods, Glucose emerged as the most influential predictor, followed by BMI and Age, aligning with established clinical risk factors. Features such as Pregnancies and DiabetesPedigreeFunction exhibited moderate impact, while Insulin and Skin Thickness had minimal effect on predictions. By comparing the advantages and limitations of different XAI methods, this research fosters trust in ML-driven diabetes diagnostics, enabling more transparent and informed decision-making. The study offers a framework for ethical AI integration in clinical practice, advancing responsible AI use in diabetes management.
ER  - 

TY  - CONF
TI  - Balancing Artificial Intelligence Advancements and Patient Data Privacy in Philippine Healthcare: A Review of Literature
T2  - 2025 5th International Conference on Soft Computing for Security Applications (ICSCSA)
SP  - 1513
EP  - 1521
AU  - J. G. Caw-It
AU  - G. Ricalde
AU  - J. C. Bustillo
PY  - 2025
KW  - Ethics
KW  - Privacy
KW  - Technological innovation
KW  - Data privacy
KW  - Law
KW  - Explainable AI
KW  - Medical services
KW  - Data governance
KW  - Artificial intelligence
KW  - Computer security
KW  - Artificial Intelligence
KW  - Cybersecurity
KW  - Ethical AI
KW  - Explainable AI
KW  - Healthcare Privacy
KW  - Philippine Healthcare Systems
DO  - 10.1109/ICSCSA66339.2025.11170964
JO  - 2025 5th International Conference on Soft Computing for Security Applications (ICSCSA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 5th International Conference on Soft Computing for Security Applications (ICSCSA)
Y1  - 4-6 Aug. 2025
AB  - Artificial intelligence (AI) is transforming healthcare worldwide, yet its integration raises critical concerns regarding patient privacy and ethical use, particularly in developing countries like the Philippines. This study aims to analyze the intersection of AI innovation and patient privacy in the Philippine healthcare system through a thematic literature review A total of 21 peer-reviewed sources published between 2017 and 2024 were analyzed thematically. Four dominant focus areas were identified: AI applications in healthcare, cybersecurity and privacy, integration of emerging technologies, and explainable and ethical AI. The literature predominantly emphasizes theoretical models, with limited practical deployment. Significant gaps exist in implementation, data governance, ethical alignment, and infrastructure adaptation, particularly in applying global AI frameworks to local contexts. This review highlights the need for localized pilot programs, culturally relevant ethical standards, and improved cybersecurity systems. These insights inform national policy and offer actionable guidance for responsible AI integration in Philippine healthcare.
ER  - 

TY  - CONF
TI  - Privacy-Focused Federated Learning for Mental Health Data with XAI Techniques
T2  - 2025 6th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)
SP  - 1338
EP  - 1343
AU  - V. Meena
AU  - G. Raghavender
AU  - S. Jayakar
AU  - J. S. Kumar
PY  - 2025
KW  - Performance evaluation
KW  - Training
KW  - Data privacy
KW  - Accuracy
KW  - Quantum computing
KW  - Federated learning
KW  - Explainable AI
KW  - Mental health
KW  - Medical services
KW  - Data models
KW  - privacy
KW  - mental healthcare data
KW  - WESAD
KW  - Explainable AI
KW  - federated learning
DO  - 10.1109/ICDICI66477.2025.11134972
JO  - 2025 6th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 6th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)
Y1  - 9-11 July 2025
AB  - Protecting the privacy of sensitive data is paramount in the field of mental health care, keeping in mind the amount of data that the healthcare applications produce. In such cases, federated learning comes into the picture, which has emerged as a leading solution for training decentralized models while preserving the confidential information from the models. To overcome some vital issues, we have come up with an approach of solving the problem using Federated Learning which is communication-efficient and specifically designed for mental healthcare applications. This model's framework depends on federated averaging that involves averaging of weights to update the models' parameters. We evaluate the model's working on a classic mental health care dataset called WESAD from the UCI ML repository with identically distributed data. Our results highlight the promise of the FL model, demonstrating its potential to transform the field. To gain confidence in our results, we have utilized explainable artificial intelligence, for tabular data so that we can be sure of our model's predictions. The model performed well with an overall accuracy of 92.56%, a precision score of 92.67%, a recall of 92.56%, and an F1 score of 92.25%.
ER  - 

TY  - CONF
TI  - Enhancing Speech Emotion Recognition with Explainable AI and Feature Importance Analysis
T2  - 2025 2nd International Conference on Next-Generation Computing, IoT and Machine Learning (NCIM)
SP  - 1
EP  - 6
AU  - H. Hasnat
AU  - K. Akter
AU  - N. Tabassum
PY  - 2025
KW  - Emotion recognition
KW  - Analytical models
KW  - Accuracy
KW  - Explainable AI
KW  - Speech recognition
KW  - Speech enhancement
KW  - Predictive models
KW  - Feature extraction
KW  - Recording
KW  - Root mean square
KW  - SER
KW  - Explainable AI
KW  - Model Interpretability
DO  - 10.1109/NCIM65934.2025.11160160
JO  - 2025 2nd International Conference on Next-Generation Computing, IoT and Machine Learning (NCIM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 2nd International Conference on Next-Generation Computing, IoT and Machine Learning (NCIM)
Y1  - 27-28 June 2025
AB  - This study proposes a lightweight 1D-CNN architecture that integrates Explainable AI (XAI) techniques to address the interpretability gap in Speech Emotion Recognition (SER). The model employs three hierarchical convolutional blocks with batch normalization and max-pooling, processing acoustic features including Zero-Crossing Rate, Root Mean Square, and Mel-Frequency Cepstral Coefficients extracted from a combined dataset of 9,730 samples from TESS, SAVEE, RAVDESS, and CREMA-D corpora. Local Interpretable Model-Agnostic Explanations (LIME) for instance-level interpretation and Integrated Gradients for batch-level feature importance analysis are implemented and this the first work to do so. The proposed model achieves 97.05% accuracy across seven emotion classes while providing transparent decision explanations. The integration of dual XAI techniques enables both individual prediction explanations and aggregated feature importance insights, making the model suitable for applications requiring both high performance and interpretability. This work demonstrates that competitive accuracy can be achieved without sacrificing model transparency in SER.
ER  - 

TY  - CONF
TI  - AI-Driven Whole-Exome Sequencing: Advancing Variant Interpretation and Precision Medicine
T2  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
SP  - 1
EP  - 5
AU  - F. Aburub
AU  - M. Al-Remawi
AU  - R. A. Abdel-Rahem
AU  - F. Al-Akayleh
AU  - A. S. A. A. Agha
PY  - 2025
KW  - Deep learning
KW  - Proteins
KW  - Sequential analysis
KW  - Ethics
KW  - Accuracy
KW  - Translation
KW  - Annotations
KW  - Precision medicine
KW  - Artificial intelligence
KW  - Diseases
KW  - Whole-Exome Sequencing
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Genetic Diagnostics
KW  - Precision Medicine
DO  - 10.1109/ICCIAA65327.2025.11013653
JO  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
Y1  - 28-30 April 2025
AB  - Whole-exome sequencing (WES) has revolutionized genomic medicine by enabling high-resolution analyses of the protein-coding regions responsible for numerous genetic diseases. Despite its advantages, processing and interpreting the massive datasets generated by WES remain challenging, particularly given the diversity of genetic variants and the need for clinically actionable insights. Artificial intelligence (AI), which includes machine learning (ML) and deep learning (DL), has emerged as a transformative approach, providing data-driven solutions for variant calling, functional annotation, and pathogenicity modeling. By integrating multi-omics data, clinical records, and large-scale WES outputs, AI-driven methods can pinpoint disease-associated variants, discover novel biomarkers, and guide personalized treatment strategies. This article highlights current AI applications in WES, discusses technical and ethical considerations, and outlines future directions for integrating AI into precision medicine workflows. Ultimately, AI offers the potential to enhance diagnostic accuracy, streamline variant interpretation, and improve patient care in genetic diagnostics.
ER  - 

TY  - CONF
TI  - Deep Learning Framework with Explainable AI for Accurate and Interpretable Brain Tumor Segmentation
T2  - 2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)
SP  - 1
EP  - 6
AU  - D. ND
AU  - K. R. Naik
AU  - C. H G
AU  - A. L
AU  - D. C
PY  - 2025
KW  - Deep learning
KW  - Training
KW  - Data privacy
KW  - Accuracy
KW  - Federated learning
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Brain tumors
KW  - Robustness
KW  - Real-time systems
KW  - MRI-based brain tumor diagnosis
KW  - Explainable AI (XAI)
KW  - Federated Learning (FL)
KW  - Deep learning
KW  - Data privacy
KW  - Interpretable predictions
KW  - Diagnostic reliability
KW  - Clinical applications
DO  - 10.1109/ICISCN64258.2025.10934555
JO  - 2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)
Y1  - 24-25 Jan. 2025
AB  - Brain tumor detection by MRI scan is an imperative medical concern that necessitates state-of-the-art techniques for accurate and timely detection. This paper proposes an approach integrating Explainable AI, Federated Learning, and deep learning to improve accuracy, privacy, and trust. Federated learning allows collaborative learning for the model without accessing each other’s data. Explanation using SHAP and LIME techniques results in interpretable predictions from an AI system, enabling greater clinician trust in these AI systems. In accuracy, the proposed system could achieve an accuracy of 96.8%, with sensitivity at 96.5%, specificity at 96.4%, and an F1-score at 96.6% more than the traditional CNN model. The future direction in this approach will be enhanced in interpretability, robustness, scalability, and in dataset diversity to further boost generalization. This research methodology is bridging AI research to clinical application that offers reliable diagnoses through practices of privacy preservation, moves healthcare through innovation, and evokes trust in AI-driven solutions.
ER  - 

TY  - CONF
TI  - IoT-Driven Air Quality Monitoring and Predictive Analysis with Explainable AI
T2  - 2025 6th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)
SP  - 1434
EP  - 1439
AU  - M.Sobhana
AU  - L. D. Katakam
AU  - P. Ramya Sri
AU  - M. Lingamallu
PY  - 2025
KW  - Temperature sensors
KW  - Explainable AI
KW  - Atmospheric modeling
KW  - Data visualization
KW  - Predictive models
KW  - Real-time systems
KW  - Sensors
KW  - Internet of Things
KW  - Public healthcare
KW  - Monitoring
KW  - Air pollution
KW  - Internet of Things (IoT)
KW  - Machine Learning
KW  - Real-time monitoring
KW  - Explainable AI (XAI)
DO  - 10.1109/ICICV64824.2025.11085506
JO  - 2025 6th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 6th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)
Y1  - 17-19 June 2025
AB  - Air pollution is a critical concern in urban environments due to its impact on public health and quality of life. A real-time air quality monitoring and prediction model is introduced that integrates Internet of Things (IoT) sensors with Machine Learning (ML) models to detect and predict pollutant concentrations. The system collects data on nitrogen dioxide (NO2), sulfur dioxide (SO2), carbon monoxide (CO), and ozone (O3) using MQ-7, MQ-136, MQ-9, and MQ-131 sensors, along with DHT11 for temperature and humidity. The data is preprocessed to train ML models, including Decision Tree, Random Forest, Gradient Boosting, and Support Vector Classifier. The Decision Tree model achieved the highest accuracy of 99.68% in predicting pollutant levels. Explainable AI (XAI) techniques, such as LIME, are employed to interpret prediction results enhancing model transparency and building user trust. A web-based dashboard visualizes real-time data and prediction outputs, enabling policymakers, environmental agencies, and the public to make informed decisions. The proposed system offers a scalable and interpretable solution for proactive urban air quality management.
ER  - 

TY  - JOUR
TI  - XAI for Industry 5.0—Concepts, Opportunities, Challenges, and Future Directions
T2  - IEEE Open Journal of the Communications Society
SP  - 2706
EP  - 2729
AU  - T. R. Gadekallu
AU  - P. Kumar Reddy Maddikunta
AU  - P. Boopathy
AU  - N. Deepa
AU  - R. Chengoden
AU  - N. Victor
AU  - W. Wang
AU  - W. Wang
AU  - Y. Zhu
AU  - K. Dev
PY  - 2025
KW  - Surveys
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Smart manufacturing
KW  - Fifth Industrial Revolution
KW  - Computer science
KW  - Production systems
KW  - Collaboration
KW  - Agriculture
KW  - Technological innovation
KW  - XAI
KW  - AI
KW  - industry 5.0
KW  - smart factories
KW  - smart healthcare
KW  - E-governance
KW  - smart transportation
KW  - education 5.0
KW  - agriculture 5.0
KW  - energy 5.0
DO  - 10.1109/OJCOMS.2024.3473891
JO  - IEEE Open Journal of the Communications Society
IS  - 
SN  - 2644-125X
VO  - 6
VL  - 6
JA  - IEEE Open Journal of the Communications Society
Y1  - 2025
AB  - Industry 5.0 has become a reality now and it is a paradigm that integrates contemporary innovations and concepts. Artificial Intelligence (AI) is a key component and asset of the industrial transformation which allows intelligent devices to perform functionalities such as self-examination, assessment, and evaluation autonomously. AI-based methodologies using ML and deep learning assist manufacturers and industrialists in forecasting service requirements and minimizing downtime. Recent research has discovered a remarkable change in the processes, systems, applications, and products in industries. Also, there is a significant challenge with the explainability of the decisions provided by the models using deep learning algorithms and their inadequate ability to be coupled with each other. Therefore, Explainable artificial intelligence (XAI) is required without compromising the efficiency of the models developed using deep learning algorithms. XAI investigates and develops algorithms, techniques, and models that produce human-comprehensible explanations of AI-based systems and can increase transparency and performance. The explainability nature of XAI will help humans understand the model and the reason behind the predictions, thus improving the model’s transparency and the reliability of the outcomes. Furthermore, an Industry 5.0-enabled environment has a variety of data from varied sources, and this multi-source information must be fused to derive meaningful and optimal decisions. Therefore, all AI-integrated applications must derive actionable insights through information fusion. Hence, the adoption of XAI methodologies in Industry 5.0 can help humans make trustworthy decisions for critical applications requiring information fusion. In this paper, we present a state-of-the-art survey on adopting XAI in Industry 5.0. We discuss the adoption of XAI in various applications such as smart factories, smart Healthcare, E-Governance, smart transportation, Education 5.0, Agriculture 5.0, and Energy 5.0. Finally, some research issues and future directions of integrating XAI with Industry 5.0 are also discussed and highlighted to promote more study in the potential field.
ER  - 

TY  - CONF
TI  - Predictive Modelling for Chronic Kidney Disease: A Hybrid Approach Using Exploratory Data Analysis, Machine Learning and Explainable AI
T2  - 2025 8th International Conference on Electronics, Materials Engineering & Nano-Technology (IEMENTech)
SP  - 1
EP  - 6
AU  - A. Nandi
AU  - P. Paul
AU  - A. K. Das
AU  - A. Ghosh
AU  - A. Singh
AU  - A. Gupta
PY  - 2025
KW  - Analytical models
KW  - Data analysis
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Chronic kidney disease
KW  - Data models
KW  - Reliability
KW  - Chronic Kidney Disease
KW  - Machine Learning
KW  - EDA
KW  - Explainable AI
DO  - 10.1109/IEMENTech65115.2025.10959403
JO  - 2025 8th International Conference on Electronics, Materials Engineering & Nano-Technology (IEMENTech)
IS  - 
SN  - 2767-9934
VO  - 
VL  - 
JA  - 2025 8th International Conference on Electronics, Materials Engineering & Nano-Technology (IEMENTech)
Y1  - 31 Jan.-2 Feb. 2025
AB  - Chronic Kidney Disease (CKD) is a growing health crisis in India, affecting nearly 17% of the population. Due to its often asymptomatic nature, CKD is frequently diagnosed only in advanced stages, leading to expensive treatments and poor patient outcomes. To address this, we explore the potential of AI and machine learning to enhance early detection and predict CKD progression, enabling timely interventions. This paper presents a Machine Learning (ML) model with Exploratory Data Analysis (EDA) for CKD prediction, emphasizing both accuracy and interpretability, which is critical for clinical adoption. We integrate Explainable AI (XAI) techniques, specifically LIME, to provide healthcare professionals with clear insights into model predictions, promoting trust and transparency. Our approach combines predictive power with interpretability, offering a practical solution for early CKD detection and improving clinical decision-making. We discuss the challenges and future directions of integrating AI in healthcare, with the aim of reducing CKD-related healthcare costs and improving patient outcomes in India.
ER  - 

TY  - CONF
TI  - An Explainable AI-Centric Approach for Healthcare: A Review
T2  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
SP  - 1565
EP  - 1568
AU  - S. Sharma
AU  - K. P. Sharma
AU  - K. Saini
PY  - 2025
KW  - Diabetic retinopathy
KW  - Accuracy
KW  - Explainable AI
KW  - Reviews
KW  - Medical services
KW  - Prediction algorithms
KW  - Cognition
KW  - Security
KW  - XAI
KW  - AI
KW  - Retinopathy
KW  - Artificial Intelligence
KW  - Diabetic Retinopathy
DO  - 10.1109/ICCSAI64074.2025.11064197
JO  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
IS  - 
SN  - 
VO  - 3
VL  - 3
JA  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
Y1  - 4-6 April 2025
AB  - In the past, doctors used photographs of the back of the eye and other tests to find diabetic retinopathy. But sometimes, these tests weren't clear enough, and mistakes happened. With more and more people getting diabetes, we need faster and better ways to check their eyes. Artificial intelligence (AI) is like a computer program that can help doctors find diabetic retinopathy faster and more accurately. But sometimes, it's hard to understand how AI makes results. That's where XAI comes in. XAI helps us understand why AI makes certain decisions. This makes doctors feel more confident in using AI and helps patients understand their condition better. Diabetic Retinopathy (DR) is a severe eye problem that happens because of diabetes. It can make you lose your eyesight. We need to find it early and take care of it properly. Using a smart way called Explainable Artificial Intelligence (XAI) can help us do this better.
ER  - 

TY  - JOUR
TI  - Expanding AI’s Role in Healthcare Applications: A Systematic Review of Emotional and Cognitive Analysis Techniques
T2  - IEEE Access
SP  - 69129
EP  - 69160
AU  - P. K. Nag
AU  - A. Bhagat
AU  - R. Vishnu Priya
PY  - 2025
KW  - Artificial intelligence
KW  - Medical services
KW  - Systematic literature review
KW  - Emotion recognition
KW  - Mental health
KW  - Deep learning
KW  - Social networking (online)
KW  - Market research
KW  - Ethics
KW  - Depression
KW  - AI in healthcare
KW  - cognitive assessment
KW  - data security in AI
KW  - deep learning applications
KW  - emotion detection
KW  - mental health analytics
KW  - NLP in medical texts
KW  - patient-centered approaches
KW  - sentiment evaluation
DO  - 10.1109/ACCESS.2025.3562131
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 13
VL  - 13
JA  - IEEE Access
Y1  - 2025
AB  - This systematic literature review (SLR) analyzes the various applications of artificial intelligence (AI) in healthcare, with a particular emphasis on the integration of emotive and cognitive analytical frameworks. The primary aim of this investigation is to thoroughly evaluate the influence of AI technology on patient care by analyzing emotional processes and enabling patient-centered solutions. In this research, we investigate the cognitive and emotional approaches to sentiment analysis and other modeling and forecasting methods using AI. Primary sources include patients’ reviews, online health exchanges and doctors’ narratives. Key aspects of the present state of affairs are advances in the development of machine learning algorithms for emotion recognition, intracellular fusion of cognitive and affective modes of analysis, and the application of artificial intelligence for the enhancement of clinical support systems. Moreover, these technologies have significantly improved individualized clinical approaches, expedited the early identification of mental health problems, and strengthened the rationale for therapeutic treatments. Despite recent advancements, the discipline still faces numerous persistent obstacles. Pressing issues include the ethical implications of using artificial intelligence, the need to protect patient privacy, and the complexity of detecting biases in algorithms. Nevertheless, the impact of AI on healthcare practices is indisputable, indicating a future marked by a more intelligent, efficient, empathetic, and patient-centered healthcare system. This study examines the consequences of artificial intelligence in healthcare by analyzing its importance in emotional and cognitive computing, tracking ongoing developments, and promoting the use of AI in healthcare while considering individual requirements.
ER  - 

TY  - CONF
TI  - Establishing a Balance between Precision and Openness in AI Systems for Essential Decision-Making: Interpretable Deep Learning Models
T2  - 2025 International Conference on Emerging Systems and Intelligent Computing (ESIC)
SP  - 658
EP  - 663
AU  - A. L. Priya
AU  - M. Kumar Gajula
AU  - S. P. Karuppiah
AU  - J. Ganesh B
AU  - J. J. J
AU  - S. Rajesh Kumar
PY  - 2025
KW  - Deep learning
KW  - Ethics
KW  - Accuracy
KW  - Attention mechanisms
KW  - Decision making
KW  - Closed box
KW  - Predictive models
KW  - Data models
KW  - Topology
KW  - Artificial intelligence
KW  - Interpretability
KW  - Deep Learning
KW  - Precision
KW  - Openness
KW  - Black-box models
KW  - Explainable AI (XAI)
KW  - Attention mechanisms
DO  - 10.1109/ESIC64052.2025.10962662
JO  - 2025 International Conference on Emerging Systems and Intelligent Computing (ESIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Emerging Systems and Intelligent Computing (ESIC)
Y1  - 8-9 Feb. 2025
AB  - A growing number of industries, like healthcare, banking, and autonomous systems, are placing their trust in AI systems to make important decisions, which calls for the creation of models that are transparent and have high predictive precision. In this research, we concentrate on interpretable deep learning models as a means to tackle the overarching problem of making AI systems both precise and open (interpretable). Despite the remarkable accuracy that deep learning models have shown in areas like medical diagnostics, image recognition, and natural language processing, the lack of transparency, accountability, and trust caused by their "black-box" nature is a major concern, particularly when decisions with potentially life-changing implications are at stake. There are ethical questions about justice, prejudice, and explainability brought up by the absence of interpretability, which also makes it harder for regulated areas to use AI. The goal of this study is to investigate possible approaches that can make deep learning models both very accurate and easily interpretable. To help non-technical stakeholders better comprehend model predictions, we explore state-of-the-art strategies including attention mechanisms, layer-wise relevance propagation, and post-hoc explanation methods (e.g., SHAP, LIME). Combining model-based and data-driven decision-making, we also provide new architectures that intrinsically promote openness. In addition, the study delves into the costs and benefits of model performance against interpretability, looking at how interpretability limitations might affect accuracy and how accuracy could affect interpretability. Several real-world case studies are used to assess these trade-offs. These case studies include autonomous driving systems, financial fraud detection, and predictive healthcare analytics. All of these systems need to be highly accurate and interpretable in order to meet regulatory requirements and maintain user trust. Based on our research, interpretable models have the potential to perform similarly to opaque models while also offering further advantages including increased user trust, simpler debugging, and compliance with legal and ethical requirements. Contributing to the continuing conversation on responsible AI, this study provides developers and legislators with practical insights by outlining a methodology for striking a balance between transparency and accuracy. The goal of future research is to enhance these methods even further by combining symbolic reasoning with neural network topologies to create hybrid models that are both accurate and easy to understand. This effort could improve the transparency and accountability of AI systems, which could speed up their use in high-stakes decision-making situations.
ER  - 

TY  - CONF
TI  - Artificial Intelligence in Lipidomics: Advancing Biomarker Discovery, Pathway Analysis, and Precision Medicine
T2  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
SP  - 01
EP  - 05
AU  - M. Al-Remawi
AU  - F. Aburub
AU  - F. Al-Akayleh
AU  - R. A. Abdel-Rahem
AU  - A. S. A. Ali Agha
PY  - 2025
KW  - Deep learning
KW  - Microorganisms
KW  - Precision medicine
KW  - Biological system modeling
KW  - Systems biology
KW  - Standardization
KW  - Predictive models
KW  - Lipidomics
KW  - Real-time systems
KW  - Artificial intelligence
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Lipidomics
KW  - Multi-Omics
DO  - 10.1109/ICCIAA65327.2025.11013531
JO  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
Y1  - 28-30 April 2025
AB  - Lipidomics, a branch of metabolomics, provides critical insights into cellular processes and disease mechanisms by profiling lipid species in biological systems. Despite its potential, traditional lipidomics faces significant challenges, including high-dimensional data, variability across experiments, and difficulties integrating with other omics layers. Artificial intelligence (AI) has emerged as a transformative tool, leveraging machine learning and deep learning algorithms to overcome these limitations. AI enables precise lipid biomarker discovery for diseases such as Alzheimer's, diabetes, and cancer, facilitates the elucidation of lipid metabolic pathways, and accelerates drug discovery through improved target identification and pharmacokinetics. Additionally, AI-driven lipidomics supports personalized medicine by tailoring therapeutic strategies based on individual lipidomic profiles and advances environmental research by identifying lipid alterations under stress conditions. However, challenges such as data heterogeneity, lack of standardization, and limited interpretability of AI models remain. Future advancements, including real-time AI-powered lipidomics, federated learning for collaborative research, and ethical frameworks, promise to address these barriers. The field is poised to revolutionize systems biology and precision medicine by integrating lipidomics with cutting-edge AI methodologies, offering profound implications for diagnostics, therapeutics, and environmental health.
ER  - 

TY  - CONF
TI  - Advancing Ethical Ai: Emerging Trends in Transparency, Fairness, and Explainability
T2  - 2025 International Conference on Circuit, Systems and Communication (ICCSC)
SP  - 1
EP  - 8
AU  - H. MAJJATE
AU  - Y. BELLARHMOUCH
AU  - A. JEGHAL
AU  - A. YAHYAOUY
AU  - L. LAAOUINA
AU  - H. TAIRI
AU  - K. A. ZIDANI
PY  - 2025
KW  - Ethics
KW  - Data privacy
KW  - Systematics
KW  - Explainable AI
KW  - Education
KW  - Medical services
KW  - Market research
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Standards
KW  - Ethical Artificial Intelligence
KW  - Explainable AI (XAI)
KW  - Fairness
KW  - Transparency
KW  - Responsible AI
KW  - Data Privacy
DO  - 10.1109/ICCSC66714.2025.11134950
JO  - 2025 International Conference on Circuit, Systems and Communication (ICCSC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Circuit, Systems and Communication (ICCSC)
Y1  - 19-20 June 2025
AB  - Artificial Intelligence (AI) has emerged as a revolutionary technology across various domains, including healthcare, finance, education, and autonomous systems. However, the implementation of AI, particularly deep learning intelligence, brings about serious issues concerning data privacy, transparency and user security. These concerns are currently critical, as addressing them is important for building trust, achieving public acceptance, and ensuring fairness and accountability in AI applications. To tackle these challenges, new subfields have been developed, including Explainable AI (XAI), Fairness-Aware Machine Learning, and Responsible AI. These emerging subfields of AI emphasise the importance of creating AI systems that prioritise interpretability, transparency, and trustworthiness while aligning advancements with society's collective values, ethical principles, and regulatory standards. However, a key challenge remains in balancing AI performance with the emphasis on user privacy. This paper contributes to the extant academic discourse surrounding the ethics of artificial intelligence by conducting a comprehensive analysis of emerging trends within this evolving field. Furthermore, it proposes a systematic framework aimed at addressing these trends and provides additional recommendations for the implementation of responsible methodologies in the development of ethical AI models.
ER  - 

TY  - CONF
TI  - Predicting Co-Morbid Chronic Diseases: Hypertension and Diabetes Using Deep Learning
T2  - 2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD)
SP  - 1
EP  - 6
AU  - T. S. Perumal
AU  - K. Duraisamy
AU  - D. T
AU  - V. Dhanalakshmi
PY  - 2025
KW  - Deep learning
KW  - Hypertension
KW  - Recurrent neural networks
KW  - Federated learning
KW  - Medical services
KW  - Predictive models
KW  - Data models
KW  - Diabetes
KW  - Forecasting
KW  - Diseases
KW  - Co-Morbid Disease
KW  - Hypertension
KW  - Diabetes
KW  - Deep Learning
KW  - Convolutional Neural Networks (CNN)
KW  - Recurrent Neural Networks (RNN)
KW  - Electronic Health Records (EHR)
KW  - Predictive Analytics
KW  - Explainable AI
KW  - SHAP Values
KW  - Machine Learning
KW  - Healthcare AI
KW  - Medical Diagnosis
KW  - Risk Factor Analysis
KW  - Federated Learning
KW  - Disease Forecasting
KW  - Clinical Decision Support
KW  - Chronic Disease Prediction
DO  - 10.1109/ITIKD63574.2025.11004651
JO  - 2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on IT Innovation and Knowledge Discovery (ITIKD)
Y1  - 13-15 April 2025
AB  - This research proposes a deep learning-oriented method for forecasting the co-morbidity of hypertension and diabetes utilizing Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The main contribution of this research is the creation of a hybrid deep learning framework that proficiently merges CNNs for feature extraction and RNNs for sequence modeling, facilitating precise disease prediction from electronic health records (EHR). The model is trained using a vast dataset that includes demographic, clinical, and lifestyle factors to improve predictive accuracy. Our experimental findings indicate that the proposed model attains 92.5% accuracy, 90.1% precision, and 88.7% recall, surpassing traditional machine learning techniques. Additionally, we incorporate SHAP (Shapley Additive Explanations) values to enhance model interpretability, enabling healthcare professionals to pinpoint essential risk factors that affect predictions. The results underscore the promise of deep learning in fostering early disease detection, streamlining patient care, and lowering healthcare expenses. This research establishes a base for prospective AI-driven predictive analytics in healthcare, with potential for real-world application and further advancement through federated learning and real-time clinical data integration.
ER  - 

TY  - CONF
TI  - Enhancing Healthcare Services through Machine Learning and Artificial Intelligence Applications
T2  - 2025 4th International Conference on Creative Communication and Innovative Technology (ICCIT)
SP  - 1
EP  - 7
AU  - E. Sambodja
AU  - R. Widhawati
AU  - N. A. Zakaria
AU  - N. Lutfiani
AU  - R. Fachrurrozi
AU  - R. Z. Ikhsan
PY  - 2025
KW  - Deep learning
KW  - Ethics
KW  - Technological innovation
KW  - Government
KW  - Decision making
KW  - Medical services
KW  - Data models
KW  - Real-time systems
KW  - Artificial intelligence
KW  - Standards
KW  - Machine Learning
KW  - Artificial Intelligence
KW  - Diagnosis
KW  - Analytics
KW  - Personalization
DO  - 10.1109/ICCIT65724.2025.11167464
JO  - 2025 4th International Conference on Creative Communication and Innovative Technology (ICCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 4th International Conference on Creative Communication and Innovative Technology (ICCIT)
Y1  - 15-16 Aug. 2025
AB  - The rapid growth of digital technologies has positioned Machine Learning (ML) and Artificial Intelligence (AI) as key drivers in modernizing healthcare. This study explores recent advancements in applying ML and AI to improve disease diagnosis, predictive analytics, treatment personalization, and patient monitoring. Using a systematic literature review of peer-reviewed studies from IEEE Xplore, PubMed, and Scopus, the research prioritizes clinical relevance, methodological rigor, and innovation. Findings show that deep learning models, notably convolutional and recurrent neural networks, enhance diagnostic accuracy and chronic disease prediction. AI tools also support real-time decision-making, remote monitoring, and early detection of complications, especially in resource-limited settings. Despite their potential, challenges remain in data privacy, model transparency, and interdisciplinary collaboration. This study highlights the transformative role of AI/ML in patient-centered care and efficiency, stressing the need for ethical standards and strong data governance to guide implementation across diverse health systems.
ER  - 

TY  - CONF
TI  - Developing Explainable AI Models for Personalized Treatment Recommendations Using LLMs and EHR Data
T2  - 2025 International Conference on Computing Technologies & Data Communication (ICCTDC)
SP  - 1
EP  - 6
AU  - S. Korkanti
PY  - 2025
KW  - Accuracy
KW  - Explainable AI
KW  - Large language models
KW  - Precision medicine
KW  - MIMICs
KW  - Decision making
KW  - Medical services
KW  - Transformers
KW  - Natural language processing
KW  - Optimization
KW  - Explainable AI
KW  - Personalized Treatment
KW  - Large Language Models
KW  - Electronic Health Records
KW  - Healthcare Informatics
DO  - 10.1109/ICCTDC64446.2025.11158747
JO  - 2025 International Conference on Computing Technologies & Data Communication (ICCTDC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Computing Technologies & Data Communication (ICCTDC)
Y1  - 4-5 July 2025
AB  - Personalized treatment recommendations remain critical for better outcomes and optimization of healthcare delivery. This paper discusses an explainable AI framework for personalized treatment recommendation using large language models in conjunction with EHR data. Using the publicly available dataset MIMIC-III, this approach integrates into a unique analysis advanced natural language processing capabilities of LLMs, interpreting complex clinical narratives along with structured data. We implement explainability techniques that provide insight into the decision-making process of the AI model to ensure transparency and trust. The results show improved accuracy and clinician satisfaction compared to traditional models, underlining the potential of explainable AI in personalized medicine. This work emphasizes the need to combine large datasets with state-of-the-art AI technologies to enable trustworthy and effective healthcare solutions.
ER  - 

TY  - CONF
TI  - Hybrid CNN-RNN Deep Learning Framework for EEG-Based Mental Health Disorder Diagnosis with Explainable AI
T2  - 2025 International Conference on Intelligent and Cloud Computing (ICoICC)
SP  - 1
EP  - 6
AU  - S. T. Siddiqui
PY  - 2025
KW  - Deep learning
KW  - Recurrent neural networks
KW  - Explainable AI
KW  - Mental health
KW  - Brain modeling
KW  - Feature extraction
KW  - Depression
KW  - Electroencephalography
KW  - Real-time systems
KW  - Convolutional neural networks
KW  - Deep Learning
KW  - EEG signals
KW  - Convolutional Neural Networks (CNNs)
KW  - Recurrent Neural Networks (RNNs)
KW  - CNN-RNN Hybrid
KW  - Mental Health Detection
KW  - Explainable AI
DO  - 10.1109/ICoICC64033.2025.11052036
JO  - 2025 International Conference on Intelligent and Cloud Computing (ICoICC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Intelligent and Cloud Computing (ICoICC)
Y1  - 2-3 May 2025
AB  - Mental health diseases like depression, anxiety, and schizophrenia affect worldwide healthcare systems. Traditional diagnostic methods are subjective, thus data-driven methods are needed. EEG, a non-invasive brain activity measurement method, reveals neural patterns related with mental health disorders. EEG data processing is now automated thanks to deep learning, boosting mental disease classification and detection. This study introduces a hybrid deep learning framework that leverages both Convolutional Neural Networks (CNNs)—specialized in detecting spatial patterns—and Recurrent Neural Networks (RNNs)—suited for modeling temporal dynamics. This paper provides a Hybrid CNN-RNN Deep Learning Framework that uses CNNs for spatial feature extraction and RNNs for temporal relationships in EEG signals. The proposed approach uses Explainable AI (XAI) for clinical interpretability and an attention strategy to improve feature learning. After testing on benchmark EEG datasets, the suggested method outperformed CNN and RNN models with 88.9% accuracy. Despite promising results, dataset variability, class imbalance, and real-time processing persist. Future research should focus on multimodal data fusion, privacy-preserving federated learning, and real-time wearable EEG-based mental health monitoring. This study shows how deep learning might improve mental health diagnostics and enable clinically viable AI-powered treatments.
ER  - 

TY  - CONF
TI  - Comprehensive Study on Heart Disease Prediction and Risk Stratification using Explainable Artificial Intelligence Technique
T2  - 2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)
SP  - 1
EP  - 5
AU  - A. Gadde
AU  - S. Chintala
PY  - 2025
KW  - Heart
KW  - Explainable AI
KW  - Medical services
KW  - Predictive models
KW  - Real-time systems
KW  - Reliability
KW  - Artificial intelligence
KW  - Predictive analytics
KW  - Random forests
KW  - Diseases
KW  - explainable AI
KW  - heart disease prediction
KW  - LIME
KW  - risk stratification
KW  - SHAP
DO  - 10.1109/ICISCN64258.2025.10934680
JO  - 2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Intelligent Systems and Computational Networks (ICISCN)
Y1  - 24-25 Jan. 2025
AB  - Artificial Intelligence (AI) is becoming increasingly important in healthcare, particularly in identifying heart disease and predicting its occurrence. This article discusses how Explainable AI (XAI)-based methods are employed to assess the risks of heart disease and predict its risk stratification. The primary goal is to address the ‘black-box’ issue associated with older AI models by clarifying their decision-making processes. Other XAI methods discussed in this research include Shapley Additive Explanations (SHAP), Local Interpretable Model-agnostic Explanations (LIME), Layer-wise relevance Propagation, Partial Dependency Plots (PDP), and Gradient-Class Activation Map (Grad-CAM) method, employed for heart disease prediction and its risk stratification. Additionally, this paper discusses XAI methods that are specific to neural networks and tree-based models. Many people can better comprehend AI models using XAI, which is crucial for clinical applications. Furthermore, this paper highlights research gaps in combining XAI with Electronic Health Records (EHR) and real-time data analytics. The Dense-Net classifier method achieves an accuracy of 99.8%, recall of 98%, and F1-score of 99.8%, outperforming existing methods such as Random Forest and Extreme Boost (XG-Boost).
ER  - 

TY  - CONF
TI  - FBZX: A Novel Explainable AI based Security Model for IoT Healthcare Systems
T2  - 2025 Third International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)
SP  - 106
EP  - 110
AU  - Y. Sowjanya
AU  - S. Gopalakrishnan
AU  - R. D. Kumar
PY  - 2025
KW  - Privacy
KW  - Federated learning
KW  - Explainable AI
KW  - Medical services
KW  - Threat assessment
KW  - Real-time systems
KW  - Blockchains
KW  - Zero Trust
KW  - Security
KW  - Resilience
KW  - IoT
KW  - XAI
KW  - ZTA
KW  - Blockchain
KW  - healthcare
KW  - FL
KW  - Federated Learning
KW  - security
DO  - 10.1109/ICAISS61471.2025.11042096
JO  - 2025 Third International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 Third International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)
Y1  - 21-23 May 2025
AB  - With the rise of IoT in major fields, the risk and need of secure communication has been a question that remains unaddressed. Especially, in healthcare systems, preserving patient data from cyber threats and intruder relics as a significant challenge due to the heterogeneous and dynamic nature of IoT systems. In this paper, a novel and hybrid framework based on Explainable AI (XAI) is proposed which integrates advanced modules like AI-powered Zero Trust Architecture (ZTA), Federated Learning (FL), and Blockchain for enhancing security, decision transparency, and privacy in healthcare systems. The FL technique is used for anomaly detection while Blockchain based identity management prevents spoofing activities and promotes trust in a decentralized environment. Additionally, the infused XAI ensures interpretability and helps in making security decisions. The performance evaluation demonstrates improved threat detection accuracy (98.7%), improved resilience against cyberattacks, and reduced response time (350ms). Overall, this hybrid technique offers a high-end security solution for IoTdriven healthcare infrastructures.
ER  - 

TY  - CONF
TI  - Enhancing Elderly Care with AI: A Review of Artificial Intelligence and Machine Learning Applications
T2  - SoutheastCon 2025
SP  - 900
EP  - 905
AU  - S. T. Shrestha
AU  - A. K. Ghosh
PY  - 2025
KW  - Ethics
KW  - Privacy
KW  - Navigation
KW  - Decision making
KW  - Medical services
KW  - Machine learning
KW  - Digital intelligence
KW  - Older adults
KW  - Usability
KW  - Remote monitoring
KW  - Caregiving
KW  - Elderly Care
KW  - Healthcare
KW  - Artificial Intelligence
KW  - Machine Learning
DO  - 10.1109/SoutheastCon56624.2025.10971504
JO  - SoutheastCon 2025
IS  - 
SN  - 1558-058X
VO  - 
VL  - 
JA  - SoutheastCon 2025
Y1  - 22-30 March 2025
AB  - The growing global elderly population demands efficient, personalized, and accessible healthcare solutions. Artificial Intelligence (AI) and Machine Learning (ML) have emerged as transformative tools in elderly caregiving, enhancing disease diagnostics, treatment planning, remote monitoring, and health record management. These technologies offer greater autonomy for seniors and improved efficiency for caregivers and healthcare professionals. However, barriers such as data privacy concerns, AI's opaque decision-making (black box problem), and digital literacy challenges continue to limit their widespread adoption. This review examines strategies to improve AI transparency, strengthen privacy safeguards, and develop user-friendly interfaces, ensuring that AI-driven healthcare solutions remain accessible and trustworthy for elderly individuals. Findings emphasize the need for a balanced AI-human caregiving model, where AI complements rather than replaces human caregivers, preserving the compassionate and ethical dimensions of elderly care. Addressing these challenges will enable AI to reshape elderly caregiving-bridging technological advancements with human empathy to improve healthcare outcomes and quality of life for seniors.
ER  - 

TY  - CONF
TI  - XGBClassifier and Explainable AI for Disease Classification from Patient Symptom and Demographic Data with Late Fusion
T2  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
SP  - 1
EP  - 5
AU  - M. I. H. Abir
AU  - N. Bashar
AU  - M. S. Ahmed
AU  - H. I. Peyal
PY  - 2025
KW  - Support vector machines
KW  - Electric potential
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Medical services
KW  - Predictive models
KW  - Fatigue
KW  - Convolutional neural networks
KW  - Diseases
KW  - Lightweight Convolutional Neural Networks
KW  - CNN
KW  - SVM Classifier
KW  - Potato
KW  - Explainable AI (XAI) Techniques
KW  - SHAP
KW  - Lime
DO  - 10.1109/ECCE64574.2025.11013373
JO  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Electrical, Computer and Communication Engineering (ECCE)
Y1  - 13-15 Feb. 2025
AB  - The accurate identification of diseases based on patient symptoms and demographic data is a critical area of healthcare research, with the potential to significantly improve patient outcomes. In this study, we employed machine learning algorithms, specifically the XGBClassifier, to classify diseases based on a dataset containing patient symptoms and demographic information such as age, gender, fever, cough, fatigue, and other health indicators. To ensure model interpretability and transparency, we incorporated Explainable AI (XAI) techniques like SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), which allow us to understand and interpret the feature contributions to the model’s predictions. The results demonstrated that the XGBClassifier achieved an accuracy of 81.42%, outperforming other machine learning models tested. This study emphasizes the importance of combining XAI with machine learning for disease classification, offering greater transparency in the decision-making process, which is vital in healthcare settings.
ER  - 

TY  - CONF
TI  - Scalable Fake Review Detection: Leveraging Machine Learning for Trustworthy Online Platforms
T2  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
SP  - 1
EP  - 5
AU  - N. Bala
AU  - A. Choudhury
AU  - A. Raj
AU  - H. Poonia
PY  - 2025
KW  - Deep learning
KW  - Sentiment analysis
KW  - Adaptation models
KW  - Accuracy
KW  - Computational modeling
KW  - User-generated content
KW  - Real-time systems
KW  - Fraud
KW  - Electronic commerce
KW  - Fake news
KW  - Fake review detection
KW  - machine learning
KW  - NLP
KW  - sentiment analysis
KW  - deep learning
KW  - online platforms
KW  - e-commerce fraud
KW  - deceptive content
KW  - scalable AI
KW  - fraud detection
DO  - 10.1109/AIMLA63829.2025.11041537
JO  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
Y1  - 29-30 April 2025
AB  - Online fake review proliferation has drastically influenced market dynamics and consumer trust. Existing rule-based systems fail to effectively scale up against intelligent forms of fraud. The paper here provides a scalable machine learning framework for detecting fake reviews based on natural language processing, sentiment analysis, and deep neural networks. The system evaluates linguistic features, reviewer behavior, and metadata discrepancies in detecting malicious content. Through rigorous testing on actual datasets, the system exhibits high precision and flexibility in different e-commerce and social media platforms. The research emphasizes the need for scalable AI-based solutions in upholding platform integrity and providing stable user experiences.
ER  - 

TY  - CONF
TI  - Artificial Intelligence and Neurological Disorders: Developments and Opportunities
T2  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
SP  - 94
EP  - 99
AU  - P. K. Gupta
AU  - B. D. Mazumdar
AU  - S. Sharma
AU  - M. Agarwal
AU  - Aakriti
AU  - D. Singh
AU  - Madhur
PY  - 2025
KW  - Neurological diseases
KW  - Training
KW  - Deep learning
KW  - Computer vision
KW  - Costs
KW  - Medical services
KW  - Security
KW  - Artificial intelligence
KW  - Faces
KW  - Artificial Intelligence
KW  - Computer Vision
KW  - Deep Learning
KW  - Healthcare
KW  - Machine Learning
KW  - Neurological Disorders
DO  - 10.1109/ICCSAI64074.2025.11063890
JO  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
IS  - 
SN  - 
VO  - 3
VL  - 3
JA  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
Y1  - 4-6 April 2025
AB  - With the increased use of artificial intelligence (AI) approaches in the realm of neurological disorders (NDs), the low cost early diagnosis is becoming a reality. Further, there is always a possibility of suggesting in time training programs, treatments, etc. Consequently, the developments & the literature on the use of AI approaches in NDs is highly scattered. Through this paper, we aim to bring some sort of aggregation to use of AI in NDs and give the reader a holistic view as well as a succinct coverage. We focus on various NDs as well as myriad AI approaches. In particular we discuss, what has been accomplished so far by the use of AI in the domain of NDs and what still exist as unexplored. Finally we also discuss some challenges that use of AI in NDs face.
ER  - 

TY  - CONF
TI  - The Evolution of Machine Learning (ML) in Decision-Making: A Comprehensive Bibliometric Analysis
T2  - 2025 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
SP  - 1
EP  - 5
AU  - A. Praharaj
AU  - J. Samal
PY  - 2025
KW  - Ethics
KW  - Explainable AI
KW  - Decision making
KW  - Bibliometrics
KW  - Ecosystems
KW  - Finance
KW  - Medical services
KW  - Documentation
KW  - Market research
KW  - Research and development
KW  - Machine Learning (ML)
KW  - Decision-Making
KW  - Bibliometric Analysis
KW  - Artificial Intelligence (AI)
KW  - Explainable AI (XAI)
KW  - Interdisciplinary Research
DO  - 10.1109/ASSIC64892.2025.11158041
JO  - 2025 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
Y1  - 16-18 May 2025
AB  - This research offers a thorough bibliometric examination of machine learning (ML) in decision-making, emphasising its developing research landscape, principal contributors, thematic priorities, and new trends. Drawing on 188 peer-reviewed articles from the Scopus database, this research spans 1971-2025, showcasing two distinct documentation phases. Further, this study identifies the USA, China, and India as leading contributors, reflecting their robust R&D ecosystems and interdisciplinary collaborations. Using open-sourced tools like VOSviewer and R-Studios, the analysis uncovers six keyword clusters, emphasising applications in healthcare, finance, and commerce, along with emerging themes such as explainable AI and human-centric decision-making. This research provides actionable insights into the field's progression, offers a roadmap for future exploration, and highlights the importance of ethical and interdisciplinary approaches in leveraging ML for decision-making.
ER  - 

TY  - CONF
TI  - Ocularis-Ensuring Healthy Vision Through Intelligent Detection
T2  - 2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)
SP  - 1
EP  - 6
AU  - P. K
AU  - T. M. S G
AU  - N. P
AU  - P. S. L
AU  - S. Johnson
PY  - 2025
KW  - Deep learning
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Visual impairment
KW  - Medical services
KW  - Predictive models
KW  - Retina
KW  - Diseases
KW  - Ocular Disease
KW  - Fundus Imaging
KW  - Multimodal Approach
KW  - Machine Learning
KW  - Deep Learning
KW  - Ensembling Techniques
KW  - Explainable AI (XAI)
KW  - Diagnostic Keywords
KW  - Medical Imaging
DO  - 10.1109/RMKMATE64874.2025.11042812
JO  - 2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 2nd International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)
Y1  - 7-8 May 2025
AB  - Ocular diseases can cause vision impairment or blindness if not detected early. “Ocular Disease Intelligent Recognition (Ocularis)” is a multimodal system designed for automated ocular disease detection and classification. It utilizes a dataset of 5,000 patient records, incorporating age, diagnostic keywords, and high-resolution fundus images captured from diverse imaging devices. The model focuses on classifying eight primary disease categories. To enhance diagnostic accuracy, ensemble learning techniques are employed by integrating ResNet, EfficientNet, and VGG19, leveraging their strengths for robust classification. Additionally, SHAP-based Explainable AI (XAI) provides interpretable visualizations of model predictions, aiding clinical validation. A Google Gemini LLM-powered recommendation system further assists in generating personalized insights based on the detected condition. Ocularis is designed to be a scalable and reliable screening tool for real-world healthcare applications. Future enhancements include predictive analytics for disease progression and extending the system to broader ophthalmic applications, improving early detection and clinical decision-making.
ER  - 

TY  - CONF
TI  - An Explainable AI Framework for Vision-Based Human Fall Detection in Fog Infrastructure
T2  - 2025 IEEE International Conference on Emerging Technologies and Applications (MPSec ICETA)
SP  - 1
EP  - 6
AU  - A. Bajpai
AU  - G. Yadav
AU  - V. P. Arul Kumar
AU  - M. Chandna
AU  - R. Srisainath
AU  - N. Pandey
PY  - 2025
KW  - Deep learning
KW  - Visualization
KW  - Accuracy
KW  - Sensitivity
KW  - Explainable AI
KW  - Medical services
KW  - Predictive models
KW  - Convolutional neural networks
KW  - Fall detection
KW  - Wearable sensors
KW  - XAI
KW  - LIME
KW  - SHAP
KW  - Fog
KW  - Cloud
KW  - FDD
KW  - CNN and Fall detection
DO  - 10.1109/MPSecICETA64837.2025.11118852
JO  - 2025 IEEE International Conference on Emerging Technologies and Applications (MPSec ICETA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 IEEE International Conference on Emerging Technologies and Applications (MPSec ICETA)
Y1  - 21-23 Feb. 2025
AB  - The activity recognition using visual data is the primary emphasis of this study. One of the most significant issues that poses a threat to the lives of older people is human fall. It is possible for elderly people to suffer irreparable disability or even pass away as a result of accidental falls. Within the realm of healthcare, fall detection has emerged as a significant research topic, necessitating the development of systems that are both more dependable and efficient to intelligently categorize fall actions. Continuous monitoring of old individuals has become practical because of the growth of the Internet of Things, which includes the creation of wearable sensors, ambient sensors, and cameras. Using Deep Learning classification and a Convolutional Neural Network with three hidden layers, the work that is being offered assures the detection of falls. In order to assess the effectiveness of the Deep Learning model, the Fall Detection Dataset is used. Additionally, for intelligent classification systems to be accepted by healthcare practitioners, they need to be trustworthy. Explainable artificial intelligence models, such as LIME and SHAP, are tested in this study in order to explain the categorization of fall activity. Because of defining the limits of the input picture, the outputs of XAI models demonstrate the feature that is responsible for prediction.
ER  - 

TY  - CONF
TI  - AI-Driven Glaucoma Detection Using Deep Learning
T2  - 2025 International Conference on Sensors and Related Networks (SENNET) Special Focus on Digital Healthcare(64220)
SP  - 1
EP  - 6
AU  - B. R. Sathishkumar
AU  - S. Sudevan
AU  - M. Vignesh
AU  - R. V. Kumar
PY  - 2025
KW  - Glaucoma
KW  - Deep learning
KW  - Head
KW  - Optical computing
KW  - Optical fiber networks
KW  - Retina
KW  - Optical imaging
KW  - Optical sensors
KW  - Convolutional neural networks
KW  - Artificial intelligence
KW  - Glaucoma
KW  - permanent blindness
KW  - early detection
KW  - deep learning
KW  - artificial intelligence
KW  - retinal fundus images
KW  - convolutional neural networks
KW  - cup-to-disc ratio
KW  - optic nerve head
KW  - accuracy
KW  - sensitivity
KW  - specificity
KW  - public datasets
KW  - AI-based detection
KW  - diagnostic techniques
KW  - early glaucoma detection
KW  - ophthalmologists
KW  - patient outcomes
KW  - model interpretability
KW  - AI-assisted screening
DO  - 10.1109/SENNET64220.2025.11135953
JO  - 2025 International Conference on Sensors and Related Networks (SENNET) Special Focus on Digital Healthcare(64220)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Sensors and Related Networks (SENNET) Special Focus on Digital Healthcare(64220)
Y1  - 24-27 July 2025
AB  - Glaucoma stands as a major reason for permanent blindness worldwide yet patients experience no detectable symptoms before their vision worsens significantly. The discovery of glaucoma at its early stage remains essential to achieve proper care and treatment methods. Researchers have researched glaucoma detection in retinal fundus pictures through DL and AI technology systems to operate autonomously. The evaluation includes examining glaucoma indicator features including cup-to-disc ratio and optic nerve head structure through the use of convolutional neural networks (CNNs). The model obtained its parameters from public datasets before receiving assessment of its accuracy and performance metrics. The diagnostic capabilities of artificial intelligence outperform the reliability standards achieved with traditional diagnostic methods. The study reveals deep learning models have boosted early glaucoma identification efficiency to the point where they decrease medical professional workloads while creating better patient care results. Future research aims to make the models more understandable while also working to incorporate AI assisted screening systems into patient care environments.
ER  - 

TY  - CONF
TI  - AI-Driven Protein Discovery: Bridging Computational Innovation and Therapeutic Advancements
T2  - 2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
SP  - 1185
EP  - 1190
AU  - N. M. Chowdhury
AU  - M. S. Islam
AU  - S. Dey
AU  - R. Paul
AU  - M. A. Kadir
AU  - M. T. Islam
AU  - M. R. U. Nabi
AU  - M. A. Yusuf
PY  - 2025
KW  - Proteins
KW  - Drugs
KW  - Deep learning
KW  - Technological innovation
KW  - Ethics
KW  - Precision medicine
KW  - Computational modeling
KW  - Data models
KW  - Drug discovery
KW  - Diseases
KW  - artificial intelligence (AI)
KW  - protein discovery
KW  - machine learning (ML)
KW  - deep learning (DL)
KW  - protein structure prediction
KW  - drug repurposing
KW  - personalized medicine
DO  - 10.1109/ICSSAS66150.2025.11081108
JO  - 2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)
Y1  - 11-13 June 2025
AB  - Artificial intelligence (AI) is revolutionizing the science of protein discovery dramatically, making tremendous progress in drug discovery, protein structure prediction, and precision medicine. The current study examines the crucial role of AI, i.e., via machine learning (ML) and deep learning (DL), in transforming our knowledge, modeling, and use of proteins in therapeutic design. Such breakthrough frameworks as AlphaFold and RosettaFold have shown AI's unmatched ability to predict protein structures at experimental accuracy, minimizing time and expense associated with conventional means. Furthermore, AI's assistance in the discovery of protein-protein interactions (PPIs) and drug repurposing has presented novel avenues to boost drug discovery and enhance current drugs. The combination of multi- omics data has augmented the contribution of AI to personalized medicine to facilitate the design of protein-based therapies for individual patients to optimize therapeutic effects. Nevertheless, issues such as data quality, model interpretability, and constraints on computing power continue to hinder the extensive use of AI in protein discovery. Ethical considerations, such as data protection and transparency of AI models in clinical environments, need to be addressed with caution. Future work needs to focus on enhancing model interpretability, enriching dataset quality, and promoting the use of AI models with conventional experimental frameworks to unlock AI's transformative potential in the battle against disease and the expansion of precision medicine.
ER  - 

TY  - CONF
TI  - The Past Decade and Future of the Cross Application of Artificial Intelligence and Decision Support System
T2  - 2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
SP  - 1666
EP  - 1669
AU  - M. Li
PY  - 2025
KW  - Decision support systems
KW  - Seminars
KW  - Supply chain management
KW  - Precision medicine
KW  - Medical services
KW  - Manufacturing
KW  - Artificial intelligence
KW  - Optimization
KW  - Biomedical imaging
KW  - Logistics
KW  - Artificial intelligence (AI)
KW  - Decision support systems (DSS)
KW  - Cross-application
DO  - 10.1109/AINIT65432.2025.11036058
JO  - 2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 IEEE 6th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)
Y1  - 11-13 April 2025
AB  - The purpose of this study is to investigate the current status and future directions of the cross-application of artificial intelligence (AI) and decision support systems (DSS). With the rapid development of AI technology, AI has been widely used in decision support systems in medical, logistics, business and other fields, which greatly enhances the intelligence and adaptive ability of decision support systems. Although existing research shows the potential of AI-driven DSS in multiple domains, there are still some challenges, such as the transparency and interpretability issues of AI models, as well as data bias and big data scarcity. By analyzing the application cases of AI in medical care, logistics and other industries, this paper puts forward possible solutions to solve these challenges, and looks forward to the development of AI in decision support system in the future.
ER  - 

TY  - CONF
TI  - Towards Explainable AI: A Framework for Interpretable Deep Learning in High-Stakes Domains
T2  - 2025 5th International Conference on Soft Computing for Security Applications (ICSCSA)
SP  - 1354
EP  - 1360
AU  - I. Manga
PY  - 2025
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Predictive models
KW  - Brain modeling
KW  - Convolutional neural networks
KW  - Alzheimer's disease
KW  - Medical diagnostic imaging
KW  - Residual neural networks
KW  - Explainable AI (XAI)
KW  - Deep Learning
KW  - Alzheimer’s Disease
KW  - Interpretable Models
KW  - Grad-CAM
KW  - Custom CNN
KW  - Model Transparency
KW  - Ethical AI
KW  - Medical Imaging
KW  - Neural Networks
DO  - 10.1109/ICSCSA66339.2025.11170778
JO  - 2025 5th International Conference on Soft Computing for Security Applications (ICSCSA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 5th International Conference on Soft Computing for Security Applications (ICSCSA)
Y1  - 4-6 Aug. 2025
AB  - As deep learning models are increasingly used in high-stakes fields such as healthcare, the demand for interpretable and reliable artificial intelligence has grown. Although deep neural networks achieve state-of-the-art accuracy in complex pattern recognition tasks, their opaque "black-box" approach restricts transparency and trust, particularly in critical areas such as medical diagnosis. This paper presents a unified architecture for interpretable deep learning, integrating prediction and interpretation pipelines to certify that the AI model’s decisions are not only accurate but also explainable. The classification of Alzheimer's disease was chosen as the case study of this paper, and four Convolutional Neural Network models were analyzed: Custom CNN, MobileNetV2, ResNet50, and EfficientNetB0. These models are both trained and validated using the available dataset with brain MRI pictures. MobileNetV2 and ResNet50, when used without pretrained weights, failed to generalize due to their inability to meet the localization requirement; thus, indicating the need for architectural tweaking and domain alignment. The architecture recommended incorporates advancements such as Grad-CAM visualizations, Layer-Wise Relevance Propagation, and attention mechanisms, enabling an enhanced understanding of classified data. The domain-customization layer then synthesizes these complex findings into clinically interpretable results, narrowing the gap between AI models and medical professionals. Thus, the suggested architecture not only improves diagnostic dependability but also increases transparency, which fosters trust and compliance with ethical principles for the users.
ER  - 

TY  - CONF
TI  - Explainable Emotion Recognition Using Xception-Based Feature Extraction and Supervised Machine Learning on the RAVDESS Dataset
T2  - 2025 IEEE Medical Measurements & Applications (MeMeA)
SP  - 1
EP  - 6
AU  - S. T. Hussain Shah
AU  - S. A. Hussain Shah
AU  - K. Panagiotopoulos
AU  - J. Pigueiras-del-Real
AU  - K. Qayyum
AU  - S. B. Hussain Shah
AU  - A. Buccoliero
AU  - A. Di Terlizzi
AU  - M. A. Deriu
PY  - 2025
KW  - Support vector machines
KW  - Deep learning
KW  - Emotion recognition
KW  - Three-dimensional displays
KW  - Sensitivity
KW  - Explainable AI
KW  - Face recognition
KW  - Medical services
KW  - Feature extraction
KW  - Data augmentation
KW  - Facial emotion recognition
KW  - Explainable AI (XAI)
KW  - Deep learning
KW  - Xception network
KW  - MediaPipe
KW  - 3D face mesh
KW  - Machine learning
KW  - Emotion classification
KW  - Data augmentation
KW  - Behavioral analysis
DO  - 10.1109/MeMeA65319.2025.11068008
JO  - 2025 IEEE Medical Measurements & Applications (MeMeA)
IS  - 
SN  - 2837-5882
VO  - 
VL  - 
JA  - 2025 IEEE Medical Measurements & Applications (MeMeA)
Y1  - 28-30 May 2025
AB  - Facial emotion recognition is a valuable tool in healthcare, providing insights into emotional well-being, developmental progress, and health-related behaviors. This study presents a novel framework integrating deep learning with explainable artificial intelligence (XAI) to enhance emotion recognition from video data. Using the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), the framework begins with preprocessing, where 3D face meshes with 478 landmarks are generated using MediaPipe, and regions of interest (ROI) are extracted. Data augmentation techniques, including rotation, scaling, and translation, improve dataset variability. Feature extraction is performed using a fine-tuned Xception deep convolutional neural network, followed by classification using supervised machine learning algorithms such as SVM, KNN, ensemble methods, and ANN. Among these, the Fine Gaussian SVM (FGSVM) achieved the highest performance, with 93.87 % accuracy on both validation and test sets. The validation precision, recall, and F1-score were 94.06 %, 93.79 %, and 93.93 %, respectively, while the test set recorded 94.01 %, 93.74 %, and 93.88 %. To ensure interpretability, XAI techniques such as Grad-CAM, LIME, sensitivity occlusion, and SHAP highlight crucial facial landmarks and temporal frames influencing predictions. This study underscores the potential of combining deep learning with XAI to enhance reliability in healthcare applications, improving clinical decision-making, mental health monitoring, and human-computer interaction. A Python-based implementation of the proposed framework is available at: 10.5281/zenodo.14809940.
ER  - 

TY  - CONF
TI  - Unified Model Agnostic Computation and Explainable AI for Enhanced Accuracy and Transparency in Medical Image Classification
T2  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
SP  - 1
EP  - 5
AU  - P. P. Kashyap
AU  - P. G
AU  - S. K. S
AU  - S. K. K. N
AU  - R. P. R
AU  - S. Rastogi
PY  - 2025
KW  - Deep learning
KW  - Analytical models
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Closed box
KW  - Predictive models
KW  - Convolutional neural networks
KW  - Medical diagnostic imaging
KW  - Image classification
KW  - Medical Image Classification
KW  - Explainable AI
KW  - Deep Learning
KW  - SHAP (Shapley Additive Explanations)
KW  - LIME (Local Interpretable Model-Agnostic Explanations)
DO  - 10.1109/ICSSES64899.2025.11009270
JO  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Smart Systems for applications in Electrical Sciences (ICSSES)
Y1  - 21-22 March 2025
AB  - The application of deep learning techniques in medical image classification has revolutionized healthcare diagnostics, but challenges remain in terms of model transparency and interpretability. Despite the high performance of deep learning models, their “black-box” nature often limits their clinical adoption, as healthcare professionals require an understanding of the rationale behind automated predictions. To address this, we propose a unified framework that integrates model-agnostic explainability techniques such as Shapley Additive Explanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) with deep learning models for medical image classification. This approach not only enhances the accuracy of the model by utilizing state-of-the-art convolutional neural network (CNN) architectures but also improves transparency by providing interpretable, humanunderstandable explanations for the model's decisions. The proposed framework is evaluated using various medical image datasets, including X-rays and MRIs, and is compared against traditional deep learning models without explainability methods. Results demonstrate that the integrated approach achieves superior classification accuracy while offering critical interpretability, making it more suitable for deployment in clinical settings. This work bridges the gap between highperformance deep learning models and the need for model transparency, promoting trust in AI-driven medical image analysis tools and enhancing their practical application in realworld healthcare scenarios.
ER  - 

TY  - CONF
TI  - AI Powered Decision Support Systems for Healthcare Enhancing Diagnosis and Treatment with Deep Learning
T2  - 2025 International Conference on Intelligent Computing and Knowledge Extraction (ICICKE)
SP  - 1
EP  - 5
AU  - M. R. Kale
AU  - A. H. Mutlag
AU  - S. P
AU  - N. H. Al-Muraad
AU  - H. S. Mahdi
AU  - S. Muthuperumal
PY  - 2025
KW  - Decision support systems
KW  - Deep learning
KW  - Accuracy
KW  - Scalability
KW  - Transformers
KW  - Feature extraction
KW  - Graph neural networks
KW  - Artificial intelligence
KW  - Usability
KW  - Optimization
KW  - AI-powered healthcare
KW  - swin transformer
KW  - GNN
KW  - medical diagnosis
KW  - decision support system
DO  - 10.1109/ICICKE65317.2025.11136681
JO  - 2025 International Conference on Intelligent Computing and Knowledge Extraction (ICICKE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Intelligent Computing and Knowledge Extraction (ICICKE)
Y1  - 6-7 June 2025
AB  - This research presents a novel hybrid decision support system with artificial intelligence based on Swin Transformer and Graph Neural Networks (GNNs) to enhance diagnostic accuracy and personalized treatment in medical care. Swin Transformer efficiently extracts hierarchical features from medical imaging data, while GNNs specify patient-specific interrelations from structured Electronic Health Records (EHRs). Combining spatial and relational knowledge, the system provides better predictive performance. Implemented using Python with deep learning libraries PyTorch, the hybrid model is evaluated using real-world healthcare datasets. Experimental results show that the proposed method shows an accuracy rate of 92%, a 7–10% increase compared to individual models like CNNs, GNNs, and Swin Transformers. The performance improvement leads to more accurate disease detection and better patient outcomes. Additionally, the model provides good scalability and usability in real-world applications, especially health camp resource optimization. These promising results make the system a strong and interpretable AI platform for clinical deployment.
ER  - 

TY  - CONF
TI  - Deep Learning for Personalized Medicine: A Comprehensive Review
T2  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
SP  - 1503
EP  - 1509
AU  - I. A. Mirzapure
AU  - U. Telrandhe
AU  - M. R. Dandekar
AU  - Y. M. Salve
PY  - 2025
KW  - Deep learning
KW  - Technological innovation
KW  - Ethics
KW  - Precision medicine
KW  - Genomics
KW  - Predictive models
KW  - Real-time systems
KW  - Monitoring
KW  - Biomedical imaging
KW  - Diseases
KW  - Deep Learning
KW  - Personalized Medicine
KW  - Artificial Intelligence
KW  - Predictive Modelling
KW  - Medical Imaging
DO  - 10.1109/ICSADL65848.2025.10933299
JO  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
Y1  - 18-20 Feb. 2025
AB  - Deep learning is now considered to be almost the most potent tool in personal medicine that has changed how healthcare is perceived into data-based and most precise at the same time. It makes decisions for aficionado administration with traits in interpreting large- and complex-data-sized deep learning models that analyze multimodal information, genomics, pictures, and clinical data, into personalized care strategies meant for the personalization of each patient's characteristics. The main components of deep learning are highlighted to show various applications in personal medicine and the challenges that must be overcome to make them work optimally in clinical practice and future directions regarding real-time health monitoring, multi-omics data integration, and explainable AI systems. Most of the factors-including data privacy, model interpretability, and a much stronger regulatory framework towards challenging problems that require innovation not just across disciplinary fully coordinated research but also collaborations amongst AI researchers, clinicians, and regulatory bodies in every field coming together to put inappropriate practices using deep learning for healthcare more responsible and fairer. Deep learning is so revolutionary at the moment and there is still potential for its application and research to grow to change the future in personalized medicine according to the most advanced and latest research together with invention.
ER  - 

TY  - CONF
TI  - AXAI-CDSS: An Affective Explainable AI-Driven Clinical Decision Support System for Cannabis Use
T2  - 2025 International Conference on Activity and Behavior Computing (ABC)
SP  - 1
EP  - 14
AU  - T. Zhang
AU  - T. Chung
AU  - A. Dey
AU  - S. W. Bae
PY  - 2025
KW  - Decision support systems
KW  - Emotion recognition
KW  - Sentiment analysis
KW  - Explainable AI
KW  - Large language models
KW  - Medical services
KW  - Predictive models
KW  - Real-time systems
KW  - Inference algorithms
KW  - Artificial intelligence
KW  - Explainable Artificial Intelligence (XAI)
KW  - Passive Sensing
KW  - Affective Computing
KW  - Clinical Decision Support Systems (CDSS)
KW  - Cannabis Use Disorder
KW  - Cannabis Intoxication
KW  - Cannabis-Intoxicated Behaviors
KW  - Personalized Intervention
KW  - Large Language Models (LLMs)
KW  - Algorithmic Decisions
KW  - Transparency
KW  - Healthcare AI
KW  - Trustworthy AI
KW  - Facial Emotion Recognition
KW  - Causal Inference
DO  - 10.1109/ABC64332.2025.11118599
JO  - 2025 International Conference on Activity and Behavior Computing (ABC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Activity and Behavior Computing (ABC)
Y1  - 21-25 April 2025
AB  - As cannabis use has increased in recent years, researchers have come to rely on sophisticated machine learning models to predict cannabis use behavior and its impact on health. However, many artificial intelligence (AI) models lack transparency and interpretability due to their opaque nature, limiting their trust and adoption in real-world medical applications, such as clinical decision support systems (CDSS). To address this issue, this paper enhances algorithm explainability underlying CDSS by integrating multiple Explainable Artificial Intelligence (XAI) methods and applying causal inference techniques to clarify the models’ predictive decisions under various scenarios. By providing deeper interpretability of the XAI outputs using Large Language Models (LLMs), we provide users with more personalized and accessible insights to overcome the challenges posed by AI’s “black box” nature. Our system dynamically adjusts feedback based on user queries and emotional states, combining text-based sentiment analysis with real-time facial emotion recognition to ensure responses are empathetic, contextadaptive, and user-centered. This approach bridges the gap between the learning demands of interpretability and the need for intuitive understanding, enabling non-technical users such as clinicians and clinical researchers to interact effectively with AI models. Ultimately, this approach improves usability, enhances perceived trustworthiness, and increases the impact of CDSS in healthcare applications.
ER  - 

TY  - CONF
TI  - A Deep Learning-Based Pneumonia Detection System with Explainable AI for Medical Decision Support
T2  - 2025 11th International Conference on Communication and Signal Processing (ICCSP)
SP  - 694
EP  - 699
AU  - S. K. Behera
AU  - K. Murali Gopal
AU  - S. B. Punuri
PY  - 2025
KW  - Deep learning
KW  - Training
KW  - Pneumonia
KW  - Accuracy
KW  - Systematics
KW  - Explainable AI
KW  - Signal processing
KW  - X-ray imaging
KW  - Medical diagnostic imaging
KW  - Testing
KW  - Pneumonia
KW  - Deep Learning
KW  - Disease Detection
KW  - Chest X-ray
KW  - AI-assisted medical
KW  - Grad-CAM
KW  - Explainable AI
DO  - 10.1109/ICCSP64183.2025.11089357
JO  - 2025 11th International Conference on Communication and Signal Processing (ICCSP)
IS  - 
SN  - 2836-1873
VO  - 
VL  - 
JA  - 2025 11th International Conference on Communication and Signal Processing (ICCSP)
Y1  - 5-7 June 2025
AB  - Accurate pneumonia diagnosis is crucial for reducing mortality rates, particularly in resource-constrained healthcare facilities. A deep learning detection framework that examines pneumonia diagnosis for chest X-ray images constitutes the proposal of this research. This system leverages the EfficientNetB7 architecture with Squeeze-and-Excitation (SE) blocks, which significantly increases feature extraction and outperforms the baseline EfficientNetB0 in distinguishing pneumonia from normal cases. The data undergoes systematic division into three parts for training, validation, and testing purposes as part of a thorough model evaluation. The final model achieves an impressive detection accuracy of 98.31%, surpassing existing approaches in this domain. To enhance interpretability, Grad-CAM heat maps are employed to highlight the most influential regions in the X-ray images, aligning with clinical diagnostic needs. This visualization-driven approach improves trust and transparency in AI-assisted medical decision-making, making it a valuable tool for pneumonia diagnosis.
ER  - 

TY  - CONF
TI  - Explainable AI for Hematological Diagnostics: Integrating Image Processing and LIME for Leukemia Detection
T2  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
SP  - 846
EP  - 851
AU  - S. K
AU  - B. G S
AU  - N. A K
AU  - D. T
PY  - 2025
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Leukemia
KW  - Medical services
KW  - Signal processing
KW  - Reliability
KW  - Medical diagnostic imaging
KW  - Blood
KW  - I-driven leukemia diagnosis
KW  - Deep learning
KW  - explainable AI
KW  - Leukemia classification
KW  - Counterfactual explanations
KW  - Alternative diagnostics
KW  - AI healthcare
KW  - Medical transparency
DO  - 10.1109/ISPCC66872.2025.11039577
JO  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
IS  - 
SN  - 2643-8615
VO  - 
VL  - 
JA  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
Y1  - 6-8 March 2025
AB  - This study introduces an innovative system for diagnosing leukemia, leveraging the power of artificial intelligence (AI) combined with deep learning and Explainable AI (XAI) to build confidence in AI-driven medical tools. The system processes images of blood cells to precisely identify and classify different types of leukemia, such as Acute Lymphoblastic Leukemia (ALL), Acute Myeloid Leukemia (AML), Chronic Lymphocytic Leukemia (CLL), and Chronic Myeloid Leukemia (CML). To ensure transparency, the model employs XAI methods, delivering clear and interpretable predictions along with counterfactual explanations that explore alternative diagnostic possibilities. This not only strengthens the system’s reliability but also promotes accountability in medical decision-making. Beyond diagnosis, the system provides customized treatment recommendations based on the patient’s unique medical background, effectively connecting accurate diagnostics with practical, actionable steps. By focusing on precision, transparency, and a patient-first approach, this solution not only enhances the detection of leukemia but also builds trust and understanding among healthcare providers regarding the use of AI in critical medical scenarios.
ER  - 

TY  - CONF
TI  - AI and Automation in Organ-on-a-chip Technology: Advancing Personalized Medicine and Disease Modeling
T2  - 2025 IEEE 6th International Conference in Robotics and Manufacturing Automation (ROMA)
SP  - 40
EP  - 45
AU  - V. Vijayaragavan
AU  - V. M
AU  - P. Dharshini
AU  - K. Karunakaran
AU  - K. Kesavan
AU  - R. D. S. Raj
PY  - 2025
KW  - Adaptation models
KW  - Toxicology
KW  - Service robots
KW  - Precision medicine
KW  - Real-time systems
KW  - Drug discovery
KW  - Biosensors
KW  - In vitro
KW  - Microfluidics
KW  - Testing
KW  - Microfluidics
KW  - Artificial Intelligence
KW  - Automation
KW  - Biosensors
KW  - Smart healthcare
DO  - 10.1109/ROMA66616.2025.11155313
JO  - 2025 IEEE 6th International Conference in Robotics and Manufacturing Automation (ROMA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 IEEE 6th International Conference in Robotics and Manufacturing Automation (ROMA)
Y1  - 20-22 Aug. 2025
AB  - Organ-on-a-Chip (OoC) technology is an advanced platform that replicates human organ functioning on a microfluidic chip, offering a more precise and moral substitute for conventional in vitro and animal testing. They re-create intricate tissue interfaces, mechanical signs and biochemical environments, making them of great utility for disease modeling, drug discovery and toxicity testing. With recent advancements, the use of Artificial Intelligence (AI) and robotic automation, the interpretation of real-time biosensor signals, facilitating accurate detection of tissue responses, early indicators of toxicity and predictive modeling of disease progression will be augmented in the healthcare industry. Robotic automation enables OoC platforms utility to advance through the high-throughput fabrication, fluid manipulation and maintenance of chip cultures with minimal human involvement. This enhances consistency and decreases the possibility of human errorIn medicine, these OoCs are utilized in personalized medicine for drug screening and optimization of therapy using patient- derived cells. They are also used to model intricate medical conditions like cancer, cardiovascular disease, and neurological disorders. This review underscores the upcoming trends of automated and AI-based OoC systems with a focus on their increasing influence on clinical research, personalized medicines, diagnostics and future healthcare provisions.
ER  - 

TY  - CONF
TI  - Tutorial: Fundamentals of (and Tools for) Trustworthy Artificial Intelligence in Smart Health
T2  - 2025 Eleventh International Conference on eDemocracy & eGovernment (ICEDEG)
SP  - 13
EP  - 13
AU  - J. M. Alonso Moral
PY  - 2025
KW  - Artificial intelligence
KW  - Tutorials
KW  - Ethics
KW  - Fuzzy systems
KW  - Explainable AI
KW  - Intelligent systems
KW  - Software tools
KW  - Smart healthcare
KW  - Law
KW  - Fuzzy sets
DO  - 10.1109/ICEDEG65568.2025.11081632
JO  - 2025 Eleventh International Conference on eDemocracy & eGovernment (ICEDEG)
IS  - 
SN  - 2573-1998
VO  - 
VL  - 
JA  - 2025 Eleventh International Conference on eDemocracy & eGovernment (ICEDEG)
Y1  - 18-20 June 2025
AB  - Outline of the Tutorial Artificial Intelligence (AI) is pervading many aspects of our society. This poses challenges to avoid people being put aside when their own data are processed by AI systems, which provide decisions that may result in harmful discrimination. Our focus is on knowledge representation and how to enhance human-centered information processing in the context of Trustworthy AI. Endowing AI with trustworthiness encompasses technical and non-technical challenges. In this tutorial, in addition to technical aspects (i.e., disruptive human-centered technologies as well as human-friendly computer tools aimed at covering all phases of the design, analysis, and evaluation of trustworthy intelligent systems), we will also discuss Ethical, Legal, Socio-Economic and Cultural (ELSEC) implications of AI. Special emphasis will be placed on certifying if intelligent systems comply with European values. Assuming explainability as a prerequisite for trustworthiness, Explainable AI (XAI in short) is an endeavor to evolve AI methodologies and technology by developing intelligent systems capable of generating decisions that a human can understand, but also capable of explicitly explaining their decisions. This way, it is possible to scrutinize the underlying intelligent models and verify if automated decisions are made based on accepted rules and principles so that decisions can be trusted and their impact justified. Accordingly, intelligent systems are expected to naturally interact with humans, thus providing comprehensible explanations of decisions automatically made. Even if this tutorial will introduce the main concepts and methods in the context of XAI in general, a major focus will be on how to properly deal (and compute) with words and perceptions in generating and evaluating textual explanations for smart health. More precisely, we will consider the explainable design of Fuzzy Sets and System in combination with pre-trained Large Language Models for paving the way from interpretable machine learning to Trustworthy AI. Such systems deal naturally with uncertainty and approximate reasoning (as humans do) through computing with words and perceptions. This way, they facilitate humans to scrutinize the underlying intelligent models. Moreover, human-AI interaction is natural and faithful
ER  - 

TY  - CONF
TI  - Interpretable Deep Learning for Bone Cancer Diagnosis: An Explainable AI Perspective
T2  - 2025 8th International Conference on Computing Methodologies and Communication (ICCMC)
SP  - 897
EP  - 902
AU  - R. P. Reji
AU  - D. Shine
AU  - R. N. T
PY  - 2025
KW  - Deep learning
KW  - Visualization
KW  - Accuracy
KW  - Image analysis
KW  - Explainable AI
KW  - X-rays
KW  - Predictive models
KW  - Bones
KW  - Medical diagnostic imaging
KW  - Cancer
KW  - Bone Cancer Detection
KW  - Explainable AI (XAI)
KW  - Bone Cancer Classification
KW  - Deep Learning
KW  - ResNet-101
KW  - Grad-CAM
KW  - Medical Image Analysis
KW  - Interpretability
KW  - Computer-Aided Diagnosis
KW  - Convolutional Neural Networks
KW  - Radiological Imaging
KW  - Transfer Learning
KW  - X-ray Analysis
KW  - Cancer Diagnosis
DO  - 10.1109/ICCMC65190.2025.11140777
JO  - 2025 8th International Conference on Computing Methodologies and Communication (ICCMC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 8th International Conference on Computing Methodologies and Communication (ICCMC)
Y1  - 23-25 July 2025
AB  - Bone cancer is a dangerous disease that needs to be identified early and accurately in order to treat it properly. Conventional diagnostics such as X-rays, MRIs, and biopsies are used as a starting point to identify abnormalities but are very operator dependent, slow, and variability-prone. Deep learning has proven to possess tremendous potential in the process of automated medical image analysis with outstanding accuracy in the identification of cancer. Despite their high accuracy, the absence of interpretability is still a great hindrance for clinical implementation.This work proposes an Explainable AI (XAI) framework for deep learning based detection of bone cancer with the guarantee of both accuracy and interpretability of the predictions. The ResNet-101 model is fine-tuned for binary classification of bone cancer based on a dataset from Hugging Face with preprocessing steps involving resizing, normalization, and equalization of brightness to facilitate improved learning. Grad-CAM is incorporated to highlight the regions of the image which are most influential to the decision of the model in order for medical experts to be able to rely on explanations of AI predictions. The presented method has shown a spectacular accuracy of 94% and F1-score of 0.94, which makes it suitable for bone cancer detection. By integrating deep learning and explainability, the research presents a novel diagnostic system that not only achieves high classification accuracy but also provides visual explanations of model predictions using Grad-CAM. This twofold emphasis on performance as well as interpretability is the essence of the 'Explainable AI Perspective' of the title, with the goal of fostering trust among clinicians and enabling clinical adoption in practice. The findings indicate that the integration of interpretability into AI-based diagnostics can make them more adoptable within clinical practice, which in turn will result in better patient outcomes.
ER  - 

TY  - CONF
TI  - Clustering Techniques for Machine Learning with Electronic Healthcare Data
T2  - 2025 8th International Conference on Computing Methodologies and Communication (ICCMC)
SP  - 632
EP  - 636
AU  - M. R. Prasath
AU  - G. Ravishankar
AU  - K. R. Kumar
AU  - M. P. Ram
AU  - M. S. I. Mary
AU  - S. J. Lakshmi
PY  - 2025
KW  - Deep learning
KW  - Accuracy
KW  - Translation
KW  - Recurrent neural networks
KW  - Precision medicine
KW  - User centered design
KW  - Standardization
KW  - Risk management
KW  - Electronic medical records
KW  - Testing
KW  - Electronic Health Record
KW  - Deep Learning
KW  - Transparency
KW  - Standardization
KW  - Analytical Techniques
DO  - 10.1109/ICCMC65190.2025.11140609
JO  - 2025 8th International Conference on Computing Methodologies and Communication (ICCMC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 8th International Conference on Computing Methodologies and Communication (ICCMC)
Y1  - 23-25 July 2025
AB  - An unparalleled amount of data is being generated as a consequence of advancements in EHR-based systems. Machine learning methods, including clustering, are greatly enhanced by the complex, ever-expanding, and high-dimensional data found in EHRs. Reducing the number of dimensions is generally necessary for cluster analysis to ensure optimal processing time and minimize the disaster of dimensionality. Given the vast array of dimension reduction and cluster analysis techniques at our disposal, it becomes very challenging to determine which combination of approaches from both families yields the desired results. Accurate and practical analysis of electronic health record data requires more familiarity with the data, intermediate outcomes, setting factors, and analytical methods. Imaging, patient risk assessment, and statistical modeling are just a few areas that this study covers as it pertains to deep learning's use in electronic health record analysis. Despite ongoing challenges, the recommendations center on improving user-centered design to ease translation into clinical practice. If crucial next steps are taken to ensure transparency and standardization, machine learning for data-driven precision medicine, EHRs may be a game-changer.
ER  - 

TY  - CONF
TI  - FedSync: Synchronized and Explainable Federated Learning with XAI in IoT-Based Healthcare Data
T2  - 2025 International Conference on Smart Applications, Communications and Networking (SmartNets)
SP  - 1
EP  - 6
AU  - B. Dündar
AU  - E. A. Sezer
AU  - F. Y. Okay
AU  - S. Özdemir
PY  - 2025
KW  - Training
KW  - Performance evaluation
KW  - Deep learning
KW  - Data privacy
KW  - Explainable AI
KW  - Federated learning
KW  - Computational modeling
KW  - Synchronization
KW  - Servers
KW  - Internet of Things
KW  - IoT
KW  - federated learning
KW  - XAI
KW  - healthcare
KW  - deep learning
DO  - 10.1109/SmartNets65254.2025.11106872
JO  - 2025 International Conference on Smart Applications, Communications and Networking (SmartNets)
IS  - 
SN  - 2837-4940
VO  - 
VL  - 
JA  - 2025 International Conference on Smart Applications, Communications and Networking (SmartNets)
Y1  - 22-24 July 2025
AB  - In recent years, significant advances have been made in health systems with the widespread adoption of the Internet of Things (IoT) and AI-powered innovative services. IoT enables the continuous collection of health data, while Artificial Intelligence (AI) analyzes this data to enhance early disease diagnosis. However, traditional Deep Learning (DL) approaches generally rely on centralized systems that collect and process personal data, posing a risk of violating individual privacy. In this study, a Federated Learning (FL) approach is used to process health data obtained from wearable smart devices while preserving privacy and classifying individuals' health scores. To overcome the performance limitations of FL approaches, a novel approach, FedSync, is proposed to incorporate a parallel training mechanism. To demonstrate the superiority of the FedSync approach, results are compared with DL and FL approaches in terms of accuracy, precision, recall, and F1-score. According to the results, the proposed approach provides a 29.02% improvement in computational time performance compared to FL and a 21.01% improvement compared to DL. Furthermore, to address the lack of explainability in IoT data, Explainable AI (XAI) has been integrated into the experiments to identify the most influential features affecting the classification score. This study makes a significant contribution to the literature by presenting a synchronous FL approach supported by XAI for IoT-based health data analysis.
ER  - 

TY  - CONF
TI  - Explainable AI-Driven Heart Disease Prediction
T2  - 2025 International Conference on Visual Analytics and Data Visualization (ICVADV)
SP  - 959
EP  - 964
AU  - R. Bhuvaneswari
AU  - P. Kumar
AU  - S. Kaviya
PY  - 2025
KW  - Heart
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Visual analytics
KW  - Medical services
KW  - Predictive models
KW  - Medical diagnostic imaging
KW  - Tuning
KW  - Diseases
KW  - Machine learning
KW  - Heart disease prediction
KW  - Feature significance
KW  - Healthcare analysis
KW  - Explainable AI
DO  - 10.1109/ICVADV63329.2025.10961035
JO  - 2025 International Conference on Visual Analytics and Data Visualization (ICVADV)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Visual Analytics and Data Visualization (ICVADV)
Y1  - 4-6 March 2025
AB  - The earliest detection is crucial both to prevention and treatment of heart disease, which ranks highly in the list of worldwide health concerns. Data gathering and processing are performed as the initial phase through several health indicators such as blood pressure, cholesterol, and lifestyle factors. All the inputs are processed and computed with the chances of suffering heart disease using a state-of-art model in machine learning. From, this the best performed machine learning model are integrated in the proposed methodology which results in the better accuracy with 83.9%. For the second step, with the help of explainable AI approaches, relevant elements for each prediction of the model are highlighted. Since the performance of the model is assessed in terms of the interpretation and accuracy of its predictions, the third phase comes as the final one, to deliver medical practitioners with an accurate tool that can be put to use for an early diagnosis and helpful insight. The main objective of our approach is to make EAI models in the health care sector more transparent and user-friendly. LIME (Local Interpretable Model-agnostic Explanations) is a popular method in Explainable Artificial Intelligence (EAI) that has been utilized in the proposed work since it interprets complex machine learning models by highlighting the most significant features contributing to a prediction. In the proposed method, LIME provides insights into the features influencing the model's output, such as whether a patient is likely to have heart disease along with the features which contribute to that prediction.
ER  - 

TY  - CONF
TI  - Advancing Breast Cancer Prediction with Machine Learning and Explainable AI Techniques
T2  - 2025 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
SP  - 1
EP  - 7
AU  - T. M. A. A. Amodi
AU  - K. Bajaj
PY  - 2025
KW  - Logistic regression
KW  - Analytical models
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Decision making
KW  - Predictive models
KW  - Breast cancer
KW  - Decision trees
KW  - Random forests
KW  - Machine Learning Approach
KW  - Breast Cancer Prediction
KW  - Accuracy
KW  - XAI
KW  - LightGBM
KW  - Logistic Regression
DO  - 10.1109/ICKECS65700.2025.11035930
JO  - 2025 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Knowledge Engineering and Communication Systems (ICKECS)
Y1  - 28-29 April 2025
AB  - In Healthcare, Breast cancer remained a crucial task but recent technical advancements in machine learning have provided significant promising solutions in this field. This research aims to improve the prediction of breast cancer by utilizing several machine learning algorithms and among those which models will performs well will be analyzed. This ML approach will further focus on the interpretability of models through Explainable AI (XAI) techniques available. Dataset used in this research was obtained from a trusted source named as Kaggle, where several techniques such as preprocessing, EDA and feature importance were applied. Analysis of ML models was conducted where we have used models like XGBoost, Random Forest, LightGBM, Decision Tree and Logistic Regression. Among these models Logistic Regression outperformed all the models with the highest accuracy of 98.25%, showing its superior performance in Breast prediction. Explainable AI technique such as SHAP (Shapley Additive Explanations) was applied to further achieve transparency and reliability. Results obtained showed that XGBoost when combined with XAI provides a good knowledge about the model’s decision-making process.
ER  - 

TY  - CONF
TI  - Clinically Interpretable XAI-Based FMCW Radar Fall Detection with Reduced False Alarms
T2  - 2025 6th International Conference on Bio-engineering for Smart Technologies (BioSMART)
SP  - 1
EP  - 4
AU  - N. Gillani
AU  - T. Arslan
PY  - 2025
KW  - Support vector machines
KW  - Explainable AI
KW  - Biological system modeling
KW  - Neural networks
KW  - Radar detection
KW  - Radar
KW  - Medical services
KW  - Feature extraction
KW  - Fall detection
KW  - Random forests
KW  - Explainable AI (XAI)
KW  - FMCW Radar
KW  - Fall Detection
KW  - Healthcare
KW  - Machine Learning
DO  - 10.1109/BioSMART66413.2025.11046189
JO  - 2025 6th International Conference on Bio-engineering for Smart Technologies (BioSMART)
IS  - 
SN  - 2831-4352
VO  - 
VL  - 
JA  - 2025 6th International Conference on Bio-engineering for Smart Technologies (BioSMART)
Y1  - 14-16 May 2025
AB  - AI-based fall detection systems often limit clinical trust due to high false alarm rates and opaque decision-making. This study presents a novel Explainable AI (XAI) framework designed for FMCW radar-based fall detection. Guided by Patient and Public Involvement, a dataset of 400 falls and 500 non-fall daily activities was collected from volunteers aged 26 to 78 in controlled environments and private homes. Multiple machine learning models, including Random Forest, SVM, XGBoost, and Neural Networks, were benchmarked before and after integrating the proposed XAI framework using 35 micro-Doppler (MD) extracted features. Initially, biomechanically relevant radar features contributing to classification decisions were identified using Permutation Importance. Then, a Surrogate Decision Tree was trained to approximate the optimized model, generating human-readable classification rules for clinicians and caregivers. Finally, a counterfactual analysis module was designed to identify feature shifts in false positives and false negatives. These shifts and surrogate rules guided threshold refinements to improve decision boundaries, reducing false alarms and missed falls. Results show that false alarms were often associated with excessive MD width, while low-energy falls contributed to missed detections. Integrating the XAI framework led to improvements across all classifiers, with model accuracy increasing by 4.9% to 9.6%, false alarm rates decreasing by 1.0% to 9.0%, and missed fall rates reducing by 1.3% to 8.8%. As proof of concept, the framework has been clinically validated with expert feedback, demonstrating strong potential for real-world deployment.
ER  - 

TY  - CONF
TI  - Interpretable Deep Learning for Alzheimer Diseases Classification: Integrating XAI for Trustworthy AI-Assisted Diagnosis
T2  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
SP  - 549
EP  - 554
AU  - A. Suryavanshi
AU  - V. Kukreja
PY  - 2025
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Feature extraction
KW  - Convolutional neural networks
KW  - Medical diagnosis
KW  - Alzheimer's disease
KW  - Standards
KW  - Medical diagnostic imaging
KW  - Alzheimer’s Disease
KW  - Early Diagnosis
KW  - Convolutional Neural Network (CNN)
KW  - Explainable Artificial Intelligence (XAI)
KW  - Grad-Cam
KW  - SHAP
KW  - Medical Imaging
DO  - 10.1109/ISPCC66872.2025.11039536
JO  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
IS  - 
SN  - 2643-8615
VO  - 
VL  - 
JA  - 2025 7th International Conference on Signal Processing, Computing and Control (ISPCC)
Y1  - 6-8 March 2025
AB  - Alzheimer’s Disease (AD) moves progressively through the brain leading to neurodegeneration in millions of individuals across the world. The proper identification of AD subtypes with Normal Cognition (NC), Mild Cognitive Impairment (MCI), Moderate Alzheimer’s (MAD), and Severe Alzheimer’s (SAD) remains vital for early healthcare decisions regarding diagnosis and therapy. Traditional deep learning models including both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) demonstrate restricted classification abilities because they are designed to extract local features alone or process global dependencies. We have designed a hybrid CNN-ViT framework that joins CNN spatial understanding with ViT global context to handle enhanced medical diagnosis classification. The implementation of SHAP Grad-Cam and LIME as XAI techniques allowed healthcare professionals to interpret the model's decision-making process for clinical use. The implemented hybrid CNN-ViT model performed better than standard CNN and ViT methods along with ResNet-50 and EfficientNet-B4 variants by reaching 92.5% accuracy along with 91.2% F1-score and 94.0% AUC-ROC. The model demonstrated excellent subtype discrimination according to ROC-AUC curves in addition to its low confusion matrix analysis misclassification rates. Results from ablation studies proved that the elimination of CNN ViT or XAI components substantially deteriorated model performance which demonstrates why a mixed and intelligible approach is required. The CNN-ViT and XAI model achieves both superior classification results and trustworthy operation characteristics according to the research findings thus proving suitable for clinical use. The research delivers an important solution for closing the clinical application distance between AI diagnostics systems by delivering a robust framework for diagnosing Alzheimer's disease.
ER  - 

TY  - CONF
TI  - Heart Disease Prediction Using AI
T2  - 2025 12th International Conference on Computing for Sustainable Global Development (INDIACom)
SP  - 1
EP  - 5
AU  - A. Srivastav
AU  - A. Pandey
AU  - S. Chaturvedi
PY  - 2025
KW  - Heart
KW  - Logistic regression
KW  - Accuracy
KW  - Medical services
KW  - Predictive models
KW  - Boosting
KW  - Artificial intelligence
KW  - Random forests
KW  - Monitoring
KW  - Diseases
KW  - Artificial intelligence
KW  - heart disease prediction
KW  - machine learning
KW  - Healthcare
DO  - 10.23919/INDIACom66777.2025.11115853
JO  - 2025 12th International Conference on Computing for Sustainable Global Development (INDIACom)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 12th International Conference on Computing for Sustainable Global Development (INDIACom)
Y1  - 2-4 April 2025
AB  - As the world keeps grappling with untimely deaths caused by heart diseases, due to lack of advanced diagnostics, AI is seen as a solution to this problem. Traditional methods of diagnosing cardiovascular diseases are no longer seen as adequate, greater improvement is needed to minimize preventable deaths caused by it and this is where AI comes in. Cardiovascular diagnostic models have been constructed in this paper to assess how accurate these models are and what issues they currently face. In this research, the primary focus was to develop an AI-based cardiovascular diagnostics model that cannot only meet but also exceed the sensitivity and specificity levels of the traditional diagnostics tools. Various algorithms were employed, some of them are as follows: Naive Bayes, Gradient Boosting, Decision Tree, Random Forest, Logistic Regression, K-Nearest Neighbors. Each algorithm is trained, retested, and altered through hyperparameter optimization. Cross-validation was also incorporated to validate the models while simultaneously reducing bias. The use of these multiple algorithms in this form assures the end user of the effectiveness of the designed models. Random Forest secured the best performance in the K-fold evaluation and achieved the highest accuracy level of 94 %. The F1 Score was equal to 93.95 further supporting the model's effectiveness in distinguishing patterns from the trained dataset.Gradient Boosting was also able to showcase convincing results with a precision equal to 94.94 and F1 score equal to 92.59.
ER  - 

TY  - CONF
TI  - Enhancing Medical Image Segmentation with 3D U-Net and Explainable AI for Trustworthy Healthcare
T2  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
SP  - 1
EP  - 6
AU  - V. K V
AU  - S. M H
PY  - 2025
KW  - Deep learning
KW  - Image segmentation
KW  - Solid modeling
KW  - Visualization
KW  - Three-dimensional displays
KW  - Explainable AI
KW  - Computational modeling
KW  - Medical services
KW  - Usability
KW  - Medical diagnostic imaging
KW  - Medical Image Segmentation
KW  - 3D U-Net
KW  - Explainable AI
KW  - Deep Learning
KW  - Interpretability
KW  - Grad-CAM
KW  - Un-certainty Quantification
DO  - 10.1109/ICTEST64710.2025.11042801
JO  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
Y1  - 3-5 April 2025
AB  - Medical image segmentation is a crucial process for computer-aided diagnosis that uses precise delineation of anatomical structures to aid in clinical decisions. Deep learning models such as 3D U-Net have proven to be the most outstanding performers in the segmentation of volumetric medical images through technologies like CT and MRI scans. However, the lack of interpretability in these models raises concerns about their reliability, and the real-world medical applications would suffer from such issues. In this paper, we propose a 3D U-Net-based seg-mentation framework in conjunction with Explainable AI (XAI) methods to help model transparency and interpretability. We use Grad-CAM map to show the decision-making process of the network thereby making sure that segmentation outputs correctly represent clinically relevant features. We test our concept on publicly available medical imaging datasets, demonstrating how we have improved segmentation accuracy alongside presenting the model’s predictions with meaningful visual explanations. By drawing the link between deep learning efficiency and clinical interpretability, we hope our work will win the public’s trust in AI-assisted diagnostics and thus will make the technology more popular in real-world healthcare settings.
ER  - 

TY  - CONF
TI  - Hybrid Deep Learning and Machine Learning for Brain Tumor Detection: CNN Meets Random Forest
T2  - 2025 6th International Conference for Emerging Technology (INCET)
SP  - 1
EP  - 5
AU  - Yashu
AU  - V. Kukreja
PY  - 2025
KW  - Deep learning
KW  - Accuracy
KW  - Brain tumors
KW  - Medical services
KW  - Brain modeling
KW  - Convolutional neural networks
KW  - Reliability
KW  - Random forests
KW  - Standards
KW  - Medical diagnostic imaging
KW  - Education
KW  - Health
KW  - Brain Tumour Diagnosis
KW  - Deep Learning
KW  - Convolutional Neural Networks
KW  - Random Forest
KW  - LIME
DO  - 10.1109/INCET64471.2025.11140876
JO  - 2025 6th International Conference for Emerging Technology (INCET)
IS  - 
SN  - 2996-4490
VO  - 
VL  - 
JA  - 2025 6th International Conference for Emerging Technology (INCET)
Y1  - 23-25 May 2025
AB  - Medical imaging diagnosis heavily depends on precise and explainable AI models because brain tumor classification stands as an essential clinical task in the field. The research implements a deep learning framework which unifies Convolutional Neural Networks (CNNs) for feature extraction with Random Forest (RF) classification and Local Interpretable Model-Agnostic Explanations (LIME) interpretability. The proposed model received training and evaluation on 4,220 MRI images from the Kaggle dataset which displayed Glioma and Meningioma and Pituitary tumor contents. The proposed CNN-RF model reached 98.88% accuracy which surpassed traditional CNN network architectures. The LIME heatmaps validated how the model applied its decisions through meaningful clinical features which improved both transparency and reliability of its decisions. ROC curve analysis reported that the method achieved an AUC value of 0.988 which exceeded standard deep learning technology performance levels. A review of all tumor types showed that the confusion matrix contained only minor errors to validate broad usage across each type. The examination shows that the CNN-RF hybrid model presents itself as a dependable alternative that provides interpretability over black-box AI systems. This study provides essential contributions to AI trustworthiness development in healthcare through its accurate and explainable diagnostic framework that will help medical practice adopt AI-driven diagnostics at a higher level.
ER  - 

TY  - CONF
TI  - Explainable AI for Disease Detection and Body Constitution Analysis Using Tongue Imaging: A Deep Learning Approach for Non-Invasive Diagnostics
T2  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
SP  - 1
EP  - 6
AU  - S. M. G
AU  - N. N. Das
AU  - N. Sunil
AU  - N. V
AU  - M. Rajeev
AU  - S. Gireesan
PY  - 2025
KW  - Deep learning
KW  - Pathology
KW  - Tongue
KW  - Accuracy
KW  - Explainable AI
KW  - Medical diagnosis
KW  - Lesions
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - Diseases
KW  - Explainable AI
KW  - Convolutional Neural Network
KW  - Traditional Chinese Medicine
KW  - Tongue Imaging
KW  - Non-Invasive Tongue Diagnosis
DO  - 10.1109/ICTEST64710.2025.11042469
JO  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2025 2nd International Conference on Trends in Engineering Systems and Technologies (ICTEST)
Y1  - 3-5 April 2025
AB  - Deep learning, specifically CNNs and multi-task learning models, is revolutionizing the task of disease classification in tongue imaging. This has been applied for reporting latest results on detection of pathological characteristics such as coatings, cracks, and discoloration. Hybrid frameworks, which have combined segmentation and classification together, improved the diagnostic accuracy especially when distinguishing between related disorders. Combining data-driven methodologies with a knowledge base on traditional medicine, AI models will be able to present comprehensive health diagnoses. Integration of these scalable solutions into a simple, mobile application can make such solutions available to a much wider audience. This breakthrough demonstrates how deep learning can transform healthcare using non-invasive, precise, and objective diagnostic tools. All these models will get further refinements such as lesion types and large datasets, eventually culminating into better accurate trustworthy medical diagnostics.
ER  - 

TY  - CONF
TI  - Advancement in Explainable AI: Bringing Transparency and Interpretability to Machine Learning Models for Use in High-Stakes Decisions
T2  - 2025 International Conference on Emerging Smart Computing and Informatics (ESCI)
SP  - 1
EP  - 6
AU  - R. David
AU  - H. Shankar
AU  - P. Kura
AU  - K. Kowtarapu
AU  - U. M. S
AU  - S. Karkuzhali
PY  - 2025
KW  - Measurement
KW  - Ethics
KW  - Analytical models
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Decision making
KW  - Closed box
KW  - Medical services
KW  - Stakeholders
KW  - Transparency in AI
KW  - High-Stakes Decision-Making
KW  - Model Interpretability Techniques
KW  - Black-Box Models
KW  - LIME
KW  - SHAP
KW  - Inherently Interpretable Models
KW  - User-Centric Explainability
DO  - 10.1109/ESCI63694.2025.10988079
JO  - 2025 International Conference on Emerging Smart Computing and Informatics (ESCI)
IS  - 
SN  - 2996-1815
VO  - 
VL  - 
JA  - 2025 International Conference on Emerging Smart Computing and Informatics (ESCI)
Y1  - 5-7 March 2025
AB  - AI and ML have rapidly altered healthcare, finance, and criminal justice. These models are being used for high-stakes decision-making, raising questions about their openness and interpretability. Explainable AI (XAI), a subfield of machine learning that makes models more intelligible to users, is advancing in key applications where decisions can have a major influence on individuals and society. We begin by discussing black-box models' core issues, which can cause distrust and ethical issues by hiding the decision-making process. We show how model transparency has improved by analysing modern methods including model-agnostic methods, interpretable models, and visualisation tools. LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) can help stakeholders understand how input features affect outcomes. We also emphasise the need of domain expertise in designing successful interpretability methods and user-centric explanations customised to stakeholders' demands. XAI's effects on regulatory compliance, ethical AI practices, and stakeholder confidence are investigated, especially under governance frameworks that require AI accountability and transparency. Finally, we propose developing standardised explainability evaluation metrics, including human factors in interpretability frameworks, and exploring hybrid models that balance accuracy and transparency to bridge the gap between AI advancements and interpretability. This work promotes solutions that empower people to make informed decisions based on trustworthy, interpretable AI systems, contributing to the ethical deployment of AI technologies.
ER  - 

TY  - CONF
TI  - XAI-PSSGAN: Perception-Enhanced Spectrum Shift Generative Adversarial Network with Explainable AI System for NIR-II Fluorescence Molecular Imaging
T2  - 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)
SP  - 1
EP  - 4
AU  - L. Fu
AU  - B. Lu
AU  - L. Li
AU  - X. Shi
AU  - J. Tian
AU  - Z. Hu
PY  - 2025
KW  - Training
KW  - Image resolution
KW  - Explainable AI
KW  - Statistical analysis
KW  - Noise
KW  - Imaging
KW  - Fluorescence
KW  - Generative adversarial networks
KW  - Real-time systems
KW  - Probes
KW  - Biomedical image enhancement
KW  - cycle-consistent generative adversarial network
KW  - generative AI
KW  - explainability
KW  - NIR - II imaging
DO  - 10.1109/ISBI60581.2025.10980871
JO  - 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)
IS  - 
SN  - 1945-8452
VO  - 
VL  - 
JA  - 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)
Y1  - 14-17 April 2025
AB  - Fluorescence imaging in the second near-infrared window (NIR-II) facilitates the real-time optical contrast for in vivo biomedical imaging. However, the detection noise is an inevitable byproduct of the real-time imaging process. Additionally, fluorescent probes currently approved for clinical use remain limited within the wavelength of NIR-IIa (1000–1300 nm). NIR-IIb (1500–1700 nm) imaging has the potential to deliver higher resolution than NIR-IIa, but relies on toxic fluorescent probes not approved for human use. To support clinical high-precision imaging, we propose a NIR-II fluorescence enhanced imaging method based on perception-enhanced spectrum shift generative adversarial network with explainable AI systems (xAI-PSSGAN). The proposed method tackles the problem of limited unpaired data training, and generates NIR-IIb resembling images with enhanced resolution and reduced noise. Experimental results demonstrate that proposed method effectively improves the imaging quality and surpasses competing alternatives, which is of great significance to promote the high-quality NIR-II imaging in clinical practice.
ER  - 

TY  - CONF
TI  - Grad-CAM & Grad-CAM++ for Explainable Oral Squamous Cell Carcinoma Detection using Deep Learning on Orthopantomograms
T2  - 2025 International Conference on Sensors and Related Networks (SENNET) Special Focus on Digital Healthcare(64220)
SP  - 1
EP  - 6
AU  - A. B. George
AU  - S. Bathini
AU  - G. A
PY  - 2025
KW  - Deep learning
KW  - Heating systems
KW  - Visualization
KW  - Squamous cell carcinoma
KW  - Explainable AI
KW  - Decision making
KW  - Sensors
KW  - Convolutional neural networks
KW  - Diagnostic radiography
KW  - Cancer
KW  - OPG Radiographs
KW  - Explainable AI
KW  - OSCC Detection
KW  - Grad-Cam
KW  - Grad-Cam++
KW  - Deep Learning
DO  - 10.1109/SENNET64220.2025.11136014
JO  - 2025 International Conference on Sensors and Related Networks (SENNET) Special Focus on Digital Healthcare(64220)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Sensors and Related Networks (SENNET) Special Focus on Digital Healthcare(64220)
Y1  - 24-27 July 2025
AB  - Oral squamous cell carcinoma (OSCC), accounting for 90% of oral cancers, often eludes early diagnosis due to asymptomatic progression, necessitating advanced detection methods. While deep learning has shown promise in histopathological OSCC analysis, radiographic detection using orthopantomogram (OPG) images remains underexplored. This study addresses this gap by developing an explainable deep learning framework combining pre-trained convolutional neural networks (CNNs) with Gradient-weighted Class Activation Mapping (Grad-CAM) for OSCC detection in OPG radiographs. We trained & evaluated multiple CNN architectures using our compiled dataset containing 809 OPGs, with DenseNet121 achieving superior performance of 98.15% accuracy. The integrated Grad-CAM visualizations identified critical radiographic features influencing predictions, particularly in posterior mandible regions, aligning with clinical diagnostic patterns. These heatmaps provide transparent decision-making insights, bridging the gap between AI-driven diagnostics and clinical interpretability.
ER  - 

TY  - CONF
TI  - IoT Security Enhancements in Smart Healthcare Using Federated Learning
T2  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
SP  - 263
EP  - 267
AU  - S. Lanka
AU  - T. Moodhitaporn
PY  - 2025
KW  - Training
KW  - Sentiment analysis
KW  - Federated learning
KW  - Distributed databases
KW  - Medical services
KW  - Network security
KW  - Smart systems
KW  - Sensor systems
KW  - Internet of Things
KW  - Servers
KW  - Federated Learning (FL)
KW  - Internet of Things (IoT)
KW  - security
KW  - smart healthcare
KW  - data
KW  - Machine Learning (ML)
DO  - 10.1109/ICSADL65848.2025.10933330
JO  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL)
Y1  - 18-20 Feb. 2025
AB  - Federated learning (FL) is a distributive machine learning (ML) approach that makes use of a centralised server to assist several Internet of Things (IoT) devices in cooperatively training an ML model. IoT device local data is safeguarded since it never leaves the device in FL. Since distributed IoT devices in FL often gather their local data on their own, each device's data set may naturally constitute a unique source domain. In this research work, there will be maintaining the security enhancements for smart healthcare using federated learning. There will be securing the data of IoT -based sensors which may lose local data while using the smart systems. Removing this issue will give importance to local data also using the FL method. Thus, this research involves considering different research work and gives the research methodology for the security-enhancing process using FL in IoT enhanced smart healthcare.
ER  - 

TY  - CONF
TI  - Integrating Explainability in AI for Retinal Imaging: Enhancing Diabetic Retinopathy Diagnosis Accuracy
T2  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
SP  - 2083
EP  - 2086
AU  - S. Sharma
AU  - K. P. Sharma
AU  - K. Saini
PY  - 2025
KW  - Surveys
KW  - Diabetic retinopathy
KW  - Accuracy
KW  - Explainable AI
KW  - Scalability
KW  - Imaging
KW  - Grey matter
KW  - Medical services
KW  - Retina
KW  - Security
KW  - XAI
KW  - Explainable Artificial Intelligence (XAI)
KW  - Diabetic Retinopathy (DR)
KW  - Retinal Imaging
KW  - Grad-CAM
KW  - LIME
KW  - Multi-Stage Detection
KW  - Personalized Diagnosis
KW  - AI Scalability
KW  - Clinical Trust
KW  - Healthcare Diagnostics
DO  - 10.1109/ICCSAI64074.2025.11063736
JO  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
IS  - 
SN  - 
VO  - 3
VL  - 3
JA  - 2025 3rd International Conference on Communication, Security, and Artificial Intelligence (ICCSAI)
Y1  - 4-6 April 2025
AB  - Diabetic retinopathy (DR: preventable cause of blindness till date as it is irreversible till now.) Widely available adoption of clinical features is hard because traditional models have not been interpretable; AI (Artificial Intelligence) has so far made an exception in automating diagnosis for diabetic retinopathy. Explainable AI enables the generation of raw, simple results by machines which one as a human might agree on and hence gets re-empowers physician. In this paper, Explainable AI framework is discussed in detail. This is a novel approach to deal with the diabetic retinopathy which deals with various issues such as low-labelled-data requirement, lack of validations among various populations and sub-optimal performance in real and unseen data. The paper presents the framework that includes scalable models for resource-constrained environments, multistage detection network and differential-patient diagnosis. The framework is also generalize with the latest interpretability frameworks (i.e. Grad-CAM and LIME) where explanation can be given to clinically grey matter while keeping accuracy. Clinical multi-centre research done shown that near 94.5% accuracy versus current 88% solutions currently using the framework can lead to substantial improvements in the diagnosis from the framework. Furthermore proof-of-concept studies in a smaller setting have shown that it is scalable and physician surveys found 40% increase in trust so too. The vast and critical knowledge gaps are filled by the presented work, allowing for solution to enhance the diagnostic scalability accuracy and transparency. These results explain the support of explainable AI towards the retinal imaging forward, especially at diabetic retinopathy diagnosis and will towards new breakthroughs in clinical predictive AI.
ER  - 

TY  - CONF
TI  - Early Detection of Chronic Kidney Disease in Diabetic Patients using Explainable AI
T2  - 2025 5th International Conference on Pervasive Computing and Social Networking (ICPCSN)
SP  - 948
EP  - 954
AU  - S. V
AU  - D. S
AU  - D. K. M
AU  - S. M
AU  - V. K
PY  - 2025
KW  - Support vector machines
KW  - Logistic regression
KW  - Accuracy
KW  - Explainable AI
KW  - Social networking (online)
KW  - Biological system modeling
KW  - Chronic kidney disease
KW  - Reliability
KW  - Random forests
KW  - Diseases
KW  - Diabetes-induced CKD
KW  - Sustainable healthcare
KW  - Machine Learning
KW  - Explainable AI (XAI)
KW  - Early Detection
KW  - SHAP
KW  - Logistic Regression
DO  - 10.1109/ICPCSN65854.2025.11035905
JO  - 2025 5th International Conference on Pervasive Computing and Social Networking (ICPCSN)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 5th International Conference on Pervasive Computing and Social Networking (ICPCSN)
Y1  - 14-16 May 2025
AB  - Chronic kidney disease – is an increasing stage that substantially contributes to global morbidity and mortality. Early detection is important for slowing disease advancement and enhancing patient outcomes. However, conventional diagnostic methods often fail to provide early alerts due to the mild and progressive onset of CKD. This work utilizes machine learning methods to improve early detection accuracy while maintaining model clarity through Explainable AI (XAI). A detailed dataset was pre-processed, incorporating missing value analysis to enhance data integrity. Machine learning models, including Linear Discriminant Analysis (LDA), Random Forest (RF), Support Vector Machine (SVM), and Logistic Regression (LR), were used and examined for their predictive effectiveness. Logistic Regression was chosen for additional analysis using SHapley Additive exPlanations (SHAP) due to its linear structure, which enhances a more logical interpretation of attribute influences. SHAP was utilized to detect key biomarkers influencing CKD advancement, thereby improving model clarity and aiding clinical judgment. This work aims to improve CKD detection using ML and AI techniques, promising model interpretability and reliability in medical fields.
ER  - 

TY  - CONF
TI  - Leveraging Expert Input for Robust and Explainable AI-Assisted Lung Cancer Detection in Chest X-rays
T2  - 2025 IEEE 13th International Conference on Healthcare Informatics (ICHI)
SP  - 576
EP  - 587
AU  - A. Rafferty
AU  - R. Ramaesh
AU  - A. Rajan
PY  - 2025
KW  - Limiting
KW  - Explainable AI
KW  - Perturbation methods
KW  - Lung cancer
KW  - Robustness
KW  - Medical diagnosis
KW  - Informatics
KW  - X-ray imaging
KW  - Medical diagnostic imaging
KW  - Resilience
KW  - Explainable AI
KW  - Medical Imaging
KW  - Trustworthiness
KW  - Interpretability
KW  - Robustness
KW  - Machine Learning
KW  - Lung Cancer
DO  - 10.1109/ICHI64645.2025.00071
JO  - 2025 IEEE 13th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2025 IEEE 13th International Conference on Healthcare Informatics (ICHI)
Y1  - 18-21 June 2025
AB  - Deep learning models show significant potential for advancing AI-assisted medical diagnostics, particularly in detecting lung cancer through medical image modalities such as chest X-rays. However, the black-box nature of these models poses challenges to their interpretability and trustworthiness, limiting their adoption in clinical practice. This study examines both the interpretability and robustness of a high-performing lung cancer detection model based on InceptionV3, utilizing a public dataset of chest X-rays and radiological reports. We evaluate the clinical utility of multiple explainable AI (XAI) techniques, including both post-hoc and ante-hoc approaches, and find that existing methods often fail to provide clinically relevant explanations, displaying inconsistencies and divergence from expert radiologist assessments. To address these limitations, we collaborated with a radiologist to define diagnosis-specific clinical concepts and developed ClinicXAI, an expert-driven approach leveraging the concept bottleneck methodology. ClinicXAI generated clinically meaningful explanations which closely aligned with the practical requirements of clinicians while maintaining high diagnostic accuracy. We also assess the robustness of ClinicXAI in comparison to the original InceptionV3 model by subjecting both to a series of widely utilized adversarial attacks. Our analysis demonstrates that ClinicXAI exhibits significantly greater resilience to adversarial perturbations. These findings underscore the importance of incorporating domain expertise into the design of interpretable and robust AI systems for medical diagnostics, paving the way for more trustworthy and effective AI solutions in healthcare.
ER  - 

TY  - CONF
TI  - Mobile App for Multi-Disease Detection in Lungs Using CNN
T2  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
SP  - 1
EP  - 6
AU  - M. Kiruthikha
AU  - S. Prabakar
AU  - J. J. Leo
AU  - M. M. Firdous
AU  - R. Srikash
PY  - 2025
KW  - Deep learning
KW  - Pneumonia
KW  - Accuracy
KW  - Quantization (signal)
KW  - Computational modeling
KW  - Medical services
KW  - Real-time systems
KW  - Mobile applications
KW  - Convolutional neural networks
KW  - X-ray imaging
KW  - Lung Disease Detection
KW  - Convolutional Neural Networks
KW  - Chest X-Ray Analysis
KW  - Deep Learning
KW  - Mobile Health Applications
KW  - AI in Healthcare
DO  - 10.1109/AIMLA63829.2025.11041147
JO  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
Y1  - 29-30 April 2025
AB  - Diseases of the lung like pneumonia, tuberculosis, and COPD are important global health concerns that commonly result in serious complications or death from delayed or incorrect diagnoses. Conventional approaches to diagnosis like chest X-rays and biopsies are largely dependent on skilled interpretation that is time- consuming and subject to human error. This work suggests the creation of a mobile app that utilizes Convolutional Neural Networks (CNNs) to automatically detect and classify various lung diseases based on chest X-ray images. The app shall be created as an API-based system to make it accessible and process data in real-time for medical practitioners. Using deep learning methods, the system seeks to improve diagnostic accuracy, minimize the need for human interpretation, and deliver fast insights into lung diseases. Early model training on pre-processed datasets has shown promising classification accuracy, especially between bacterial and viral pneumonia. The intended application will be a deployable and scalable solution, enabling early diagnosis and enhancing healthcare accessibility, particularly in resource-constrained environments.
ER  - 

TY  - CONF
TI  - Interpreting Cervical Cancer Risk Predictions Using Optimized Random Forest and Explainable AI Techniques
T2  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
SP  - 1677
EP  - 1682
AU  - S. G
AU  - N. Meenakshisundaram
PY  - 2025
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Predictive models
KW  - Medical diagnosis
KW  - Risk management
KW  - Cervical cancer
KW  - Random forests
KW  - Medical diagnostic imaging
KW  - Resilience
KW  - Machine Learning Algorithms
KW  - Cervical Cancer
KW  - Random Forest
KW  - Explainable AI (XAI)
KW  - LIME
DO  - 10.1109/ICOEI65986.2025.11013521
JO  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 8th International Conference on Trends in Electronics and Informatics (ICOEI)
Y1  - 24-26 April 2025
AB  - Cervical cancer remains a major public health concern, requiring accurate and interpretable risk prediction tools for early diagnosis. Machine learning models often lack transparency, limiting their adoption in clinical settings. This study proposes a robust framework using an optimized Random Forest classifier for cervical cancer risk prediction, combined with Explainable AI (XAI) techniques such as LIME. The Random Forest model is optimized using key hyperparameters to enhance predictive accuracy. LIME provides local explanations, visualizing feature contributions for individual predictions, while global feature importance identifies the most influential risk factors. To ensure clinical relevance, feature scaling reversal is applied, enabling interpretable outputs. The proposed framework achieves high accuracy, offers transparent decision-making, and enhances trust among medical professionals. This study demonstrates the potential of explainable machine learning for improving cervical cancer diagnosis.
ER  - 

TY  - CONF
TI  - Transforming Obesity Care Through Artificial Intelligence: Real-Case Implementations and Personalized Solutions
T2  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
SP  - 1
EP  - 5
AU  - M. Al-Remawi
AU  - R. A. Abdel-Rahem
AU  - F. Al-Akayleh
AU  - F. Aburub
AU  - A. S. A. Ali Agha
PY  - 2025
KW  - Deep learning
KW  - Obesity
KW  - Ethics
KW  - Weight control
KW  - Force measurement
KW  - Imaging
KW  - Medical services
KW  - Genetics
KW  - Real-time systems
KW  - Wearable sensors
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Obesity
KW  - Precision Nutrition
KW  - Health Informatics
DO  - 10.1109/ICCIAA65327.2025.11013698
JO  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 1st International Conference on Computational Intelligence Approaches and Applications (ICCIAA)
Y1  - 28-30 April 2025
AB  - Obesity is a multifactorial condition that significantly increases the risk of metabolic syndrome, cardiovascular disease, and various other comorbidities. Conventional obesity management strategies-encompassing body mass index (BMI) measurements, dietary counseling, and caloric tracking-often fail to account for the heterogeneity of individual genetic, behavioral, and metabolic factors. Artificial intelligence (AI), particularly machine learning (ML) and deep learning (DL), has emerged as a transformative approach, offering data-driven, personalized solutions that integrate electronic health records (EHRs), wearable sensor data, and multi-omics information. This article examines contemporary AI applications for obesity care, highlighting real-case implementations, technical and ethical considerations, and the future potential for optimizing patient outcomes. By leveraging advanced analytics, healthcare providers can enhance diagnostic precision, tailor interventions to individualized needs, and support sustained weight control, thus redefining the paradigm of obesity management.
ER  - 

TY  - CONF
TI  - Investigating the Impact of Artificial Intelligence on Cybersecurity in the Global Health Sector
T2  - 2025 IST-Africa Conference (IST-Africa)
SP  - 1
EP  - 10
AU  - H. Moongela
AU  - T. Mayayise
PY  - 2025
KW  - Ethics
KW  - Data protection
KW  - Medical services
KW  - Threat assessment
KW  - Real-time systems
KW  - Malware
KW  - Artificial intelligence
KW  - Computer security
KW  - Cyberattack
KW  - Systematic literature review
KW  - Artificial Intelligence
KW  - Cybersecurity
KW  - Healthcare
DO  - 10.23919/IST-Africa67297.2025.11060545
JO  - 2025 IST-Africa Conference (IST-Africa)
IS  - 
SN  - 2576-8581
VO  - 
VL  - 
JA  - 2025 IST-Africa Conference (IST-Africa)
Y1  - 28-30 May 2025
AB  - This study explores the impact of Artificial Intelligence (AI) on the cybersecurity of global healthcare systems. While AI plays a crucial role in enhancing the defence mechanisms of healthcare systems and detecting fraud, it can also be employed to develop malware and other forms of cyber-attacks. A systematic literature review, guided by the PRISMA flow diagram, was conducted on peerreviewed journal articles and conference papers published since 2019-2024 from the SCOPUS database. The findings indicate that AI strengthens healthcare cybersecurity, but is also used to generate cyber-attacks, including malware, adversarial attacks, and the exploitation of sensitive training data. The study emphasises the need for further measures in AI to mitigate the growing cyber risks in the health sector.
ER  - 

TY  - CONF
TI  - Artificial Intelligence in Alzheimer’s Diagnosis: A Comparative Review of Machine Learning and Deep Learning Approaches
T2  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
SP  - 1
EP  - 6
AU  - T. N. Charanya
AU  - M. A
AU  - N. S
PY  - 2025
KW  - Deep learning
KW  - Support vector machines
KW  - Technological innovation
KW  - Sensitivity
KW  - Reviews
KW  - Explainable AI
KW  - Computational modeling
KW  - Data models
KW  - Alzheimer's disease
KW  - Standards
DO  - 10.1109/ICDSAAI65575.2025.11011613
JO  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
Y1  - 28-29 March 2025
AB  - Alzheimer's Disease (AD) is a neurological condition that progresses over time, making early detection and treatment complex. Recently, artificial intelligence (AI) is used as medical tools for predicting illnesses. With the emergence of artificial intelligence as a new wave in healthcare, providing data-driven approaches to disease prediction, traditional computational approaches are compared with modern AI-based methods in diagnosing AD in this review. Whereas traditional ML techniques favor more simple, interpretable models, DL is generally more capable of dealing with high-dimensional, intricate data, particularly from imaging modalities like MRI. Additionally, the use of explainable AI is more transparent and clinically useful. Hybrid approaches, also based on the leverage of the power of different AI approaches, hold promise in circumventing issues like data heterogeneity and model interpretability are also addressed. In this research, the aim is to promote increased effort towards optimizing the sensitivity and promptness of AD diagnosis through cutting-edge AI method by examining the strengths and weaknesses of such approaches. Keywords: Machine learning, deep learning, Alzheimer's, disease, prediction, algorithms, Logistic Regression, Decision trees, SVM, conventional neural networks, XAI techniques, predictive modeling.
ER  - 

TY  - CONF
TI  - Integrating Machine Learning with Explainable AI in Healthcare Analytics for Diabetes Prediction
T2  - 2025 Eleventh International Conference on eDemocracy & eGovernment (ICEDEG)
SP  - 179
EP  - 189
AU  - K. A. Bamfo
AU  - D. Zeidan
AU  - L. Terán
PY  - 2025
KW  - Analytical models
KW  - Logistic regression
KW  - Accuracy
KW  - Explainable AI
KW  - Medical services
KW  - Predictive models
KW  - Diabetes
KW  - Glucose
KW  - Random forests
KW  - Synthetic data
KW  - explainable artificial intelligence
KW  - SHAP
KW  - ELI5
KW  - electronic medical records
KW  - diabetes
DO  - 10.1109/ICEDEG65568.2025.11081581
JO  - 2025 Eleventh International Conference on eDemocracy & eGovernment (ICEDEG)
IS  - 
SN  - 2573-1998
VO  - 
VL  - 
JA  - 2025 Eleventh International Conference on eDemocracy & eGovernment (ICEDEG)
Y1  - 18-20 June 2025
AB  - This study introduces an innovative and comprehensive framework for predicting diabetes, skillfully integrating classical machine learning models with explainable artificial intelligence (XAI) techniques. This integration not only enhances predictive accuracy but also significantly improves clinical interpretability, making the results more accessible to healthcare professionals. To achieve this, we utilized a carefully constructed synthetic electronic medical records (EMR) dataset that closely replicates real-world trends associated with Type I diabetes. Our analysis involved training logistic regression and random forest models, demonstrating competitive performance levels. Distinct from previous research that often prioritizes accuracy at the expense of interpretability, our approach uniquely harnesses the capabilities of SHAP and ELI5 to deliver detailed, feature-specific explanations for each model prediction. This dual focus not only provides insights into the critical risk factors influencing diabetes-such as glucose levels, body mass index (BMI), and age-but also tackles the potential biases that can arise from using synthetic data. The findings from our study present a replicable framework that effectively bridges the divide between highperformance predictive modeling and the pressing need for clear, transparent insights in healthcare analytics. The implications of our research aim to empower practitioners with actionable information that can enhance patient care and outcomes in diabetes management.
ER  - 

TY  - CONF
TI  - Brain Tumor Classification and Detection by Explainable AI, Xception and EfficientNet Models for Improving Performance Metrices
T2  - 2025 5th International Conference on Intelligent Technologies (CONIT)
SP  - 1
EP  - 6
AU  - N. Gangala
AU  - P. K. Kalangi
AU  - P. Vutti
AU  - A. A. Jajimuga
AU  - B. P. Goud
AU  - G. S. Krishna
PY  - 2025
KW  - Deep learning
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Reviews
KW  - Magnetic resonance imaging
KW  - Transfer learning
KW  - Brain tumors
KW  - Transforms
KW  - Brain modeling
KW  - Explainable AI
KW  - xception
KW  - efficientnet
KW  - MRI
KW  - Brain tumor
KW  - classification
DO  - 10.1109/CONIT65521.2025.11167506
JO  - 2025 5th International Conference on Intelligent Technologies (CONIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 5th International Conference on Intelligent Technologies (CONIT)
Y1  - 20-22 June 2025
AB  - The brain tissue develops tumors through abnormal cellular growth that either remains benign or transforms into malignancy. The accurate detection together with proper classification of brain tumors leads to decisions about treatment options and enhances patient survival potential. MRI scan interpretation through radiological assessment for brain tumor diagnosis currently depends on lengthy manual review by experts who face human interpretive bias along with errors. The current diagnostic obstacles emphasize why automated precise and interpretable medical diagnostics systems need development. This research develops an explainable deep learning system by applying transfer learning models Xception and EfficientNetB0 to perform multiclass brain tumor identification through MRI image analysis. The proposed framework handles deep learning system black boxes through Grad-CAM (Gradient-weighted Class Activation Mapping) implementation as an explainable AI (XAI) technique. Part of this framework shows visual explanations about model decisions through MRI regions that most heavily influence classification results. The proposed method succeeds in both achieving accurate diagnoses and delivering diagnostic explanations to healthcare practitioners thus creating conditions for AI-driven diagnosis acceptance. Medical professionals utilize the framework because it grants them explicit information about how predictions are generated to enable better decision-making. This tool possesses the capability to transform brain tumor diagnosis by delivering customized time-sensitive treatments which enhance patient results.
ER  - 

TY  - CONF
TI  - Optimizing Disease Diagnosis and Treatment Through AI and Deep Learning Algorithms
T2  - 2025 6th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)
SP  - 98
EP  - 102
AU  - J. Vedula
AU  - T. A. Kakani
AU  - R. Gupta
AU  - M. Mohammed
AU  - K. Hudani
AU  - J. Logeshwaran
PY  - 2025
KW  - Deep learning
KW  - Ethics
KW  - Data privacy
KW  - Recurrent neural networks
KW  - Prediction algorithms
KW  - Natural language processing
KW  - Medical diagnosis
KW  - Convolutional neural networks
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - Neural Networks
KW  - Natural Language Processing
KW  - Artificial Intelligence
KW  - Disease Diagnosis
KW  - Deep Learning
KW  - ML
DO  - 10.1109/ICICV64824.2025.11085913
JO  - 2025 6th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 6th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)
Y1  - 17-19 June 2025
AB  - A Primer for Cancer Center Leaders Session 2 Natural Language Processing for Biomedical Text Medical data is not only numeric but also composed of unstructured text. These algorithms listen to various medical imaging, genomic data, and electronic health records to find correlations that can predict different diseases. Using convolutional neural networks to analyze images and recurrent neural networks to process sequential data, AI systems improve diagnostic accuracy and minimize the risk of human error. Additionally, deep learning algorithms targets patient-oriented drug administration by predicting therapeutic responses of individual patients, enhancing treatment response. Incorporating AI into clinical workflows allows us to synthesize vast datasets in real-time, provide clinicians with action items, and advocate for evidence-based medicine. However, problems including data privacy, model interpretability, and the need for large, annotated datasets continue. Such solutions in the form of explainable AI and deep learning would play an integral role in promoting the usage of these technologies over a longer duration in the medical ecosystem. This work shows how AI and deep learning can open avenues that may fundamentally change disease detection and treatments, leading to improved diagnosis and treatments tailored to the individual patient.
ER  - 

TY  - CONF
TI  - Optimizing Symptom Prediction in Healthcare: An Integrative Machine Learning Approach to Regression and Classification Models
T2  - 2025 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)
SP  - 1
EP  - 6
AU  - N. R. Goddilla
AU  - A. Sivasangari
PY  - 2025
KW  - Training
KW  - Analytical models
KW  - Accuracy
KW  - Machine learning
KW  - Predictive models
KW  - Complexity theory
KW  - Medical diagnosis
KW  - Ensemble learning
KW  - Random forests
KW  - Diseases
KW  - Integrative machine learning
KW  - disease symptom prediction
KW  - Regression algorithms
KW  - Classification algorithms
KW  - Healthcare analytics
KW  - Ensemble methods
KW  - Feature engineering
KW  - Imbalanced data
KW  - Random Forest classifier
DO  - 10.1109/IITCEE64140.2025.10915417
JO  - 2025 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)
Y1  - 16-17 Jan. 2025
AB  - This research explores the application of combined machine learning techniques for predicting disease symptoms, aiming to support early diagnosis and improve healthcare outcomes. By analyzing both regression and classification models, the study evaluates their respective performance, strengths, and challenges in predicting symptoms. Using a comprehensive dataset with medical records, demographic data, and clinical variables, we developed predictive models to capture the complexities of healthcare data. To enhance accuracy and manage issues like data imbalance and feature diversity, ensemble methods and feature engineering were applied, with the Random Forest classifier achieving a leading accuracy of 91.84%. Additionally, the study emphasizes model interpretability, which is crucial for healthcare professionals to trust and adopt these predictive tools. The findings provide valuable guidance for healthcare professionals and data scientists in choosing machine learning approaches suited to specific medical applications. By bridging the roles of regression and classification models, this research offers a well-rounded perspective on the potential of predictive analytics in healthcare.
ER  - 

TY  - CONF
TI  - From Data to Diagnosis: A Review of Machine Learning Models for Postpartum Depression Prediction
T2  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
SP  - 1
EP  - 6
AU  - S. S
AU  - S. C P
PY  - 2025
KW  - Training
KW  - Data privacy
KW  - Analytical models
KW  - Accuracy
KW  - Reviews
KW  - Medical services
KW  - Predictive models
KW  - Depression
KW  - Data models
KW  - Wearable sensors
KW  - PPD Prediction
KW  - HER
KW  - Machine Learning
KW  - AUC
DO  - 10.1109/AIMLA63829.2025.11040647
JO  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 3rd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
Y1  - 29-30 April 2025
AB  - A common mental health condition that affects new mothers, postpartum depression (PPD) has serious repercussions for the health of both the unborn child's and the mother's health. To lessen its effects, early detection and action are essential. By examining a variety of data sources, such as demographic data, electronic health records (EHR), social media activity, wearable sensor data, and genetic markers, machine learning (ML) models have become increasingly effective tools for predicting PPD in recent years. The methodologies, datasets, feature selection strategies, and predictive performance of several ML-based PPD prediction models are all compared in this review. Also examined at the difficulties these models present, such as biases in training datasets, data privacy, and interpretability and compared the Performance (AUC, Accuracy, etc.).
ER  - 

TY  - CONF
TI  - Explainable AI in Psychology: Enhancing Transparency and Trust in Mental Health Applications
T2  - 2025 International Conference on Computational Innovations and Engineering Sustainability (ICCIES)
SP  - 1
EP  - 7
AU  - P. U. G. Sri
AU  - T. L. Bharath
AU  - J. Umamageswaran
PY  - 2025
KW  - Surveys
KW  - Ethics
KW  - Technological innovation
KW  - Explainable AI
KW  - Mental health
KW  - Reliability theory
KW  - Predictive models
KW  - Cognition
KW  - Sustainable development
KW  - Random forests
KW  - Explainable AI (XAI)
KW  - Mental health diagnostics
KW  - Tree SHAP
KW  - Clinical interpretability
KW  - Random Forest Model
KW  - Ethical AI
KW  - Feature engineering
DO  - 10.1109/ICCIES63851.2025.11032593
JO  - 2025 International Conference on Computational Innovations and Engineering Sustainability (ICCIES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Computational Innovations and Engineering Sustainability (ICCIES)
Y1  - 24-26 April 2025
AB  - Integrating Explainable Artificial Intelligence (XAI) into psychology represents a transfor mational increase in diagnostics and psychotherapies. The research proposes a novel framework that extensively uses Tree SHAP, an XAI technique to enable global interpretability from ensemble models such as Random Forests. The framework addresses the evidence concerning the black-box-proof approach with AI systems: a transparent and interpretable framework. It is well suited to the utility of psychological practice and research. By providing intuition on the contribution of the features of the proposed model, clinicians can understand the possible reasons behind the predictions that engender trust and allow safe congruence with psychological theories. Upon validation of the psychological data sets, the framework demonstrated an increase in interpretability by 15% compared to the baseline methods. According to the survey, 92% of the clinicians found the explanations ’highly satisfactory.’ The model achieved a 20% reduction in false positives, improving algorithmic reliability, and reducing misdiagnosis. These improvements underscore Tree SHAP’s ability to boost model performance to assist reasoning and decision making in clinical examinations. In addition to application to diagnostics, this framework extends to applications in early intervention systems, personalized psychotherapy, and educational psychology. By integrating domain-specific interpretability, this framework ensures cross-cultural adaptability. This approach further manages ethical concerns, thus positioning it to operate well within a larger canvas for worldwide psychological application. Computational efficiency only helps its case for practicality in real-world clinical and research application settings. This new framework will establish high-level comfort in relation to scientific integrity while staying true to human values in the new premises of AI applications in psychology.
ER  - 

TY  - JOUR
TI  - Explainable AI for Sensor Signal Interpretation to Revolutionize Human Health Monitoring: A Review
T2  - IEEE Access
SP  - 115990
EP  - 116024
AU  - A. S. Alharthi
AU  - A. Alqurashi
AU  - T. Essa Alharbi
AU  - M. M. Alammar
AU  - N. Aldosari
AU  - H. R. E. H. Bouchekara
AU  - Y. A. Sha’aban
AU  - M. Shoaib Shahriar
AU  - A. Al Ayidh
PY  - 2025
KW  - Medical services
KW  - Monitoring
KW  - Explainable AI
KW  - Diseases
KW  - Robot sensing systems
KW  - Reviews
KW  - Biomedical monitoring
KW  - Prediction algorithms
KW  - Machine learning
KW  - Accuracy
KW  - Explainable AI human health
KW  - gait
KW  - Parkinson’s disease
KW  - stroke
KW  - depression
KW  - cancer
KW  - heart disease
KW  - Alzheimer’s disease
DO  - 10.1109/ACCESS.2025.3585764
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 13
VL  - 13
JA  - IEEE Access
Y1  - 2025
AB  - The complexity of sensor signal patterns in healthcare, coupled with the variability of physiological data present significant challenges in developing reliable diagnostic and monitoring. While machine learning (ML) has greatly advanced sensor-based health analysis, its decision-making processes often lack transparency, raising concerns about reliability and clinical adoption. This review explores the role of Explainable Artificial Intelligence (XAI) in enhancing the interpretability of ML models for sensor signal analysis, particularly in applications such as pressure sensors, wearable inertial sensors, imaging sensors (MRI, CT, X-ray), ECG, EEG, and wearable health tracking. A systematic literature review was conducted across multiple databases to identify studies applying XAI techniques to sensor-based health monitoring. The review categorizes nine trending XAI methods used to interpret ML-driven analyses of biosignals, evaluating their advantages and limitations in different healthcare scenarios. The findings emphasize the importance of transparency in ML-driven sensor analysis, which is critical for building trust and real-time clinical decision-making and wearable healthcare applications. Despite its potential, XAI for sensor signals faces challenges related to model scalability, real-time processing, and clinician interpretability. The review identifies key research gaps in integrating XAI into sensor-based healthcare systems, emphasizing the need for robust validation methods and user-friendly explanations. Ultimately, XAI offers a promising path toward revolutionizing sensor-driven health monitoring, though further advancements are necessary to fully integrate explainability into real-world clinical and assistive applications.
ER  - 

TY  - CONF
TI  - Federated Learning and Explainable AI for Analyzing Heterogeneous Healthcare Data
T2  - 2025 International Conference on Communication, Computing, Networking, and Control in Cyber-Physical Systems (CCNCPS)
SP  - 250
EP  - 255
AU  - K. K. Keba
AU  - N. K. Suryadevara
AU  - A. Negi
PY  - 2025
KW  - Training
KW  - Data privacy
KW  - Accuracy
KW  - Federated learning
KW  - Explainable AI
KW  - MIMICs
KW  - Medical services
KW  - Real-time systems
KW  - Usability
KW  - Biomedical imaging
KW  - Federated Learning
KW  - Explainable AI
KW  - ICU Medication Management
KW  - SHAP
KW  - LIME
KW  - Privacy-Preserving Analytics
KW  - Healthcare AI
KW  - XGBoost
DO  - 10.1109/CCNCPS66785.2025.11135638
JO  - 2025 International Conference on Communication, Computing, Networking, and Control in Cyber-Physical Systems (CCNCPS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Communication, Computing, Networking, and Control in Cyber-Physical Systems (CCNCPS)
Y1  - 10-12 June 2025
AB  - This paper presents an interpretable Federated Learning (FL) framework for predicting medication order cancellations in Intensive Care Unit (ICU) settings, balancing predictive performance with stringent patient privacy requirements. Our approach addresses the urgent demand for explainable AI in healthcare by integrating privacy-preserving distributed training with robust model interpretability. Leveraging heterogeneous data sources including MIMIC-III tabular records, medical imaging, and PDF-based lab reports we developed and evaluated two machine learning algorithms. XGBoost achieved the highest performance, with 92.24% accuracy, 92.23% precision, and an F1-score of 90.92%. To enhance trust and clinical usability, we applied SHAP and LIME for both global and local interpretability, uncovering critical features such as medication amount, infusion rate, and temporal factors influencing cancellation decisions. Our federated architecture enables secure, collaborative training across institutions, crucial for sensitive healthcare domains. This work lays a foundation for scalable, privacy-conscious, and interpretable AI systems in critical care, and sets the stage for future research involving multimodal data, including audio-visual inputs and real-time analytics.
ER  - 

TY  - CONF
TI  - Integrating Interpretability into Deep Learning Models for Mammogram-Based Breast Cancer Detection
T2  - 2025 IEEE International Conference on Information Reuse and Integration and Data Science (IRI)
SP  - 283
EP  - 288
AU  - J. Ndirangu
AU  - L. Bouzar-Benlabiod
PY  - 2025
KW  - Deep learning
KW  - Training
KW  - Accuracy
KW  - Predictive models
KW  - Performance metrics
KW  - Breast cancer
KW  - Mammography
KW  - Rough surfaces
KW  - Public healthcare
KW  - Tumors
KW  - Interpretability
KW  - XAI
KW  - DeepSHAP
KW  - PartitionSHAP
KW  - Breast Cancer Detection
KW  - CNN
DO  - 10.1109/IRI66576.2025.00060
JO  - 2025 IEEE International Conference on Information Reuse and Integration and Data Science (IRI)
IS  - 
SN  - 2835-5776
VO  - 
VL  - 
JA  - 2025 IEEE International Conference on Information Reuse and Integration and Data Science (IRI)
Y1  - 6-8 Aug. 2025
AB  - Breast cancer is one of the most common cancers found in women, affecting 1 in 8 Canadian women over their lifetime, according to the Public Health Agency of Canada. The early diagnosis of breast cancer can be extremely beneficial for reducing the spread of cancer cells and the risk of death. Machine learning (ML) systems can help healthcare professionals diagnose breast cancer with better accuracy by identifying underlying patterns in the images that radiologists might find difficult to detect early on. While deep learning (DL) models can obtain high accuracies, their black-box nature inhibits their ability to explain how they came to their conclusion, creating distrust between DL systems and radiologists. This research employs two CNN model architectures - AlexNet and Houby & Yassin's CNN - to classify recently obtained mammographic images from the KAU-BCMD dataset into BI-RAD categories (0-6). To resolve the problem of non-explainability, two variants of SHapley Additive exPlanations (SHAP), an explainability approach based on Shapley values, have been used to assess the role that each feature has in the prediction process.
ER  - 

TY  - CONF
TI  - Leveraging Artificial Intelligence for Colon Cancer: Trends, Challenges and Innovations
T2  - 2025 International Conference on Automation and Computation (AUTOCOM)
SP  - 364
EP  - 369
AU  - S. Takkar
AU  - M. Rakhra
PY  - 2025
KW  - Deep learning
KW  - Ethics
KW  - Technological innovation
KW  - Computational modeling
KW  - Buildings
KW  - Transfer learning
KW  - Surgery
KW  - Colorectal cancer
KW  - Data models
KW  - Artificial intelligence
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Deep Learning
KW  - Cancer Diagnosis
KW  - Cancer Treatment
KW  - Healthcare
DO  - 10.1109/AUTOCOM64127.2025.10956246
JO  - 2025 International Conference on Automation and Computation (AUTOCOM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 International Conference on Automation and Computation (AUTOCOM)
Y1  - 4-6 March 2025
AB  - Artificial Intelligence (AI) has made huge enhancements in colon cancer research through its methods for diagnosis, treatment, and custom detection of one of the most common and fatal cancers suffered globally. Through AI deep learning and machine learning methods, tremendous possibilities have been realized in automating, for example, polyp detection in colonoscopy images, cancer classification from histopathological slides, and even predicting disease treatment outcomes. The current direction is towards transfer learning, hybrid model building and AI applications in surgery robots aimed at more accurate diagnosis and treatment procedures. Besides them, however, they have serious disadvantages, access to computer learning systems and datasets are limited, AI models come without explanation, embedding of artificial intelligence into clinical practices becomes troublesome. In future development, emphasize should be on creating databases, explainable ethics AI approaches, building bridges between developers and practitioners in medicine. The objective of the paper is how to incorporate research that has been done in moving colon cancer AI systems into practice, understanding the AI satellites put on colon cancer research.
ER  - 

TY  - CONF
TI  - AI Ethics and Bias in Machine Learning Models in AR and VR: a Focus on Education, Healthcare and Finance
T2  - 2025 IEEE 2nd International Workshop on Future Intelligent Technologies for Young Researchers (FITYR)
SP  - 12
EP  - 21
AU  - S. P. Arrojula
AU  - D. Bhaskaran
AU  - H. Mehta
PY  - 2025
KW  - Ethics
KW  - Solid modeling
KW  - Privacy
KW  - Machine learning algorithms
KW  - Prevention and mitigation
KW  - Finance
KW  - Medical services
KW  - Artificial intelligence
KW  - System analysis and design
KW  - Monitoring
KW  - Artificial Intelligence Ethics
KW  - Augmented Reality (AR)
KW  - Algorithmic Bias
KW  - Machine Learning Fairness
KW  - Explainable AI
KW  - Transparency in AI
KW  - Bias mitigation in AI
KW  - Responsible Immersive AI Systems
DO  - 10.1109/FITYR68680.2025.00006
JO  - 2025 IEEE 2nd International Workshop on Future Intelligent Technologies for Young Researchers (FITYR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 IEEE 2nd International Workshop on Future Intelligent Technologies for Young Researchers (FITYR)
Y1  - 21-24 July 2025
AB  - The integration of Artificial Intelligence (AI) and Machine Learning (ML) into Augmented Reality (AR) and Virtual Reality (VR) environments is transforming how users experience education, healthcare, and digital engagement. However, the combination of immersive interfaces with AI-driven personalization introduces heightened risks of algorithmic bias, user discrimination, and ethical oversight gaps. In spatially responsive and sensor-rich environments, even subtle biases in training data or system design can result in unequal treatmentsuch as misrepresented avatars, exclusionary learning experiences, or inequitable diagnostic insights. This paper explores the ethical dimensions of deploying AI systems within AR/VR technologies, focusing on high-impact domains like educational simulations, clinical training, and immersive digital services. Drawing on recent frameworks such as XRAI-Ethics and EVRIM, we review the sources and consequences of bias in extended AI systems and propose best practices to detect, mitigate, and monitor them across the development lifecycle. Our analysis synthesizes technical strategies, inclusive design principles, and policy recommendations to advance fairness, transparency, and accountability in immersive AI applications. Provided these findings, we propose a high-level framework that should provide a roadmap for stakeholders seeking to build responsible, equitable, and human-centered AI systems within emerging virtual platforms.
ER  - 

TY  - CONF
TI  - A Review on the Various Ethical Issues of the Dangers Associated with Artificial Intelligence
T2  - 2025 7th International Conference on Intelligent Sustainable Systems (ICISS)
SP  - 1660
EP  - 1665
AU  - H. N. Gadbail
AU  - P. Verma
AU  - D. Pokle
AU  - S. Zade
AU  - A. Waghale
AU  - N. Jumbde
PY  - 2025
KW  - Training
KW  - Ethics
KW  - Data privacy
KW  - Reviews
KW  - Computational modeling
KW  - Threat assessment
KW  - Production facilities
KW  - Artificial intelligence
KW  - Computer security
KW  - Robots
KW  - Artificial Intelligence
KW  - Ethics
KW  - Autonomy
KW  - Privacy
KW  - Accountability
KW  - Bias
KW  - Transparency
KW  - Risk management
DO  - 10.1109/ICISS63372.2025.11076367
JO  - 2025 7th International Conference on Intelligent Sustainable Systems (ICISS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2025 7th International Conference on Intelligent Sustainable Systems (ICISS)
Y1  - 12-14 March 2025
AB  - In our everyday life depends heavily on Artificial Intelligence (AI) systems that power self-driving cars, healthcare facilities, media services, financial institutions, robot factories and internet technology. AI creates important ethical risks when it intrudes into privacy, puts people at risk and affects their employment. This review article studies how AI affects ethics by examining potential problems with suggested solutions to manage them. This paper shows all current ethical risks of AI technology while providing guidance on building responsible artificial intelligence. This review shows why ethical values need to be part of AI creation and reveals good directions to make AI systems work better with humans and make society thrive. This study works as the starting point for researchers’ prevention plans and AI ethics practitioners who need a complete understanding of ethical AI system development and its problems.
ER  - 

