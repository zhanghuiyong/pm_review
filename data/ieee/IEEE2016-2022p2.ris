TY  - JOUR
TI  - Evaluating Privacy-Preserving Machine Learning in Critical Infrastructures: A Case Study on Time-Series Classification
T2  - IEEE Transactions on Industrial Informatics
SP  - 7834
EP  - 7842
AU  - D. Mercier
AU  - A. Lucieri
AU  - M. Munir
AU  - A. Dengel
AU  - S. Ahmed
PY  - 2022
KW  - Data models
KW  - Cryptography
KW  - Training
KW  - Privacy
KW  - Machine learning
KW  - Informatics
KW  - Critical infrastructure
KW  - Critical infrastructure
KW  - differential privacy (DP)
KW  - eXplainable AI (XAI)
KW  - federated learning (FL)
KW  - privacy-preserving machine learning (PPML)
KW  - secure sharing
KW  - time-series classification
DO  - 10.1109/TII.2021.3124476
JO  - IEEE Transactions on Industrial Informatics
IS  - 11
SN  - 1941-0050
VO  - 18
VL  - 18
JA  - IEEE Transactions on Industrial Informatics
Y1  - Nov. 2022
AB  - With the advent of machine learning in applications of critical infrastructure such as healthcare and energy, privacy is a growing concern in the minds of stakeholders. It is pivotal to ensure that neither the model nor the data can be used to extract sensitive information used by attackers against individuals or to harm whole societies through the exploitation of critical infrastructure. The applicability of machine learning in these domains is mostly limited due to a lack of trust regarding the transparency and the privacy constraints. Various safety-critical use cases (mostly relying on time-series data) are currently underrepresented in privacy-related considerations. By evaluating several privacy-preserving methods regarding their applicability on time-series data, we validated the inefficacy of encryption for deep learning, the strong dataset dependence of differential privacy, and the broad applicability of federated methods.
ER  - 

TY  - CONF
TI  - Adding Explainability to Machine Learning Models to Detect Chronic Kidney Disease
T2  - 2022 IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI)
SP  - 297
EP  - 302
AU  - M. A. Islam
AU  - K. Nittala
AU  - G. Bajwa
PY  - 2022
KW  - Measurement
KW  - Biological system modeling
KW  - Pipelines
KW  - Predictive models
KW  - Prediction algorithms
KW  - Data models
KW  - Rough surfaces
KW  - chronic kidney disease
KW  - CKD
KW  - explainability
KW  - interpretability
KW  - machine learning
KW  - neural network
DO  - 10.1109/IRI54793.2022.00069
JO  - 2022 IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI)
Y1  - 9-11 Aug. 2022
AB  - Chronic Kidney Disease is a common term for multiple heterogeneous diseases in the kidneys. It is also known as Chronic Renal Disease. Chronic kidney disease (CKD) has a gradual loss of glomerular filtration rate (GFR) over three months. The patient does not observe any significant symptoms in the earlier stage of CKD, and it is not identifiable without clinical tests like urine and blood tests. Patients with CKD would have a higher chance of developing heart disease. CKD is a progressive and often irreversible process of renal function decline, which may reach an endpoint of end-stage renal failure, requiring renal replacement therapy. It is critical to diagnose progressive CKD at an early stage and predict patients prone to developing the disease further for timely therapeutic interventions. As such, researchers have expended enormous efforts in the development of novel biomarkers that may identify subjects with early CKD at risk of progression. In this study, we have developed an explainable machine learning model to predict chronic kidney disease by implementing an automated data pipeline using the Random Forest ensemble learning trees model and feature selection algorithm. The explainability of the proposed model has been assessed in terms of feature importance and explainability metrics. Three explainability methods; LIME, SHAP, and SKATER have been applied to interpret the developed model and to compare the explainability results using Interpretability, Fidelity, and Fidelity-to-Interpretability ratio as the explainability metrics.
ER  - 

TY  - CONF
TI  - Accountable AI for Healthcare IoT Systems
T2  - 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)
SP  - 20
EP  - 28
AU  - P. Bagave
AU  - M. Westberg
AU  - R. Dobbe
AU  - M. Janssen
AU  - A. Y. Ding
PY  - 2022
KW  - Privacy
KW  - Decision making
KW  - Focusing
KW  - Medical services
KW  - Regulation
KW  - Internet of Things
KW  - Security
KW  - Accountability
KW  - Trustworthiness
KW  - Healthcare AI
KW  - Internet of Things (IoT)
DO  - 10.1109/TPS-ISA56441.2022.00013
JO  - 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE 4th International Conference on Trust, Privacy and Security in Intelligent Systems, and Applications (TPS-ISA)
Y1  - 14-17 Dec. 2022
AB  - Various AI systems have taken a unique space in our daily lives, helping us in decision-making in critical as well as non-critical scenarios. Although these systems are widely adopted across different sectors, they have not been used to their full potential in critical domains such as the healthcare sector enabled by the Internet of Things (IoT). One of the important hindering factors for adoption is the implication for accountability of decisions and outcomes affected by an AI system, where the term accountability is understood as a means to ensure the performance of a system. However, this term is often interpreted differently in various sectors. Since the EU GDPR regulations and the US congress have emphasised the importance of enabling accountability in AI systems, there is a strong demand to understand and conceptualise this term. It is crucial to address various aspects integrated with accountability and understand how it affects the adoption of AI systems. In this paper, we conceptualise these factors affecting accountability and how it contributes to a trustworthy healthcare AI system. By focusing on healthcare IoT systems, our conceptual mapping will help the readers understand what system aspects those factors are contributing to and how they affect the system trustworthiness. Besides illustrating accountability in detail, we also share our vision towards causal interpretability as a means to enhance accountability for healthcare AI systems. The insights of this paper shall contribute to the knowledge of academic research on accountability, and benefit AI developers and practitioners in the healthcare sector.
ER  - 

TY  - CONF
TI  - Characteristics of Symptom Tracking App Data and its Potential Use for XAI
T2  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
SP  - 620
EP  - 625
AU  - C. Duncan
AU  - T. Nagai
PY  - 2022
KW  - Medical services
KW  - Machine learning
KW  - Predictive models
KW  - Data models
KW  - Informatics
KW  - Monitoring
KW  - Chronic illness
KW  - Self-reported Data
KW  - Health tracking
DO  - 10.1109/ICHI54592.2022.00126
JO  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
Y1  - 11-14 June 2022
AB  - It is common for people with chronic illnesses to keep records about their health, and this is increasingly being done through symptom tracking apps. These apps allow users to log information about the symptoms they experience, and often incorporate other features such as medication and activity tracking. This creates a rich source of data about the individual's health, but at this stage is not clear how beneficial this data is to users. There is an opportunity for this data to be further utilised to assist users in understanding their health conditions. This paper proposes that this could be done through the application of Explainable AI and presents work-in-progress research. The final goal of this research is to utilise Explainable AI to provide personal insights' into users' self-tracking health-related data. As a first step, this work characterises the features of a large publicly available data-set $(n_{users}=42,283, n_{reports}$ 7,976,223) so that the general features of self-reported symptom tracking data may be better understood.
ER  - 

TY  - JOUR
TI  - System Design for a Data-Driven and Explainable Customer Sentiment Monitor Using IoT and Enterprise Data
T2  - IEEE Access
SP  - 117140
EP  - 117152
AU  - A. Nguyen
AU  - S. Foerstel
AU  - T. Kittler
AU  - A. Kurzyukov
AU  - L. Schwinn
AU  - D. Zanca
AU  - T. Hipp
AU  - S. D. Jun
AU  - M. Schrapp
AU  - E. Rothgang
AU  - B. Eskofier
PY  - 2021
KW  - Feature extraction
KW  - Predictive models
KW  - Decision support systems
KW  - Data models
KW  - Machine learning
KW  - Monitoring
KW  - Medical devices
KW  - Customer service
KW  - decision support system
KW  - IoT data
KW  - explainable AI
KW  - machine learning
KW  - big data
KW  - industrial AI
DO  - 10.1109/ACCESS.2021.3106791
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - The most important goal of customer service is to keep the customer satisfied. However, service resources are always limited and must prioritize specific customers. Therefore, it is essential to identify customers who potentially become unsatisfied and might lead to escalations. Data science on IoT data (especially log data) for machine health monitoring and analytics on enterprise data for customer relationship management (CRM) have mainly been researched and applied independently. This paper presents a data-driven decision support system framework that combines IoT and enterprise data to model customer sentiment and predicts escalations. The proposed framework includes a fully automated and interpretable machine learning pipeline using state-of-the-art methods. The framework is applied in a real-world case study with a major medical device manufacturer providing data from a fleet of thousands of high-end medical devices. An anonymized version of this industrial benchmark is released for the research community based on the presented case study, which has interesting and challenging properties. In our extensive experiments, we achieve a Recall@50 of 50.0 % for the task of predicting customer escalations. In addition, we show that combining IoT and enterprise data can improve prediction results and ease troubleshooting. Additionally, we propose a practical workflow for end-users when applying the proposed framework.
ER  - 

TY  - JOUR
TI  - On Interpretability of Artificial Neural Networks: A Survey
T2  - IEEE Transactions on Radiation and Plasma Medical Sciences
SP  - 741
EP  - 760
AU  - F. -L. Fan
AU  - J. Xiong
AU  - M. Li
AU  - G. Wang
PY  - 2021
KW  - Deep learning
KW  - Neural networks
KW  - Taxonomy
KW  - Data models
KW  - Training
KW  - Deep learning
KW  - interpretability
KW  - neural networks
KW  - survey
DO  - 10.1109/TRPMS.2021.3066428
JO  - IEEE Transactions on Radiation and Plasma Medical Sciences
IS  - 6
SN  - 2469-7303
VO  - 5
VL  - 5
JA  - IEEE Transactions on Radiation and Plasma Medical Sciences
Y1  - Nov. 2021
AB  - Deep learning as performed by artificial deep neural networks (DNNs) has achieved great successes recently in many important areas that deal with text, images, videos, graphs, and so on. However, the black-box nature of DNNs has become one of the primary obstacles for their wide adoption in mission-critical applications such as medical diagnosis and therapy. Because of the huge potentials of deep learning, the interpretability of DNNs has recently attracted much research attention. In this article, we propose a simple but comprehensive taxonomy for interpretability, systematically review recent studies on interpretability of neural networks, describe applications of interpretability in medicine, and discuss future research directions, such as in relation to fuzzy logic and brain science.
ER  - 

TY  - JOUR
TI  - FCE: Feedback Based Counterfactual Explanations for Explainable AI
T2  - IEEE Access
SP  - 72363
EP  - 72372
AU  - M. Suffian
AU  - P. Graziani
AU  - J. M. Alonso
AU  - A. Bogliolo
PY  - 2022
KW  - Predictive models
KW  - Feature extraction
KW  - Decision trees
KW  - Prediction algorithms
KW  - Radio frequency
KW  - Measurement
KW  - Decision making
KW  - Counterfactual explanations
KW  - explainable AI
KW  - human-in-the-loop
KW  - interactive machine learning
KW  - user feedback
DO  - 10.1109/ACCESS.2022.3189432
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 10
VL  - 10
JA  - IEEE Access
Y1  - 2022
AB  - Artificial Intelligence can provide quite accurate predictions for critical applications (e.g., healthcare), but lacks the ability to explain its internal mechanism in most applications which require high interaction with humans. Even if many studies analyze machine learning models and their learning behavior and eventually provide an interpretation of the inner mechanics of these models, these studies often entail a simpler surrogate model, which generates explanations by producing a piece of interpretable information such as feature scores. The crucial caveat against these studies is the lack of human involvement in the design and evaluation of explanations, consequently giving rise to trust issues and lack of acceptance and understanding. To this end, we address this limitation by involving humans in the counterfactual explanation generation process which is enriched with user feedback, thus enhancing the automated explanations which are better aligned with user expectations. In this paper, we propose a user feedback based counterfactual explanation approach (FCE) for explainable Artificial Intelligence. In our work, we utilize feedback in two ways: first, to customize the explanations by providing the acceptable ranges in the feature space where to look for feasible counterfactuals, and second, to evaluate the generated explanations.
ER  - 

TY  - JOUR
TI  - Deep Semisupervised Multitask Learning Model and Its Interpretability for Survival Analysis
T2  - IEEE Journal of Biomedical and Health Informatics
SP  - 3185
EP  - 3196
AU  - S. Chi
AU  - Y. Tian
AU  - F. Wang
AU  - Y. Wang
AU  - M. Chen
AU  - J. Li
PY  - 2021
KW  - Analytical models
KW  - Deep learning
KW  - Neural networks
KW  - Data models
KW  - Task analysis
KW  - Hazards
KW  - Biological system modeling
KW  - Deep learning
KW  - model interpretability
KW  - multitask learning
KW  - semisupervised learning
KW  - survival analysis
DO  - 10.1109/JBHI.2021.3064696
JO  - IEEE Journal of Biomedical and Health Informatics
IS  - 8
SN  - 2168-2208
VO  - 25
VL  - 25
JA  - IEEE Journal of Biomedical and Health Informatics
Y1  - Aug. 2021
AB  - Survival analysis is a commonly used method in the medical field to analyze and predict the time of events. In medicine, this approach plays a key role in determining the course of treatment, developing new drugs, and improving hospital procedures. Most of the existing work in this area has addressed the problem by making strong assumptions about the underlying stochastic process. However, these assumptions are usually violated in the real-world data. This paper proposed a semisupervised multitask learning (SSMTL) method based on deep learning for survival analysis with or without competing risks. SSMTL transforms the survival analysis problem into a multitask learning problem that includes semisupervised learning and multipoint survival probability prediction. The distribution of survival times and the relationship between covariates and outcomes were modeled directly without any assumptions. Semisupervised loss and ranking loss are used to deal with censored data and the prior knowledge of the nonincreasing trend of the survival probability. Additionally, the importance of prognostic factors is determined, and the time-dependent and nonlinear effects of these factors on survival outcomes are visualized. The prediction performance of SSMTL is better than that of previous models in settings with or without competing risks, and the effects of predictors are successfully described. This study is of great significance for the exploration and application of deep learning methods involving medical structured data and provides an effective deep-learning-based method for survival analysis with complex-structured clinical data.
ER  - 

TY  - CONF
TI  - Explainable Pulmonary Disease Diagnosis with Prompt-Based Knowledge Extraction
T2  - 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 1816
EP  - 1819
AU  - H. Zhang
AU  - C. Xu
AU  - J. Li
AU  - P. Liang
AU  - X. Zeng
AU  - H. Ren
AU  - W. Cheng
AU  - K. Wu
PY  - 2022
KW  - Deep learning
KW  - Training
KW  - Databases
KW  - Pulmonary diseases
KW  - Medical diagnosis
KW  - Data mining
KW  - Task analysis
KW  - knowledge extraction
KW  - explainable diagnosis
DO  - 10.1109/BIBM55620.2022.9995532
JO  - 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 6-8 Dec. 2022
AB  - Recent studies show that deep learning models perform well in many medical tasks such as medical imaging and automated diagnosis. With qualified training datasets, some models can achieve or even surpass expert-level performance on some tasks. However, as a typical black-box-style approach, deep learning lacks theoretical interpretability, which is especially important for medical tasks. On the other hand, there are many sources of domain knowledge for medical diagnosis from human experts, such as clinical guidelines. How to sufficiently integrate human knowledge in the model is crucial for explainable diagnosis. In this paper, we propose a novel framework for explainable automated diagnosis that leverages explicit medical knowledge. We automate the knowledge extraction from textual clinical guidelines with prompt-based learning, train a set of weighted first-order logical rules with constructed evidence database, and finally infer the diagnosis result with integrated knowledge and multi-sourced data. We instantiate the framework for pulmonary disease diagnosis, and our experiments on a real dataset show that our method outperforms the state-of-the-art baselines in accuracy and interpretability.
ER  - 

TY  - CONF
TI  - Machine Learning for Medical and Healthcare Data Analysis and Modelling: Case Studies and Performance Comparisons of Different Methods
T2  - 2022 27th International Conference on Automation and Computing (ICAC)
SP  - 1
EP  - 6
AU  - B. Sun
AU  - H. -L. Wei
PY  - 2022
KW  - Measurement
KW  - Data analysis
KW  - Medical services
KW  - Predictive models
KW  - Benchmark testing
KW  - Data models
KW  - Robustness
KW  - Machine learning
KW  - binary classification
KW  - ensemble learning
KW  - K-fold validation
KW  - MCC
DO  - 10.1109/ICAC55051.2022.9911176
JO  - 2022 27th International Conference on Automation and Computing (ICAC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 27th International Conference on Automation and Computing (ICAC)
Y1  - 1-3 Sept. 2022
AB  - In smart healthcare, binary classification is one of the most important tasks for disease or clinical outcome prediction. Machine learning (ML) methods have great potential to discover knowledge from data. While there are a wide range of ML methods that are potentially useful, the selection, design and implementation of an appropriate method from a variety of ML models for specific prediction tasks is still quite challenging in many medical and healthcare data modelling tasks. This paper investigates, evaluate and compares nine typical representative ML methods using four clinical and healthcare datasets. The main attention is paid to the strengths and weaknesses of these methods and provide suggestions to researchers in related fields. Results demonstrate that ensemble learning (such as Adaboost and random forest) outperform the other approaches in prediction accuracy and interpretability.
ER  - 

TY  - CONF
TI  - Explainable Feature Learning for Predicting Neonatal Intensive Care Unit (NICU) Admissions
T2  - 2021 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)
SP  - 69
EP  - 74
AU  - G. Marvin
AU  - M. G. R. Alam
PY  - 2021
KW  - Representation learning
KW  - Support vector machines
KW  - Pediatrics
KW  - Costs
KW  - Predictive models
KW  - Robustness
KW  - Resource management
KW  - Explainable AI (XAI)
KW  - Predictive Medicine
KW  - Perinatal Quality Management
KW  - Maternal-Fetal and Neonatal Medicine
KW  - Feature Learning
KW  - Neonatal Emergencies
KW  - NICU Facility Management
DO  - 10.1109/BECITHCON54710.2021.9893719
JO  - 2021 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Biomedical Engineering, Computer and Information Technology for Health (BECITHCON)
Y1  - 4-5 Dec. 2021
AB  - Neonatal Intensive Care Units (NICU) service costs are rapidly growing due to the higher resource utilization intensity. This in turn increases the healthcare costs for NICU patients besides the inaccessibility and unpreparedness of both NICU service providers and patient caretakers hence an increase in neonatal mortality and morbidity. There a lot of contributors to NICU admissions but the exiting methods consider very limited features to precisely predict NICU admissions. In this paper, we present a robust Explainable Artificial Intelligence approach that allows machines to interpretably learn from a pool of possible contributing features in order to predict an NICU admission. Our machine learning approach interpretably illustrates the thought process of admission prediction to the physician and patient. This provides transparent and trustable insights for the precise, proactive, personalized and participatory NICU medical diagnostics and treatment plans for the patient. We statistically and visually present Random Forest and Logistic Regression prediction explanations using SHAP, LIME and ELI5 techniques. This predictive technological approach can preventively increase success of maternal and neonatal monitoring and treatment plans. It can also enhance proactive management of NICU facilities (resources) by the responsible facility administrators most especially in resource constrained settings.
ER  - 

TY  - CONF
TI  - Machine Learning and Blockchain Techniques Used in Healthcare System
T2  - 2019 IEEE Pune Section International Conference (PuneCon)
SP  - 1
EP  - 5
AU  - N. V. Pardakhe
AU  - V. M. Deshmukh
PY  - 2019
KW  - Surveys
KW  - Industries
KW  - Data analysis
KW  - Costs
KW  - Accuracy
KW  - Medical services
KW  - Machine learning
KW  - Organizations
KW  - Blockchains
KW  - Security
KW  - Machine Learning
KW  - Blockchain
KW  - Security
DO  - 10.1109/PuneCon46936.2019.9105710
JO  - 2019 IEEE Pune Section International Conference (PuneCon)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE Pune Section International Conference (PuneCon)
Y1  - 18-20 Dec. 2019
AB  - Information systems and computerization nowadays need very faster, secure & easier data analysis techniques. It is also required to maintain efficiency and accuracy in data analysis. So machine learning & Blockchain techniques have been continuously used in the data analysis & security in various fields from medicine to organization and education to energy applications. This study applies classification of machine learning & Blockchain techniques to process the data and survey of selection methods, query strategies, applications and security. In highly secure data, Security issues are solved by using Blockchain Technology.Blockchain technology is rapidly gaining attention towards the Security of confidential data. The healthcare industry is one of the fields of organization where high risk involves & its attracted attention of many technological organizations so this field required the security for securing their data. The Blockchain is generally used for providing the security to secure the high sensitive data. By using the Blockchain technology there are numerous opportunities for healthcare industry to achieve & gain. Such as reduced transaction costs, increased transparency for regulatory reporting, efficient healthcare data management and healthcare records universality as well as able to access data from any location. In the context of smart healthcare system blockchain may provide distinct benefits, particularly from a context-aware perspective where efficient and personalized solutions may be provided to citizens and the society. This paper provides a comprehensive survey of relationship between Machine learning and blockchain techniques related to smart healthcare system. In addition, we are going to discussed several challenges can comes for actually implementing machine learning secure healthcare system using blockchain based Technology.
ER  - 

TY  - CONF
TI  - Explainable Prediction of Medical Codes through Automated Knowledge Graph Curation Framework
T2  - 2022 19th International Bhurban Conference on Applied Sciences and Technology (IBCAST)
SP  - 331
EP  - 336
AU  - M. Khalid
AU  - H. A. Khattak
AU  - A. Ahmad
AU  - S. A. Chan Bukhari
PY  - 2022
KW  - Deep learning
KW  - Knowledge engineering
KW  - Codes
KW  - Semantics
KW  - Insurance
KW  - Learning (artificial intelligence)
KW  - Predictive models
KW  - Electronic Health Records
KW  - Knowledge Graph
KW  - Machine learning (ML)
KW  - Deep learning (DL)
KW  - eXplainable Artificial Intelligence (XAI)
KW  - Ontology
DO  - 10.1109/IBCAST54850.2022.9990551
JO  - 2022 19th International Bhurban Conference on Applied Sciences and Technology (IBCAST)
IS  - 
SN  - 2151-1411
VO  - 
VL  - 
JA  - 2022 19th International Bhurban Conference on Applied Sciences and Technology (IBCAST)
Y1  - 16-20 Aug. 2022
AB  - Medical Coding (MC) converts medical diagnoses, procedures, equipment, and services into alphanumeric codes. Automated Medical Code prediction systems use Machine Learning and Deep Learning techniques. They could save insurance companies, Government agencies, and medical staff from the hassle of reading lengthy summaries to prepare insurance claims for reimbursement of expenses and fees. If not wholly correct, these machine-understandable codes can still help the medical coders to ease their task of predicting accurate medical codes. Despite being helpful, labor-saving, and efficient, these decision support systems fizzle out due to scarcity of domain knowledge. Knowledge Graphs containing a network of concepts, their meta-data, and a hierarchy of relationships in specific domains can help build the applications of Computer Assisted Coding (CAC) with much more accurate and precise results glued with Explainability. Unfortunately, domain-specific Knowledge Graphs (KG) creation is a grueling task, as it requires healthcare experts’ intervention to annotate the medical concepts. Our proposed approach is to create a framework for the Automated Generation of Knowledge Graphs, which is yet a manual and time-consuming process, with the help of Natural Language Processing, Ontology-based information retrieval, and a semantic enrichment process. The created domain-specific Knowledge graph is then fused with a Deep Learning model. Predictions made by the Deep Learning model were compared before and after the consolidation of the domain-specific Knowledge Graph. We created a web-based application to save users from the complexity of the Deep learning model and knowledge graphs. Results proved the significance of the combination of Knowledge graphs and Artificial Intelligence. Produced results yield increased precision and accuracy with fewer false-positive results.
ER  - 

TY  - CONF
TI  - Extending SOUP to ML Models When Designing Certified Medical Systems
T2  - 2021 IEEE/ACM 3rd International Workshop on Software Engineering for Healthcare (SEH)
SP  - 32
EP  - 35
AU  - V. Stirbu
AU  - T. Granlund
AU  - J. Helén
AU  - T. Mikkonen
PY  - 2021
KW  - Medical devices
KW  - Medical services
KW  - Machine learning
KW  - Predictive models
KW  - Product development
KW  - Safety
KW  - Complexity theory
KW  - medical-devices
KW  - regulations
KW  - soup
KW  - machine-learning
DO  - 10.1109/SEH52539.2021.00013
JO  - 2021 IEEE/ACM 3rd International Workshop on Software Engineering for Healthcare (SEH)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE/ACM 3rd International Workshop on Software Engineering for Healthcare (SEH)
Y1  - 3-3 June 2021
AB  - Software of Unknown Provenance, SOUP, refers to a software component that is already developed and widely available from a 3rd party, and that has not been developed, to be integrated into a medical device. From regulatory perspective, SOUP software requires special considerations, as the developers' obligations related to design and implementation are not applied to it. In this paper, we consider the implications of extending the concept of SOUP to machine learning (ML) models. As the contribution, we propose practical means to manage the added complexity of 3rd party ML models in regulated development.
ER  - 

TY  - JOUR
TI  - Assessing Trustworthy AI in Times of COVID-19: Deep Learning for Predicting a Multiregional Score Conveying the Degree of Lung Compromise in COVID-19 Patients
T2  - IEEE Transactions on Technology and Society
SP  - 272
EP  - 289
AU  - H. Allahabadi
AU  - J. Amann
AU  - I. Balot
AU  - A. Beretta
AU  - C. Binkley
AU  - J. Bozenhard
AU  - F. Bruneault
AU  - J. Brusseau
AU  - S. Candemir
AU  - L. A. Cappellini
AU  - S. Chakraborty
AU  - N. Cherciu
AU  - C. Cociancig
AU  - M. Coffee
AU  - I. Ek
AU  - L. Espinosa-Leal
AU  - D. Farina
AU  - G. Fieux-Castagnet
AU  - T. Frauenfelder
AU  - A. Gallucci
AU  - G. Giuliani
AU  - A. Golda
AU  - I. van Halem
AU  - E. Hildt
AU  - S. Holm
AU  - G. Kararigas
AU  - S. A. Krier
AU  - U. Kühne
AU  - F. Lizzi
AU  - V. I. Madai
AU  - A. F. Markus
AU  - S. Masis
AU  - E. W. Mathez
AU  - F. Mureddu
AU  - E. Neri
AU  - W. Osika
AU  - M. Ozols
AU  - C. Panigutti
AU  - B. Parent
AU  - F. Pratesi
AU  - P. A. Moreno-Sánchez
AU  - G. Sartor
AU  - M. Savardi
AU  - A. Signoroni
AU  - H. -M. Sormunen
AU  - A. Spezzatti
AU  - A. Srivastava
AU  - A. F. Stephansen
AU  - L. B. Theng
AU  - J. J. Tithi
AU  - J. Tuominen
AU  - S. Umbrello
AU  - F. Vaccher
AU  - D. Vetter
AU  - M. Westerlund
AU  - R. Wurth
AU  - R. V. Zicari
PY  - 2022
KW  - Artificial intelligence
KW  - COVID-19
KW  - Pandemics
KW  - Medical services
KW  - Ethics
KW  - Radiology
KW  - Lung
KW  - Deep learning
KW  - Artificial intelligence
KW  - case study
KW  - COVID-19
KW  - ethical tradeoff
KW  - ethics
KW  - explainable AI
KW  - healthcare
KW  - pandemic
KW  - radiology
KW  - trust
KW  - trustworthy AI
KW  - Z-Inspection®
DO  - 10.1109/TTS.2022.3195114
JO  - IEEE Transactions on Technology and Society
IS  - 4
SN  - 2637-6415
VO  - 3
VL  - 3
JA  - IEEE Transactions on Technology and Society
Y1  - Dec. 2022
AB  - This article’s main contributions are twofold: 1) to demonstrate how to apply the general European Union’s High-Level Expert Group’s (EU HLEG) guidelines for trustworthy AI in practice for the domain of healthcare and 2) to investigate the research question of what does “trustworthy AI” mean at the time of the COVID-19 pandemic. To this end, we present the results of a post-hoc self-assessment to evaluate the trustworthiness of an AI system for predicting a multiregional score conveying the degree of lung compromise in COVID-19 patients, developed and verified by an interdisciplinary team with members from academia, public hospitals, and industry in time of pandemic. The AI system aims to help radiologists to estimate and communicate the severity of damage in a patient’s lung from Chest X-rays. It has been experimentally deployed in the radiology department of the ASST Spedali Civili clinic in Brescia, Italy, since December 2020 during pandemic time. The methodology we have applied for our post-hoc assessment, called Z-Inspection®, uses sociotechnical scenarios to identify ethical, technical, and domain-specific issues in the use of the AI system in the context of the pandemic.
ER  - 

TY  - CONF
TI  - Machine Learning: A promising in-silico approach to curb antimicrobial resistance
T2  - 2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)
SP  - 1008
EP  - 1013
AU  - A. Kukreti
AU  - Y. Hasija
PY  - 2022
KW  - Productivity
KW  - Data integrity
KW  - Antibiotics
KW  - Microscopy
KW  - Machine learning
KW  - Market research
KW  - Genetics
KW  - antibiotics
KW  - antimicrobial resistance
KW  - artificial intelligence
KW  - COVID-19
KW  - deep learning
KW  - halicin
KW  - machine learning
DO  - 10.1109/ICACCS54159.2022.9784964
JO  - 2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)
IS  - 
SN  - 2575-7288
VO  - 1
VL  - 1
JA  - 2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)
Y1  - 25-26 March 2022
AB  - Antimicrobial resistance (AMR) is a concern to public health, prompting the development of novel strategies for combating AMR. While the use of machine learning (ML) to AMR is in its infancy, it has made significant progress as a diagnosis tool, owing to the growing availability of phenotypic/genotypic datasets and much faster computational power. While applying ML in AMR research is viable, its use is limited. It has been used to predict antimicrobial susceptibility genotypes/phenotypes, discover novel antibiotics, and improve diagnosis when combined with spectroscopic and microscopy methods. ML implementation in healthcare settings has challenges to adoption due to concerns about model interpretability and data integrity. The focus of this review is to outline the significant benefits and drawbacks along with the salient trends reported in recent studies.
ER  - 

TY  - CONF
TI  - Explainability Analysis of Black Box SVM models for Hepatic Steatosis Screening
T2  - 2022 IEEE Healthcare Innovations and Point of Care Technologies (HI-POCT)
SP  - 22
EP  - 25
AU  - R. Deo
AU  - S. Panigrahi
PY  - 2022
KW  - Support vector machines
KW  - Analytical models
KW  - Technological innovation
KW  - Liver diseases
KW  - Computational modeling
KW  - Biological system modeling
KW  - Sociology
DO  - 10.1109/HI-POCT54491.2022.9744067
JO  - 2022 IEEE Healthcare Innovations and Point of Care Technologies (HI-POCT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE Healthcare Innovations and Point of Care Technologies (HI-POCT)
Y1  - 10-11 March 2022
AB  - Non-Alcoholic Fatty Liver Disease (NAFLD) or HS is one of the major causes of chronic liver diseases worldwide. Identifying the NAFLD condition at an early stage allows for preventative care and potential disease remission.To this end, our research group is addressing this issue by developing a computational model for decision support for Hepatic Steatosis (HS) or NAFLD screening. Our recent work included the development of machine learning models using seven physiological parameters (demographic, lipids, and liver biochemical parameters). Although the developed models show potential for screening, there is a need for further improving the model performance. Considering the complex nature of this condition and its interaction with different physiological parameters, we identified the contribution of the individual parameters in predicting the target (HS). The objective of this paper is to identify how different features contribute to a given model prediction by using an explainable artificial intelligence (XAI) technique called Partial Dependency. Results from partial dependency analysis and plots are summarized in this paper along with insights related to model performance. We identified the top three individual important predictors (ALT, AST, and Glucose levels) for both male and female. The models both for the male and female populations were analyzed separately to incorporate the pathobiological difference in NAFLD morphology in male vs female population.Clinical Relevance—The current study and obtained results do not have immediate clinical implications. However, this work paves the path for a potential computational model, which after required validation and testing, could be used as a decision support system for Hepatic Steatosis screening.
ER  - 

TY  - CONF
TI  - Inspecting a Machine Learning Based Clinical Risk Calculator: A Practical Perspective
T2  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
SP  - 325
EP  - 330
AU  - Q. -G. Thurier
AU  - N. Hua
AU  - L. Boyle
AU  - A. Spyker
PY  - 2019
KW  - Calculators
KW  - Predictive models
KW  - Machine learning
KW  - Medical services
KW  - Training
KW  - Pipelines
KW  - Data models
KW  - Machine learning
KW  - Medical information systems
DO  - 10.1109/CBMS.2019.00073
JO  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
IS  - 
SN  - 2372-9198
VO  - 
VL  - 
JA  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
Y1  - 5-7 June 2019
AB  - Health is reaching a point where machines are more accurate than humans, or at least as accurate but with less effort, in more and more applications. However, accuracy alone is not enough, explanation and understanding is equally important to clinicians, governments, and patients. Possibly leading to loss of health benefits potentially realized through increasingly accurate algorithms. However, various techniques exist for auditing machine learning systems via insightful visualisations. Modelling best practices, parallel computations and open source technologies facilitate implementation of these techniques. This paper leverages several of these methods to increase interpretability for a black-box clinical risk calculator, hopefully opening the door to a better adoption of modern machine learning pipelines in the healthcare sector.
ER  - 

TY  - CONF
TI  - Length of Stay Analysis at Neonatal Care Units with Data Science - Preliminary Results
T2  - 2021 IEEE International Symposium on Medical Measurements and Applications (MeMeA)
SP  - 1
EP  - 6
AU  - J. R. Cordeiro
AU  - O. Postolache
PY  - 2021
KW  - Pediatrics
KW  - Technological innovation
KW  - Statistical analysis
KW  - Machine learning
KW  - Predictive models
KW  - Data science
KW  - Tools
KW  - Length-of-stay
KW  - Neonatal Intensive Care Unit
KW  - Data Science
KW  - eXplainable Artificial Intelligence
DO  - 10.1109/MeMeA52024.2021.9478748
JO  - 2021 IEEE International Symposium on Medical Measurements and Applications (MeMeA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Symposium on Medical Measurements and Applications (MeMeA)
Y1  - 23-25 June 2021
AB  - The paper presents preliminary results to the length-of-stay (LOS) analysis on neonatal intensive care units, taking as base a 10 year dataset that encompasses data from all neonatal Portuguese units. Unlike conventional studies based on statistical analysis, this investigation uses data science techniques, including machine learning hyperparameters optimization automatization and explainable artificial intelligence (XAI) tools, to understand the relevance of factors and their impact on LOS. In this work was designed and implemented an accurate LOS prediction model which results have been included in the paper. Through XAI techniques application, it was already possible to confirm insights claimed by state-of-the-art studies, namely about the importance of birth weight and gestational age for LOS as well as the importance of hospital-acquired infections. The work presents as novelty the weights (quantification) of these factors as well as the introduction of two new factors, which were not considered until now. The study makes part of ongoing work and will continue to provide better insights for better investment LOS in neonatal care units.
ER  - 

TY  - CONF
TI  - Multi-inhabitant and explainable Activity Recognition in Smart Homes
T2  - 2021 22nd IEEE International Conference on Mobile Data Management (MDM)
SP  - 264
EP  - 266
AU  - L. Arrotta
PY  - 2021
KW  - Deep learning
KW  - Conferences
KW  - Training data
KW  - Smart homes
KW  - Medical services
KW  - Activity recognition
KW  - Probabilistic logic
DO  - 10.1109/MDM52706.2021.00054
JO  - 2021 22nd IEEE International Conference on Mobile Data Management (MDM)
IS  - 
SN  - 2375-0324
VO  - 
VL  - 
JA  - 2021 22nd IEEE International Conference on Mobile Data Management (MDM)
Y1  - 15-18 June 2021
AB  - The sensor-based detection of Activities of Daily Living (ADLs) in smart home environments can be exploited to provide healthcare applications, like remotely monitoring fragile subjects living in their habitations. However, ADLs recognition methods have been mainly investigated with a focus on singleinhabitant scenarios. The major problem in multi-inhabitant settings is data association: assigning to each resident the environmental sensors’ events that he/she triggered. Furthermore, Deep Learning (DL) solutions have been recently explored for ADLs recognition, with promising results. Nevertheless, the main drawbacks of these methods are their need for large amounts of training data, and their lack of interpretability. This paper summarizes some contributions of my Ph.D. research, in which we are designing explainable multi-inhabitant approaches for ADLs recognition. We have already investigated a hybrid knowledge- and data-driven solution that exploits the high-level context of each resident to perform data association. Currently, we are studying semi-supervised techniques to mitigate the data scarcity issue, and explainable Artificial Intelligence (XAI) methods to make DL classifiers for ADLs more transparent.
ER  - 

TY  - JOUR
TI  - Explainable Prediction of Acute Myocardial Infarction Using Machine Learning and Shapley Values
T2  - IEEE Access
SP  - 210410
EP  - 210417
AU  - L. Ibrahim
AU  - M. Mesinovic
AU  - K. -W. Yang
AU  - M. A. Eid
PY  - 2020
KW  - Electrocardiography
KW  - Myocardium
KW  - Machine learning
KW  - Predictive models
KW  - Databases
KW  - Training
KW  - Feature extraction
KW  - Machine learning
KW  - biomedical informatics
KW  - predictive models
KW  - acute myocardial infarction
DO  - 10.1109/ACCESS.2020.3040166
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - The early and accurate detection of the onset of acute myocardial infarction (AMI) is imperative for the timely provision of medical intervention and the reduction of its mortality rate. Machine learning techniques have demonstrated great potential in aiding disease diagnosis. In this paper, we present a framework to predict the onset of AMI using 713,447 extracted ECG samples and associated auxiliary data from the longitudinal and comprehensive ECG-ViEW II database, previously unexplored in the field of machine learning in healthcare. The framework is realized with two deep learning models, a convolutional neural network (CNN) and a recurrent neural network (RNN), and a decision-tree based model, XGBoost. Synthetic minority oversampling technique (SMOTE) was utilized to address class imbalance. High prediction accuracy of 89.9%, 84.6%, 97.5% and ROC curve areas of 90.7%, 82.9%, 96.5% have been achieved for the best CNN, RNN, and XGBoost models, respectively. Shapley values were utilized to identify the features that contributed most to the classification decision with XGBoost, demonstrating the high impact of auxiliary inputs such as age and sex. This paper demonstrates the promising application of explainable machine learning in the field of cardiovascular disease prediction.
ER  - 

TY  - CONF
TI  - Towards Explainability in mHealth Application for Mitigation of Forward Head Posture in Smartphone Users
T2  - 2022 IEEE International Conference on E-health Networking, Application & Services (HealthCom)
SP  - 49
EP  - 55
AU  - R. O. Oyeleke
AU  - B. G. Sorinolu
PY  - 2022
KW  - Visualization
KW  - Head
KW  - Machine learning algorithms
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Magnetic heads
KW  - Explainable AI
KW  - mHealth
KW  - smartphone
KW  - forward head posture
KW  - efficientNet CNN
KW  - physiatry
DO  - 10.1109/HealthCom54947.2022.9982740
JO  - 2022 IEEE International Conference on E-health Networking, Application & Services (HealthCom)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on E-health Networking, Application & Services (HealthCom)
Y1  - 17-19 Oct. 2022
AB  - Machine learning (ML) algorithms have recorded tremendous successes in many areas, notably healthcare. With increasing computing power of mobile devices, mobile health (mHealth) applications are embedded with ML models to learn users behavior and influence positive lifestyle changes. Although ML algorithms have shown impressive predictive power over the years, nonetheless, it is necessary that their inferences and recommendations are also explainable. Explainability can promote users’ trust, particularly when ML algorithms are deployed in high-stake domains such as healthcare. In this study, first, we present our proposed situation-aware mobile application called Smarttens coach app that we developed to assist smartphone users in mitigating forward head posture. It embeds an efficientNet CNN model to predict forward head posture in smartphone users by analyzing head posture images of the users. Our Smarttens coach app achieved a state-of-the-art accuracy score of 0.99. However, accuracy score alone does not tell users the whole story about how Smarttens coach app draws its inference on predicted posture binary class. This lack of explanation to justify the predicted posture class label could negatively impact users’ trust in the efficacy of the app. Therefore, we further validated our Smarttens coach app posture prediction efficacy by leveraging an explainable AI (XAI) framework called LIME to generate visual explanations for users’ predicted head posture class label.
ER  - 

TY  - CONF
TI  - Explainable Deep Learning for Readmission Prediction with Tree-GloVe Embedding
T2  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
SP  - 138
EP  - 147
AU  - J. Jiang
AU  - S. Hewner
AU  - V. Chandola
PY  - 2021
KW  - Deep learning
KW  - Codes
KW  - Hospitals
KW  - Computational modeling
KW  - Taxonomy
KW  - Computer architecture
KW  - Predictive models
KW  - Readmission Prediction
KW  - XAI
KW  - Structural Embedding
DO  - 10.1109/ICHI52183.2021.00031
JO  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
Y1  - 9-12 Aug. 2021
AB  - Preventable hospital readmissions have been identified as one of the primary targets for improving the efficiency of the current healthcare system. Over the past decade, several data-driven solutions for predicting readmissions have been presented. While maintaining high predictive accuracy is the obvious main goal for such solutions, ensuring explainability of the model and its predictions, is equally important for adoption in the healthcare domain. Unfortunately, most solutions have struggled to strike an optimal balance between accuracy and explainability. Linear models only provide moderately accurate results while complex machine learning models are non-explainable black boxes, which precludes them from being used effectively within the decision support systems in the hospitals. We propose a solution that integrates domain knowledge, in the form of a hierarchical taxonomy defined for disease codes, into the learning framework to advance state-of-the-art in readmission prediction. We first propose a novel tree-structured embedding method to map disease codes into an explainable domain-guided representation. Next, we propose an attention-driven recurrent deep learning architecture. Results on two healthcare claims data sets show that the proposed model outperforms state-of-the-art methods proposed for this task, both in terms of accuracy and explainability.
ER  - 

TY  - CONF
TI  - Machine Learning Based Clinical Decision Support and Clinician Trust
T2  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
SP  - 1
EP  - 1
AU  - J. Schwartz
AU  - K. Cato
PY  - 2020
KW  - Decision support systems
KW  - Conferences
KW  - Decision making
KW  - Machine learning
KW  - Medical services
KW  - Informatics
KW  - machine learning
KW  - clinical decision support
KW  - understandability
KW  - trust
DO  - 10.1109/ICHI48887.2020.9374365
JO  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
Y1  - 30 Nov.-3 Dec. 2020
AB  - Machine learning is burgeoning in the clinical decision support domain, with the potential to bolster the power of decision support systems, improving data-informed clinical decision making. However, barriers persist to the adoption and regular use of machine-learning based clinical decision support systems (ML-CDSS), including the fact that many systems lack model transparency and understandability, precluding clinicians’ trust. One strategy for increasing model understandability and subsequent trust is incorporating clinical expertise in development of the machine learning model. However, clinician requirements for trusting ML-CDSS and evidence on the impact of involving clinical experts in model development for the purposes of facilitating trust are lacking.
ER  - 

TY  - CONF
TI  - DeepSynthBody: the beginning of the end for data deficiency in medicine
T2  - 2021 International Conference on Applied Artificial Intelligence (ICAPAI)
SP  - 1
EP  - 8
AU  - V. Thambawita
AU  - S. A. Hicks
AU  - J. Isaksen
AU  - M. H. Stensen
AU  - T. B. Haugen
AU  - J. Kanters
AU  - S. Parasa
AU  - T. de Lange
AU  - H. D. Johansen
AU  - D. Johansen
AU  - H. L. Hammer
AU  - P. Halvorsen
AU  - M. A. Riegler
PY  - 2021
KW  - Biological system modeling
KW  - Pipelines
KW  - Data protection
KW  - Machine learning
KW  - Medical services
KW  - Generative adversarial networks
KW  - Regulation
KW  - DeepSynthBody
KW  - synthetic medical data
KW  - deep synthetic human body
KW  - synthetic data
KW  - GAN
KW  - DeepSynth augmentation
KW  - privacy issue
KW  - medical data privacy
KW  - multi-model DeepSynth
KW  - DeepSynth explainable AI
KW  - explainable DeepSynth
DO  - 10.1109/ICAPAI49758.2021.9462062
JO  - 2021 International Conference on Applied Artificial Intelligence (ICAPAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Applied Artificial Intelligence (ICAPAI)
Y1  - 19-21 May 2021
AB  - Limited access to medical data is a barrier on developing new and efficient machine learning solutions in medicine such as computer-aided diagnosis, risk assessments, predicting optimal treatments and home-based personal healthcare systems. This paper presents DeepSynthBody: a novel framework that overcomes some of the inherent restrictions and limitations of medical data by using deep generative adversarial networks to produce synthetic data with characteristics similar to the real data, so-called DeepSynth (deep synthetic) data. We show that DeepSynthBody can address two key issues commonly associated with medical data, namely privacy concerns (as a result of data protection rules and regulations) and the high costs of annotations. To demonstrate the full pipeline of applying DeepSynthBody concepts and user-friendly functionalities, we also describe a synthetic medical dataset generated and published using our framework. DeepSynthBody opens a new era of machine learning applications in medicine with a synthetic model of the human body.
ER  - 

TY  - JOUR
TI  - Interpretability Analysis of One-Year Mortality Prediction for Stroke Patients Based on Deep Neural Network
T2  - IEEE Journal of Biomedical and Health Informatics
SP  - 1903
EP  - 1910
AU  - S. Zhang
AU  - J. Wang
AU  - L. Pei
AU  - K. Liu
AU  - Y. Gao
AU  - H. Fang
AU  - R. Zhang
AU  - L. Zhao
AU  - S. Sun
AU  - J. Wu
AU  - B. Song
AU  - H. Dai
AU  - R. Li
AU  - Y. Xu
PY  - 2022
KW  - Stroke (medical condition)
KW  - Correlation
KW  - Predictive models
KW  - Medical diagnostic imaging
KW  - Diseases
KW  - Data models
KW  - Prognostics and health management
KW  - Stroke
KW  - deep learning
KW  - model inter- pretability
KW  - prognostic prediction
KW  - feature reconstruction
DO  - 10.1109/JBHI.2021.3123657
JO  - IEEE Journal of Biomedical and Health Informatics
IS  - 4
SN  - 2168-2208
VO  - 26
VL  - 26
JA  - IEEE Journal of Biomedical and Health Informatics
Y1  - April 2022
AB  - Clinically, physicians collect the benchmark medical data to establish archives for a stroke patient and then add the follow up data regularly. It has great significance on prognosis prediction for stroke patients. In this paper, we present an interpretable deep learning model to predict the one-year mortality risk on stroke. We design sub-modules to reconstruct features from original clinical data that highlight the dissimilarity and temporality of different variables. The model consists of Bidirectional Long Short-Term Memory (Bi-LSTM), in which a novel correlation attention module is proposed that takes the correlation of variables into consideration. In experiments, datasets are collected clinically from the department of neurology in a local AAA hospital. It consists of 2,275 stroke patients hospitalized in the department of neurology from 2014 to 2016. Our model achieves a precision of 0.9414, a recall of 0.9502 and an F1-score of 0.9415. In addition, we provide the analysis of the interpretability by visualizations with reference to clinical professional guidelines.
ER  - 

TY  - CONF
TI  - A Survey on Challenges in Designing Cognitive Engines
T2  - 2020 6th International Conference on Web Research (ICWR)
SP  - 165
EP  - 171
AU  - A. M. Saghiri
PY  - 2020
KW  - Computational modeling
KW  - Operating systems
KW  - Decision making
KW  - Medical services
KW  - Computer networks
KW  - Internet of Things
KW  - Cognitive systems
KW  - Cognitive Systems
KW  - Cognitive Engine
KW  - Digitalized Healthcare
KW  - Body-Mind Operating System
DO  - 10.1109/ICWR49608.2020.9122273
JO  - 2020 6th International Conference on Web Research (ICWR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 6th International Conference on Web Research (ICWR)
Y1  - 22-23 April 2020
AB  - The primary goal of cognitive computing is to design a digitalized model that is able to mimic human thinking processes. The cognitive engine is in charge of implementing the functionality of a cognitive system. Nowadays, cognitive engines are used as a self-organized management mechanism in different fields such as computer networks, Internet of Things (IoT), and Robotics. This is because the management algorithms of these fields are going to be very complex and therefore human thinking models as digitalized models are required for fast and accurate decision making. In this paper, we summarize challenges in designing cognitive engines. Then, a set of challenges in designing the cognitive engine for body-mind operating system in the digitalized healthcare system is obtained. In the literature, our survey and also suggested case study in the healthcare system have not been considered yet.
ER  - 

TY  - CONF
TI  - Explainable AI for Early Detection of Health Changes Via Streaming Clustering
T2  - 2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
SP  - 1
EP  - 6
AU  - W. Wu
AU  - J. M. Keller
AU  - M. Skubic
AU  - M. Popescu
PY  - 2022
KW  - Annotations
KW  - Clustering algorithms
KW  - Machine learning
KW  - Predictive models
KW  - Linguistics
KW  - Prediction algorithms
KW  - Behavioral sciences
KW  - explainable ai
KW  - streaming clustering
DO  - 10.1109/FUZZ-IEEE55066.2022.9882813
JO  - 2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
IS  - 
SN  - 1558-4739
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
Y1  - 18-23 July 2022
AB  - The ability to explain the predictions of machine learning models has become increasingly important, especially in healthcare applications. Streaming clustering is an effective tool to recognize normal baseline patterns and to detect early signs of changes in data streams. However, many streaming clustering algorithms are not designed to explain to the users how predictions are made. In this paper, we extend a streaming clustering algorithm, the sequential possibilistic Gaussian mixture model (SPGMM) for early detection of health change to provide algorithm explainability for the results. Four approaches are discussed to explain either the cluster differences or the reason for the algorithm warnings: (i) linguistic summarization for warnings; (ii) annotation distribution of clusters; (iii) SHapley Additive exPlanations (SHAP); (iv) functional health score. The four approaches are validated on one older adult monitored with a collection of motion, bed, and depth sensors over three years. The results obtained on the older adult show that the four approaches aid understanding of how the clusters and warnings are generated, providing strong support for clinicians to take corresponding actions.
ER  - 

TY  - CONF
TI  - Medical Chest X-ray Image Enhancement Based on CLAHE and Wiener Filter for Deep Learning Data Preprocessing
T2  - 2022 IEEE 14th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
SP  - 1
EP  - 6
AU  - R. P. C. Gamara
AU  - P. James M. Loresco
AU  - A. A. Bandala
PY  - 2022
KW  - Deep learning
KW  - Measurement
KW  - Wiener filters
KW  - Information filters
KW  - Medical diagnosis
KW  - Information technology
KW  - X-ray imaging
KW  - deep learning
KW  - medical x-ray images
KW  - CLAHE
KW  - Wiener filter
DO  - 10.1109/HNICEM57413.2022.10109585
JO  - 2022 IEEE 14th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
IS  - 
SN  - 2770-0682
VO  - 
VL  - 
JA  - 2022 IEEE 14th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
Y1  - 1-4 Dec. 2022
AB  - In medical imaging, an X-ray image generated using a flat panel detector (digital) typically has poor image quality, affecting the capability of successful medical diagnosis based on the images. The image enhancement process intends to provide better interpretability of the information contained in the images. The main problems considered for medical images include poor quality and low contrast. Therefore, the general objectives of image enhancement include contrast improvement and noise reduction. This study proposes an upgraded X-ray image enhancement hybrid algorithm that utilizes and consists of the Contrast Limited Adaptive Histogram Equalization (CLAHE) method combined with the Wiener filter. Based on the performance metrics results, including MSE, PSNR, and Entropy, as compared to the existing CLAHE method only, the proposed methodology has a lower MSE signifying lower error, a higher PSNR representing a lower amount of distortion, and higher information entropy which indicates higher obtained information. Furthermore, the implementation of the proposed approach is applied to 6000 X-ray images before deep learning classification modeling, which significantly improved from 50% to 78% validation accuracy. Therefore, the proposed method improves the image enhancement methodology and can substantially assist in diagnosing diseases.
ER  - 

TY  - JOUR
TI  - Testing the Generalizability of an Automated Method for Explaining Machine Learning Predictions on Asthma Patients’ Asthma Hospital Visits to an Academic Healthcare System
T2  - IEEE Access
SP  - 195971
EP  - 195979
AU  - Y. Tong
AU  - A. I. Messinger
AU  - G. Luo
PY  - 2020
KW  - Respiratory system
KW  - Predictive models
KW  - Hospitals
KW  - Data models
KW  - Machine learning
KW  - Diseases
KW  - Asthma
KW  - automated explanation
KW  - extreme gradient boosting
KW  - machine learning
KW  - patient care management
KW  - predictive model
DO  - 10.1109/ACCESS.2020.3032683
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - Asthma puts a tremendous overhead on healthcare. To enable effective preventive care to improve outcomes in managing asthma, we recently created two machine learning models, one using University of Washington Medicine data and the other using Intermountain Healthcare data, to predict asthma hospital visits in the next 12 months in asthma patients. As is common in machine learning, neither model supplies explanations for its predictions. To tackle this interpretability issue of black-box models, we developed an automated method to produce rule-style explanations for any machine learning model's predictions made on imbalanced tabular data and to recommend customized interventions without lowering the prediction accuracy. Our method exhibited good performance in explaining our Intermountain Healthcare model's predictions. Yet, it stays unknown how well our method generalizes to academic healthcare systems, whose patient composition differs from that of Intermountain Healthcare. This study evaluates our automated explaining method's generalizability to the academic healthcare system University of Washington Medicine on predicting asthma hospital visits. We did a secondary analysis on 82,888 University of Washington Medicine data instances of asthmatic adults between 2011 and 2018, using our method to explain our University of Washington Medicine model's predictions and to recommend customized interventions. Our results showed that for predicting asthma hospital visits, our automated explaining method had satisfactory generalizability to University of Washington Medicine. In particular, our method explained the predictions for 87.6% of the asthma patients whom our University of Washington Medicine model accurately predicted to experience asthma hospital visits in the next 12 months.
ER  - 

TY  - JOUR
TI  - Explainable AI for Multimodal Credibility Analysis: Case Study of Online Beauty Health (Mis)-Information
T2  - IEEE Access
SP  - 127985
EP  - 128022
AU  - V. Wagle
AU  - K. Kaur
AU  - P. Kamat
AU  - S. Patil
AU  - K. Kotecha
PY  - 2021
KW  - Blogs
KW  - Symbiosis
KW  - Medical services
KW  - Information integrity
KW  - Reliability
KW  - Internet
KW  - Credibility analysis
KW  - deep learning
KW  - misinformation
KW  - natural language processing
KW  - multimodal analysis
KW  - transfer learning
KW  - explainable AI
DO  - 10.1109/ACCESS.2021.3111527
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - “One person’s data or experience is another person’s information” this has become the golden rule of the 21st century which has resulted in a massive reservoir of data and immense amounts of information generation. However, there is no control over the source of this information, accessibility of this information, or the quality of it, which has given rise to the presence of “misinformation.” The research community has reacted by proposing frameworks and difficulties, which are helpful for (different subtasks of) recognizing misinformation. Most of these frameworks, however, fail to consider all the aspects that can contribute to making information “credible”. Furthermore, a valid explanation for each considered feature’s contribution to the model’s decision stands missing in most work. With this in mind, the authors have attempted to produce a system that yields highly accurate decisions, thus effectively separating credible health blogs from their non-credible counterparts while providing valid user-friendly explanations. The study proposes an Explainable AI-assisted Multimodal Credibility Assessment System that examines the credibility of the platform where the blog is hosted, the credibility of the author of the blog and the credibility of the images that contribute to the blog. This novel framework contributes to the existing body of knowledge by assessing the credibility of misleading beauty blogs using multiple crucial modalities which would lead to an insightful information consumption by the users. The proposed pipeline was successfully implemented on multiple carefully curated datasets and correctly identified 274 non credible blogs out of 321 blogs with an accuracy of 97.5%, Precision of 0.973 & F1score of 0.986. Further, the Explainable AI model, with the help of several visualizations displayed the feature contributions for each blog & it’s impact and magnitude in a concise comprehensible format. The framework can be further customized and applied to various domains where presence of misinformation is of high concern such as pharmaceutical drug information, pandemic management, financial advisories, online healthcare services and cyber frauds.
ER  - 

TY  - CONF
TI  - An Interpretability Analysis of Travel Decision Learning in Cyber-Physical-Social-Systems
T2  - 2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI)
SP  - 340
EP  - 343
AU  - M. Guo
AU  - P. Ye
AU  - X. Liu
AU  - S. Liu
AU  - G. Xiong
AU  - L. Zhang
PY  - 2021
KW  - Deep learning
KW  - Social networking (online)
KW  - Scalability
KW  - Perturbation methods
KW  - Neurons
KW  - Transportation
KW  - Transforms
KW  - Deep Learning
KW  - Interpretability
KW  - Cyber-Physical-Social-System
KW  - Travel Behavior
DO  - 10.1109/DTPI52967.2021.9540202
JO  - 2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI)
Y1  - 15 July-15 Aug. 2021
AB  - In the few decades, the rise of Internet and social media has fundamentally changed the management, control, and operation modes of modern engineering and social systems like defense, energy, critical health care and transportation.
ER  - 

TY  - CONF
TI  - Toward Efficient Automation of Interpretable Machine Learning
T2  - 2018 IEEE International Conference on Big Data (Big Data)
SP  - 4940
EP  - 4947
AU  - B. Kovalerchuk
AU  - N. Neuhaus
PY  - 2018
KW  - Classification algorithms
KW  - Prediction algorithms
KW  - Machine learning
KW  - Mathematical model
KW  - Machine learning algorithms
KW  - Computational modeling
KW  - Neural networks
KW  - machine learning
KW  - explainability
KW  - interpretability
KW  - accuracy
KW  - classifier
KW  - visualization
KW  - visual model
KW  - dominant intervals
DO  - 10.1109/BigData.2018.8622433
JO  - 2018 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Big Data (Big Data)
Y1  - 10-13 Dec. 2018
AB  - Developing more efficient automated methods for interpretable machine learning (ML) is an important and longterm machine-learning goal. Recent studies show that unintelligible "black" box models, such as Deep Learning Neural Networks, often outperform more interpretable "grey" or "white" box models such as Decision Trees, Bayesian networks, Logic Relational models and others. Being forced to choose between accuracy and interpretability, however, is a major obstacle in the wider adoption of ML in healthcare and other domains where decisions requires both facets. Due to human perceptual limitations in analyzing complex multidimensional relations in ML, complex ML must be "degraded" to the level of human understanding, thereby also degrading model accuracy. To address this challenge, this paper presents the Dominance Classifier and Predictor (DCP) algorithm, capable of automating the process of discovering human-understandable machine learning models that are simple and visualizable. The success of DCP is shown on the benchmark Wisconsin Breast Cancer dataset with the higher accuracy than the accuracy known for other interpretable methods on these data. Furthermore, the DCP algorithm shortens the accuracy gap between interpretable and non-interpretable models on these data. The DCP explanation includes both interpretable mathematical and visual forms. Such an approach opens a new opportunity for producing more accurate and domain-explainable ML models.
ER  - 

TY  - CONF
TI  - Comparing Interpretable AI Approaches for the Clinical Environment: an Application to COVID-19
T2  - 2022 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)
SP  - 1
EP  - 8
AU  - M. A. Onari
AU  - M. S. Nobile
AU  - I. Grau
AU  - C. Fuchs
AU  - Y. Zhang
AU  - A. -K. Boer
AU  - V. Scharnhorst
PY  - 2022
KW  - COVID-19
KW  - Computational modeling
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Feature extraction
KW  - Data models
KW  - Machine Learning
KW  - Explainable Artificial Intel-ligence
KW  - Interpretability
KW  - Trust
KW  - COVID-19
DO  - 10.1109/CIBCB55180.2022.9863020
JO  - 2022 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)
Y1  - 15-17 Aug. 2022
AB  - Machine Learning (ML) models play an important role in healthcare thanks to their remarkable performance in predicting complex phenomena. During the COVID-19 pandemic, different ML models were implemented to support decisions in the medical settings. However, clinical experts need to ensure that these models are valid, provide clinically useful information, and are implemented and used correctly. In this vein, they need to understand the logic behind the models to be able to trust them. Hence, developing transparent and interpretable models has increasing relevance. In this work, we applied four interpretable ML models including logistic regression, decision tree, pyFUME, and RIPPER to classify suspected COVID-19 patients based on clinical data collected from blood samples. After preprocessing the data set and training the models, we evaluate the models based on their predictive performance. Then, we illustrate that interpretability can be achieved in different ways. First, SHAP explanations are built from logistic regression and decision trees to obtain the features' importance. Then, the potential of pyFUME and RIPPER in providing inherent interpretability are reflected. Finally, potential ways to achieve trust in future studies are briefly discussed.
ER  - 

TY  - JOUR
TI  - Explainable Deep Learning Approach for Multilabel Classification of Antimicrobial Resistance With Missing Labels
T2  - IEEE Access
SP  - 113073
EP  - 113085
AU  - M. Tharmakulasingam
AU  - B. Gardner
AU  - R. L. Ragione
AU  - A. Fernando
PY  - 2022
KW  - Genomics
KW  - Bioinformatics
KW  - Predictive models
KW  - Immune system
KW  - Deep learning
KW  - Data models
KW  - Antibiotics
KW  - Artificial intelligence
KW  - Neural networks
KW  - Multilabel classification
KW  - deep neural network
KW  - multi-drug AMR
KW  - missing labels
KW  - explainable AI
DO  - 10.1109/ACCESS.2022.3216896
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 10
VL  - 10
JA  - IEEE Access
Y1  - 2022
AB  - Predicting Antimicrobial Resistance (AMR) from genomic sequence data has become a significant component of overcoming the AMR challenge, especially given its potential for facilitating more rapid diagnostics and personalised antibiotic treatments. With the recent advances in sequencing technologies and computing power, deep learning models for genomic sequence data have been widely adopted to predict AMR more reliably and error-free. There are many different types of AMR; therefore, any practical AMR prediction system must be able to identify multiple AMRs present in a genomic sequence. Unfortunately, most genomic sequence datasets do not have all the labels marked, thereby making a deep learning modelling approach challenging owing to its reliance on labels for reliability and accuracy. This paper addresses this issue by presenting an effective deep learning solution, Mask-Loss 1D convolution neural network (ML-ConvNet), for AMR prediction on datasets with many missing labels. The core component of ML- ConvNet utilises a masked loss function that overcomes the effect of missing labels in predicting AMR. The proposed ML-ConvNet is demonstrated to outperform state-of-the-art methods in the literature by 10.5%, according to the F1 score. The proposed model’s performance is evaluated using different degrees of the missing label and is found to outperform the conventional approach by 76% in the F1 score when 86.68% of labels are missing. Furthermore, the ML-ConvNet was established with an explainable artificial intelligence (XAI) pipeline, thereby making it ideally suited for hospital and healthcare settings, where model interpretability is an essential requirement.
ER  - 

TY  - CONF
TI  - Explainable image analysis for decision support in medical healthcare
T2  - 2021 IEEE International Conference on Big Data (Big Data)
SP  - 4667
EP  - 4674
AU  - R. Corizzo
AU  - Y. Dauphin
AU  - C. Bellinger
AU  - E. Zdravevski
AU  - N. Japkowicz
PY  - 2021
KW  - Deep learning
KW  - Dimensionality reduction
KW  - COVID-19
KW  - Pulmonary diseases
KW  - Magnetic resonance imaging
KW  - Computed tomography
KW  - Neural networks
KW  - Deep Learning
KW  - Clustering
KW  - XAI
KW  - Healthcare
KW  - COVID-19
DO  - 10.1109/BigData52589.2021.9671335
JO  - 2021 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Big Data (Big Data)
Y1  - 15-18 Dec. 2021
AB  - Recent advances in medical imaging and deep learning have enabled the efficient analysis of large databases of images. Notable examples include the analysis of computed tomography (CT), magnetic resonance imaging (MRI), and X-ray. While the automatic classification of images has proven successful, adopting such a paradigm in the medical healthcare setting is unfeasible. Indeed, the physician in charge of the detailed medical assessment and diagnosis of patients cannot trust a deep learning model’s decisions without further explanations or insights about their classification outcome. In this study, rather than relying on classification, we propose a new method that leverages deep neural networks to extract a representation of images and further analyze them through clustering, dimensionality reduction for visualization, and class activation mapping. Thus, the system does not make decisions on behalf of physicians. Instead, it helps them make a diagnosis. Experimental results on lung images affected by Pneumonia and Covid-19 lesions show the potential of our method as a tool for decision support in a medical setting. It allows the physician to identify groups of similar images and highlight regions of the input that the model deemed important for its predictions.
ER  - 

TY  - CONF
TI  - Evaluating Hierarchical Medical Workflows using Feature Importance
T2  - 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)
SP  - 265
EP  - 270
AU  - U. Pawar
AU  - C. T. Culbert
AU  - R. O'Reilly
PY  - 2021
KW  - Measurement
KW  - Heart
KW  - Medical services
KW  - Machine learning
KW  - Learning (artificial intelligence)
KW  - Complexity theory
KW  - Diseases
KW  - Explainable Artificial Intelligence
KW  - Hierarchical Medical Settings
KW  - Feature Importance
KW  - Machine Learning
DO  - 10.1109/CBMS52027.2021.00075
JO  - 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)
IS  - 
SN  - 2372-9198
VO  - 
VL  - 
JA  - 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)
Y1  - 7-9 June 2021
AB  - The applicability and utility of Artificial Intelligence (AI) based solutions has been demonstrated widely in the healthcare domain via the automated analysis of medical information. However, the adoption rate of AI-based healthcare systems is inhibited due to their complicated nature. Also, the hierarchical nature of the medical settings adds a layer of complexity in understanding how a Machine Learning (ML) model functions when subjected to varying workflows. Variation in models' performance, and individual features' contribution, needs to be effectively quantified such that medical practitioners can understand the models, and validate their operation if widespread adoption is to be enabled. In this paper, a hierarchical medical workflow for understanding the operation of ML in a healthcare-based setting is proposed. Its utility is demonstrated in the context of heart disease classification. Explainable Artificial Intelligence (XAI) is incorporated in the form of Feature Importance (FI) scores and correlated with an ML model's performance metrics (Accuracy, F1-score). This provides a multi-stakeholder perspective aligned with the hierarchy as experienced in a real-world medical setting. The paper contributes a methodology for accommodating an enhanced understanding of diverse hierarchical healthcare settings that would benefit from the adoption of AI-based systems.
ER  - 

TY  - JOUR
TI  - Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis
T2  - IEEE Journal of Biomedical and Health Informatics
SP  - 1589
EP  - 1604
AU  - B. Shickel
AU  - P. J. Tighe
AU  - A. Bihorac
AU  - P. Rashidi
PY  - 2018
KW  - Machine learning
KW  - Electronic medical records
KW  - Medical diagnostic imaging
KW  - Informatics
KW  - Hospitals
KW  - Clinical informatics
KW  - deep learning
KW  - electronic health records
KW  - machine learning
KW  - survey
DO  - 10.1109/JBHI.2017.2767063
JO  - IEEE Journal of Biomedical and Health Informatics
IS  - 5
SN  - 2168-2208
VO  - 22
VL  - 22
JA  - IEEE Journal of Biomedical and Health Informatics
Y1  - Sept. 2018
AB  - The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHRs). While primarily designed for archiving patient information and performing administrative healthcare tasks like billing, many researchers have found secondary use of these records for various clinical informatics applications. Over the same period, the machine learning community has seen widespread advances in the field of deep learning. In this review, we survey the current research on applying deep learning to clinical tasks based on EHR data, where we find a variety of deep learning techniques and frameworks being applied to several types of clinical applications including information extraction, representation learning, outcome prediction, phenotyping, and deidentification. We identify several limitations of current research involving topics such as model interpretability, data heterogeneity, and lack of universal benchmarks. We conclude by summarizing the state of the field and identifying avenues of future deep EHR research.
ER  - 

TY  - CONF
TI  - Predicting Liquidity Ratio of Mutual Funds via Ensemble Learning
T2  - 2020 IEEE International Conference on Big Data (Big Data)
SP  - 5441
EP  - 5450
AU  - K. Kong
AU  - R. Liu
AU  - Y. Zhang
AU  - Y. Chen
PY  - 2020
KW  - Regulators
KW  - Mutual funds
KW  - Machine learning
KW  - Predictive models
KW  - Big Data
KW  - Tools
KW  - Robustness
KW  - XAI
KW  - liquidity ratio
KW  - mutual funds
KW  - Ensemble Learning
KW  - fund position
DO  - 10.1109/BigData50022.2020.9378486
JO  - 2020 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Big Data (Big Data)
Y1  - 10-13 Dec. 2020
AB  - We are entering a new era of AI, in which the core technology is machine learning. However, many machine learning models are opaque and not intuitive enough, making it difficult for users to understand how AI systems make decisions. In some scenarios, especially in the fields of finance, healthcare, automatic driving, users have a strong demand for the interpretability of the model. Although AI systems provide a lot of benefits, if the decision and behavior cannot be explained to users or regulators, the effectiveness and development of the systems will be restricted. To gain the trust of users, Explainable AI is necessary.Daily prediction of mutual fund holdings can be a very useful tool. If we can predict the daily holding positions of large mutual funds, we can gain insights into the sentiments of institutional investors which shed lights into the outlook of the market. How to get daily holding from the delayed disclosure information? We leverage on another source of key information - the price of a mutual fund is updated daily, often released a few hours after the market closes. Therefore, we can utilize the daily price fluctuation, combined with quarterly revealed holdings, to make daily predictions of mutual fund holdings. In this paper, we proposed an Ensemble Learning model to predict liquidity ratio of mutual funds. The model has strong interpretability, which is beneficial to users, developers and regulators and all parties involved. Compared with the real fund position data only disclosed once a quarter, our model effectuates timely and efficient high-frequency calculation. In the process of modeling, we creatively apply the framework of Ensemble Learning to portfolio decomposition for the first time. The Ensemble Learning model leverages the diversity of base learners to improve the overall prediction performance. Extensive empirical results on China A-Shares market show that our model can achieve superior accuracy, robustness, and generalization ability.
ER  - 

TY  - CONF
TI  - Deep Learning, Blockchain based Multi-layered Authentication and Security Architectures
T2  - 2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
SP  - 476
EP  - 485
AU  - H. S. K. Sheth
AU  - I. A. K
AU  - A. K. Tyagi
PY  - 2022
KW  - Deep learning
KW  - Data privacy
KW  - Q-learning
KW  - Authentication
KW  - Sensor systems
KW  - Data models
KW  - Security
KW  - Authentication Methods
KW  - Deep Learning
KW  - Blockchain
KW  - Multi Layered Security Architecture
KW  - Health
KW  - IoT
KW  - Deep Q Learning
DO  - 10.1109/ICAAIC53929.2022.9793179
JO  - 2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)
Y1  - 9-11 May 2022
AB  - Because of its ability to make educated judgments, deep learning has gained massive attention in recent years. Many of today's deep learning systems rely on centralized servers and lack operational transparency, traceability, dependability, security, and trustworthy data provenance. Furthermore, using centralized data to train deep learning models exposes them to the single point of failure issue. The relevance of combining blockchain technology and deep learning is highlighted in this study. Deep-learning (DL) techniques are used to authenticate and identify abnormalities and provide security for systems (in cryptographic and biometric systems). Confidentiality and effectiveness must be balanced since network sensors are energy-constrained devices, which is the most crucial idea to consider when establishing a security system that relies on deep-learning techniques and blockchain. Centralized systems have several flaws. Mostly, a network with significant demand for smart devices creates a prodigious amount of data. There is always the possibility that one or more of the centralized network's major components would fail, causing a catastrophic (or full) system failure. The data acquired by the centralized cloud storage frequently necessitates third-party modification. This may result in data breaches, jeopardizing the privacy of the end-user. This paper primarily focuses on the security models proposed in terms of authentication and security using Blockchain, Deep Learning, or the integration of both based on certain characteristics to identify and organize the literature, inclouding type, models, consensus protocols, applications, services, and deployment goals.
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence for 6G: Improving Trust between Human and Machine
T2  - IEEE Communications Magazine
SP  - 39
EP  - 45
AU  - W. Guo
PY  - 2020
KW  - Mathematical model
KW  - Computational modeling
KW  - Wireless communication
KW  - Optimization
KW  - Analytical models
KW  - Deep learning
KW  - 6G mobile communication
DO  - 10.1109/MCOM.001.2000050
JO  - IEEE Communications Magazine
IS  - 6
SN  - 1558-1896
VO  - 58
VL  - 58
JA  - IEEE Communications Magazine
Y1  - June 2020
AB  - As 5G mobile networks are bringing about global societal benefits, the design phase for 6G has started. Evolved 5G and 6G will need sophisticated AI to automate information delivery simultaneously for mass autonomy, human machine interfacing, and targeted healthcare. Trust will become increasingly critical for 6G as it manages a wide range of mission-critical services. As we migrate from traditional mathematical model-dependent optimization to data-dependent deep learning, the insight and trust we have in our optimization modules decrease. This loss of model explainability means we are vulnerable to malicious data, poor neural network design, and the loss of trust from stakeholders and the general public -- all with a range of legal implications. In this review, we outline the core methods of explainable artificial intelligence (XAI) in a wireless network setting, including public and legal motivations, definitions of explainability, performance vs. explainability trade-offs, and XAI algorithms. Our review is grounded in case studies for both wireless PHY and MAC layer optimization and provide the community with an important research area to embark upon.
ER  - 

TY  - CONF
TI  - Pneumonia Detection On Chest X-Ray Using Radiomic Features And Contrastive Learning
T2  - 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)
SP  - 247
EP  - 251
AU  - Y. Han
AU  - C. Chen
AU  - A. Tewfik
AU  - Y. Ding
AU  - Y. Peng
PY  - 2021
KW  - Deep learning
KW  - Pulmonary diseases
KW  - Biological system modeling
KW  - Neural networks
KW  - Radiology
KW  - Feature extraction
KW  - Delays
KW  - radiomics
KW  - medical imaging
KW  - CNN
KW  - chest X-ray
KW  - neural networks
KW  - interpretability
DO  - 10.1109/ISBI48211.2021.9433853
JO  - 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)
IS  - 
SN  - 1945-8452
VO  - 
VL  - 
JA  - 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)
Y1  - 13-16 April 2021
AB  - Chest X-ray becomes one of the most common medical diagnoses due to its noninvasiveness. The number of chest X-ray images has skyrocketed, but reading chest X-rays still has been manually performed by radiologists, which creates huge burnouts and delays. Traditionally, radiomics, as a subfield of radiology that can extract a large number of quantitative features from medical images, demonstrates its potential to facilitate medical imaging diagnosis before the deep learning era. With the rise of deep learning, the explainability of deep neural networks on chest X-ray diagnosis remains opaque. In this study, we proposed a novel framework that leverages radiomics features and contrastive learning to detect pneumonia in chest X-ray. Experiments on the RSNA Pneumonia Detection Challenge dataset show that our model achieves superior results to several state-of-the-art models ($\gt 10$ % in F1-score) and increases the model’s interpretability.
ER  - 

TY  - CONF
TI  - Novel Machine Learning Ensemble Models for Active Diabetes Diagnosis
T2  - 2021 International Conference on Technological Advancements and Innovations (ICTAI)
SP  - 466
EP  - 471
AU  - M. A. M. Iesa
AU  - A. P. Shirahatt
AU  - H. Sharma
AU  - M. K. Goyal
AU  - A. Shrivastava
AU  - B. Vajrala
PY  - 2021
KW  - Technological innovation
KW  - Machine learning algorithms
KW  - Medical services
KW  - Manuals
KW  - Forestry
KW  - Real-time systems
KW  - Diabetes
KW  - Deep Learning
KW  - Random Forest
KW  - Diabetes
KW  - Healthcare
KW  - GBDT
KW  - Interpretability
DO  - 10.1109/ICTAI53825.2021.9673166
JO  - 2021 International Conference on Technological Advancements and Innovations (ICTAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Technological Advancements and Innovations (ICTAI)
Y1  - 10-12 Nov. 2021
AB  - Now that diabetes is on the rise as a silent killer, it will have catastrophic consequences if preventative measures are not implemented soon enough. Given that diabetes cannot be cured after it has been diagnosed in a patient, early identification of the illness is critical in diabetes. Doctors are finding it more difficult to do manual detection as the number of patients grows on a daily basis. We can do some automobile detection using tools like Machine Learning. This issue of diabetes diagnosis has been the subject of much study up to this point. Using two ensemble machine learning algorithms like Random Forest and GBDT, this study does predictive analysis. Pima Indians Diabetes Dataset, which includes data of diabetes patients, was used in this study to conduct different experiments. The findings are presented in this article. This article also addresses the significance of output interpretability in the healthcare sector and explains how providing output interpretability together with the machine learning model’s output on the patient record would assist physicians in real-time (in the future).
ER  - 

TY  - CONF
TI  - Inexplicable AI in Medicine as a Form of Epistemic Oppression
T2  - 2022 IEEE International Symposium on Technology and Society (ISTAS)
SP  - 1
EP  - 5
AU  - C. Herzog
PY  - 2022
KW  - Instruments
KW  - Decision making
KW  - Closed box
KW  - Medical services
KW  - Approximation algorithms
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Explicable AI
KW  - Responsible AI
KW  - Medicine
KW  - Medical AI
KW  - Epistemic Oppression
DO  - 10.1109/ISTAS55053.2022.10227139
JO  - 2022 IEEE International Symposium on Technology and Society (ISTAS)
IS  - 
SN  - 2158-3412
VO  - 1
VL  - 1
JA  - 2022 IEEE International Symposium on Technology and Society (ISTAS)
Y1  - 10-12 Nov. 2022
AB  - This contribution portrays inexplicable AI in medicine as a form of epistemic exclusion, i.e., as the marginalization or complete elimination of important stakeholders from the knowledge generation process. To show this, I will first briefly characterize and exemplify the notion of explicability as per Floridi in the medical domain as being instrumental to support accountability through intelligible explanatory interfaces. I will then follow Dotson in delineating three different orders of epistemic oppression, the third being irreducible to either social or political oppression. I will continue to argue that inexplicable AI in medicine, in its tendency to severely hinder a decision-making process that is either shared interprofessionally and/or between patient and physician, may, under certain conditions, amount to third-order epistemic exclusion. I will discuss that it follows that the adoption of inexplicable AI in medicine may severely hamper progress to support health as conceived holistically along the lines of the WHO’s constitution. Instead, the use of inexplicable AI in medicine may bring about short-term benefits, which may be tempting, but should not outweigh the long-term advantages promised by a patient-centered and individualized form of medicine. In summary, this contribution adds a novel conceptual take on the issues involved with adopting black-box AI in the medical domain that goes beyond a merely utility-based argumentation, but—in addition—depicts it as a form of epistemic exclusion, which is also wrong in itself.
ER  - 

TY  - CONF
TI  - Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations
T2  - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
SP  - 1524
EP  - 1533
AU  - M. Watson
AU  - B. A. Shiekh Hasan
AU  - N. A. Moubayed
PY  - 2022
KW  - Deep learning
KW  - Training
KW  - Medical conditions
KW  - Neural networks
KW  - MIMICs
KW  - Logic gates
KW  - Market research
KW  - Explainable AI
KW  - Fairness
KW  - Accountability
KW  - Privacy and Ethics in Vision Datasets
KW  - Evaluation and Comparison of Vision Algorithms
KW  - Deep Learning
KW  - Medical Imaging/Imaging for Bioinformatics/Biological and Cell Microscopy
DO  - 10.1109/WACV51458.2022.00159
JO  - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
IS  - 
SN  - 2642-9381
VO  - 
VL  - 
JA  - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
Y1  - 3-8 Jan. 2022
AB  - Deep Learning of neural networks has progressively become more prominent in healthcare with models reaching, or even surpassing, expert accuracy levels. However, these success stories are tainted by concerning reports on the lack of model transparency and bias against some medical conditions or patients’ sub-groups. Explainable methods are considered the gateway to alleviate many of these concerns. In this study we demonstrate that the generated explanations are volatile to changes in model training that are perpendicular to the classification task and model structure. This raises further questions about trust in deep learning models for healthcare. Mainly, whether the models capture underlying causal links in the data or just rely on spurious correlations that are made visible via explanation methods. We demonstrate that the output of explainability methods on deep neural networks can vary significantly by changes of hyper-parameters, such as the random seed or how the training set is shuffled. We introduce a measure of explanation consistency which we use to highlight the identified problems on the MIMIC-CXR dataset. We find explanations of identical models but with different training setups have a low consistency: ≈ 33% on average. On the contrary, kernel methods are robust against any orthogonal changes, with explanation consistency at 94%. We conclude that current trends in model explanation are not sufficient to mitigate the risks of deploying models in real life healthcare applications.
ER  - 

TY  - CONF
TI  - Explainable Machine Learning Models for Prediction of Smoking Cessation Outcome in New Zealand
T2  - 2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS)
SP  - 764
EP  - 768
AU  - I. C. Medina
AU  - M. Mohaghegh
PY  - 2022
KW  - Pregnancy
KW  - Sociology
KW  - Predictive models
KW  - Prediction algorithms
KW  - Data models
KW  - Decision trees
KW  - Reliability
KW  - smoking cessation
KW  - interpretable machine learning
KW  - CatBoost classifier
KW  - SHAP TreeExplainer
KW  - New Zealand
DO  - 10.1109/COMSNETS53615.2022.9668458
JO  - 2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS)
IS  - 
SN  - 2155-2509
VO  - 
VL  - 
JA  - 2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS)
Y1  - 4-8 Jan. 2022
AB  - In many countries including New Zealand, tobacco use has been considered as the leading cause of preventable diseases and death. Among the adult smokers, more than one fourth makes quit attempts every year. Therefore, selecting the most suitable smoking cessation program and support system is crucial to help smokers achieve the desirable outcomes. A total of 14,443 enrolments were analyzed in this study using machine learning techniques based on tree ensemble and gradient boosted trees. The CatBoost model was adopted which performed better in all data set analyzed compared to Decision Trees, Random Forest or XGBoost. Predictive models were constructed for different priority populations and SHAP TreeExplainer was used to determine the most important predictors associated with smoking cessation outcome. These interpretable machine learning models could help support healthcare providers in making reliable decisions in assisting smokers throughout their quit smoking journey.
ER  - 

TY  - CONF
TI  - Trustworthy Explanations for Knowledge Discovered from E-Health Records
T2  - 2022 IEEE International Conference on E-health Networking, Application & Services (HealthCom)
SP  - 246
EP  - 251
AU  - B. S. Dhaliwal
AU  - R. Imran
AU  - C. K. Leung
AU  - E. W. R. Madill
PY  - 2022
KW  - COVID-19
KW  - Soft sensors
KW  - Medical services
KW  - Machine learning
KW  - Big Data
KW  - Data science
KW  - Knowledge discovery
KW  - E-health
KW  - COVID-19
KW  - data analytics
KW  - data science
KW  - explainability
KW  - explainable artificial intelligence (XAI)
KW  - health informatics
KW  - hospitalization
KW  - interpretability
KW  - machine learning
KW  - prediction
KW  - trustworthy AI
KW  - visual analytics
DO  - 10.1109/HealthCom54947.2022.9982786
JO  - 2022 IEEE International Conference on E-health Networking, Application & Services (HealthCom)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on E-health Networking, Application & Services (HealthCom)
Y1  - 17-19 Oct. 2022
AB  - In the current era of big data, very large amounts of data are generating at a rapid rate from a wide variety of rich data sources. Electronic health (e-health) records are examples of the big data. With the technological advancements, more healthcare practice has gradually been supported by electronic processes and communication. This enables health informatics, in which computer science meets the healthcare sector to address healthcare and medical problems. Embedded in the big data are valuable information and knowledge that can be discovered by data science, data mining and machine learning techniques. Many of these techniques apply "opaque box" approaches to make accurate predictions. However, these techniques may not be crystal clear to the users. As the users not necessarily be able to clearly view the entire knowledge discovery (e.g., prediction) process, they may not easily trust the discovered knowledge (e.g., predictions). Hence, in this paper, we present a system for providing trustworthy explanations for knowledge discovered from e-health records. Specifically, our system provides users with global explanations for the important features among the records. It also provides users with local explanations for a particular record. Evaluation results on real-life e-health records show the practicality of our system in providing trustworthy explanations to knowledge discovered (e.g., accurate predictions made).
ER  - 

TY  - CONF
TI  - Interpretability of deep learning models: A survey of results
T2  - 2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)
SP  - 1
EP  - 6
AU  - S. Chakraborty
AU  - R. Tomsett
AU  - R. Raghavendra
AU  - D. Harborne
AU  - M. Alzantot
AU  - F. Cerutti
AU  - M. Srivastava
AU  - A. Preece
AU  - S. Julier
AU  - R. M. Rao
AU  - T. D. Kelley
AU  - D. Braines
AU  - M. Sensoy
AU  - C. J. Willis
AU  - P. Gurram
PY  - 2017
KW  - Machine learning
KW  - Training
KW  - Training data
KW  - Data models
KW  - Brain modeling
KW  - Predictive models
KW  - Neurons
DO  - 10.1109/UIC-ATC.2017.8397411
JO  - 2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)
Y1  - 4-8 Aug. 2017
AB  - Deep neural networks have achieved near-human accuracy levels in various types of classification and prediction tasks including images, text, speech, and video data. However, the networks continue to be treated mostly as black-box function approximators, mapping a given input to a classification output. The next step in this human-machine evolutionary process - incorporating these networks into mission critical processes such as medical diagnosis, planning and control - requires a level of trust association with the machine output. Typically, statistical metrics are used to quantify the uncertainty of an output. However, the notion of trust also depends on the visibility that a human has into the working of the machine. In other words, the neural network should provide human-understandable justifications for its output leading to insights about the inner workings. We call such models as interpretable deep networks. Interpretability is not a monolithic notion. In fact, the subjectivity of an interpretation, due to different levels of human understanding, implies that there must be a multitude of dimensions that together constitute interpretability. In addition, the interpretation itself can be provided either in terms of the low-level network parameters, or in terms of input features used by the model. In this paper, we outline some of the dimensions that are useful for model interpretability, and categorize prior work along those dimensions. In the process, we perform a gap analysis of what needs to be done to improve model interpretability.
ER  - 

TY  - CONF
TI  - Predicting Dengue Outbreaks with Explainable Machine Learning
T2  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
SP  - 940
EP  - 947
AU  - R. Aleixo
AU  - F. Kon
AU  - R. Rocha
AU  - M. S. Camargo
AU  - R. Y. De Camargo
PY  - 2022
KW  - Measurement
KW  - Training
KW  - Support vector machines
KW  - Computational modeling
KW  - Urban areas
KW  - Predictive models
KW  - Data models
KW  - Dengue Fever
KW  - Epidemiology
KW  - Public Health
KW  - Machine Learning
KW  - Explainable AI
DO  - 10.1109/CCGrid54584.2022.00114
JO  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 22nd IEEE International Symposium on Cluster, Cloud and Internet Computing (CCGrid)
Y1  - 16-19 May 2022
AB  - Seasonal infectious diseases, such as dengue, have been causing great losses in many countries around the world in terms of deaths, quality of life, and economic burden. In Brazil, this is relevant not only in large cities such as Rio de Janeiro and São Paulo but, according to the Ministry of Health, in another 500 cities throughout the country. Predicting the occurrence of diseases, such as dengue bursts, can be a valuable instrument for public health management as health officials can better prepare and redirect resources to the affected areas. In this paper, we present an explainable machine learning model to forecast the number of dengue occurrences in a large metropolis, Rio de Janeiro. We focus on explainable models, which provide health authorities with the reasons for outbreak predictions, allowing them to plan their actions accordingly. We trained a gradient boosting decision tree algorithm (CatBoost) with data from the National System of Information on Notifiable Diseases (SINAN), weather data, and socio-demographic data from The Brazilian Institute of Geoaraphy and Statistics (IBGE).
ER  - 

TY  - CHAP
TI  - Moving the AI Needle
T2  - The AI Book: The Artificial Intelligence Handbook for Investors, Entrepreneurs and FinTech Visionaries
SP  - 75
EP  - 77
AU  - Hendrik Abel
AU  - Ulrich Kleipa&#xdf;
PY  - 2020
KW  - Artificial intelligence
KW  - Insurance
KW  - Predictive models
KW  - Chatbots
KW  - Analytical models
KW  - Explainable AI
KW  - Standards organizations
KW  - Risk management
KW  - Process control
KW  - Needles
DO  - 10.1002/9781119551966.ch21
PB  - Wiley
SN  - 9781119551867
UR  - http://ieeexplore.ieee.org/document/10954317
AB  - Summary <p>The healthcare sector is one of the biggest digital data providers, accounting for one third of global digital data volumes. This chapter aims to identify relevant artificial intelligence (AI) use cases that really move the needle for health insurers and give an overview on key strategic imperatives to derive an AI strategy that can best utilize their untapped data treasures. A wealth of use cases for AI has emerged that are gaining increasing traction both outside and inside health insurance. In general, use cases can be classified into three different levels: standard AI solutions, tailor&#x2010;made AI solutions, and explainable AI solutions. There is no right or wrong approach; however, health insurers need to start treating AI as a strategic asset that can help them in achieving their overall strategic goals. The chapter provides a four&#x2010;step approach that can help decision&#x2010;makers find the right starting point, and to plot a way forward.</p>
ER  - 

TY  - CONF
TI  - Demystify the Black-box of Deep Learning Models for COVID-19 Detection from Chest CT Radiographs
T2  - 2021 24th International Conference on Computer and Information Technology (ICCIT)
SP  - 1
EP  - 6
AU  - M. N. Islam
AU  - M. Hasan
AU  - A. K. M. Masum
AU  - M. Z. Uddin
AU  - M. G. R. Alam
PY  - 2021
KW  - COVID-19
KW  - Deep learning
KW  - Radiography
KW  - Computed tomography
KW  - Computational modeling
KW  - Medical services
KW  - Data models
KW  - COVID-19
KW  - transfer learning
KW  - explainable AI
KW  - CT Imaging
DO  - 10.1109/ICCIT54785.2021.9689784
JO  - 2021 24th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 24th International Conference on Computer and Information Technology (ICCIT)
Y1  - 18-20 Dec. 2021
AB  - Covid 19 continues to have a catastrpoic effect on the world, causing terrible spots to appear all over the place. Due to global epidemics and doctor and healthcare personel shortages, developing an AI-based system to detect COVID in a timely and cost-effective method has become a requirement. It is also essential to detect covid from chest X-ray and CT radiographs due to their accuracy in detecting lung infection and as well as to understand the severity. Moreover, though the number of infected people around the globe is enormous, the amount of covid data set to build an AI system is scarce and scattered. In this letter, we presented a Chest CT scan data (HRCT) set for Covid and healthy patients considering a varying range of severity of COVID, which we published on kaggle, that can assist other researchers to contribute to healthcare AI. We also developed three deep learning approaches for detecting covid quickly and cheaply. Our three transfer learning-based approaches, Inception v3, Resnet 50, and VGG16, achieve accuracy of 99.8%, 91.3%, and 99.3%, respectively on unseen data. We delve deeper into the black boxes of those models to demonstrate how our model comes to a certain conclusion, and we found that, despite the low accuracy of the model based on VGG16, it detects the covid spot of images well, which we believe may further assist doctors in visualizing which regions are affected.
ER  - 

TY  - CONF
TI  - An Interpretable Deep Learning System for Automatically Scoring Request for Proposals
T2  - 2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)
SP  - 851
EP  - 855
AU  - S. Maji
AU  - A. Appe
AU  - R. Bali
AU  - A. Ghosh Chowdhury
AU  - V. Chikka Raghavendra
AU  - V. M. Bhandaru
PY  - 2021
KW  - Deep learning
KW  - Conferences
KW  - Medical services
KW  - Writing
KW  - Transformers
KW  - Natural language processing
KW  - Proposals
KW  - NLP
KW  - Deep Learning
KW  - Healthcare
KW  - Automatic Scoring Systems
DO  - 10.1109/ICTAI52525.2021.00136
JO  - 2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)
IS  - 
SN  - 2375-0197
VO  - 
VL  - 
JA  - 2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)
Y1  - 1-3 Nov. 2021
AB  - The Managed Care system within Medicaid (US Healthcare) uses Request For Proposals (RFP) to award contracts for various healthcare and related services. RFP responses are very detailed documents (hundreds of pages) submitted by competing organisations to win contracts. Subject matter expertise and domain knowledge play an important role in preparing RFP responses along with analysis of historical submissions. Automated analysis of these responses through Natural Language Processing (NLP) systems can reduce time and effort needed to explore historical responses, and assisting in writing better responses. Our work draws parallels between scoring RFPs and essay scoring models, while highlighting new challenges and the need for interpretability. Typical scoring models focus on word level impacts to grade essays and other short write-ups. We propose a novel Bi-LSTM and a transformer based regression model, and provide deeper insight into phrases which latently impact scoring of responses. We contend the merits of our proposed methodology using extensive quantitative experiments. We also qualitatively assess the impact of important phrases using human evaluators. Finally, we introduce a novel problem statement that can be used to further improve the state of the art in NLP based automatic scoring systems.
ER  - 

TY  - JOUR
TI  - VBridge: Connecting the Dots Between Features and Data to Explain Healthcare Models
T2  - IEEE Transactions on Visualization and Computer Graphics
SP  - 378
EP  - 388
AU  - F. Cheng
AU  - D. Liu
AU  - F. Du
AU  - Y. Lin
AU  - A. Zytek
AU  - H. Li
AU  - H. Qu
AU  - K. Veeramachaneni
PY  - 2022
KW  - Predictive models
KW  - Decision making
KW  - Tools
KW  - Computational modeling
KW  - Visual analytics
KW  - Hospitals
KW  - Task analysis
KW  - Explainable Artificial Intelligence
KW  - Healthcare
KW  - Visual Analytics
KW  - Decision Making
DO  - 10.1109/TVCG.2021.3114836
JO  - IEEE Transactions on Visualization and Computer Graphics
IS  - 1
SN  - 1941-0506
VO  - 28
VL  - 28
JA  - IEEE Transactions on Visualization and Computer Graphics
Y1  - Jan. 2022
AB  - Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.
ER  - 

TY  - CONF
TI  - An Explainable Computer Vision in Histopathology: Techniques for Interpreting Black Box Model
T2  - 2022 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)
SP  - 392
EP  - 398
AU  - S. Bhattacharjee
AU  - Y. -B. Hwang
AU  - K. Ikromjanov
AU  - R. I. Sumon
AU  - H. -C. Kim
AU  - H. -K. Choi
PY  - 2022
KW  - Computer vision
KW  - Visualization
KW  - Histopathology
KW  - Computational modeling
KW  - Receivers
KW  - Robustness
KW  - Artificial intelligence
KW  - explainable computer vision
KW  - histopathology
KW  - artificial intelligence
KW  - black box
KW  - prostate cancer
DO  - 10.1109/ICAIIC54071.2022.9722656
JO  - 2022 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)
Y1  - 21-24 Feb. 2022
AB  - Computer vision is a field of artificial intelligence (AI) that is being used increasingly in histopathology to identify pathologies in slide images with a high degree of accuracy. In this paper, we focus on the different interpreting techniques of explainable computer vision (XCV). Analysis of histopathology images is a challenging task, and specialized knowledge is mandatory to make AI decisions. To carry out this analysis, a deep learning model has been used to classify and differentiate the scoring (i.e., benign and malignant) of Prostate cancer (PCa). However, the AI models are complex and opaque, and it is important to understand model decision-making. Therefore, to address this problem, we present three techniques for accountability and transparency of the model, namely Activation Layer Visualization (ALV), Local Interpretable Model-Agnostic Explanation (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-CAM). XCV is AI in which the results of the black-box model can be understood by humans. The robustness of our model has been confirmed by using an external test dataset including 100 histopathology images. The model performance has been evaluated using the receiver operating characteristic (ROC) curve.
ER  - 

TY  - CONF
TI  - X-MIR: EXplainable Medical Image Retrieval
T2  - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
SP  - 1544
EP  - 1554
AU  - B. Hu
AU  - B. Vasu
AU  - A. Hoogs
PY  - 2022
KW  - COVID-19
KW  - Measurement
KW  - Computer vision
KW  - Image retrieval
KW  - Medical services
KW  - Machine learning
KW  - Skin
KW  - Explainable AI
KW  - Fairness
KW  - Accountability
KW  - Privacy and Ethics in Vision Medical Imaging/Imaging for Bioinformatics/Biological and Cell Microscopy
DO  - 10.1109/WACV51458.2022.00161
JO  - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
IS  - 
SN  - 2642-9381
VO  - 
VL  - 
JA  - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
Y1  - 3-8 Jan. 2022
AB  - Despite significant progress in the past few years, machine learning systems are still often viewed as "black boxes," which lack the ability to explain their output decisions. In high-stakes situations such as healthcare, there is a need for explainable AI (XAI) tools that can help open up this black box. In contrast to approaches which largely tackle classification problems in the medical imaging domain, we address the less-studied problem of explainable image retrieval. We test our approach on a COVID-19 chest X-ray dataset and the ISIC 2017 skin lesion dataset, showing that saliency maps help reveal the image features used by models to determine image similarity. We evaluated three different saliency algorithms, which were either occlusion-based, attention-based, or relied on a form of activation mapping. We also develop quantitative evaluation metrics that allow us to go beyond simple qualitative comparisons of the different saliency algorithms. Our results have the potential to aid clinicians when viewing medical images and addresses an urgent need for interventional tools in response to COVID-19. The source code is publicly available at: https://gitlab.kitware.com/brianhhu/x-mir.
ER  - 

TY  - CONF
TI  - Comprehension of Contextual Semantics Across Clinical Healthcare Domains
T2  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
SP  - 479
EP  - 480
AU  - K. Miller
PY  - 2022
KW  - Limiting
KW  - Transfer learning
KW  - Semantics
KW  - Medical services
KW  - Transformers
KW  - Information retrieval
KW  - Natural language processing
KW  - natural language processing
KW  - electronic health records
KW  - knowledge graph
KW  - transformer architectures
DO  - 10.1109/ICHI54592.2022.00077
JO  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
Y1  - 11-14 June 2022
AB  - The widespread lack of adoption of clinical notetaking standards has rendered information retrieval from Electronic Health Records (EHRs) especially challenging using traditional Natural Language Processing (NLP) techniques. Clinical note authors too commonly adopt their own note-taking structures and styles, limiting the applicability of rule-based and statistical models. While the context of any given sentence within a note carries important implied information, context is notoriously difficult for a language model to infer. However, recent advances in deep learning NLP methods such as pre-training on domain-specific corpora, novel embedding structures, and transformer architectures have enabled an awareness of context not previously attainable. In this work, I study the application of these evidenced NLP approaches to a gold standard annotated corpus of primary care notes of multiple Mayo Clinic EHR systems. The strongly labelled data will be supplemented with large volumes of weakly labelled data curated using distant supervision. The combined dataset will be used to train and evaluate context classification and section boundary detection models that classify the current context of a sentence given adjacent text segments. Once validated against primary care corpora, transfer learning methods will enable access to shared knowledge across more specific clinical domains, enabling generalizability across clinical domains and a degree of transparency into the shared aspects of the integrated model.
ER  - 

TY  - CONF
TI  - Exploring LRP and Grad-CAM visualization to interpret multi-label-multi-class pathology prediction using chest radiography
T2  - 2022 IEEE 35th International Symposium on Computer-Based Medical Systems (CBMS)
SP  - 258
EP  - 263
AU  - M. U. Alam
AU  - J. R. Baldvinsson
AU  - Y. Wang
PY  - 2022
KW  - Radiography
KW  - Deep learning
KW  - Heating systems
KW  - Decision support systems
KW  - Pathology
KW  - Neural networks
KW  - Data visualization
KW  - Deep Learning
KW  - Interpretability
KW  - LRP
KW  - Grad-CAM
KW  - Chest X-ray
KW  - Visualization
KW  - Clinical Decision Support System
DO  - 10.1109/CBMS55023.2022.00052
JO  - 2022 IEEE 35th International Symposium on Computer-Based Medical Systems (CBMS)
IS  - 
SN  - 2372-9198
VO  - 
VL  - 
JA  - 2022 IEEE 35th International Symposium on Computer-Based Medical Systems (CBMS)
Y1  - 21-23 July 2022
AB  - The area of interpretable deep neural networks has received increased attention in recent years due to the need for transparency in various fields, including medicine, healthcare, stock market analysis, compliance with legislation, and law. Layer-wise Relevance Propagation (LRP) and Gradient-weighted Class Activation Mapping (Grad-CAM) are two widely used algorithms to interpret deep neural networks. In this work, we investigated the applicability of these two algorithms in the sensitive application area of interpreting chest radiography images. In order to get a more nuanced and balanced outcome, we use a multi-label classification-based dataset and analyze the model prediction by visualizing the outcome of LRP and Grad-CAM on the chest radiography images. The results show that LRP provides more granular heatmaps than Grad-CAM when applied to the CheXpert dataset classification model. We posit that this is due to the inherent construction difference of these algorithms (LRP is layer-wise accumulation, whereas Grad-CAM focuses primarily on the final sections in the model's architecture). Both can be useful for understanding the classification from a micro or macro level to get a superior and interpretable clinical decision support system.
ER  - 

TY  - JOUR
TI  - Toward Global Validation Standards for Health AI
T2  - IEEE Communications Standards Magazine
SP  - 64
EP  - 69
AU  - M. Wenzel
AU  - T. Wiegand
PY  - 2020
KW  - Artificial intelligence
KW  - Task analysis
KW  - Data models
KW  - Medical services
KW  - Benchmark testing
KW  - ITU
DO  - 10.1109/MCOMSTD.001.2000006
JO  - IEEE Communications Standards Magazine
IS  - 3
SN  - 2471-2833
VO  - 4
VL  - 4
JA  - IEEE Communications Standards Magazine
Y1  - September 2020
AB  - Machine learning (ML) and artificial intelligence (AI) methods hold great potential for healthcare, for example, for purposes of diagnosis or prognosis that include a wide range of pattern recognition tasks. Ensuring that health ML/AI models are trustworthy will consequently become increasingly important soon. The ITU/WHO focus group on "AI for Health" is working on validation standards for health AI that can help to assess the quality of the powerful but complex technologies in a comparable and transparent manner. In particular, standardized benchmarking can serve as a valuable tool to determine the merits and limits of different health ML/AI models. In this article, ongoing work of the ITU/WHO initiative is introduced and set into perspective with related digital health and AI standardization efforts.
ER  - 

TY  - CONF
TI  - Artificial Intelligence and Blockchain for Future Cyber Security Application
T2  - 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
SP  - 802
EP  - 805
AU  - F. Xiaohua
AU  - C. Marc
AU  - E. Elias
AU  - H. Khalid
PY  - 2021
KW  - Big Data
KW  - Proof of Work
KW  - Blockchains
KW  - Information and communication technology
KW  - Security
KW  - Reliability
KW  - Problem-solving
KW  - AI (Artificial intelligence)
KW  - Blockchain Technology
KW  - Consensus
KW  - Distributed Ledgers
KW  - Immutability
KW  - Big Data Analysis
KW  - Digital Forensics Evidence
DO  - 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00133
JO  - 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)
Y1  - 25-28 Oct. 2021
AB  - AI (Artificial intelligence) application on Big Data had been developed fast. AI cyber security defense for the facing threats were required. Blockchain technology was invented in 2008 with BTC (Bit coin. This technology could be benefited alongside the custom of Blockchain, AI, Big Data and so on. There were a rapid progress in the advancement of Blockchain. This subject had recently become a discussion topic in the ICT (Information and Communications Technology) world. In this paper, AI security is discussed from the initial stage. Suggestion: In this paper, we discussed the impact of AI security from the initial stage and its impact and benefits to IT engineers, ICT students and CS (Computer Sciences) academic researchers, using a case study of medical records with personal recognizable identification privacy information that needs strict access control security. We considered its need for trustworthy cyber security, anti-fake, anti-alteration and transaction accounting transparency reputation to be applied to the NHS (National Health Service). Lastly, the paper provided some necessarily analysis. Blockchain technology had trustworthy cyber security, anti-fake, anti-alteration and transaction accounting transparency reputation to be considered to be applied to NHS (National Health Service). This short paper provided some analysis necessarily.
ER  - 

TY  - CONF
TI  - Enabling scalable AI for Digital Health: interoperability, consent and ethics support
T2  - 2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW)
SP  - 18
EP  - 27
AU  - Z. Milosevic
PY  - 2021
KW  - Training
KW  - Ethics
KW  - Conferences
KW  - Buildings
KW  - Ecosystems
KW  - Electronic healthcare
KW  - Artificial intelligence
KW  - interoperability
KW  - digital health
KW  - consent
KW  - ethics deontic logic
KW  - ODP enterprise language (ODP-EL)
KW  - AI
KW  - FHIR
DO  - 10.1109/EDOCW52865.2021.00028
JO  - 2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW)
IS  - 
SN  - 2325-6605
VO  - 
VL  - 
JA  - 2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW)
Y1  - 25-29 Oct. 2021
AB  - This paper proposes an approach for building scalable AI applications in digital health, with a specific focus on addressing interoperability, consent and ethics challenges. These challenges need to be considered in the context of increasingly available tooling for streamlined model development, training, validation, and deployment, while accommodating novel solutions for explainable AI support for clinicians. Such an approach is required because digital health ecosystems involve many data type created by different systems, and often used as part of workflows over different jurisdictional boundaries. Interoperability solutions are needed to support technical and business agreements between parties providing data and services, including knowledge intensive services, such as ML and AI. Computable expression of consent and ethics policies are needed to control how patient information is used, including compliance with regulative rules, possibly from different policy contexts. Our approach, based on the latest interoperability and enterprise policy standards may provide a useful guidance for the practitioners building scalable AI solutions for digital health.
ER  - 

TY  - JOUR
TI  - Gaining Insights Into Patient Satisfaction Through Interpretable Machine Learning
T2  - IEEE Journal of Biomedical and Health Informatics
SP  - 2215
EP  - 2226
AU  - N. Liu
AU  - S. Kumara
AU  - E. Reich
PY  - 2021
KW  - Hospitals
KW  - Machine learning
KW  - Medical services
KW  - Data models
KW  - Task analysis
KW  - Predictive models
KW  - Trajectory
KW  - Healthcare analytics
KW  - interpretable machine learning
KW  - mixed integer programming
KW  - patient satisfaction
KW  - survey data mining
DO  - 10.1109/JBHI.2020.3038194
JO  - IEEE Journal of Biomedical and Health Informatics
IS  - 6
SN  - 2168-2208
VO  - 25
VL  - 25
JA  - IEEE Journal of Biomedical and Health Informatics
Y1  - June 2021
AB  - Patient satisfaction is a key performance indicator of patient-centered care and hospital reimbursement. To discover the major factors that affect patient experiences is considered as an effective way to formulate corrective actions. A patient during his/her healthcare journey interacts with multiple health professionals across different service units. The health-related data generated at each step of the journey is a valuable resource for extracting actionable insights. In particular, self-reported satisfaction survey and the associated patient electronic health records play an important role in the hospital-patient interaction analysis. In this paper, we propose an interpretable machine learning framework to formulate the patient satisfaction problem as a supervised learning task and utilize a mixed-integer programming model to identify the most influential factors. The proposed framework transforms heterogeneous data into human-understandable features and integrates feature transformation, variable selection, and coefficient learning into the optimization process. Therefore, it can achieve desirable model performance while maintaining excellent model interpretability, which paves the way for successful real-world applications.
ER  - 

TY  - CONF
TI  - Improve the interpretability by decision tree regression: exampled by an insurance dataset
T2  - 2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)
SP  - 325
EP  - 330
AU  - S. Dong
AU  - D. Fei
PY  - 2021
KW  - Correlation coefficient
KW  - Costs
KW  - Linear regression
KW  - Sociology
KW  - Insurance
KW  - Companies
KW  - Medical services
KW  - decision tree
KW  - linear regression
KW  - EDA
KW  - Proactive Coping Theory
KW  - interpretability
DO  - 10.1109/ICCEAI52939.2021.00065
JO  - 2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)
Y1  - 27-29 Aug. 2021
AB  - Rapidly rising national health care expenditure is a problem for both developed and developing countries. Based on the data of medical insurance of insurance companies, this study explores the influencing factors of medical insurance cost. Furthermore, the influencing factors are used as characteristic variables to establish decision tree regression model and linear regression model, and predict the medical insurance cost. The main conclusions are as follows: (1) The characteristics of “region” and “sex” do not affect the insurance cost.(2) Smoking has the greatest influence on insurance cost. Smoking is a characteristic of body mass index (BMI) and has a driving effect on insurance cost. (3) The regression correlation coefficient of decision tree is about 81%, and the linear regression correlation coefficient is 65%, that is, the prediction result of decision tree is more accurate.
ER  - 

TY  - JOUR
TI  - Counterfactual Building and Evaluation via eXplainable Support Vector Data Description
T2  - IEEE Access
SP  - 60849
EP  - 60861
AU  - A. Carlevaro
AU  - M. Lenatti
AU  - A. Paglialonga
AU  - M. Mongelli
PY  - 2022
KW  - Support vector machines
KW  - Data privacy
KW  - Solid modeling
KW  - Prototypes
KW  - Minimization
KW  - Machine learning
KW  - Counterfactuals
KW  - support vector data description
KW  - eXplainable machine learning
DO  - 10.1109/ACCESS.2022.3180026
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 10
VL  - 10
JA  - IEEE Access
Y1  - 2022
AB  - Increasingly in recent times, the mere prediction of a machine learning algorithm is considered insufficient to gain complete control over the event being predicted. A machine learning algorithm should be considered reliable in the way it allows to extract more knowledge and information than just having a prediction at hand. In this perspective, the counterfactual theory plays a central role. By definition, a counterfactual is the smallest variation of the input such that it changes the predicted behaviour. The paper addresses counterfactuals through Support Vector Data Description (SVDD), empowered by explainability and metric for assessing the counterfactual quality. After showing the specific case in which an analytical solution may be found (under Euclidean distance and linear kernel), an optimisation problem is posed for any type of distances and kernels. The vehicle platooning application is the use case considered to demonstrate how the outlined methodology may offer support to safety-critical applications as well as how explanation may shed new light into the control of the system at hand.
ER  - 

TY  - CONF
TI  - Attack-agnostic Adversarial Detection on Medical Data Using Explainable Machine Learning
T2  - 2020 25th International Conference on Pattern Recognition (ICPR)
SP  - 8180
EP  - 8187
AU  - M. Watson
AU  - N. Al Moubayed
PY  - 2021
KW  - Training
KW  - Deep learning
KW  - Perturbation methods
KW  - MIMICs
KW  - Medical services
KW  - Predictive models
KW  - Feature extraction
KW  - Adversarial Attacks
KW  - Explainability
KW  - SHAP
KW  - Medical Data
DO  - 10.1109/ICPR48806.2021.9412560
JO  - 2020 25th International Conference on Pattern Recognition (ICPR)
IS  - 
SN  - 1051-4651
VO  - 
VL  - 
JA  - 2020 25th International Conference on Pattern Recognition (ICPR)
Y1  - 10-15 Jan. 2021
AB  - Explainable machine learning has become increasingly prevalent, especially in healthcare where explainable models are vital for ethical and trusted automated decision making. Work on the susceptibility of deep learning models to adversarial attacks has shown the ease of designing samples to mislead a model into making incorrect predictions. In this work, we propose a model agnostic explainability-based method for the accurate detection of adversarial samples on two datasets with different complexity and properties: Electronic Health Record (EHR) and chest X-ray (CXR) data. On the MIMIC-III and Henan-Renmin EHR datasets, we report a detection accuracy of 77% against the Longitudinal Adversarial Attack. On the MIMIC-CXR dataset, we achieve an accuracy of 88%; significantly improving on the state of the art of adversarial detection in both datasets by over 10% in all settings. We propose an anomaly detection based method using explainability techniques to detect adversarial samples which is able to generalise to different attack methods without a need for retraining.
ER  - 

TY  - JOUR
TI  - Preventing and Controlling Epidemics Through Blockchain-Assisted AI-Enabled Networks
T2  - IEEE Network
SP  - 34
EP  - 41
AU  - S. Otoum
AU  - I. Al Ridhawi
AU  - H. T. Mouftah
PY  - 2021
KW  - Infectious diseases
KW  - COVID-19
KW  - Pandemics
KW  - Surveillance
KW  - Design methodology
KW  - Medical services
KW  - Reliability
KW  - Epidemics
KW  - Coronaviruses
KW  - Viruses (medical)
DO  - 10.1109/MNET.011.2000628
JO  - IEEE Network
IS  - 3
SN  - 1558-156X
VO  - 35
VL  - 35
JA  - IEEE Network
Y1  - May/June 2021
AB  - The COVID-19 pandemic, which spread rapidly in late 2019, has revealed that the use of computing and communication technologies provides significant aid in preventing, controlling, and combating infectious diseases. With the ongoing research in next-generation networking (NGN), the use of secure and reliable communication and networking is of utmost importance when dealing with users' health records and other sensitive information. Through the adaptation of artificial-intelligence-enabled NGN, the shape of healthcare systems can be altered to achieve smart and secure healthcare capable of coping with epidemics that may emerge at any given moment. In this article, we envision a cooperative and distributed healthcare framework that relies on state-of-the-art computing, communication, and intelligence capabilities, namely, federated learning, mobile edge computing, and blockchain, to enable epidemic (or suspicious infectious disease) discovery, remote monitoring, and fast health authority response. The introduced framework can also enable secure medical data exchange at the edge and between different health entities. This technique, coupled with the low latency and high bandwidth functionality of 5G and beyond networks, would enable mass surveillance, monitoring, and analysis to occur at the edge. Challenges, issues, and design guidelines are also discussed in this article with highlights on some trending solutions.
ER  - 

TY  - CHAP
TI  - AI
T2  - The AI Book: The Artificial Intelligence Handbook for Investors, Entrepreneurs and FinTech Visionaries
SP  - 20
EP  - 22
AU  - Bonnie Buchanan
PY  - 2020
KW  - Artificial intelligence
KW  - Patents
KW  - Companies
KW  - Web and internet services
KW  - Urban areas
KW  - Face recognition
KW  - Surveillance
KW  - Investment
KW  - Industries
KW  - Europe
DO  - 10.1002/9781119551966.ch7
PB  - Wiley
SN  - 9781119551867
UR  - http://ieeexplore.ieee.org/document/10953925
AB  - Summary <p>Artificial intelligence (AI) is impacting industries ranging from IT to financial services, healthcare, education, surveillance and regulation. Between 2012 and 2016 the US invested $18.2 billion into AI compared with $2.6 billion in China. This chapter presents an analysis of AI in China and the West. China, the United Kingdom (UK) and the European Union (EU) have adopted a government&#x2010;led approach to AI. The United States&#x2019; (US) AI strategy has been dominated (and self&#x2010;regulated) by big tech companies. Singapore's AI structure emphasizes a more &#x201c;human&#x2010;centric&#x201d; approach that includes explainability, transparency and fairness to establish public trust in AI. China outpaces the US in terms of both deep learning and AI&#x2010;related patent publications, and the gap is closing for machine learning&#x2010;related patent publications. The US companies focus on machine learning, speech recognition and speech synthesis, whereas the Chinese ones focus on AI searching and facial recognition.</p>
ER  - 

TY  - CONF
TI  - SPACES: Explainable Multimodal AI for Active Surveillance, Diagnosis, and Management of Adverse Childhood Experiences (ACEs)
T2  - 2021 IEEE International Conference on Big Data (Big Data)
SP  - 5843
EP  - 5847
AU  - N. Ammar
AU  - P. Zareie
AU  - M. E. Hare
AU  - L. Rogers
AU  - S. Madubuonwu
AU  - J. Yaun
AU  - A. Shaban-Nejad
PY  - 2021
KW  - Pediatrics
KW  - Medical conditions
KW  - Surveillance
KW  - Sociology
KW  - Semantics
KW  - Big Data
KW  - Resource management
KW  - Explainable AI
KW  - Multimodal AI
KW  - Adverse Childhood Experiences
KW  - Recommender System
KW  - Disease Surveillance
DO  - 10.1109/BigData52589.2021.9671303
JO  - 2021 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Big Data (Big Data)
Y1  - 15-18 Dec. 2021
AB  - Adverse Childhood Experiences (ACEs) are a public health crisis. The American Academy of Pediatrics (AAP) recommends routine screening for ACEs. Current challenges in practice include a lack of validated screening tools, lack of resources to address issues found on screening, and the inability to translate population outcomes to individual patient care. Health care providers, and researchers are seeking innovative approaches and tools for ACEs screening, diagnosis, management, and continuous monitoring. Multimodal AI is an emerging concept that combines different input modalities to train AI agents to learn more accurate results by using both content and context. We present the Semantic Platform for Adverse Childhood Experiences Surveillance (SPACES), an explainable multimodal AI platform to facilitate ACEs surveillance and diagnosis of related health conditions, and subsequent interventions. We utilize a bottom-up approach to multimodal, explainable knowledge graph-based learning to derive recommendations and insights for better resource allocation and care management. SPACEs provides a novel approach to active ACEs surveillance by utilizing 360-degree views about patients and populations.
ER  - 

TY  - JOUR
TI  - RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records
T2  - IEEE Transactions on Visualization and Computer Graphics
SP  - 299
EP  - 309
AU  - B. C. Kwon
AU  - M. -J. Choi
AU  - J. T. Kim
AU  - E. Choi
AU  - Y. B. Kim
AU  - S. Kwon
AU  - J. Sun
AU  - J. Choo
PY  - 2019
KW  - Machine learning
KW  - Medical diagnostic imaging
KW  - Task analysis
KW  - Predictive models
KW  - Computational modeling
KW  - Visual analytics
KW  - Data models
KW  - Interactive Artificial Intelligence
KW  - XAI (Explainable Artificial Intelligence)
KW  - Interpretable Deep Learning
KW  - Healthcare
DO  - 10.1109/TVCG.2018.2865027
JO  - IEEE Transactions on Visualization and Computer Graphics
IS  - 1
SN  - 1941-0506
VO  - 25
VL  - 25
JA  - IEEE Transactions on Visualization and Computer Graphics
Y1  - Jan. 2019
AB  - We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.
ER  - 

TY  - CONF
TI  - Explainable Diagnosis, Lesion Segmentation and Quantification of COVID-19 Infection from CT Images Using Convolutional Neural Networks
T2  - 2022 IEEE 13th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)
SP  - 0171
EP  - 0178
AU  - N. Darapaneni
AU  - S. AT
AU  - K. K. S
AU  - A. R. Paduri
AU  - S. R. R
AU  - S. R
AU  - J. S. S. J
AU  - S. S
PY  - 2022
KW  - COVID-19
KW  - Image segmentation
KW  - Pandemics
KW  - Computed tomography
KW  - Pulmonary diseases
KW  - Medical services
KW  - Mobile communication
KW  - Explainable AI (XAI)
KW  - Convolutional Neural Network
KW  - Region of Interest Extraction
KW  - COVID-19
KW  - Segmentation
KW  - CT
DO  - 10.1109/IEMCON56893.2022.9946520
JO  - 2022 IEEE 13th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)
IS  - 
SN  - 2644-3163
VO  - 
VL  - 
JA  - 2022 IEEE 13th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)
Y1  - 12-15 Oct. 2022
AB  - The health crisis caused by the COVID-19 pandemic has led to unprecedented research efforts to build AI solutions that can assist healthcare systems. In this work, we propose a novel CNN-based system that detects COVID-19 infection and other pneumonia from CT scans, segments COVID-specific lesions, namely Ground Glass Opacities (GGO) and Consolidations (CL), and computes the percentage of lungs that have been affected by COVID and provides an explanation of the basis in which the diagnosis has been made through the comparison of class activation maps pertaining to the diagnosis with the segmented lesions. This can assist healthcare setups in the rapid contactless screening of COVID-19 and assess the stage and severity of the disease, while also providing some level of transparency on the rationale behind the model's decisions. Based on the initial results of the interpretation of the model's decisions, all the non-lung areas from the CT images were removed using a contour detection-based region of interest (ROI) extraction approach. This was done to prevent the model from making decisions based on details in non-lung areas, which are clinically irrelevant for COVID diagnosis. This is the first work to utilize such a contour detection-based ROI extraction approach for medical images, based on our study. The model has achieved a mean F1 score of 0.87 for multi-label classification (COVID, Common Pneumonia & Normal) and a Dice Similarity Coefficient (DSC) of 0.8066 for lesion segmentation which has exceeded the DSC achieved by 6 out of 7 lesion segmentation models referenced in our study.
ER  - 

TY  - CONF
TI  - Sanity Check for Shapley values-based explanations of Deep Neural Networks Predictions
T2  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
SP  - 644
EP  - 646
AU  - R. Tatano
AU  - A. Mastropietro
AU  - E. Busto
AU  - F. Vaccarino
PY  - 2022
KW  - Deep learning
KW  - Sensitivity
KW  - Computational modeling
KW  - Neural networks
KW  - Medical services
KW  - Prediction algorithms
KW  - Solids
KW  - XAI
KW  - SHAP
KW  - Sanity Check
DO  - 10.1109/ICHI54592.2022.00130
JO  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
Y1  - 11-14 June 2022
AB  - Explanation methods have been introduced to un-cover the black-box of Deep Neural Networks (DNNs). The interpretability of the prediction is crucial in medicine, where the use of black-box algorithms has an impact on human life. An important class of explanation methods is based on the Shapley values. Since no ground truth explanation is available, assessing the explanation faithfulness is difficult. In this paper, a comparison among Shapley values-based explanations is performed, and a sanity check of their faithfulness from the literature is applied. The results show that these methods pass the check and they can be suited to explain DNN in healthcare scenarios.
ER  - 

TY  - JOUR
TI  - Interpretability Analysis of Heartbeat Classification Based on Heartbeat Activity’s Global Sequence Features and BiLSTM-Attention Neural Network
T2  - IEEE Access
SP  - 109870
EP  - 109883
AU  - R. Li
AU  - X. Zhang
AU  - H. Dai
AU  - B. Zhou
AU  - Z. Wang
PY  - 2019
KW  - Electrocardiography
KW  - Heart beat
KW  - Heart rate variability
KW  - Feature extraction
KW  - Medical services
KW  - Databases
KW  - Deep learning
KW  - Heartbeat activity’s global sequence features
KW  - BiLSTM-attention neural network
KW  - interpretability
KW  - heartbeat classification
DO  - 10.1109/ACCESS.2019.2933473
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 7
VL  - 7
JA  - IEEE Access
Y1  - 2019
AB  - Arrhythmia is a disease that threatens human life. Therefore, timely diagnosis of arrhythmia is of great significance in preventing heart disease and sudden cardiac death. The BiLSTM-Attention neural network model with heartbeat activity's global sequence features can effectively improve the accuracy of heartbeat classification. Firstly, the noise is removed by the continuous wavelet transform method. Secondly, the peak of the R wave is detected by the tagged database, and then the P-QRS-T wave morphology and the RR interval are extracted. This feature set is heartbeat activity's global sequence features, which combines single heartbeat morphology and 21 consecutive RR intervals. Finally, the Bi-LSTM algorithm and the BiLSTM-Attention algorithm are used to identify heartbeat category respectively, and the MIT-BIH arrhythmia database is used to verify the algorithm. The results show that the BiLSTM-Attention model combined with heartbeat activity's global sequence features has higher interpretability than other methods discussed in this paper.
ER  - 

TY  - CONF
TI  - Trustworthy AI in the Age of Pervasive Computing and Big Data
T2  - 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
SP  - 1
EP  - 6
AU  - A. Kumar
AU  - T. Braud
AU  - S. Tarkoma
AU  - P. Hui
PY  - 2020
KW  - Artificial intelligence
KW  - Ethics
KW  - Data privacy
KW  - Biological system modeling
KW  - Training
KW  - Pervasive computing
KW  - Robustness
KW  - Artificial Intelligence
KW  - Pervasive Computing
KW  - Ethics
KW  - Data Fusion
KW  - Transparency
KW  - Privacy
KW  - Fairness
KW  - Accountability
KW  - Federated Learning
DO  - 10.1109/PerComWorkshops48775.2020.9156127
JO  - 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)
Y1  - 23-27 March 2020
AB  - The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.
ER  - 

TY  - CONF
TI  - Explaining Therapy Predictions with Layer-Wise Relevance Propagation in Neural Networks
T2  - 2018 IEEE International Conference on Healthcare Informatics (ICHI)
SP  - 152
EP  - 162
AU  - Y. Yang
AU  - V. Tresp
AU  - M. Wunderle
AU  - P. A. Fasching
PY  - 2018
KW  - Medical services
KW  - Biological neural networks
KW  - Prediction algorithms
KW  - Machine learning
KW  - Neurons
KW  - Task analysis
KW  - Explainable Neural Network
KW  - Clinical Decision Support
KW  - Layer-wise Relevance Propagation
DO  - 10.1109/ICHI.2018.00025
JO  - 2018 IEEE International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Healthcare Informatics (ICHI)
Y1  - 4-7 June 2018
AB  - In typical data analysis projects in biology and healthcare, simpler predictive models, such as regressions and decision trees, enjoy more popularity than more complex and expressive ones, such as neural networks. One reason for this is that the functioning of simpler models is easier to explain, which greatly increases user acceptance. A neural network, on the contrary, is often regarded as a black box model, because its very strength in modeling complex interactions also makes its operation almost impossible to explain. Still, neural networks remain very interesting tools, since they have demonstrated promising performance in a variety of predictive tasks, such as medical image classification and segmentation, as well as clinical event prediction, i.e., in the modeling of therapy decisions and survival time. In this work, we attempt to improve the explainability of neural networks applied in healthcare. We propose to apply the Layer-wise Relevance Propagation algorithm to explain clinical decisions proposed by deep modern neural networks. This algorithm is able to highlight the features that lead to the probabilistic prediction of therapy decisions for each individual patient. We evaluate the feature-oriented explanations generated by the algorithm with clinical experts. We show that the features, which are identified by the algorithm to be relevant, largely agree with clinical knowledge and guidelines. We believe that being able to explain machine learning based decisions greatly improves transparency and acceptance of neural network models applied in the clinical domain.
ER  - 

TY  - CONF
TI  - Interpretable AI Model-Based Predictions of ECG changes in COVID-recovered patients
T2  - 2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART)
SP  - 1
EP  - 5
AU  - A. Gupta
AU  - J. Jain
AU  - S. Poundrik
AU  - M. K. Shetty
AU  - M. P. Girish
AU  - M. D. Gupta
PY  - 2021
KW  - COVID-19
KW  - Heart
KW  - Economics
KW  - Analytical models
KW  - Hospitals
KW  - Biological system modeling
KW  - Electrocardiography
KW  - ECG signal analysis
KW  - COVID-19
KW  - post-COVID subjects
KW  - interpretable AI
KW  - XAI
DO  - 10.1109/BioSMART54244.2021.9677747
JO  - 2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART)
Y1  - 8-10 Dec. 2021
AB  - COVID-19 has caused immense social and economic losses throughout the world. Subjects recovered from COVID are learned to have complications. Some studies have shown a change in the heart rate variability (HRV) in COVID-recovered subjects compared to the healthy ones. This change indicates an increased risk of heart problems among the survivors of moderate-to-severe COVID. Hence, this study is aimed at finding HRV features that get altered in COVID-recovered subjects compared to healthy subjects. Data of COVID-recovered and healthy subjects were collected from two hospitals in Delhi, India. Seven ML models have been built to classify healthy versus COVID-recovered subjects. The best-performing model was further analyzed to explore the ranking of altered heart features in COVID-recovered subjects via AI interpretability. Ranking of these features can indicate cardiovascular health status to doctors, who can provide support to the COVID-recovered subjects for timely safeguard from heart disorders. To the best of our knowledge, this is the first study with an in-depth analysis of the heart status of COVID-recovered subjects via ECG analysis.
ER  - 

TY  - CONF
TI  - Blockchain-based Smart Contract with Machine Learning for Insurance Claim Verification
T2  - 2021 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques (ICEECCOT)
SP  - 247
EP  - 252
AU  - K. Alnavar
AU  - C. N. Babu
PY  - 2021
KW  - Support vector machines
KW  - Smart contracts
KW  - Insurance
KW  - Organizations
KW  - Blockchains
KW  - Peer-to-peer computing
KW  - Secure storage
KW  - Blockchain
KW  - Machine Learning
KW  - Smart Contract
KW  - Ethereum
KW  - Insurance
KW  - Remix
KW  - Solidity
KW  - Ganache
KW  - Python
KW  - Web3
DO  - 10.1109/ICEECCOT52851.2021.9707964
JO  - 2021 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques (ICEECCOT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 5th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques (ICEECCOT)
Y1  - 10-11 Dec. 2021
AB  - Blockchain is a decentralized, immutable, peer-to-peer network which provides trust, transparency, and tamper resistance and these features have led to various applications such as bitcoin and ethereum. Various structures of Blockchains are designed in fields such as e-health as it can facilitate safe and secure storage of the patient’s information in the health care system. Data integration problem in healthcare can be overcome by bringing a decentralized system in the hospital organization. This paper focuses on the use of Blockchains in managing medical records for insurance claim. By removing the central administrator, the suggested architecture uses Ethereum smart contracts to create a tamperproof and transparent healthcare system and ensure the integrity of sensitive patient data. Additionally, the smart contract keeps the patient well-informed and governs the communication between all the participants in the network. The proposed approach provides a solution to eliminate fraudulent insurance claims using Machine Learning (ML) with Blockchain. Machine Learning techniques such as Random Forest Classifier, Support Vector Machines are used along with Python and Solidity languages and an accuracy of 88% is achieved on the healthcare data set.
ER  - 

TY  - CONF
TI  - Artificial Intelligence for Public Health
T2  - 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 1
EP  - 2
AU  - K. P. Bennett
PY  - 2019
KW  - Translation
KW  - Surveillance
KW  - Semantics
KW  - Neural networks
KW  - Mortality
KW  - Transforms
KW  - Organizations
KW  - Predictive models
KW  - Trajectory
KW  - Public healthcare
DO  - 10.1109/BIBM47256.2019.8983112
JO  - 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 18-21 Nov. 2019
AB  - In this talk, we examine artificial intelligence approaches for extracting actionable insights from health care data in order to improve public health. Our goal is to simultaneously identify subpopulations with distinct health risks and health trajectories and find the distinct risk factors or determinants associated each subpopulation. These determinants can then be used treatments, programs, and policies in order to reduce mortality and comorbidity and provide more efficient healthcare. We examine novel cadre machine learning approaches that combine predictive neural network modeling with more traditional statistical epidemiology methods for risk and survival analyses. We embed the cadre methods into a Semantically Targeted Analytics (Semantalytics) System that combines semantics, inference, automatic machine learning, and explainable AI. The AI system translate the public health questions to an analysis plan, prepares data, conducts analysis and reports results with visualization and text. We demonstrated these approach on public health care surveillance datasets and electronic medical records. The award winning “MortalityMinder” app examines the social determinants of “Deaths Despair” (deaths from suicide and substance abuse) and other causes of mortality that are unexpectedly rising in the United States. Other applications include association of environment toxins associated with diseases, high needs patient management for a health management organization, and emergency department readmissions. We conclude with the discussion of the open challenges to create population health AI systems that can transform health care questions into data-driven actionable-insights on the fly.
ER  - 

TY  - JOUR
TI  - Using Wearables and Machine Learning to Enable Personalized Lifestyle Recommendations to Improve Blood Pressure
T2  - IEEE Journal of Translational Engineering in Health and Medicine
SP  - 1
EP  - 13
AU  - P. -H. Chiang
AU  - M. Wong
AU  - S. Dey
PY  - 2021
KW  - Wearable computers
KW  - Feature extraction
KW  - Data models
KW  - Predictive models
KW  - Hypertension
KW  - Time series analysis
KW  - Biomedical monitoring
KW  - Blood pressure
KW  - hypertension
KW  - machine learning
KW  - personalized modeling
KW  - smart healthcare
DO  - 10.1109/JTEHM.2021.3098173
JO  - IEEE Journal of Translational Engineering in Health and Medicine
IS  - 
SN  - 2168-2372
VO  - 9
VL  - 9
JA  - IEEE Journal of Translational Engineering in Health and Medicine
Y1  - 2021
AB  - Background: Blood pressure (BP) is an essential indicator for human health and is known to be greatly influenced by lifestyle factors, like activity and sleep factors. However, the degree of impact of each lifestyle factor on BP is unknown and may vary between individuals. Our goal is to investigate the relationships between BP and lifestyle factors and provide personalized and precise recommendations to improve BP, as opposed to the current practice of general lifestyle recommendations. Method: Our proposed system consists of automated data collection using home BP monitors and wearable activity trackers and feature engineering techniques to address time-series data and enhance interpretability. We propose Random Forest with Shapley-Value-based Feature Selection to offer personalized BP modeling and top lifestyle factor identification, and subsequent generation of precise recommendations based on the top factors. Result: In collaboration with UC San Diego Health and Altman Clinical and Translational Research Institute, we performed a clinical study, applying our system to 25 patients with elevated BP or stage I hypertension for three consecutive months. Our study results validate our system’s ability to provide accurate personalized BP models and identify the top features which can vary greatly between individuals. We also validate the effectiveness of personalized recommendations in a randomized controlled experiment. After receiving recommendations, the subjects in the experimental group decreased their BPs by 3.8 and 2.3 for systolic and diastolic BP, compared to the decrease of 0.3 and 0.9 for the subjects without recommendations. Conclusion: The study demonstrates the potential of using wearables and machine learning to develop personalized models and precise lifestyle recommendations to improve BP.
ER  - 

TY  - CONF
TI  - Enabling the efficiency of Blockchain Technology in Tele-Healthcare with Enhanced EMR
T2  - 2020 International Conference on Computer Science, Engineering and Applications (ICCSEA)
SP  - 1
EP  - 6
AU  - S. K
AU  - C. Priya
PY  - 2020
KW  - Privacy
KW  - Smart contracts
KW  - Authentication
KW  - Blockchain
KW  - Diabetes
KW  - Cryptography
KW  - Diseases
KW  - Blockchain
KW  - Electronic Medical Record (EMR)
KW  - Smart contracts
KW  - Tele-Healthcare
KW  - Diabetes
KW  - Electronic Health Record (EHR)
KW  - Machine Learning
KW  - Decentralization
DO  - 10.1109/ICCSEA49143.2020.9132922
JO  - 2020 International Conference on Computer Science, Engineering and Applications (ICCSEA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 International Conference on Computer Science, Engineering and Applications (ICCSEA)
Y1  - 13-14 March 2020
AB  - A blockchain is a growing list of records, called blocks, that are linked using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp and a transaction data. A blockchain consists of a structure of data that represents a financial ledger entry, or a record of transactions. Each transaction is digitally signed to ensure its authenticity, therefore no one tampers with it. So the ledger itself along with an existing transactions within it are assumed to be with great integrity. The basic advantages of blockchain technology includes the features of decentralization, Peer to peer network, immutability, security and transparency. If once the data has been re-entered into the record, it will not get deleted instead, it gets updated. Every block in the blockchain has a permanent timestamp that indicates authentication and verification. Here, we promote the facility of having a smart contract between the patient and a healthcare sector. It highly concentrates in the field of diabetes as it is a rapidly growing disease in today's world. It requires regular checkups to keep the disease in control. This paper consolidates the features of blockchain technology to produce an enhanced EMR in the field of diabetes.
ER  - 

TY  - CONF
TI  - Illicit Account Detection in the Ethereum Blockchain Using Machine Learning
T2  - 2021 International Conference on Information Technology (ICIT)
SP  - 488
EP  - 493
AU  - R. F. Ibrahim
AU  - A. Mohammad Elian
AU  - M. Ababneh
PY  - 2021
KW  - Privacy
KW  - Machine learning algorithms
KW  - Government
KW  - Education
KW  - Finance
KW  - Medical services
KW  - Time measurement
KW  - Ethereum
KW  - Blockchain
KW  - Fraud Detection
KW  - machine learning
KW  - Illicit account detection
DO  - 10.1109/ICIT52682.2021.9491653
JO  - 2021 International Conference on Information Technology (ICIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Information Technology (ICIT)
Y1  - 14-15 July 2021
AB  - Blockchain is a platform technology for the cryptocurrency’s applications like Bitcoin and Ethereum. The purpose of the blockchain is to eliminate the need for third trusted parties such as banks. In recent years and because of the properties of this technology like immutability and transparency, the technology was extended beyond cryptocurrencies and was exploited by various sectors like education, healthcare, finance, energy, government, and IoT providing more privacy, faster transactions and more security. In this research, we investigated Illicit accounts on Ethereum blockchain and proposed a Fraud detection model using three different machine learning algorithms: decision tree (j48), Random Forest and K-nearest neighbors (KNN). These algorithms were applied on a data set obtained from Kaggle.com containing 42 features. We have used the correlation coefficient to select the most effective features and built a new data set using 6 features only. Our research results show a significant improvement in time measurements using the three algorithms and an improvement in the F measure using the Random Forest algorithm.
ER  - 

TY  - CONF
TI  - Development of an Explainable Prediction Model of Heart Failure Survival by Using Ensemble Trees
T2  - 2020 IEEE International Conference on Big Data (Big Data)
SP  - 4902
EP  - 4910
AU  - P. A. Moreno-Sanchez
PY  - 2020
KW  - Heart
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Tools
KW  - Big Data
KW  - Task analysis
KW  - Heart failure survival prediction
KW  - explainable artificial intelligence
KW  - machine learning
KW  - ensemble trees
KW  - features importance
DO  - 10.1109/BigData50022.2020.9378460
JO  - 2020 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Big Data (Big Data)
Y1  - 10-13 Dec. 2020
AB  - Cardiovascular diseases (CVD) are the leading cause of death globally. Heart failure prediction, one of the CVD manifestations, has become a priority for doctors, however, up to date clinical practice usually has failed to reach high accuracy in such tasks. Machine learning offers advantages not only for clinical prediction but also for feature ranking improving the interpretation of the outputs by clinical professionals. Thus, the concept of eXplainable Artificial Intelligence (XAI) is aimed to cope with the lack of explainability of machine learning models in the healthcare domain, in this case, and provide healthcare professionals with patient-tailored decision-making tools that improve treatments and diagnostics. This paper presents a heart failure survival prediction model development by using ensemble trees machine learning techniques. Extreme Gradient Boosting (XGBoost) is demonstrated as the classifier with most accurate results (83% accuracy with unseen data) over the other ensemble trees options. Moreover, a features selection preprocessing is made in order to assess which relevant features contribute to the model's results. Next, in terms of improving the explainability of the model developed, a study of features importance is carried out showing the "follow up time period" feature as the most relevant. Finally, a quantitative evaluation of the interpretability and fidelity of the model developed is performed obtaining a balanced ratio between these two indicators.
ER  - 

TY  - CONF
TI  - Interpretable Sentiment Analysis based on Deep Learning: An overview
T2  - 2020 IEEE Pune Section International Conference (PuneCon)
SP  - 65
EP  - 70
AU  - S. Jawale
AU  - S. D. Sawarkar
PY  - 2020
KW  - Deep learning
KW  - Sentiment analysis
KW  - Social networking (online)
KW  - Transfer learning
KW  - Mental health
KW  - Medical services
KW  - Reliability
KW  - Sentiment Analysis
KW  - Deep learning
KW  - NLP
KW  - Interpretable methods
DO  - 10.1109/PuneCon50868.2020.9362361
JO  - 2020 IEEE Pune Section International Conference (PuneCon)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE Pune Section International Conference (PuneCon)
Y1  - 16-18 Dec. 2020
AB  - Sentiment analysis (SA) or emotion AI or opinion mining uses natural language processing (NLP). Sentiment Analysis identify, study, quantify, obtain, tacit states and subject related information. Broad spectrum of areas influenced due to Sentiment Analysis such as policy making by the government, finding mental health of individuals, finding misuse of drugs in healthcare, fraud detection in the financial sector, covid-19 awareness and impact, Cyber-crime etc. As the amplitude of social media data increases day by day, there is a need to automatically address sentiment analysis. Deep learning handles it very well. It gives very good accuracy but incomprehensibility in decision strategy. For better decision-making trust, believe, fairness, reliability, and unbiasing is important. This paper explores the work done in this area along with popular techniques to address interpretability in sentiment analysis and its evaluation criteria.
ER  - 

TY  - CONF
TI  - An Interpretable Deep Learning Framework for Health Monitoring Systems: A Case Study of Eye State Detection using EEG Signals
T2  - 2020 IEEE Symposium Series on Computational Intelligence (SSCI)
SP  - 211
EP  - 218
AU  - A. Tahmassebi
AU  - J. Martin
AU  - A. Meyer-Baese
AU  - A. H. Gandomi
PY  - 2020
KW  - Brain modeling
KW  - Electroencephalography
KW  - Data models
KW  - Training
KW  - Predictive models
KW  - Load modeling
KW  - Medical services
DO  - 10.1109/SSCI47803.2020.9308230
JO  - 2020 IEEE Symposium Series on Computational Intelligence (SSCI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE Symposium Series on Computational Intelligence (SSCI)
Y1  - 1-4 Dec. 2020
AB  - Effective monitoring and early detection of deterioration in patients play an essential role in healthcare. This includes minimizing the number of emergency encounters, reducing the length of hospitalization stay, re-admission rates of the patients, and etc. Cutting-edge methods in artificial intelligence (AI) have the ability to significantly improve outcomes. However, the struggle to interpret these black box models presents a serious problem to the healthcare industry. When selecting a model, the decision to sacrifice accuracy for interpretability must be made. In this paper, we propose an interpretable framework with the ability of real-time prediction. To demonstrate the predictive power of the framework, a case study on eye state detection using electroencephalogram (EEG) signals was employed to investigate how a deep neural network (DNN) model makes a prediction, and how that prediction can be interpreted. The promising results can be used to employ more advanced models in healthcare solutions without any concern of sacrificing the interpretation.
ER  - 

TY  - CONF
TI  - Global Interpretation for Patient Similarity Learning
T2  - 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 589
EP  - 594
AU  - M. Huai
AU  - C. Miao
AU  - J. Liu
AU  - D. Wang
AU  - J. Chou
AU  - A. Zhang
PY  - 2020
KW  - Predictive models
KW  - Statistics
KW  - Sociology
KW  - Optimization
KW  - Sensitivity
KW  - Labeling
KW  - Task analysis
KW  - Patient similarity learning
KW  - model interpretations
KW  - global explanations
DO  - 10.1109/BIBM49941.2020.9313255
JO  - 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 16-19 Dec. 2020
AB  - As an important family of learning problems in healthcare domain, patient similarity learning has received much attention in recent years. Patient similarity learning aims to measure the similarity between a pair of patients according to their historical clinical information, which helps to improve the clinical predictions of the patient of interest. Although patient similarity learning has achieved tremendous success in many real-world applications, the lack of transparency behind the behavior of the learned patient similarity model impedes users from trusting the predicted results, which hampers its further applications in the real world. To tackle this problem, in this paper, we investigate how to enable interpretation in patient similarity learning and propose a global interpretation method for patient similarity learning. Based on the proposed global interpretation method, we can identify a minimal sufficient subset of data features that are sufficient in themselves to justify the global predictions made by the well-trained patient similarity model. The identified minimal sufficient feature subset can help us to better understand the overall behaviors of the learned model across different subpopulations of patients. We also conduct experiments on real-world datasets to evaluate the performance of the proposed global interpretation method.
ER  - 

TY  - JOUR
TI  - Graph Convolutional Networks for Drug Response Prediction
T2  - IEEE/ACM Transactions on Computational Biology and Bioinformatics
SP  - 146
EP  - 154
AU  - T. Nguyen
AU  - G. T. T. Nguyen
AU  - T. Nguyen
AU  - D. -H. Le
PY  - 2022
KW  - Drugs
KW  - Genomics
KW  - Bioinformatics
KW  - Computer architecture
KW  - Deep learning
KW  - Predictive models
KW  - Feature extraction
KW  - Drug response prediction
KW  - interpretability
KW  - deep learning
KW  - graph convolutional network
KW  - saliency map
DO  - 10.1109/TCBB.2021.3060430
JO  - IEEE/ACM Transactions on Computational Biology and Bioinformatics
IS  - 1
SN  - 1557-9964
VO  - 19
VL  - 19
JA  - IEEE/ACM Transactions on Computational Biology and Bioinformatics
Y1  - 1 Jan.-Feb. 2022
AB  - Background: Drug response prediction is an important problem in computational personalized medicine. Many machine-learning-based methods, especially deep learning-based ones, have been proposed for this task. However, these methods often represent the drugs as strings, which are not a natural way to depict molecules. Also, interpretation (e.g., what are the mutation or copy number aberration contributing to the drug response) has not been considered thoroughly. Methods: In this study, we propose a novel method, GraphDRP, based on graph convolutional network for the problem. In GraphDRP, drugs were represented in molecular graphs directly capturing the bonds among atoms, meanwhile cell lines were depicted as binary vectors of genomic aberrations. Representative features of drugs and cell lines were learned by convolution layers, then combined to represent for each drug-cell line pair. Finally, the response value of each drug-cell line pair was predicted by a fully-connected neural network. Four variants of graph convolutional networks were used for learning the features of drugs. Results: We found that GraphDRP outperforms tCNNS in all performance measures for all experiments. Also, through saliency maps of the resulting GraphDRP models, we discovered the contribution of the genomic aberrations to the responses. Conclusion: Representing drugs as graphs can improve the performance of drug response prediction. Availability of data and materials: Data and source code can be downloaded athttps://github.com/hauldhut/GraphDRP.
ER  - 

TY  - CONF
TI  - Deep Embedded Clustering for Data-Driven ECG Exploration Using Continuous Wavelet Transforms
T2  - 2019 International Conference on Information and Digital Technologies (IDT)
SP  - 551
EP  - 556
AU  - M. P. Wachowiak
AU  - J. J. Moggridge
AU  - R. Wachowiak-Smolikova
PY  - 2019
KW  - Dimensionality reduction
KW  - Time-frequency analysis
KW  - Continuous wavelet transforms
KW  - Statistical analysis
KW  - Visual analytics
KW  - Smart healthcare
KW  - Electrocardiography
KW  - Wavelet analysis
KW  - Physiology
KW  - Standards
KW  - machine learning
KW  - deep clustering
KW  - continuous wavelet transform
KW  - ECG
KW  - dimensionality reduction
KW  - visual analytics
DO  - 10.1109/DT.2019.8813501
JO  - 2019 International Conference on Information and Digital Technologies (IDT)
IS  - 
SN  - 2575-677X
VO  - 
VL  - 
JA  - 2019 International Conference on Information and Digital Technologies (IDT)
Y1  - 25-27 June 2019
AB  - Due to the length, complexity, and inter-subject variation of physiological signals acquired non-invasively, datadriven analysis is increasingly valuable in Smart Health, precision medicine, and in studying physiological dynamics. Knowledge discovery in medical research requires that domain experts analyze complex, high-dimensional data and signals, which is facilitated by dimensionality reduction and clustering. The current paper presents an unsupervised machine learning approach to clustering time-frequency features from ECG records using deep embedded clustering, which optimizes a clustering metric that maps high-dimensional time-frequency data to a lower dimensional latent space. These clusters can be obtained in arbitrarily low dimensions, and are subsequently analyzed with visual analytics to uncover structures and patterns. This technique is applied to time segments of continuous wavelet transforms of ECG records, representing a variety of conditions. Preliminary results on publicly-available ECG records indicate that deep embedded clustering produces a low-dimensional learned representation of time-frequency characteristics that facilitates signal exploration and improves interpretability.
ER  - 

TY  - CONF
TI  - Communication Demand in the National Airspace – A Federated Learning Approach
T2  - 2022 Integrated Communication, Navigation and Surveillance Conference (ICNS)
SP  - 1
EP  - 11
AU  - N. Schimpf
AU  - H. Li
AU  - E. Knoblock
AU  - R. Apaza
PY  - 2022
KW  - Training
KW  - Navigation
KW  - Atmospheric modeling
KW  - Surveillance
KW  - Modulation
KW  - Bandwidth
KW  - Collaborative work
DO  - 10.1109/ICNS54818.2022.9771527
JO  - 2022 Integrated Communication, Navigation and Surveillance Conference (ICNS)
IS  - 
SN  - 2155-4951
VO  - 
VL  - 
JA  - 2022 Integrated Communication, Navigation and Surveillance Conference (ICNS)
Y1  - 5-7 April 2022
AB  - Within the national airspace system (NAS), efficient use of spectrum remains a challenge; as UAS and UAM missions evolve, the amount of mission-critical aircraft communications are expected to significantly grow. To accommodate the increased demand, NASA Glenn Research Center is investigating artificial intelligence approaches that could dynamically allocate spectrum; however, these solutions are driven by communication and aviation data items, many of which are not directly available. One such cornerstone data item is communication demand, parameterizing the needs within a sector in terms that may directly inform spectrum allocation, such as channel access duration, bandwidth, and modulation type. This paper considers the complexity of predicting communication demand as a function of NAS behaviors. Unlike prior prediction work in communications, this research must consider how the NAS may be impacted by external factors - such as convective weather and closures - rather than estimating demand from time-series forecasting alone. Much of this research considers a federated learning design to predict communication events in terms of the type of event occurring (sector coordination, conflict resolution, etc). To do so, an investigation of products from Sherlock Data Warehouse is conducted, identifying the trends, sufficiency, and correlations of each product to identified events. Additionally, a preliminary discussion for inferring associations between these event types and their communication parameters (duration, bandwidth, modulation) is presented. By utilizing federated learning, imbalances in the types of events and data present throughout the NAS can inform local models without impairing global training. Furthermore, the two-stage approach proposed allows for robust and speculative communication modelling, as communication techniques continue to evolve. As a result, this model enables a generalized approach to understanding NAS communications which is able to inform long-term changes to aviation spectrum management.
ER  - 

TY  - CONF
TI  - LIVE: A Local Interpretable model-agnostic Visualizations and Explanations
T2  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
SP  - 245
EP  - 254
AU  - P. Shi
AU  - A. Gangopadhyay
AU  - P. Yu
PY  - 2022
KW  - Deep learning
KW  - Location awareness
KW  - Limiting
KW  - Data visualization
KW  - Medical services
KW  - Predictive models
KW  - Data models
KW  - Deep learning
KW  - Interpretability
KW  - Visualization
KW  - Decision tree
DO  - 10.1109/ICHI54592.2022.00045
JO  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
Y1  - 11-14 June 2022
AB  - Deep learning has been successfully applied to various types of classification and prediction tasks including images, text, and tabular data, however interpretability of deep learning models still remains challenges, which becomes a limiting factor for further applications.Most of current interpretable models are limited to how to interpret the deep learning models. In this paper, we propose a simple but effective novel Local Interpretable model-agnostic Visualizations and Explanations (LIVE) algorithm through combining decision tree and deep learning models. The proposed LIVE algorithm is validated through 4 different data sets with different types of deep learning models including convolutional neural network (CNN) and long short-term memory models (LSTM). Our experiment results show that 1) LIVE algorithm could generally consistently help improve the deep learning models by 0.02 0.03 in AUC; 2) Our LIVE algorithm is a weakly supervised localization tool for region of interest identification; 3) Our LIVE algorithm could be applied to different deep learning models and different types of data.
ER  - 

TY  - CONF
TI  - RetainEXT: Enhancing Rare Event Detection and Improving Interpretability of Health Records using Temporal Neural Networks
T2  - 2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)
SP  - 01
EP  - 06
AU  - S. Ramchand
AU  - G. Tsang
AU  - D. Cole
AU  - X. Xie
PY  - 2022
KW  - COVID-19
KW  - Recurrent neural networks
KW  - Pandemics
KW  - Hospitals
KW  - Receivers
KW  - Machine learning
KW  - Predictive models
KW  - Artificial intelligence
KW  - Data mining
KW  - Electronic health records
KW  - COVID-19
KW  - Attention networks
DO  - 10.1109/BHI56158.2022.9926906
JO  - 2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)
IS  - 
SN  - 2641-3604
VO  - 
VL  - 
JA  - 2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)
Y1  - 27-30 Sept. 2022
AB  - A recurring theme during the pandemic was the shortage of hospital beds. Despite all efforts, the healthcare system still faces 25 % of resource strain felt during the first peak of coronavirus. Digitisation of Electronic Healthcare Records (EHRs) and the pandemic have brought about many successful applications of Recurrent Neural Networks (RNNs) to predict patients' current and future states. Despite their strong per-formance, it remains a challenge for users to delve into the black box which has heavily influenced researchers to utilise more interpretable techniques such as ID-Convolutional neural networks. Others focus on using more interpretable machine learning techniques but only achieve high performance on a select subset of patients. By collaborating with medical experts and artificial intelligence scientists, our study improves on the REverse Time AttentIoN EX model, a feature and visit level attention network, for increased interpretability and usability of RNNs in predicting COVID-19-related hospitalisations. We achieved 82.40 % area under the receiver operating characteristic curve and showcased effective use of the REverse Time AttentIoN EXTension model and EHRs in understanding how individual medical codes contribute to hospitalisation risk prediction. This study provides a guideline for researchers aiming to design interpretable temporal neural networks using the power of RNNs and data mining techniques.
ER  - 

TY  - JOUR
TI  - A Novel Smart Healthcare Design, Simulation, and Implementation Using Healthcare 4.0 Processes
T2  - IEEE Access
SP  - 118433
EP  - 118471
AU  - A. Kumar
AU  - R. Krishnamurthi
AU  - A. Nayyar
AU  - K. Sharma
AU  - V. Grover
AU  - E. Hossain
PY  - 2020
KW  - Medical services
KW  - Blockchain
KW  - Smart contracts
KW  - Market research
KW  - Industries
KW  - Cloud computing
KW  - Block
KW  - blockchain 3.0
KW  - healthcare 4.0
KW  - Industrial IoT (IIoT)
KW  - Industry 4.0
KW  - Internet of Things (IoT)
KW  - mining optimization
KW  - smart solution
KW  - transaction
DO  - 10.1109/ACCESS.2020.3004790
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - Blockchain technology is found to have its applicability in almost every domain because of its advantages such as crypto-security, transparency, immutability, decentralized data network. In present times, a smart healthcare system with a blockchain data network and healthcare 4.0 processes provides transparency, easy and faster accessibility, security, efficiency, etc. Healthcare 4.0 trends include industry 4.0 processes such as the internet of things (IoT), industrial IoT (IIoT), cognitive computing, artificial intelligence, cloud computing, fog computing, edge computing, etc. The goal of this work is to design a smart healthcare system and it is found to be possible through integration and interoperability of Blockchain 3.0 and Healthcare 4.0 in consideration with healthcare ground-realities. Here, healthcare 4.0 processes used for data accessibility are targeted to be validated through statistical simulation-optimization methods and algorithms. The blockchain is implemented in the Ethereum network, and with associated programming languages, tools, and techniques such as solidity, web3.js, Athena, etc. Further, this work prepares a comparative and comprehensive survey of state-of-the-art blockchain-based smart healthcare systems. The comprehensive survey includes methodology, applications, requirements, outcomes, future directions, etc. A list of groups, organizations, and enterprises are prepared that are working in electronic health records (EHR), electronic medical records (EMR) or electronic personal records (EPR) mainly, and a comparative analysis is drawn concerning adopting the blockchain technology in their processes. This work has explored optimization algorithms applicable to Healthcare 4.0 trends and improves the performance of blockchain-based decentralized applications for the smart healthcare system. Further, smart contracts and their designs are prepared for the proposed system to expedite the trust-building and payment systems. This work has considered simulation and implementation to validate the proposed approach. Simulation results show that the Gas value required (indicating block size and expenditure) lies within current Etherum network Gas limits. The proposed system is active because block utilization lies above 80%. Automated smart contract execution is below 20 seconds. A good number (average 3 per simulation time) is generated in the network that indicates a health competition. Although there is error observed in simulation and implementation that lies between 0.55% and 4.24%, these errors are not affecting overall system performance because simulated and actual (taken in state-of-the-art) data variations are negligible.
ER  - 

TY  - CONF
TI  - Using knowledge Graphs to Enhance the Interpretability of Clinical Decision Support Model
T2  - 2020 International Conference on Computer Science and Management Technology (ICCSMT)
SP  - 115
EP  - 122
AU  - J. Huang
AU  - L. Xiao
AU  - J. Yang
AU  - S. Chen
PY  - 2020
KW  - Human computer interaction
KW  - Decision support systems
KW  - Analytical models
KW  - Correlation
KW  - Knowledge based systems
KW  - Decision making
KW  - Machine learning
KW  - Clinical Decision Support Systems(CDSSs)
KW  - Human-Computer Interaction (HCI)
KW  - SNOMED CT
KW  - Interpretation Knowledge Base (IKB)
DO  - 10.1109/ICCSMT51754.2020.00030
JO  - 2020 International Conference on Computer Science and Management Technology (ICCSMT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 International Conference on Computer Science and Management Technology (ICCSMT)
Y1  - 20-22 Nov. 2020
AB  - Current clinical practice relies heavily on technology to support decision-making. In particular, machine learning is increasingly used in decision support systems. This can be attributed to information overload, a fact that clinicians cannot consider all available information. The disadvantage of this method is that this kind of Clinical Decision Support Systems (CDSSs) is usually a black box, and it can’t understand its decision-making reasons. However, in a healthcare environment, trust and accountability are important issues, and such systems should best be interpretable. In contrast, other areas rely almost entirely on observational or subjective patient reported questionnaires to quantify medical conditions. Developers need to use cognitive science based Human-Computer Interaction (HCI) research methods to design practice models, including user-centered iterative design and common standards. The main work of this paper is to propose a clinical decision support model with enhanced interpretability, including an automated interface generation engine. In the design of personalized decision-making, enhance the universality of decision-making push. The clinical evidence is input and displayed in the form of tables, and the medical concepts and their matching with SNOMED CT terms, consistent navigation, and finally displayed in the form of knowledge spectrum. Enhance the flexibility of interaction and integrate workflow seamlessly. As a result, domain experts can get advice quickly and take appropriate actions at convenient points in the workflow without additional effort or delay. Optimizing the interaction and availability of CDSS with providers can enhance the use of CDSS. The iterative design of CDSS improves the usability of the system and the user’s popularity score. Our analysis shows that modern machine learning methods can provide interpretability compatible with domain Interpretation Knowledge Base (IKB) and traditional method ranking. Future work should focus on replicating these findings in other datasets and further testing different interpretable methods.
ER  - 

TY  - CONF
TI  - nuhealthsoft: A Nutritional and Health Data Processing Software Tool from a patient’s perspective
T2  - 2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)
SP  - 386
EP  - 393
AU  - D. P. Panagoulias
AU  - M. Virvou
AU  - G. A. Tsihrintzis
PY  - 2022
KW  - Microservice architectures
KW  - Production
KW  - Medical services
KW  - Reliability engineering
KW  - Software reliability
KW  - Stakeholders
KW  - Task analysis
KW  - Biomarkers
KW  - nutritional biomarkers
KW  - microservices
KW  - rational unified process
KW  - user experience
KW  - explainable A.I
DO  - 10.1109/SITIS57111.2022.00065
JO  - 2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 16th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)
Y1  - 19-21 Oct. 2022
AB  - To ensure the alignment between developers, engineers and other stakeholders in the various steps of a production cycle, the utilisation of one of the many development methodologies is imperative for the successful transition from an idea to a useful and reliable end product. The rational unified process is the most precise and most cited iterative software engineering process that can ensure that alignment, trough comprehensive and simple separate steps, taken within pre - set restrictions [1].Modern software has to be easily managed and easy to scale. By scalable software, we refer to software, where functionality and processing power can be easily attached. For that reason more often than not, the software is divided in smaller components that serve different purposes and are tasked with different responsibilities. Those components can be deployed independently or as as part of greater distributed systems and are easier to collaborate on and to manage. This development framework is referred to as a microservices architecture.Medical applications for treatment, disease prevention and health optimisation can greatly benefit from machine learning and artificial intelligence. The success of such applications is better defined by its ability to include patients and users with different requirements and abilities.In this study we look into the development process of nuhealthsoft, an A.I infused medical application, that uses blood analysis and dieatary variables and other health data for the identification of health states and risk factors, such us metabolic syndrome and high blood pressure. The main characteristics of the application will be outlined through the R.U.P methodology. The microservices architecture and the patients’ needs and requirements are the key issues to be addressed in this paper, through the aforementioned methodology and are the forefront, all through the development cycle.
ER  - 

TY  - CONF
TI  - Visualizing the Interpretation of a Criterion-Driven System that Automatically Evaluates the Quality of Health News: An Exploratory Study of Two Approaches
T2  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
SP  - 01
EP  - 03
AU  - X. Liu
AU  - S. McRoy
PY  - 2022
KW  - Visualization
KW  - Computational modeling
KW  - Decision making
KW  - Medical services
KW  - Machine learning
KW  - Reliability
KW  - Informatics
KW  - Health Misinformation
KW  - Machine Learning
KW  - LIME
KW  - Interpretable AI
DO  - 10.1109/ICHI54592.2022.00109
JO  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
Y1  - 11-14 June 2022
AB  - Numerous studies have established the capability of machine learning in detecting health misinformation, but none addresses model interpretability. We propose two approaches to visualization for a criteria-based system that automatically evaluates health news quality. The model's interpretability arises from providing sentence-level evidence to support the document-level automatic criterion evaluation outcome. The two approaches were both implemented and then evaluated on four unseen cases, for one of ten established criteria.
ER  - 

TY  - CONF
TI  - Explainable data-driven modeling of patient satisfaction survey data
T2  - 2017 IEEE International Conference on Big Data (Big Data)
SP  - 3869
EP  - 3876
AU  - N. Liu
AU  - S. Kumara
AU  - E. Reich
PY  - 2017
KW  - Hospitals
KW  - Predictive models
KW  - Data models
KW  - Training data
KW  - Pain
KW  - Vegetation
KW  - explainable machine learning
KW  - data mining
KW  - lazy lasso
KW  - patient satisfaction
KW  - healthcare management
DO  - 10.1109/BigData.2017.8258391
JO  - 2017 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2017 IEEE International Conference on Big Data (Big Data)
Y1  - 11-14 Dec. 2017
AB  - In the personalized patient-centered healthcare, self-reported patient satisfaction survey data plays an important role. Given the patient survey data, it is necessary to identify the drivers of patient satisfaction and explain them so that such patterns can be used in future as well as necessary corrective actions can be taken. In healthcare, both accuracy and interpretability are important criteria for choosing a reliable predictive model for analyzing patient data. Usually, complex models such as Random Forest, neural networks can achieve high prediction accuracy but lack necessary interpretation to their prediction results. In this paper, we address this problem by proposing a local explanation method to interpret complex model prediction results. First, we build a predictive model using Random Forest to fit the patient satisfaction data. Second, we utilize local explanation method to provide insights into the Random Forest prediction results so as to discover true reasons behind patient experiences and overall ratings. Specifically, our approach allows us to interpret patient's overall rating of a hospital at the individual level, and find out the set of the most influential factors for each patient. We focus on all unhappy patients to investigate the top reasons for patient dissatisfaction. Our approach and findings will help to establish guidelines for a quality healthcare.
ER  - 

TY  - CONF
TI  - Development and validation of ECG rhythm classification on a multitude of data sources using Deep Learning
T2  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
SP  - 101
EP  - 105
AU  - V. Krishna Prasad
AU  - K. M. Nithin Raj
AU  - S. Muthya
AU  - R. S. Nair
PY  - 2021
KW  - Deep learning
KW  - Strips
KW  - Analytical models
KW  - Sensitivity
KW  - Electrocardiography
KW  - Sensitivity and specificity
KW  - Predictive models
KW  - Deep Learning
KW  - Electrocardiogram
KW  - Arrhythmia
KW  - Grad-CAM
KW  - Computer Vision
DO  - 10.1109/ICHI52183.2021.00026
JO  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
Y1  - 9-12 Aug. 2021
AB  - Electrocardiogram (ECG) serves a key role in understanding human health conditions and necessitates targeted treatments. Attempts to automate ECG interpretations using different algorithmic paradigms have existed for decades. However, a robust system or an approach that can automate the detection of a wide range of arrhythmia categories from ECGs captured or acquired from a multitude of sources remains a challenge, given the signal quality and extend of associated artifacts vary. The approach described in this paper takes the application of Deep Learning (DL) in arrhythmia detection to a new dimension, in which it uses class-discriminative visualization to improve interpretability and transparency as an additional step to validate the algorithm. A Convolutional Neural Network (CNN) of 20 rhythm categories was developed using 193,492 single-lead ten-second ECG strips. External validation was done against a test set of 5606 strips collected using Holter device (n=4447), patch (n=311), diagnostic ECG machine (n=806), and smartwatch (n=42), covering a wide variety of devices available in the current market. For IoU (Intersection over Union) score, GradCAM (Gradient-weighted Class Activation Mapping) analysis representing the region weighted by the model for inference was performed on 266 strips against Certified Cardiographic Technician annotations. The model with a weighted average F1 score of 0.94 across 20 rhythm categories had high sensitivity and specificity for critical arrhythmias like Atrial Fibrillation (0.997, 0.955) and Ventricular Tachycardia (0.999, 0.915). Besides, the mean IoU score calculated was 0.56, meaning the region of arrhythmia had a great impact on the predictions made by the model.
ER  - 

TY  - CONF
TI  - Exploiting Explanations for Model Inversion Attacks
T2  - 2021 IEEE/CVF International Conference on Computer Vision (ICCV)
SP  - 662
EP  - 672
AU  - X. Zhao
AU  - W. Zhang
AU  - X. Xiao
AU  - B. Lim
PY  - 2021
KW  - Privacy
KW  - Semantics
KW  - Data visualization
KW  - Medical services
KW  - Predictive models
KW  - Data models
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Datasets and evaluation
KW  - Recognition and classification
DO  - 10.1109/ICCV48922.2021.00072
JO  - 2021 IEEE/CVF International Conference on Computer Vision (ICCV)
IS  - 
SN  - 2380-7504
VO  - 
VL  - 
JA  - 2021 IEEE/CVF International Conference on Computer Vision (ICCV)
Y1  - 10-17 Oct. 2021
AB  - The successful deployment of artificial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artificial intelligence (XAI) provides more information to help users to understand model decisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identified several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve significantly higher inversion performance than using the target model prediction only. These XAI-aware inversion models were designed to exploit the spatial knowledge in image explanations. To understand which explanations have higher privacy risk, we analyzed how various explanation types and factors influence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for non-explainable target models by exploiting explanations of surrogate models through attention transfer. This method first inverts an explanation from the target prediction, then reconstructs the target image. These threats highlight the urgent and significant privacy risks of explanations and calls attention for new privacy preservation techniques that balance the dual-requirement for AI explainability and privacy.
ER  - 

TY  - CONF
TI  - An Investigation of Interpretable Deep Learning for Adverse Drug Event Prediction
T2  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
SP  - 337
EP  - 342
AU  - J. Rebane
AU  - I. Karlsson
AU  - P. Papapetrou
PY  - 2019
KW  - Medical diagnostic imaging
KW  - Deep learning
KW  - Drugs
KW  - Predictive models
KW  - Computational modeling
KW  - Training
KW  - Data models
KW  - deep learning
KW  - attention mechanism
KW  - adverse drug events
KW  - medical records
DO  - 10.1109/CBMS.2019.00075
JO  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
IS  - 
SN  - 2372-9198
VO  - 
VL  - 
JA  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
Y1  - 5-7 June 2019
AB  - A variety of deep learning architectures have been developed for the goal of predictive modelling in regards to detecting health diagnoses in medical records. Several models have placed strong emphases on temporal attention mechanisms and decay factors as a means to include highly temporally relevant information regarding the recency of medical event occurrence while facilitating medical code-level interpretability. In this study we utilise such models with a novel Electronic Patient Record (EPR) data set consisting of both diagnoses and medication data for the purpose of Adverse Drug Event (ADE) prediction. As such, a main contribution of this work is an empirical evaluation of two state-of-the-art deep learning architectures in terms of objective performance metrics for ADE prediction. We also assess the importance of attention mechanisms in regards to their usefulness for medical code-level interpretability, which may facilitate novel insights pertaining to the nature of ADE occurrence within the health care domain.
ER  - 

TY  - CONF
TI  - Non-transfer Deep Learning of Optical Coherence Tomography for Post-hoc Explanation of Macular Disease Classification
T2  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
SP  - 48
EP  - 52
AU  - R. Arefin
AU  - M. D. Samad
AU  - F. A. Akyelken
AU  - A. Davanian
PY  - 2021
KW  - Optical filters
KW  - Deep learning
KW  - Computational modeling
KW  - Transfer learning
KW  - Retina
KW  - Feature extraction
KW  - Convolutional neural networks
KW  - Convolutional neural network
KW  - Macular disease
KW  - Ophthalmology
KW  - Explainable AI
KW  - Feature extraction
KW  - Filter kernels
KW  - Medical imaging
DO  - 10.1109/ICHI52183.2021.00020
JO  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
Y1  - 9-12 Aug. 2021
AB  - Deep transfer learning is widely used for medical image classification by leveraging models that are pretrained by natural images. This choice may introduce unnecessary model complexity that can limit explanations of such model outcomes in clinical practice. To investigate this hypothesis, we develop a configurable deep convolutional neural network (CNN) to classify four macular disease types using retinal optical coherence tomography (OCT) images. Our proposed non-transfer deep CNN model (acc: 97.9%) outperforms existing transfer learning models such as ResNet-50 (acc: 89.0%), ResNet-101 (acc: 96.7%), VGG-19 (acc: 93.3%), and Inception-V3 (acc: 95.8%) in the same retinal OCT image classification task. Our post-hoc analysis of the model extracted image features reveals that only eight out of 256 CNN filter kernels are active at the final convolutional layer. The convolutional responses of these eight selective filters yield image features that efficiently separate four macular disease classes even when projected onto two-dimensional principal component space. Our findings suggest that a large portion of deep learning parameters and computations are redundant for retinal OCT image classification, which intensifies when using transfer learning. Additionally, we provide clinical interpretations of our misclassified test images identifying manifest artifacts, shadowing of useful texture, false texture representing fluids, and other confounding factors. These clinical explanations along with model optimization via filter selection can improve the classification accuracy, computational costs, and explainability of deep model outcomes.
ER  - 

TY  - CONF
TI  - Hierarchical visual case-based reasoning for supporting breast cancer therapy
T2  - 2019 Fifth International Conference on Advances in Biomedical Engineering (ICABME)
SP  - 1
EP  - 4
AU  - J. -B. Lamy
AU  - B. Sekar
AU  - G. Guezennec
AU  - J. Bouaud
AU  - B. Séroussi
PY  - 2019
KW  - Surgery
KW  - Visualization
KW  - Chemotherapy
KW  - Breast cancer
KW  - Color
KW  - Cognition
KW  - breast cancer
KW  - case-based reasoning
KW  - explainable artificial intelligence
KW  - XAI
DO  - 10.1109/ICABME47164.2019.8940223
JO  - 2019 Fifth International Conference on Advances in Biomedical Engineering (ICABME)
IS  - 
SN  - 2377-5696
VO  - 
VL  - 
JA  - 2019 Fifth International Conference on Advances in Biomedical Engineering (ICABME)
Y1  - 17-19 Oct. 2019
AB  - Breast cancer therapy is particularly complex. Case-based reasoning (CBR) is an approach that can support clinicians when prescribing a therapy, and that is able to explain its recommendation to the clinicians. In a previous work, we proposed a visual CBR approach for helping clinicians to choose a treatment between four main categories (e.g. surgery, chemotherapy). However, these are broad categories and clinicians need more details about the treatment, e.g. several surgeries exist such as lumpectomy. Here, we extend our visual CBR approach for fully supporting the therapy for breast cancer, using a hierarchical approach: first, decide the category, then decide the exact treatment, etc.
ER  - 

TY  - JOUR
TI  - Data Extrapolation From Learned Prior Images for Truncation Correction in Computed Tomography
T2  - IEEE Transactions on Medical Imaging
SP  - 3042
EP  - 3053
AU  - Y. Huang
AU  - A. Preuhs
AU  - M. Manhart
AU  - G. Lauritsch
AU  - A. Maier
PY  - 2021
KW  - Image reconstruction
KW  - Deep learning
KW  - Computed tomography
KW  - Extrapolation
KW  - Detectors
KW  - Image quality
KW  - Robustness
KW  - Deep learning
KW  - robustness
KW  - interpretability
KW  - truncation correction
KW  - computed tomography
DO  - 10.1109/TMI.2021.3072568
JO  - IEEE Transactions on Medical Imaging
IS  - 11
SN  - 1558-254X
VO  - 40
VL  - 40
JA  - IEEE Transactions on Medical Imaging
Y1  - Nov. 2021
AB  - Data truncation is a common problem in computed tomography (CT). Truncation causes cupping artifacts inside the field-of-view (FOV) and anatomical structures missing outside the FOV. Deep learning has achieved impressive results in CT reconstruction from limited data. However, its robustness is still a concern for clinical applications. Although the image quality of learning-based compensation schemes may be inadequate for clinical diagnosis, they can provide prior information for more accurate extrapolation than conventional heuristic extrapolation methods. With extrapolated projection, a conventional image reconstruction algorithm can be applied to obtain a final reconstruction. In this work, a general plug-and-play (PnP) method for truncation correction is proposed based on this idea, where various deep learning methods and conventional reconstruction algorithms can be plugged in. Such a PnP method integrates data consistency for measured data and learned prior image information for truncated data. This shows to have better robustness and interpretability than deep learning only. To demonstrate the efficacy of the proposed PnP method, two state-of-the-art deep learning methods, FBPConvNet and Pix2pixGAN, are investigated for truncation correction in cone-beam CT in noise-free and noisy cases. Their robustness is evaluated by showing false negative and false positive lesion cases. With our proposed PnP method, false lesion structures are corrected for both deep learning methods. For FBPConvNet, the root-mean-square error (RMSE) inside the FOV can be improved from 92HU to around 30HU by PnP in the noisy case. Pix2pixGAN solely achieves better image quality than FBPConvNet solely for truncation correction in general. PnP further improves the RMSE inside the FOV from 42HU to around 27HU for Pix2pixGAN. The efficacy of PnP is also demonstrated on real clinical head data.
ER  - 

TY  - JOUR
TI  - GSIC: A New Interpretable System for Knowledge Exploration and Classification
T2  - IEEE Access
SP  - 108544
EP  - 108554
AU  - T. -P. Nguyen
AU  - S. Nguyen
AU  - D. Alahakoon
AU  - V. -N. Huynh
PY  - 2020
KW  - Medical services
KW  - Bayes methods
KW  - Prototypes
KW  - Task analysis
KW  - Self-organizing feature maps
KW  - Machine learning
KW  - Data models
KW  - Classification
KW  - interpretable machine learning
KW  - knowledge discovery
DO  - 10.1109/ACCESS.2020.3001428
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - Machine learning and data mining techniques have been developed rapidly in recent times. In tasks such as classification, machine learning techniques have been shown to equal to and even surpass human performance. However, high performance models are usually complex, opaque and have low interpretability thus making it difficult to explain the underlying behaviors of those models that lead to the final outcomes. In many domains such as medicine and healthcare, interpretability is one of the most important factors when considering the adoption of those models. In this paper, we propose a two-stage binary classification system applicable for healthcare (or general) data that benefits from a high level of interpretability and can at the same time achieve the results comparable to commonly used classification techniques. The motivation behind the proposed system is the lack of effective classification methods for handling data generated by various distributions (such as healthcare or banking data) that can harmonize both performance and interpretability perspectives. In this work, we tackle the problem by applying divide and conquer strategy on a new disentangled representation of the underlying data. The merit of our system is evaluated by a classification experiment with a wide range of real data and popular transparent and black-box models. Furthermore, a use case in data of sepsis patients staying in the ICU (Intensive Care Unit) is depicted to prove the interpretability of the proposed model.
ER  - 

