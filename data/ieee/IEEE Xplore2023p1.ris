TY  - CONF
TI  - Visualizing Research on Explainable Artificial Intelligence for Medical and Healthcare
T2  - 2023 4th International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)
SP  - 1
EP  - 6
AU  - S. Ali
AU  - A. S. Imran
AU  - Z. Kastrati
AU  - S. M. Daudpota
PY  - 2023
KW  - Visualization
KW  - Image segmentation
KW  - Systematics
KW  - Bibliometrics
KW  - Medical services
KW  - Machine learning
KW  - Market research
KW  - xai
KW  - explainable
KW  - interpretability
KW  - artificial intelligence
KW  - machine learning
KW  - deep learning
KW  - medical
KW  - healthcare
KW  - bibliometric analysis
DO  - 10.1109/iCoMET57998.2023.10099343
JO  - 2023 4th International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 4th International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)
Y1  - 17-18 March 2023
AB  - Understanding complex machine learning and artificial intelligence models have always been challenging because these models are black-box, and often we don't know what information models rely upon to infer. Explainable Artificial Intelligence (XAI) has emerged as a new exciting field to explain and understand these machine learning models as humans can understand and improve them. In the past few years, there have been numerous research articles on explainable artificial intelligence for medical and healthcare. 1687 documents are being studied and analysed using bibliometric methods in this work. There are certain systematic reviews on the same topic, but this study is the first of its kind to use a quantitative method to analyze a large number of publications. The results of this study show that the research in this field took pace in 2011, and there have been quite many publications in the following years. We have also identified top-cited journals and articles. Through thematic analysis, we have found some important thematic areas of research in the field of XAI for medical and healthcare. The findings showed that the USA is the global leader in XAI research, followed by China and Canada at second and third place, respectively.
ER  - 

TY  - CONF
TI  - Machine Learning Revolution in Early Disease Detection for Healthcare: Advancements, Challenges, and Future Prospects
T2  - 2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)
SP  - 638
EP  - 643
AU  - K. P. Reddy
AU  - M. Satish
AU  - A. Prakash
AU  - S. M. Babu
AU  - P. P. Kumar
AU  - B. S. Devi
PY  - 2023
KW  - Deep learning
KW  - Systematics
KW  - Precision medicine
KW  - Medical services
KW  - Prediction algorithms
KW  - Real-time systems
KW  - Natural language processing
KW  - Machine Learning
KW  - Early Disease Detection
KW  - Machine Learning Advancements
KW  - Healthcare
DO  - 10.1109/ICCCMLA58983.2023.10346963
JO  - 2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE 5th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)
Y1  - 7-8 Oct. 2023
AB  - This paper explored the integration of machine learning into healthcare has revolutionized early disease detection, offering a multidimensional approach to data analysis. Advanced algorithms, rooted in deep learning, process diverse datasets encompassing medical records, genetics, and imaging data, enabling subtle pattern detection. Deep learning, predictive analytics, natural language processing, anomaly detection, and personalized medicine have ushered in a proactive healthcare era, leading to better patient outcomes, reduced misdiagnosis, and cost-effective treatments. Systematic reviews underscore machine learning's impact across various medical domains. The future holds promise with enhanced data integration, interdisciplinary collaboration, explainable AI, real-time monitoring, global healthcare accessibility, ethical considerations, and continuous learning, ultimately reshaping healthcare for the better.
ER  - 

TY  - CONF
TI  - Explainable AI for Healthcare: An Approach Towards Interpretable Healthcare Models
T2  - 2023 24th International Arab Conference on Information Technology (ACIT)
SP  - 1
EP  - 7
AU  - K. Zahoor
AU  - N. Z. Bawany
AU  - U. Ghani
PY  - 2023
KW  - Deep learning
KW  - Data privacy
KW  - Pneumonia
KW  - Explainable AI
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - X-ray imaging
KW  - Explainable AI
KW  - Deep learning
KW  - Transfer learning
KW  - Healthcare
KW  - LIME
KW  - SHAP
KW  - Grad-CAM
DO  - 10.1109/ACIT58888.2023.10453740
JO  - 2023 24th International Arab Conference on Information Technology (ACIT)
IS  - 
SN  - 2831-4948
VO  - 
VL  - 
JA  - 2023 24th International Arab Conference on Information Technology (ACIT)
Y1  - 6-8 Dec. 2023
AB  - Artificial intelligence (AI) along with deep learning techniques has become an integral part of almost all aspects of life. One of the domains significantly impacted by this technological revolution is healthcare. Deep learning-based AI systems assist clinicians and medical professionals in disease diagnosis, personalized treatment, and monitoring through wearables, among other applications. Despite its expedient integration in healthcare, the trustworthiness of deep learning models remains a concern, primarily due to a lack of understanding of their underlying processes. However, Explainable AI (XAI) offers explanations through various methods, including Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive explanation (SHAP), and GRAD-CAM. XAI is utilized to enhance transparency, allowing users to understand and trust AI decisions. In this study, we present deep learning models for the classification of pneumonia disease in Chest X-ray Images followed by their explanations. Convolutional Neural Networks (CNNs) and other pre-trained models, including VGG16, MobileNetV3, and ResNet50, were used for classification of images as ‘normal’ or ‘pneumonia’. The VGG16 model, known for its exceptional image understanding capabilities, achieved the highest accuracy, with an impressive 93% score. Further, we used XAI techniques including SHAP, LIME, and Grad-CAM for explanation of models. LIME and Grad-CAM provided more accurate results than SHAP in our experiments. This approach was taken to evaluate the fairness and transparency of the model. The insights gained from XAI can be used to refine and improve machine learning models by identifying areas of weakness or misinterpretation which increases overall model robustness.
ER  - 

TY  - CONF
TI  - Explainable AI (XAI) in Healthcare - Tools and Regulations
T2  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
SP  - 151
EP  - 152
AU  - P. Raif
AU  - R. Suchanek-Raif
AU  - E. Tkacz
PY  - 2023
KW  - Industries
KW  - Digital transformation
KW  - Bibliographies
KW  - Medical services
KW  - Data science
KW  - Regulation
KW  - Biology
KW  - Explainable AI (XAI)
KW  - Interpretable Machine Learning (IML)
KW  - Trustworthy AI
KW  - risk
KW  - regulatory affairs
KW  - explainability
KW  - interpretability
KW  - digital transformation in healthcare
DO  - 10.1109/IEEECONF58974.2023.10405096
JO  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
Y1  - 7-9 Dec. 2023
AB  - Explainable AI (XAI) techniques are becoming an increasingly important element in the development of trustworthy AI solutions in healthcare research and industry. In this paper, we present the result of our literature review on selected aspects of XAI in healthcare such as the motivation behind XAI, regulations and available methods.
ER  - 

TY  - CONF
TI  - An Overview Of Interpretability Techniques For Explainable Artificial Intelligence (XAI) In Deep Learning-Based Medical Image Analysis
T2  - 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)
SP  - 175
EP  - 182
AU  - P. D. S
AU  - R. Kumar K
AU  - V. S
AU  - N. K
AU  - A. K
PY  - 2023
KW  - Deep learning
KW  - Analytical models
KW  - Image analysis
KW  - Computational modeling
KW  - Neural networks
KW  - Medical services
KW  - Computer architecture
KW  - Explainable Artificial Intelligence (XAI)
KW  - XAI architecture
KW  - Medical Image Analysis
KW  - Interpretability method
KW  - XAI approaches
KW  - Explanation methods
KW  - Visual explanation
KW  - Deep neural network.
DO  - 10.1109/ICACCS57279.2023.10113001
JO  - 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)
IS  - 
SN  - 2575-7288
VO  - 1
VL  - 1
JA  - 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)
Y1  - 17-18 March 2023
AB  - In the past two years, technology has undergone significant changes that have had a major impact on healthcare systems. Artificial intelligence (AI) is a key component of this change, and it can assist doctors with various healthcare systems and intelligent health systems. AI is crucial in diagnosing common diseases, developing new medications, and analyzing patient information from electronic health records. However, one of the main issues with adopting AI in healthcare is the lack of transparency, as doctors must interpret the output of the AI. Explainable AI (XAI) is extremely important for the healthcare sector and comes into play in this regard. With XAI, doctors, patients, and other stakeholders can more easily examine a decision's reliability by knowing its reasoning due to XAI's interpretable explanations. Deep learning is used in this study to discuss explainable artificial intelligence (XAI) in medical image analysis. The primary goal of this paper is to provide a generic six-category XAI architecture for classifying DL-based medical image analysis and interpretability methods.The interpretability method/XAI approach for medical image analysis is often categorized based on the explanation and technical method. In XAI approaches, the explanation method is further sub-categorized into three types: text-based, visual-based, and examples-based. In interpretability technical method, it was divided into nine categories. Finally, the paper discusses the advantages, disadvantages, and limitations of each neural network-based interpretability method for medical imaging analysis.
ER  - 

TY  - CONF
TI  - Future Directions of Artificial Intelligence and Machine Learning in Healthcare: A Systematic Analysis and Mapping Study
T2  - 2023 6th International Conference on Information Systems and Computer Networks (ISCON)
SP  - 1
EP  - 6
AU  - H. M. Mishra
AU  - B. Ahmed
AU  - M. Shuja
AU  - A. Qtaishat
AU  - M. Kumar
PY  - 2023
KW  - Privacy
KW  - Systematics
KW  - Roads
KW  - Transfer learning
KW  - Machine learning
KW  - Medical services
KW  - Search problems
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Healthcare
DO  - 10.1109/ISCON57294.2023.10111959
JO  - 2023 6th International Conference on Information Systems and Computer Networks (ISCON)
IS  - 
SN  - 2832-143X
VO  - 
VL  - 
JA  - 2023 6th International Conference on Information Systems and Computer Networks (ISCON)
Y1  - 3-4 March 2023
AB  - In order to organise previously conducted research on using machine learning in medical contexts, the authors of this paper employ a method known as systematic mapping. In order to do this, we looked through the abstracts of a variety of publications, including medical journals, healthcare periodicals, and conference proceedings, looking for the phrase “use of artificial intelligence and machine learning in healthcare.” After doing a search on Google Scholar, which resulted in the retrieval of 500 papers, we classified these studies in accordance with their objectives, approaches, primary concerns, and illnesses. With the use of this technique, we were able to organise our findings into the five categories that are as follows: privacy and security; a framework for privacy and security; interpretable machine learning; medical image assessment; electronic health record processing; and transfer learning. In addition, we found that the evaluation of medical images is the topic that receives the most research, that interpretable machine learning and explainable artificial intelligence are becoming increasingly popular, and that most authors are primarily interested in cancer research. To restate our mission, we want to provide future generations of scholars with an accurate picture of where the field is now and where it is headed.
ER  - 

TY  - CONF
TI  - Explainable AI in Orthopedics: Challenges, Opportunities, and Prospects
T2  - 2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)
SP  - 1374
EP  - 1380
AU  - S. Amirian
AU  - L. A. Carlson
AU  - M. F. Gong
AU  - I. Lohse
AU  - K. R. Weiss
AU  - J. F. Plate
AU  - A. P. Tafti
PY  - 2023
KW  - Data privacy
KW  - Explainable AI
KW  - Surgery
KW  - Collaboration
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Explainable AI
KW  - XAI
KW  - Explainable Machine Learning
KW  - AI-Powered Healthcare
KW  - Health Informatics
DO  - 10.1109/CSCE60160.2023.00230
JO  - 2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 Congress in Computer Science, Computer Engineering, & Applied Computing (CSCE)
Y1  - 24-27 July 2023
AB  - While artificial intelligence (AI) has made many successful applications in various domains, its adoption in health-care lags a little bit behind other high-stakes settings. Several factors contribute to this slower uptake, including regulatory frameworks, patient privacy concerns, and data heterogeneity. However, one significant challenge that impedes the implementation of AI in healthcare, particularly in orthopedics, is the lack of explainability and interpretability around AI models. Addressing the challenge of explainable AI (XAI) in orthopedics requires developing AI models and algorithms that prioritize transparency and interpretability, allowing clinicians, surgeons, and patients to understand the contributing factors behind any AI-powered predictive or descriptive models. The current contribution outlines several key challenges and opportunities that manifest in XAI in orthopedic practice. This work empha-sizes the need for interdisciplinary collaborations between AI practitioners, orthopedic specialists, and regulatory entities to establish standards and guidelines for the adoption of XAI in orthopedics.
ER  - 

TY  - CONF
TI  - Smart Healthcare Monitoring System: Integrating IoT, Deep Learning, and XGBoost for Real-time Patient Diagnosis
T2  - 2023 OITS International Conference on Information Technology (OCIT)
SP  - 708
EP  - 713
AU  - K. Paulraj
AU  - N. Soms
AU  - S. D. Samuel Azariya
AU  - S. P. S
AU  - J. E. J
AU  - V. Sureshkumar
PY  - 2023
KW  - Deep learning
KW  - Technological innovation
KW  - Decision making
KW  - Real-time systems
KW  - Medical diagnosis
KW  - Internet of Things
KW  - Monitoring
KW  - Smart Healthcare Monitoring System
KW  - IoT
KW  - Deep Learning
KW  - XGBoost
KW  - Real-time Patient Diagnosis
KW  - Wearable Devices
KW  - Medical Sensors
KW  - Accuracy
KW  - Precision
KW  - Healthcare Practices
KW  - Data Privacy
KW  - Model Interpretability
KW  - and Clinical Decision-making
DO  - 10.1109/OCIT59427.2023.10431108
JO  - 2023 OITS International Conference on Information Technology (OCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 OITS International Conference on Information Technology (OCIT)
Y1  - 13-15 Dec. 2023
AB  - Integrating the Internet of Things (IoT), Deep Learning (DL), and the XGBoost algorithm has paved the way for transformative advancements in healthcare. This paper presents a pioneering study on a "Smart Healthcare Monitoring System" that harnesses the synergy of these technologies for real-time patient diagnosis. With the escalating demand for accurate and swift medical assessments, our work addresses the crucial need for timely and precise diagnosis. The proposed system amalgamates IoT-enabled wearable devices and sensors to capture comprehensive patient data. A hybrid approach is employed to leverage this data, comprising a Deep Learning model for intricate pattern recognition and the XGBoost algorithm for rapid decision-making. Nonetheless, challenges such as data security and model interpretability are acknowledged, highlighting avenues for future research. This paper underscores the transformative potential of the IoT-Deep Learning-XGBoost integration, offering a robust foundation for further innovation in intelligent healthcare systems. As healthcare delivery evolves, our work underscores the promise of this interdisciplinary fusion in revolutionizing patient diagnosis and care.
ER  - 

TY  - CONF
TI  - Stroke Probability Prediction from Medical Survey Data: AI-Driven Analysis with Insightful Feature Importance using Explainable AI (XAI)
T2  - 2023 26th International Conference on Computer and Information Technology (ICCIT)
SP  - 1
EP  - 6
AU  - S. B. Akter
AU  - S. Akter
AU  - T. S. Pias
PY  - 2023
KW  - Surveys
KW  - Radio frequency
KW  - Explainable AI
KW  - Computational modeling
KW  - Medical services
KW  - Stroke (medical condition)
KW  - Predictive models
KW  - Stroke Probability Prediction
KW  - Behavioral Risk Factor Surveillance System (BRFSS)
KW  - Stacking
KW  - Hybrid Learning
KW  - Explainable AI (XAI)
KW  - Residual Networks
KW  - AI in Health Care
DO  - 10.1109/ICCIT60459.2023.10441480
JO  - 2023 26th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 26th International Conference on Computer and Information Technology (ICCIT)
Y1  - 13-15 Dec. 2023
AB  - Prioritizing dataset dependability, model performance, and interoperability is a compelling demand for improving stroke probability prediction from medical surveys using AI in healthcare. These collective efforts are required to enhance the field of stroke probability assessment and demonstrate the transformational potential of AI in healthcare. This effective study leverages the CDC’s published 2021 BRFSS dataset to explore AI-based stroke probability prediction. Numerous substantial and notable contributions have been established from this study. To start with, the dataset’s dependability is improved through preprocessing and oversampling techniques that overcome the challenges of missing data and class imbalance. In order to identify the most promising models, eight different AI models are meticulously evaluated including DT, RF, GNB, RusBoost, AdaBoost, CNN, ANN, and MLP. The study combines top-performing models using fusion approaches such as soft voting, hard voting, and stacking to demonstrate the combined prediction performance. The stacking-based model combined with GNB, RF, and AdaBoost demonstrated superior performance, achieving a specificity of 74.06% and a sensitivity of 74.20%. The work also employs explainable AI (XAI) approaches to highlight the subtle contributions of important features across both healthy and stroke cases, improving model interpretability. The comprehensive approach to stroke probability prediction employed in this study enhanced dataset reliability, model performance, and interpretability, demonstrating AI’s fundamental impact in healthcare.
ER  - 

TY  - CONF
TI  - The Use of Explainable Artificial Intelligence in Medicine
T2  - 2023 IEEE International Conference on E-health Networking, Application & Services (Healthcom)
SP  - 251
EP  - 252
AU  - B. Wen
AU  - A. Chaddad
PY  - 2023
KW  - Explainable AI
KW  - Federated learning
KW  - Decision making
KW  - Medical services
KW  - Cognition
KW  - Complexity theory
KW  - Medical diagnostic imaging
KW  - XAI
KW  - machine learning
KW  - medicine
DO  - 10.1109/Healthcom56612.2023.10472365
JO  - 2023 IEEE International Conference on E-health Networking, Application & Services (Healthcom)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on E-health Networking, Application & Services (Healthcom)
Y1  - 15-17 Dec. 2023
AB  - Artificial intelligence (AI) is revolutionizing the medical field in various ways. However, despite the successful implementation of various AI models in the healthcare field, the complexity, high dimensionality, and nonlinearity of AI hinder its widespread application due to the opacity of the models. The role of eXplainable artificial intelligence (XAI) is basically the same, providing clinicians and experts with predictive, diagnostic, and explanatory information about clinical decision-making patterns, and providing patients with explanations of the reasoning process and results generated by AI, thereby improving their acceptance and trust in such explanations.
ER  - 

TY  - CONF
TI  - The Metaverse for Intelligent Healthcare using XAI, Blockchain, and Immersive Technology
T2  - 2023 IEEE International Conference on Metaverse Computing, Networking and Applications (MetaCom)
SP  - 612
EP  - 616
AU  - M. A. I. Mozumder
AU  - T. P. T. A.
AU  - R. I. Sumon
AU  - S. M. I. Uddin
AU  - A. Athar
AU  - H. -C. Kim
PY  - 2023
KW  - Training
KW  - Metaverse
KW  - Surgery
KW  - Mixed reality
KW  - Medical services
KW  - Transforms
KW  - Organizations
KW  - metaverse
KW  - healthcare
KW  - immersive Technology
KW  - artificial intelligence
KW  - blockchain
DO  - 10.1109/MetaCom57706.2023.00107
JO  - 2023 IEEE International Conference on Metaverse Computing, Networking and Applications (MetaCom)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Metaverse Computing, Networking and Applications (MetaCom)
Y1  - 26-28 June 2023
AB  - The metaverse combines the real world and the virtual world, allowing people to interact with their avatars in a setting supported by cutting-edge technologies like 5G or 6G internet, virtual reality (VR), augmented reality (AR), mixed reality (MR), extended reality, blockchain, digital twins, internet of things, and artificial intelligence (AI), all of which are enhanced by practically limitless data. The metaverse, a term coined to describe the virtual world instead of physical worlds, has the potential to revolutionize general healthcare. The Metaverse already provides opportunities for remote patient care, medical education, surgeries training, anti-aging healthcare, and medical research opportunities with implementation. According to the studies, the metaverse in medical domain revenue is expected to reach $640B by 2027. Different fields such as the financial industry, manufacturing industry, media, entertainment, etc. are taking advantage of healthcare tools to bring state-of-the-art technology into their domain. In this study, we are going to discuss the current state of metaverse technology in digital healthcare and its uses in the metaverse. We will also discuss the challenges that the metaverse presents for the healthcare industry. Also, we will conclude with a discussion of the future of metaverse in healthcare and the potential impact it could have on the delivery of healthcare services. Finally, this paper aims to provide an XAI, BC, and Immersive technology-based framework for metaverse healthcare and its potential to improve patient outcomes and transform the healthcare industry.
ER  - 

TY  - CONF
TI  - Improving Prospective Healthcare Outcomes by Leveraging Open Data and Explainable AI
T2  - 2023 3rd International Conference on Computing and Information Technology (ICCIT)
SP  - 486
EP  - 492
AU  - G. Alam
AU  - I. McChesney
AU  - P. Nicholl
AU  - J. Rafferty
PY  - 2023
KW  - Training
KW  - Medical services
KW  - Prediction algorithms
KW  - Market research
KW  - Data models
KW  - Reliability
KW  - Artificial intelligence
KW  - Open data
KW  - Data sets
KW  - Healthcare
KW  - Explainable AI
KW  - AI balances
KW  - AI bias
DO  - 10.1109/ICCIT58132.2023.10273878
JO  - 2023 3rd International Conference on Computing and Information Technology (ICCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 3rd International Conference on Computing and Information Technology (ICCIT)
Y1  - 13-14 Sept. 2023
AB  - The use of Artificial Intelligence (AI) in healthcare can improve diagnosis and clinical workflows, advancing standards and improving outcomes. AI for healthcare raises significant ethical and legal challenges. AI model development and deployment require transparency and interpretability. Transparency is the capacity to comprehend how an AI model makes recommendations, whereas interpretability is the ability to understand why. One such issue is the lack of transparency and interpretability of AI models, which might lead to a lack of trust in its decisions. Explainable AI (XAI) is an emerging research field that tries to solve this problem by creating AI models that can explain their recommendations. AI models need enough data to be accurate and reliable, thus they must be trained on various and extensive data sets. Open data and data sets (ODDS) can aid AI model development and training. Medical images, lab reports, and patient data are used to train algorithms to identify trends and make predictions in healthcare. Open data can help XAI in healthcare to enable AI models to be transparent, accountable, and interpretable. Open data makes possible AI model validation and replication. This is especially crucial in healthcare, where AI model flaws or biases can have serious implications. This paper discusses the use of ODDS and XAI to improve healthcare. ODDS's capability to promote trust and openness and best practices for using XAI in healthcare are also explored. The benefits and challenges of employing XAI and ODDS in their current forms are also discussed.
ER  - 

TY  - CONF
TI  - Deep Learning and Explainable Machine Learning on Hair Disease Detection
T2  - 2023 IEEE 5th Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)
SP  - 150
EP  - 153
AU  - W. W. Heng
AU  - N. A. Abdul-Kadir
PY  - 2023
KW  - Deep learning
KW  - Hair
KW  - Sensitivity
KW  - Databases
KW  - Biological system modeling
KW  - Scalp
KW  - Medical services
KW  - Deep learning
KW  - explainable machine learning
KW  - XAI
KW  - hair loss
KW  - scalp disease
DO  - 10.1109/ECBIOS57802.2023.10218472
JO  - 2023 IEEE 5th Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE 5th Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)
Y1  - 2-4 June 2023
AB  - Deep learning algorithms have been widely used for various healthcare research because it helps eliminate the need for manual feature extraction which requires specialist expertise and is time-consuming. However, deep learning models have low interpretability in their classification results and hence low trust and practical usage in clinical settings. To overcome this reliability issue, explainable machine learning (XAI) can be used to understand the effect of the different networks and the extracted features on the classification results. In this study, multiple convolutional neural networks were trained and tested on hairy scalp images for the detection of hair diseases. In addition to standard performance metrics including accuracy, sensitivity, and specificity, we further investigated the models' interpretability using three XAI techniques including Local Interpretable Model-Agnostic Explanations, Gradient-weighted Class Activation Mapping, and occlusion sensitivity. The result of using XAI techniques revealed that the model's high classification accuracy did not necessarily coincide with its applicability or practicality. The application of XAI techniques in this study provided valuable insights into the contributions made by different groups of pixels to the model's decision-making process. This method helped identify potential model biases, which could then be utilized to facilitate informed adjustments for the improvement of the model's robustness.
ER  - 

TY  - CONF
TI  - Random Forest Classification in Healthcare Decision Support for Disease Diagnosis
T2  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
SP  - 1
EP  - 7
AU  - A. Badhoutiya
AU  - R. P. Verma
AU  - A. Shrivastava
AU  - K. Laxminarayanamma
AU  - A. L. N. Rao
AU  - A. K. Khan
PY  - 2023
KW  - Measurement
KW  - Adaptation models
KW  - Technological innovation
KW  - Scalability
KW  - Forestry
KW  - Learning (artificial intelligence)
KW  - Medical diagnosis
KW  - Random Forest
KW  - healthcare decision support
KW  - interpretability
KW  - diagnostic accuracy
KW  - machine learning
DO  - 10.1109/ICAIIHI57871.2023.10489244
JO  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
Y1  - 29-30 Dec. 2023
AB  - The use of random forests Classification in medical decision support for diagnosing diseases is investigated in this study. Based on interpretivism, the study uses a design that is descriptive in nature, a deductive methodology, and secondary data to evaluate the interpretability, adaptability, and effect of the model on diagnostic accuracy. The findings indicate that performance metrics are promising and that methods such as SHAP values have improved interpretability. Random Forest's potential is emphasized through comparing it with traditional methods, and its adaptability is further highlighted by its generalization across various healthcare domains. Refinement opportunities are found through critical analysis, which includes dealing with skepticism and overfitting. Additional model optimization and joint creation of user-friendly interfaces are among the recommendations. Further research should concentrate on improving interpretability, examination scalability, evaluating real-world impact, and fine-tuning advanced hyperparameters.
ER  - 

TY  - CONF
TI  - Development and Evaluation of an Explainable Diagnostic AI for Alzheimer's Disease
T2  - 2023 International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS)
SP  - 1
EP  - 6
AU  - D. Yilmaz
PY  - 2023
KW  - Deep learning
KW  - Support vector machines
KW  - Pathology
KW  - Magnetic resonance imaging
KW  - Transfer learning
KW  - Predictive models
KW  - Convolutional neural networks
KW  - Machine learning
KW  - Alzheimer's disease
KW  - healthcare
KW  - neuroimaging
KW  - deep learning
DO  - 10.1109/CAISAIS59399.2023.10270042
JO  - 2023 International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS)
Y1  - 3-5 Sept. 2023
AB  - Alzheimer's Disease (AD) is a progressive neurode-generative disease that is estimated to affect 24.3 million people worldwide, and with an expected rise in cases, an accurate and efficient diagnosis method is necessary. Machine learning has been implemented for diagnosing AD in previous studies utilizing various modalities of data. More specifically, deep learning has been shown to perform at very high accuracy rates. However, many healthcare applications of deep learning paradigms lack trust due to their lack of interpretability. To provide evidence for the potential of deep learning to meet established standards of accurate diagnosis, we have designed an explainable diagnostic machine learning model for predicting AD severity levels. Using two open-source Magnetic Resonance Imaging (MRI) datasets, we develop and evaluate a Convolutional Neural Network (CNN). An accuracy rate of 99.9% was achieved using the CNN model, which outperforms other models trained on the same dataset. To improve model transparency, we leverage the explainable AI technique SHapley Additive exPlanations (SHAP) to interpret the predictions of the CNN. The implementation of explainable AI demonstrates that the predictions of the model are influenced by well-known pathological indicators of AD.
ER  - 

TY  - CONF
TI  - EXplainable Artificial Intelligence (XAI) for MRI brain tumor diagnosis: A survey
T2  - 2023 International Conference on Cyberworlds (CW)
SP  - 171
EP  - 178
AU  - H. Charaabi
AU  - H. Mzoughi
AU  - R. Hamdi
AU  - M. Njah
PY  - 2023
KW  - Surveys
KW  - Deep learning
KW  - Training
KW  - Magnetic resonance imaging
KW  - Brain modeling
KW  - Cognition
KW  - Artificial intelligence
KW  - eXplainable Artificial Intelligence
KW  - XAI
KW  - post-hoc XAI
KW  - intrinsic XAI
KW  - brain tumor diagnosis
KW  - Medical Imaging Data
KW  - MRI
DO  - 10.1109/CW58918.2023.00033
JO  - 2023 International Conference on Cyberworlds (CW)
IS  - 
SN  - 2642-3596
VO  - 
VL  - 
JA  - 2023 International Conference on Cyberworlds (CW)
Y1  - 3-5 Oct. 2023
AB  - The results of the Deep Learning (DL) are indisputable in different fields and in particular that of the medical diagnosis. The black box nature of this tool has left the doctors very cautious with regard to its estimates. The eXplainable Artificial Intelligence (XAI) recently seemed to lift this challenge by providing explanations to the DL estimates. Several works are published in the literature offering explanatory methods. We are interested in this survey to present an overview on the application of XAI in Deep Learning-based Magnetic Resonance Imaging (MRI) image analysis for Brain Tumor (BT) diagnosis. In this survey, we divide these XAI methods into four groups, the group of the intrinsic methods and three groups of post-hoc methods which are the activation based, the gradientr based and the perturbation based XAI methods. These XAI tools improved the confidence on the DL based brain tumor diagnosis.
ER  - 

TY  - CONF
TI  - An Explainable AI Framework for Artificial Intelligence of Medical Things
T2  - 2023 IEEE Globecom Workshops (GC Wkshps)
SP  - 2097
EP  - 2102
AU  - A. Amin
AU  - K. Hasan
AU  - S. Zein-Sabatto
AU  - D. Chimba
AU  - I. Ahmed
AU  - T. Islam
PY  - 2023
KW  - Training
KW  - Industries
KW  - Explainable AI
KW  - Instruments
KW  - Refining
KW  - Medical services
KW  - Real-time systems
KW  - Explainable AI (XAI)
KW  - Maximum Voting Classifier
KW  - Internet of Medical Things
KW  - Intelligent Healthcare System
KW  - Health
DO  - 10.1109/GCWkshps58843.2023.10464798
JO  - 2023 IEEE Globecom Workshops (GC Wkshps)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE Globecom Workshops (GC Wkshps)
Y1  - 4-8 Dec. 2023
AB  - The healthcare industry has been revolutionized by the convergence of Artificial Intelligence of Medical Things (AIoMT), allowing advanced data-driven solutions to improve healthcare systems. With the increasing complexity of Artificial Intelligence (AI) models, the need for Explainable Artificial Intelligence (XAI) techniques become paramount, particularly in the medical domain, where transparent and interpretable decision-making becomes crucial. Therefore, in this work, we leverage a custom XAI framework, incorporating techniques such as Local Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for the domain of AIoMT. The proposed framework enhances the effectiveness of strategic healthcare methods and aims to instill trust and promote understanding in AI-driven medical applications. Moreover, we utilize a majority voting technique that aggregates predictions from multiple convolutional neural networks (CNNs) and leverages their collective intelligence to make robust and accurate decisions in the healthcare system. Building upon this decision-making process, we apply the XAI framework to brain tumor detection as a use case demon strating accurate and transparent diagnosis. Evaluation results underscore the exceptional performance of the XAI framework, achieving high precision, recall, and F1 scores with a training accuracy of 99% and a validation accuracy of 98%. Combining advanced XAI techniques with ensemble-based deep-learning (DL) methodologies allows for precise and reliable brain tumor diagnoses as an application of AIoMT.
ER  - 

TY  - CONF
TI  - Recent Advances, Challenges, and Applications of Deep Learning in Healthcare Systems for Medical Diagnosis and Treatment
T2  - 2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS)
SP  - 716
EP  - 724
AU  - R. C. Patra
AU  - G. Saritha
AU  - G. A. Raghuwanshi
AU  - P. Parthiban
AU  - V. Ashok
PY  - 2023
KW  - Deep learning
KW  - Data privacy
KW  - Medical services
KW  - Weaving
KW  - Trajectory
KW  - Medical diagnosis
KW  - Convolutional neural networks
KW  - Deep Learning
KW  - Healthcare Systems
KW  - Medical Diagnosis
KW  - Medical Treatment
KW  - Convolutional Neural Networks
KW  - Personalized Treatment
KW  - Disease Trajectory Prediction
KW  - Drug Discovery
KW  - Data Privacy
KW  - Model Interpretability
KW  - Healthcare Management
DO  - 10.1109/ICTACS59847.2023.10390124
JO  - 2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS)
Y1  - 1-3 Nov. 2023
AB  - In the realm of healthcare, the integration of Deep Learning (DL) stands as a potent force, propelling advancements in medical diagnosis and treatment. This paper navigates recent strides, persistent hurdles, and emerging applications within this synergy. DL basics are elucidated initially, demonstrating neural networks' role in data analysis. Progressing further, we unveil DL's robust applications in medical diagnosis, particularly via Convolutional Neural Networks, revolutionizing image-based disease detection. Yet, this frontier is not devoid of challenges, encompassing data privacy concerns and model interpretability. In tandem, DL empowers personalized treatment by predicting disease trajectories and expediting drug discovery. Expanding horizons, DL streamlines healthcare management through resource optimization and digitized records. In conclusion, this synthesis of DL and healthcare signifies a transformative trajectory, promising refined diagnostics and patient-centric treatments, though not devoid of ethical and technical intricacies.
ER  - 

TY  - CONF
TI  - Explainable AI-Based Malicious Traffic Detection and Monitoring System in Next-Gen IoT Healthcare
T2  - 2023 International Conference on Smart Applications, Communications and Networking (SmartNets)
SP  - 1
EP  - 6
AU  - E. Gürbüz
AU  - Ö. Turgut
AU  - İ. Kök
PY  - 2023
KW  - Data privacy
KW  - Privacy
KW  - Patient monitoring
KW  - Telemedicine
KW  - Decision making
KW  - Medical services
KW  - Security
KW  - Internet of Things
KW  - Healthcare security
KW  - intrusion detection
KW  - Explainable Artificial Intelligence(XAI)
KW  - Interpretable Machine Learning (IML)
DO  - 10.1109/SmartNets58706.2023.10215896
JO  - 2023 International Conference on Smart Applications, Communications and Networking (SmartNets)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Smart Applications, Communications and Networking (SmartNets)
Y1  - 25-27 July 2023
AB  - In recent years, there has been a surge in IoT healthcare applications, ranging from wearable health monitors and remote patient monitoring systems to smart medical devices, telemedicine platforms, and personalized health tracking and management tools. The purpose of these applications is to improve treatment outcomes, streamline healthcare delivery, and enable data-driven decision-making. However, due to the sensitive nature of health data and the critical role that these applications play in people’s lives, ensuring their security and privacy has become a paramount concern. To address this issue, we developed an explainable malicious traffic detection and monitoring system based on Machine Learning (ML) and Deep Learning (DL) models. The proposed system involves the use of Explainable Artificial Intelligence (XAI) methods such as LIME, SHAP, ELI5, and Integrated Gradients(IG) to ensure the interpretability and explainability of the developed models. Finally, we demonstrate the high accuracy of the developed models in detecting attacks on the intensive care patient dataset. Furthermore, we ensure the transparency and interpretability of the model outcomes by presenting them through the Shapash Monitor interface, which can be easily accessed by both experts and non-experts.
ER  - 

TY  - CONF
TI  - A Novel Attention-based Explainable Deep Learning Framework Towards Medical Image Classification
T2  - 2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS)
SP  - 1
EP  - 8
AU  - G. W. Muoka
AU  - D. Yi
AU  - C. C. Ukwuoma
AU  - M. D. Martin
AU  - A. A. Aydin
AU  - M. A. Al-Antari
PY  - 2023
KW  - Deep learning
KW  - COVID-19
KW  - Analytical models
KW  - Lung cancer
KW  - Predictive models
KW  - Retina
KW  - Breast cancer
KW  - Medical Imaging
KW  - Deep Learning
KW  - Attention Mechanism
KW  - Explainable AI(XAI)
DO  - 10.1109/ISAS60782.2023.10391289
JO  - 2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS)
Y1  - 23-25 Nov. 2023
AB  - Deep learning applications for medical image classification have shown remarkable promise, particularly incorporating attention-based neural networks. This is particularly relevant in medical imaging, where the integration of Artificial Intelligence assists with various imaging tasks, including classification, segmentation, and detection. Deep learning is revolutionizing medical research and playing a significant role in advancing personalized clinical treatment. However, the lack of interpretability in these models presents a significant obstacle to their adoption in clinical practice. Therefore, there is a growing need for a comprehensive understanding of artificial intelligence systems and their internal mechanisms, capabilities, and limitations, which is the focus of the field of explainable AI. This study proposes a novel attention-based explainable deep learning framework for medical image classification tasks, including Covid-19, breast cancer (BreakHis), lung cancer (LC2500), and Retinal optical coherence tomography (OCT). The proposed framework recorded overall accuracies of 98% (Covid-19 Radiography), 95% (BreakHis), 99.8% (LC2500), and 95%(OCT). For visual analysis of the outcomes, we employ and use the LIME, SHAP, and ELI-5 to analyze the achieved results. The study’s primary goal is to bridge the gap between the high performance achieved by attention-based models and the necessity for transparency and interpretability in medical image diagnostics.
ER  - 

TY  - STD
TI  - IEEE Draft Guide for an Architectural Framework for Explainable Artificial Intelligence
T2  - IEEE P2894/D8, August 2023
SP  - 1
EP  - 51
PY  - 2023
KW  - IEEE Standards
KW  - Artificial intelligence
KW  - Software architecture
KW  - AI
KW  - architectural framework
KW  - artificial intelligence
KW  - explainable AI
KW  - explainable artificial intelligence
KW  - IEEE 2894™
KW  - machine learning
KW  - XAI
DO  - 
JO  - IEEE P2894/D8, August 2023
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE P2894/D8, August 2023
Y1  - 10 Aug. 2023
AB  - A new wave of artificial intelligence applications that offer extensive benefits to our daily lives has been led to by dramatic success in machine learning. The loss of explainability during this transition, however, means vulnerability to vicious data, poor model structure design, and suspicion of stakeholders and the general public--all with a range of legal implications. The study of explainable AI (XAI), which is an active research field that aims to make AI systems results more understandable to humans, has been called for by this dilemma. This is a field with great hopes for improving the trust and transparency of AI-based systems and is considered a necessary route for AI to move forward. A technological blueprint for building, deploying, and managing machine learning models, while meeting the requirements of transparent and trustworthy AI by adopting a variety of XAI methodologies, is provided by this guide. It defines the architectural framework and application guidelines for explainable AI, including: description and definition of XAI; the types of XAI methods and the application scenarios to which each type applies; and performance evaluation of XAI.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence in Healthcare Applications: A Systematic Review
T2  - 2023 International Scientific Conference on Computer Science (COMSCI)
SP  - 1
EP  - 8
AU  - B. Costa
AU  - P. Georgieva
PY  - 2023
KW  - Industries
KW  - Computer science
KW  - Systematics
KW  - Decision making
KW  - Taxonomy
KW  - Closed box
KW  - Medical services
KW  - explainable artificial intelligence
KW  - healthcare
KW  - medical applications
KW  - machine learning
DO  - 10.1109/COMSCI59259.2023.10315829
JO  - 2023 International Scientific Conference on Computer Science (COMSCI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Scientific Conference on Computer Science (COMSCI)
Y1  - 18-20 Sept. 2023
AB  - Current artificial intelligence (AI) advances and progress in medicine created a new challenge for medical AI. The” black-box” nature of AI methods has created a discussion on the use of explainability techniques to build trust and provide transparency in the AI decision-making process. A study of current state-of-the-art approaches in Explainable Artificial Intelligence (XAI) was conducted using Preferred Reporting Items on Systematic Reviews and Meta-analysis (PRISMA) research technology. In this systematic review, we provide an overview of current XAI techniques based on different taxonomies. Finally, we discuss the applications and challenges that come with the application of explainability methods in the healthcare industry.
ER  - 

TY  - CONF
TI  - Understanding AI: Interpretability and Transparency in Machine Learning Models
T2  - 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)
SP  - 613
EP  - 617
AU  - R. Dugyala
AU  - S. K. Singh
AU  - M. Saleh Al Ansari
AU  - C. Gunasundari
AU  - K. Aswini
AU  - G. Sandhya
PY  - 2023
KW  - Heart
KW  - Computational modeling
KW  - Decision making
KW  - Medical services
KW  - Machine learning
KW  - Data models
KW  - Diseases
KW  - Artificial intelligence
KW  - Machine learning
KW  - Interpretability
KW  - Transparency
KW  - Health care
DO  - 10.1109/UPCON59197.2023.10434665
JO  - 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)
IS  - 
SN  - 2687-7767
VO  - 10
VL  - 10
JA  - 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)
Y1  - 1-3 Dec. 2023
AB  - Artificial intelligence (AI) is recognized as a valuable tool in various healthcare uses for diagnosing and therapeutic decision-making. Due to the tremendous rise in accessible data and processing capacity, machine learning (ML) models have performed well or greater than doctors in numerous activities. The AI platform needs to be transparent, resilient, and interpretable to adhere to the principles of trusted AI. Present ML systems are alluded to as black boxes because of the absence of understanding of the mechanics related to the decision-making procedures. As a result, before ML can be implemented into ordinary healthcare processes, its transparency and interpretability must be understood. To address this issue, this study presents a method for understanding the transparency and interpretability of ML suggestion systems. We particularly adapt the suggested technique to a chronic condition that is frequent in seniors: heart disease. The suggested approach illustrates the fundamental cause for these suggestions and increases patient trust and interpretability of ML models by assessing the influence of various patient features on the suggestions.
ER  - 

TY  - CONF
TI  - Explainable AI (XAI): Explained
T2  - 2023 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)
SP  - 1
EP  - 6
AU  - G. P. Reddy
AU  - Y. V. P. Kumar
PY  - 2023
KW  - Ethics
KW  - Additives
KW  - Law
KW  - Social networking (online)
KW  - Focusing
KW  - Finance
KW  - Medical services
KW  - Black box models
KW  - Explainable AI (XAI)
KW  - Interpretability
KW  - Local Interpretable Model-Agnostic Explanations (LIME)
KW  - SHapley Additive exPlanations (SHAP)
KW  - Transparency
KW  - Trust in AI
DO  - 10.1109/eStream59056.2023.10134984
JO  - 2023 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)
IS  - 
SN  - 2690-8506
VO  - 
VL  - 
JA  - 2023 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)
Y1  - 27-27 April 2023
AB  - Artificial intelligence (AI) has become an integral part of our lives; from the recommendations we receive on social media to the diagnoses made by medical professionals. However, as AI continues to grow more complex, the “black box” nature of many AI models has become a cause for concern. The main objective of Explainable AI (XAI) research is to produce AI models that are easily interpretable and understandable by humans. In this view, this paper presents an overview of XAI and its techniques for creating interpretable models, specifically focusing on Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP). Furthermore, this paper delves into the various applications of XAI in different domains, including healthcare, finance, and law. Additionally, the ethical and legal implications of using XAI are mentioned. Finally, the paper discusses various challenges and future research directions of XAI.
ER  - 

TY  - CONF
TI  - Evaluating SHAP’s Robustness in Precision Medicine: Effect of Filtering and Normalization
T2  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 3157
EP  - 3164
AU  - M. Sobhan
AU  - A. M. Mondal
PY  - 2023
KW  - Logistic regression
KW  - Machine learning algorithms
KW  - Uncertainty
KW  - Filtering
KW  - Precision medicine
KW  - Transcriptomics
KW  - Sociology
KW  - explainable machine learning
KW  - robust AI
KW  - patient-specific biomarkers
KW  - precision medicine
KW  - SHAP
DO  - 10.1109/BIBM58861.2023.10385704
JO  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 2156-1133
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 5-8 Dec. 2023
AB  - Local interpretation of explainable AI, SHAP (SHapley Additive exPlanations), in disease classification problems offers significant feature scores for each sample, potentially identifying precision medicine targets. Tailoring treatments based on individual genetic and molecular targets can enhance therapeutic outcomes while minimizing side effects. However, the suitability of SHAP's local interpretation at the patient level remains uncertain. It generates different sets of patient-specific genes in various runs, even with consistent overall accuracies. This uncertainty challenges the reliability of SHAP’s local interpretations for precision medicine applications. Not only that, different filtering criteria and normalization techniques may influence the contribution scores of patient-specific features. To validate our hypothesis, SHAP was applied to machine learning algorithms from different genres to identify patient-specific feature contributions from the breast cancer subtype classification problem. The program underwent multiple runs to assess the robustness of SHAP.Our study demonstrates that shallow machine learning algorithms, like Logistic Regression, consistently provided stable and reliable results across multiple runs. In contrast, complex machine learning models like XGBoost and MLP exhibited inconsistencies across different runs. Moreover, we found that data normalization techniques, particularly z-score and min-max normalization, had a minimal effect on the performance of XGBoost models. Our study also shows that the accuracy scores of complex machine learning models remained relatively constant across different runs but produced different sets of patient-specific features. In conclusion, our findings underscore the importance of selecting appropriate filtering and normalization techniques, given the variability in SHAP results across different runs. Our study indicates that combining SHAP with shallow machine learning algorithms yields more stable and dependable results compared to complex machine learning approaches.
ER  - 

TY  - CHAP
TI  - 3 Explainable Artificial Intelligence (XAI) in the Veterinary and Animal Sciences Field
T2  - Explainable Artificial Intelligence for Biomedical Applications
SP  - 33
EP  - 56
AU  - Islam Aqib Amjad
AU  - Fatima Mahreen
AU  - Muneer Afshan
AU  - Atta Khazeena
AU  - Arslan Muhammad
AU  - Fatima Zaheer C-Neen
AU  - Muneer Sadia
AU  - Murtaza Maheen
PY  - 2023
DO  - 
PB  - River Publishers
SN  - 9788770228848
UR  - http://ieeexplore.ieee.org/document/10158447
AB  - Since its first appearance, artificial intelligence has been ensuring revolutionary outcomes in the context of real-world problems. At this point, it has strong relations with biomedical and today&#x2019;s intelligent systems compete with human capabilities in medical tasks. However, advanced use of artificial intelligence causes intelligent systems to be black-box. That situation is not good for building trustworthy intelligent systems in medical applications. For a remarkable amount of time, researchers have tried to solve the black-box issue by using modular additions, which have led to the rise of the term: interpretable artificial intelligence. As the literature matured (as a result of, in particular, deep learning), that term transformed into explainable artificial intelligence (XAI). This book provides an essential edited work regarding the latest advancements in explainable artificial intelligence (XAI) for biomedical applications. It includes not only introductive perspectives but also applied touches and discussions regarding critical problems as well as future insights. Topics discussed in the book include: &#x2022; XAI for the applications with medical images &#x2022; XAI use cases for alternative medical data/task &#x2022; Different XAI methods for biomedical applications &#x2022; Reviews for the XAI research for critical biomedical problems. Explainable Artificial Intelligence for Biomedical Applications is ideal for academicians, researchers, students, engineers, and experts from the fields of computer science, biomedical, medical, and health sciences. It also welcomes all readers of different fields to be informed about use cases of XAI in black-box artificial intelligence. In this sense, the book can be used for both teaching and reference source purposes.
ER  - 

TY  - CONF
TI  - An XAI Approach to Predictive Analytics of Pancreatic Cancer
T2  - 2023 International Conference on Information Technology (ICIT)
SP  - 343
EP  - 348
AU  - S. B
AU  - M. S. Bhargavi
PY  - 2023
KW  - Deep learning
KW  - Uncertainty
KW  - Computed tomography
KW  - Pancreatic cancer
KW  - Transforms
KW  - Predictive models
KW  - Prediction algorithms
KW  - Explainable Artificial Intelligence
KW  - Shapley Additive Explanations (SHAP)
KW  - Local Interpretable Model-agnostic Explanations (LIME)
KW  - Deep Learning
KW  - Pancreatic Cancer
DO  - 10.1109/ICIT58056.2023.10225991
JO  - 2023 International Conference on Information Technology (ICIT)
IS  - 
SN  - 2831-3399
VO  - 
VL  - 
JA  - 2023 International Conference on Information Technology (ICIT)
Y1  - 9-10 Aug. 2023
AB  - Despite intensive research, survival rate for pancreatic cancer, a fatal and incurable illness, has not dramatically improved in recent years. Deep learning systems have shown superhuman ability in a considerable number of activities, and recent developments in Artificial Intelligence (AI) have led to its widespread use in predictive analytics of pancreatic cancer. However, the improvement in performance is the result of model complexity being raised, which transforms these systems into “black box” methods and creates uncertainty about how they function and, ultimately, how they make judgements. This ambiguity has made it difficult for deep learning algorithms to be accepted in important field like healthcare, where their benefit may be enormous. As a result, there has been a significant resurgence in recent years of scholarly interest in the topic of Explainable Artificial Intelligence (XAI), which is concerned with the creation of novel techniques for interpreting and explaining deep learning models. In this study, we utilize Computed Tomography (CT) images and Clinical data to predict and analyze pancreatic cancer and survival rate respectively. Since pancreatic tumors are small to identify, the region marking through XAI will assist medical professionals to identify the appropriate region and determine the presence of cancer. Various features are taken into consideration for survival prediction. The most prominent features can be identified with the help of XAI, which in turn aids medical professionals in making better decisions. This study mainly focuses on the XAI strategy for deep and machine learning models rather than prediction and survival methodology.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence in Clinical Decision Support Systems
T2  - 2023 IV International Conference on Neural Networks and Neurotechnologies (NeuroNT)
SP  - 3
EP  - 6
AU  - E. N. Volkov
AU  - A. N. Averkin
PY  - 2023
KW  - Decision support systems
KW  - Medical services
KW  - Artificial neural networks
KW  - Standards
KW  - Neurotechnology
KW  - Medical diagnostic imaging
KW  - Biological neural networks
KW  - artificial intelligence
KW  - explainable artificial intelligence
KW  - XAI
KW  - clinical systems
KW  - decision support systems
KW  - CDSS
KW  - healthcare
KW  - medicine
KW  - personalized medicine
KW  - expert systems
DO  - 10.1109/NeuroNT58640.2023.10175852
JO  - 2023 IV International Conference on Neural Networks and Neurotechnologies (NeuroNT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IV International Conference on Neural Networks and Neurotechnologies (NeuroNT)
Y1  - 16-16 June 2023
AB  - The development of modern medicine and the gradual transition to the Healthcare 5.0 concept implies an increased role of automated clinical decision support systems (CDSS) in the processes of diagnosis and treatment prescription. However, despite the prospects of using artificial neural networks in CDSS, there is a problem of “non-transparency” - the inability to understand the reasons for the results, which is unacceptable for systems of this kind. The use of explainable artificial intelligence in CDSS increases the level of confidence in the technology. In the study the analysis of the current state of use of explanatory artificial intelligence in CDSS.
ER  - 

TY  - CONF
TI  - Navigating Healthcare Insights: A Bird’s Eye View of Explainability with Knowledge Graphs
T2  - 2023 IEEE Sixth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)
SP  - 54
EP  - 61
AU  - S. Garg
AU  - S. Parikh
AU  - S. Garg
PY  - 2023
KW  - Drugs
KW  - Shape
KW  - Biological system modeling
KW  - Decision making
KW  - Medical services
KW  - Knowledge graphs
KW  - Cognition
KW  - AI Healthcare
KW  - Knowledge Graphs
KW  - eXplainable AI (XAI)
KW  - Interpretability
KW  - Knowledge-infused learning (K-iL)
KW  - Drug Discovery
DO  - 10.1109/AIKE59827.2023.00016
JO  - 2023 IEEE Sixth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)
IS  - 
SN  - 2831-7203
VO  - 
VL  - 
JA  - 2023 IEEE Sixth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)
Y1  - 25-27 Sept. 2023
AB  - Knowledge graphs (KGs) are gaining prominence in Healthcare AI, especially in drug discovery and pharmaceutical research as they provide a structured way to integrate diverse information sources, enhancing AI system interpretability. This interpretability is crucial in healthcare, where trust and transparency matter, and eXplainable AI (XAI) supports decision-making for healthcare professionals. This overview summarizes recent literature on the impact of KGs in healthcare and their role in developing explainable AI models. We cover KG workflow, including construction, relationship extraction, reasoning, and their applications in areas like Drug-Drug Interactions (DDI), Drug Target Interactions (DTI), Drug Development (DD), Adverse Drug Reactions (ADR), and bioinformatics. We emphasize the importance of making KGs more interpretable through knowledge-infused learning in healthcare. Finally, we highlight research challenges and provide insights for future directions.
ER  - 

TY  - JOUR
TI  - A Review of Trustworthy and Explainable Artificial Intelligence (XAI)
T2  - IEEE Access
SP  - 78994
EP  - 79015
AU  - V. Chamola
AU  - V. Hassija
AU  - A. R. Sulthana
AU  - D. Ghosh
AU  - D. Dhingra
AU  - B. Sikdar
PY  - 2023
KW  - Artificial intelligence
KW  - Art
KW  - Surveys
KW  - Robustness
KW  - Medical services
KW  - General Data Protection Regulation
KW  - Trusted computing
KW  - Autonomous vehicles
KW  - Medical services
KW  - Internet of Things
KW  - Network security
KW  - Computer security
KW  - Risk management
KW  - Artificial intelligence (AI)
KW  - trustworthy AI (TAI)
KW  - eXplainable AI (XAI)
KW  - autonomous vehicles
KW  - healthcare
KW  - IoT
DO  - 10.1109/ACCESS.2023.3294569
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - The advancement of Artificial Intelligence (AI) technology has accelerated the development of several systems that are elicited from it. This boom has made the systems vulnerable to security attacks and allows considerable bias in order to handle errors in the system. This puts humans at risk and leaves machines, robots, and data defenseless. Trustworthy AI (TAI) guarantees human value and the environment. In this paper, we present a comprehensive review of the state-of-the-art on how to build a Trustworthy and eXplainable AI, taking into account that AI is a black box with little insight into its underlying structure. The paper also discusses various TAI components, their corresponding bias, and inclinations that make the system unreliable. The study also discusses the necessity for TAI in many verticals, including banking, healthcare, autonomous system, and IoT. We unite the ways of building trust in all fragmented areas of data protection, pricing, expense, reliability, assurance, and decision-making processes utilizing TAI in several diverse industries and to differing degrees. It also emphasizes the importance of transparent and post hoc explanation models in the construction of an eXplainable AI and lists the potential drawbacks and pitfalls of building eXplainable AI. Finally, the policies for developing TAI in the autonomous vehicle construction sectors are thoroughly examined and eclectic ways of building a reliable, interpretable, eXplainable, and Trustworthy AI systems are explained to guarantee safe autonomous vehicle systems.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence in Medical Image Analysis: State of the Art and Prospects
T2  - 2023 XXVI International Conference on Soft Computing and Measurements (SCM)
SP  - 134
EP  - 137
AU  - E. N. Volkov
AU  - A. N. Averkin
PY  - 2023
KW  - Image analysis
KW  - Current measurement
KW  - Artificial neural networks
KW  - Artificial intelligence
KW  - Biomedical imaging
KW  - artificial intelligence
KW  - explainable artificial intelligence
KW  - explainability
KW  - neural networks
KW  - artificial neural networks
KW  - digital health
KW  - personalized medicine
KW  - medicine
KW  - healthcare
KW  - medical images
DO  - 10.1109/SCM58628.2023.10159033
JO  - 2023 XXVI International Conference on Soft Computing and Measurements (SCM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 XXVI International Conference on Soft Computing and Measurements (SCM)
Y1  - 24-26 May 2023
AB  - to increase transparency in the work of artificial intelligence systems in the analysis of medical images is called to use methods of explainable artificial intelligence. Our study provides an overview of the current state of application of explainable artificial intelligence methods for medical image analysis, and also considers potentially promising approaches to improving the technology.
ER  - 

TY  - JOUR
TI  - Artificial Intelligence and Biosensors in Healthcare and Its Clinical Relevance: A Review
T2  - IEEE Access
SP  - 61600
EP  - 61620
AU  - R. Qureshi
AU  - M. Irfan
AU  - H. Ali
AU  - A. Khan
AU  - A. S. Nittala
AU  - S. Ali
AU  - A. Shah
AU  - T. M. Gondal
AU  - F. Sadak
AU  - Z. Shah
AU  - M. U. Hadi
AU  - S. Khan
AU  - Q. Al-Tashi
AU  - J. Wu
AU  - A. Bermak
AU  - T. Alam
PY  - 2023
KW  - Medical services
KW  - Machine learning
KW  - Biological system modeling
KW  - Predictive models
KW  - Biosensors
KW  - Medical diagnostic imaging
KW  - Data models
KW  - Artificial intelligence
KW  - explainable AI
KW  - medical imaging
KW  - domain adaptation
KW  - biosensors
KW  - federated learning
KW  - big data analytics
KW  - large language models
DO  - 10.1109/ACCESS.2023.3285596
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - Data generated from sources such as wearable sensors, medical imaging, personal health records, and public health organizations have resulted in a massive information increase in the medical sciences over the last decade. Advances in computational hardware, such as cloud computing, graphical processing units (GPUs), Field-programmable gate arrays (FPGAs) and tensor processing units (TPUs), provide the means to utilize these data. Consequently, an array of sophisticated Artificial Intelligence (AI) techniques have been devised to extract valuable insights from the extensive datasets in the healthcare industry. Here, we present an overview of recent progress in AI and biosensors in medical and life sciences. We discuss the role of machine learning in medical imaging, precision medicine, and biosensors for the Internet of Things (IoT). We review the latest advancements in wearable biosensing technologies. These innovative solutions employ AI to assist in monitoring of bodily electro-physiological and electro-chemical signals, as well as in disease diagnosis. These advancements exemplify the trend towards personalized medicine, delivering highly effective, cost-efficient, and precise point-of-care treatment.Furthermore, an overview of the advances in computing technologies, such as accelerated AI, edge computing, and federated learning for medical data, are also documented. Finally, we investigate challenges in data-driven AI approaches, the potential issues generated by biosensors and IoT-based healthcare, and the distribution shifts that occur among different data modalities, concluding with an overview of future prospects.
ER  - 

TY  - CONF
TI  - A Case Study of Privacy Protection Challenges and Risks in AI-Enabled Healthcare App
T2  - 2023 IEEE Conference on Artificial Intelligence (CAI)
SP  - 296
EP  - 297
AU  - P. Wang
AU  - H. Zare
PY  - 2023
KW  - Privacy
KW  - Ethics
KW  - Data privacy
KW  - Medical services
KW  - Artificial intelligence
KW  - Guidelines
KW  - AI
KW  - healthcare
KW  - ethics
KW  - privacy
KW  - security
KW  - risks
DO  - 10.1109/CAI54212.2023.00132
JO  - 2023 IEEE Conference on Artificial Intelligence (CAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE Conference on Artificial Intelligence (CAI)
Y1  - 5-6 June 2023
AB  - Artificial intelligence (AI) is increasingly used in healthcare systems and applications (apps) with questions and debates on ethical issues and privacy risks. This research study explores and discusses the ethical challenges, privacy risks, and possible solutions related to protecting user data privacy in AI-enabled healthcare apps. The study is based on the healthcare app named Charlie in one of the fictional case studies designed by Princeton University to elucidate critical thinking and discussions on emerging ethical issues embracing AI.
ER  - 

TY  - CONF
TI  - An Explainable AI-Enabled Framework for the Diabetes Classification
T2  - 2023 IEEE International Conference on Machine Learning and Applied Network Technologies (ICMLANT)
SP  - 1
EP  - 6
AU  - C. K. Long
AU  - V. Puri
AU  - V. K. Solanki
AU  - G. Jeanette Rincón Aponte
PY  - 2023
KW  - Machine learning algorithms
KW  - Electric breakdown
KW  - Decision making
KW  - Visual impairment
KW  - Medical treatment
KW  - Machine learning
KW  - Real-time systems
KW  - XAI
KW  - Diabetes
KW  - Artificial Intelligence
KW  - Black Box
KW  - Transparency
KW  - Trust
DO  - 10.1109/ICMLANT59547.2023.10372975
JO  - 2023 IEEE International Conference on Machine Learning and Applied Network Technologies (ICMLANT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Machine Learning and Applied Network Technologies (ICMLANT)
Y1  - 14-15 Dec. 2023
AB  - Nowadays, Diabetes has become a prominent health issue that spans several categories determined by characteristics such as clinical presentation, disease progression, and pathophysiology. A real-time monitoring and management system is necessary to mitigate this life-threatening disease and safeguard individuals. The potential of Artificial Intelligence (AI) to change the management of Diabetes and enhance outcomes for persons with the condition is substantial. While AI shows promise in its ability to forecast Diabetes, it also presents a challenge due to its black-box nature. To address this concern, a proposed framework incorporating explainable AI is suggested for classifying Diabetes. This framework serves to elucidate the decisions made by the AI model. Furthermore, it was assessed using three distinct datasets categorized according to Diabetes categorization. Understanding the decision-making process of AI in the medical field fosters trust between healthcare providers and patients.
ER  - 

TY  - CONF
TI  - Artificial Intelligence in the Judiciary System of Saudi Arabia: A Literature Review
T2  - 2023 International Conference On Cyber Management And Engineering (CyMaEn)
SP  - 83
EP  - 87
AU  - A. I. Al-Alawi
AU  - A. M. A-Lmansouri
PY  - 2023
KW  - Text mining
KW  - Bibliographies
KW  - Government
KW  - Focusing
KW  - Medical services
KW  - Machine learning
KW  - Learning (artificial intelligence)
KW  - Artificial Intelligence (AI)
KW  - Machine Learning (ML)
KW  - Judicial System
KW  - Minimal human interaction
DO  - 10.1109/CyMaEn57228.2023.10050929
JO  - 2023 International Conference On Cyber Management And Engineering (CyMaEn)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference On Cyber Management And Engineering (CyMaEn)
Y1  - 26-27 Jan. 2023
AB  - Artificial intelligence (AI) is one of the prominent technological developments in the 21st century. It has gained application in diverse fields, including healthcare, agriculture, and manufacturing. Like other global countries, Saudi Arabia has accelerated its efforts to develop AI. This study aims to investigate AI techniques’ influence on the Saudi judiciary system and determine possible future developments. The study was a literature review of 10 different international and Saudi Arabia publications on Artificial Intelligence in the Judiciary System using related titles, terms, and abstracts. The findings revealed that AI techniques are promising developments in the judiciary. Research evidence supports that augmenting human expertise with appropriate AI techniques like machine learning, text mining, NLP, and machine interpretation can improve efficiency, effectiveness, accuracy, transparency, and fairness in judicial processes. However, the studies focusing on AI applications in the Saudi judicial system are also limited. Despite the limited research, with inferences from the findings from studies in other contexts. AI is a promising development that can improve the Saudi judiciary system. Although these findings provide insight, they are inferential and not an accurate explanation of the influence of AI on the Saudi judicial system. Such gaps call for future studies to determine the influence of AI on the judicial system within the Saudi context.
ER  - 

TY  - CONF
TI  - Explicable AI for surveillance and interpretation of Coronavirus using X-ray imaging
T2  - 2023 International Conference on Emerging Smart Computing and Informatics (ESCI)
SP  - 1
EP  - 6
AU  - T. Chauhan
AU  - S. Sonawane
PY  - 2023
KW  - COVID-19
KW  - Surveillance
KW  - Taxonomy
KW  - Pipelines
KW  - Closed box
KW  - Medical services
KW  - Reliability
KW  - Explainable AI
KW  - Artificial intelligence
KW  - Black box systems
KW  - Transparent systems
KW  - Medical diagnosis
KW  - Chest X-ray images
DO  - 10.1109/ESCI56872.2023.10099633
JO  - 2023 International Conference on Emerging Smart Computing and Informatics (ESCI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Emerging Smart Computing and Informatics (ESCI)
Y1  - 1-3 March 2023
AB  - Explainable AI (XAI) is one of the disciplines being investigated, with the goal of improving the transparency of black-box systems. XAI is such a technology that could assist to alleviate the black-box system by providing new ways of understanding the core thinking process of AI systems. Conside ring the healthcare domain, doctors are still not able to explain why certain decisions or forecasts had been predicted by a particular system. As a result, it imposes limitations on how and where AI technology can be implemented. And to address this problem, a taxonomy of model interpretability is framed for conceptualizing the explainability. Also, an approach with the baseline system is created which could firstly differentiate in the Covid-19 positive and Covid-19 negative chest X-ray images and an automated explainable pipeline is designed using XAI technique. This technique shows that the model is interpretable, that is the achieved results are easy to understand and can encourage medicians and patients with transparent and reliable medical journey. This article aims to help people comprehend the necessity for Explainable AI, as well as the methodological approaches used in healthcare.
ER  - 

TY  - CONF
TI  - Developing an Explainable AI Model for Predicting Patient Readmissions in Hospitals
T2  - 2023 2nd International Conference on Edge Computing and Applications (ICECAA)
SP  - 587
EP  - 592
AU  - P. Chandre
AU  - V. Vanarote
AU  - M. Kuri
AU  - A. Uttarkar
AU  - A. Dhore
AU  - S. Pathan
PY  - 2023
KW  - Industries
KW  - Visualization
KW  - Costs
KW  - Hospitals
KW  - Computational modeling
KW  - Decision making
KW  - Predictive models
KW  - readmission
KW  - machine learning
KW  - healthcare
DO  - 10.1109/ICECAA58104.2023.10212152
JO  - 2023 2nd International Conference on Edge Computing and Applications (ICECAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 2nd International Conference on Edge Computing and Applications (ICECAA)
Y1  - 19-21 July 2023
AB  - The objective of this study is to develop an AI model that can correctly identify which patients are most likely to require hospital readmission within a predetermined window of time after being discharged. Given that readmissions are linked to higher healthcare costs and poorer patient outcomes; this is a crucial problem in healthcare. The model must, nonetheless, also be explicable, which means that healthcare professionals must be able to comprehend the rationale behind why it made certain predictions. This is essential for establishing the model's credibility and making sure it is being used properly. To do this, the study may employ a range of machine learning methods renowned for their interpretability, like decision trees or random forests. Additionally, the study could investigate how to generate feature importance plots or partial dependence plots to visualize the model's decision-making process. Overall, by enhancing patient outcomes and fostering openness and confidence in the use of AI, this research subject has the potential to have a significant impact on healthcare.
ER  - 

TY  - CONF
TI  - Optimizing Healthcare Operations With Big Data and AI
T2  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
SP  - 1
EP  - 9
AU  - K. Dubey
AU  - A. Esnaashariyeh
AU  - K. Nahak
AU  - D. N. Hodade
AU  - K. Shrivastava
AU  - G. Jorvekar
PY  - 2023
KW  - Training
KW  - Technological innovation
KW  - Protocols
KW  - Medical services
KW  - Big Data
KW  - Reliability engineering
KW  - Artificial intelligence
KW  - Predictive analytics
KW  - Standards
KW  - Biomedical imaging
KW  - Big Data
KW  - Artificial Intelligence
KW  - Healthcare Operations
KW  - Clinical Decision Support
KW  - Ethical Considerations
DO  - 10.1109/ICAIIHI57871.2023.10489119
JO  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
Y1  - 29-30 Dec. 2023
AB  - To improve patient care and operational effectiveness, this study investigates the implementation of big data along with artificial intelligence (AI) into medical operations. Utilizing a deductive technique and an interpretive approach, this study collects secondary data from reliable sources using a descriptive design. While AI applications concentrate on clinical decision support, tailored treatment strategies, and predictive analytics, Big Data integration includes health records, medical imaging, genome-wide data, and patient-generated information. Private data, interconnection, biases in AI systems, and resource limitations are some of the difficulties. Transparency, justice, and patient autonomy are all important ethical considerations. Strong data security protocols, continuous attempts to mitigate prejudice, interdisciplinary training, and open governance frameworks are among the suggestions offered. Future research should focus on interoperability standards
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence for Patient Safety: A Review of Application in Pharmacovigilance
T2  - IEEE Access
SP  - 50830
EP  - 50840
AU  - S. Lee
AU  - S. Kim
AU  - J. Lee
AU  - J. -Y. Kim
AU  - M. -H. Song
AU  - S. Lee
PY  - 2023
KW  - Artificial intelligence
KW  - Drugs
KW  - Predictive models
KW  - Data models
KW  - Safety
KW  - Machine learning
KW  - Databases
KW  - Machine learning
KW  - pharmacovigilance
KW  - explainable artificial intelligence
DO  - 10.1109/ACCESS.2023.3271635
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - Explainable AI (XAI) is a methodology that complements the black box of artificial intelligence, and its necessity has recently been highlighted in various fields. The purpose of this research is to identify studies in the field of pharmacovigilance using XAI. Though there have been many previous attempts to select papers, with a total of 781 papers being confirmed, only 25 of them manually met the selection criteria. This study presents an intuitive review of the potential of XAI technologies in the field of pharmacovigilance. In the included studies, clinical data, registry data, and knowledge data were used to investigate drug treatment, side effects, and interaction studies based on tree models, neural network models, and graph models. Finally, key challenges for several research issues for the use of XAI in pharmacovigilance were identified. Although artificial intelligence (AI) is actively used in drug surveillance and patient safety, gathering adverse drug reaction information, extracting drug-drug interactions, and predicting effects, XAI is not normally utilized. Therefore, the potential challenges involved in its use alongside future prospects should be continuously discussed.
ER  - 

TY  - CONF
TI  - AI with Consciousness: Advancing Explainability in the Healthcare Sector
T2  - 2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)
SP  - 323
EP  - 328
AU  - A. Koul
AU  - S. Gupta
AU  - B. Gupta
PY  - 2023
KW  - Ethics
KW  - Explainable AI
KW  - Computational modeling
KW  - Medical services
KW  - Learning (artificial intelligence)
KW  - Brain modeling
KW  - Security
KW  - Explainable AI
KW  - Machine learning
KW  - approaches
KW  - review
KW  - Deep learning
DO  - 10.1109/ICCSAI59793.2023.10421388
JO  - 2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Communication, Security and Artificial Intelligence (ICCSAI)
Y1  - 23-25 Nov. 2023
AB  - With the quick progression of man-made brainpower (computer based intelligence/ AI) in the clinical field, there is a rising requirement for straightforwardness and interpretability of man-made intelligence models to guarantee their viable and capable arrangement. This paper means to investigate the idea of logical simulated intelligence (XAI) with regards to medical services, examining its significance, difficulties, and possible applications. The paper gives strategies and procedures for XAI, uses of XAI in medical services, moral contemplations, and future bearings
ER  - 

TY  - CONF
TI  - Detecting Vitamin A Deficiency in Schoolchildren Using an Enhanced Explainable Machine Learning Model
T2  - 2023 Advances in Science and Engineering Technology International Conferences (ASET)
SP  - 1
EP  - 5
AU  - D. A. Abuhani
AU  - J. Khan
AU  - H. Sulieman
PY  - 2023
KW  - Industries
KW  - Deep learning
KW  - Pediatrics
KW  - Electric potential
KW  - Sensitivity
KW  - Visual impairment
KW  - Medical services
KW  - Machine Learning
KW  - Target Encoding
KW  - Vitamin A
KW  - Ophthalmology
KW  - Electronic Health Records
KW  - XAI
DO  - 10.1109/ASET56582.2023.10180556
JO  - 2023 Advances in Science and Engineering Technology International Conferences (ASET)
IS  - 
SN  - 2831-6878
VO  - 
VL  - 
JA  - 2023 Advances in Science and Engineering Technology International Conferences (ASET)
Y1  - 20-23 Feb. 2023
AB  - The most prevalent avoidable cause of vision impairments in children worldwide is vitamin A deficiency. In most cases, deficiencies can be detected through blood tests. However, blood tests are less accessible and of high cost in underdeveloped countries in Africa and Southeast Asia which hinders the efforts of detecting Vitamin A deficiency soon enough to prevent further complications. With the development of machine learning and deep learning in risk-averse industries like healthcare and the expansion of electronic health records, there is a potential to use these techniques in order to arrange for a more accessible substitute to blood tests. In this study, a variety of machine learning techniques are applied to a sparse dataset of ocular symptoms and diagnoses that was obtained from Maradi, Niger, during routine eye exams carried out in a school environment. The goal is to provide an affordable, accessible, and effective clinical screening system for Vitamin A deficiency in children using solely existing health records. The LGB model achieved the best accuracy: 84.4% with a sensitivity of 81.9%, and a specificity of 84.7 which outperformers results on the same dataset recently published [1] by almost 10% in terms of accuracy, specificity, and F1-score.
ER  - 

TY  - CONF
TI  - Explainable AI for Breast Cancer Detection: A LIME-Driven Approach
T2  - 2023 16th International Conference on Developments in eSystems Engineering (DeSE)
SP  - 540
EP  - 545
AU  - T. Khater
AU  - A. Hussain
AU  - S. Mahmoud
AU  - S. Yasen
PY  - 2023
KW  - Training
KW  - Industries
KW  - Adhesives
KW  - Reviews
KW  - Decision making
KW  - Predictive models
KW  - Reliability engineering
KW  - artificial Intelligence
KW  - breast cancer
KW  - interpretability
KW  - explainable machine learning
KW  - and model-agnostic methods
DO  - 10.1109/DeSE60595.2023.10469341
JO  - 2023 16th International Conference on Developments in eSystems Engineering (DeSE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 16th International Conference on Developments in eSystems Engineering (DeSE)
Y1  - 18-20 Dec. 2023
AB  - Artificial Intelligence is transforming the healthcare industry due to the increasing accessibility of organized and unorganized information and the rapid development of analytical techniques. As artificial intelligence becomes more significant in healthcare, concerns are arising regarding the lack of transparency, explainability, and the possibility of bias in model predictions. The goal of this paper is to utilize interpretable machine learning to provide a better understanding of breast cancer using the Local Interpretable Model-agnostic Method (LIME). This study uses Local Interpretable Model-Agnostic Explanations to explain how the machine-learning model accurately classifies breast cancer cases as either Benign or Malignant. It provides a LIME plot for Benign cases, highlighting the significant role of "Bare Nuclei" where lower values strongly suggest Benign predictions. Other features like Normal Nucleoli, Marginal adhesion, single adhesion, Mitoses, and uniformity of cell size also contribute to the Benign class prediction when they fall below a particular threshold. The study also presents a LIME plot for Malignant cases, emphasizing the importance of "Bare Nuclei" and Clump thickness, where higher values indicate a higher likelihood of Malignant predictions. Other features like Normal Nucleoli, Marginal adhesion, Bland chromatin, uniformity of cell size, and Mitoses also contribute to Malignant predictions when their values exceed specific thresholds. The feature "Concave points_worst" influences the model’s benign predictions when it is below 0.07, while the "texture" feature affects predictions when it exceeds 29.41. Furthermore, the study provides further explanations for Malignant predictions based on "Concave points_worst" and "Texture." These findings provide valuable insights into the decision-making process of the model, making it more interpretable and useful for breast cancer diagnosis.
ER  - 

TY  - CONF
TI  - Applications of Artificial Intelligence Methods in Medicine
T2  - 2023 International Conference on Information Technology (ICIT)
SP  - 349
EP  - 354
AU  - M. Ivanovic
AU  - D. Macos
PY  - 2023
KW  - Decision support systems
KW  - Metaverse
KW  - Precision medicine
KW  - Sociology
KW  - Medical services
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Artificial intelligence in medicine
KW  - Agent technology
KW  - Personalization and recommendations in medicine
DO  - 10.1109/ICIT58056.2023.10226107
JO  - 2023 International Conference on Information Technology (ICIT)
IS  - 
SN  - 2831-3399
VO  - 
VL  - 
JA  - 2023 International Conference on Information Technology (ICIT)
Y1  - 9-10 Aug. 2023
AB  - In contemporary society more people suffer from chronic diseases. They are the leading cause of death worldwide. On the other hand, the world population is getting older and the need for health care is of high priority. World prominent health stakeholders have recognized the importance of development of sophisticated medical services. For development of wide variety of intelligent decision support systems collection of enormous amounts of patient's complex data is needed. Data should be properly aggregated, analyzed and acquired knowledge should be presented to the doctors/caregivers to support them in recommending adequate treatment for achieving patient's wellbeing. To achieve this in modern approaches the focus is on personalized medicine and healthcare. Additionally in such systems it is helpful inclusion of e-coaching facilities supported by agents and metaverse technologies.
ER  - 

TY  - CONF
TI  - Big Data Analytics Cloud based Smart IoT Healthcare Network
T2  - 2023 7th International Conference on Trends in Electronics and Informatics (ICOEI)
SP  - 437
EP  - 443
AU  - K. Mehta
AU  - S. Gaur
AU  - S. Maheshwari
AU  - H. Chugh
AU  - M. a. Kumar
PY  - 2023
KW  - Cloud computing
KW  - Costs
KW  - Telemedicine
KW  - Medical services
KW  - Big Data
KW  - Data science
KW  - Sensors
KW  - Healthcare
KW  - Big Data
KW  - IoT
KW  - Cloud
KW  - Analytics
KW  - Artificial Intelligence
DO  - 10.1109/ICOEI56765.2023.10125936
JO  - 2023 7th International Conference on Trends in Electronics and Informatics (ICOEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 7th International Conference on Trends in Electronics and Informatics (ICOEI)
Y1  - 11-13 April 2023
AB  - A smart healthcare network combines the advantages of sensing devices, the Internet of Things (IoT), and big data analytics to give better patient comfort and lower healthcare costs. The healthcare sector is currently faced with enormous difficulties in storing and processing the data that is generated in order to derive information from it. It is vital to create new techniques and strategies for processing the growing amount of health sector data produced by IoT devices, e-health, m-health, and telemedicine airing. The physical intricacy of medicinal data, the demand for remedial competence as well as the protection of individual medicinal data distinguish research of data mining in health sector from various data studies. This research addresses these difficulties by presenting a contemporary technique for IoT based big data execution in healthcare. a large data processing and IoT health stage platform on the cloud for healthcare system has been introduced that lowers medical data management expenses while improving safety. Remote monitoring, the use of clever algorithms, tools, and processes, quicker analysis, and the engagement of experts for better treatment references, this learning aims to deliver health sector facilities to both the ill and the healthy community. With the integration of technologies, healthcare service delivery has evolved significantly. This study suggests a brand-new big data smart healthcare framework for remotely monitoring people's everyday physical activity, both healthy and sick.
ER  - 

TY  - CONF
TI  - Bleak Medical Prognosis of Covid Patients Using Explainable Machine Learning
T2  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
SP  - 1
EP  - 5
AU  - R. Sharma
AU  - H. Pandey
AU  - A. K. Agarwal
AU  - D. Srivastava
PY  - 2023
KW  - COVID-19
KW  - Machine learning algorithms
KW  - Machine learning
KW  - Predictive models
KW  - Writing
KW  - Prediction algorithms
KW  - Risk analysis
KW  - Explainable systems
KW  - SHAP value interpretation
KW  - Machine learning
KW  - Covid-19
KW  - XAI
KW  - XGBoost
DO  - 10.1109/ICACTA58201.2023.10393824
JO  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
Y1  - 6-7 Oct. 2023
AB  - Today, artificial intelligence (AI) algorithms are used to generate predictions in many high-stakes fields, such as credit risk analysis and medical diagnosis. Therefore, AI systems are having a greater impact on people, yet many cutting-edge technologies are opaque, negating people's "right to explanation." To address this issue, experts in the field have created explainable AI to detail the rationale behind an algorithm's prediction. The focus of this article is on creating an explanation system for a predicted outcome and a machine learning prediction model for poor medical prognosis in Covid patients. Many different types of machine learning algorithms, including both foundational and augmented versions, have been implemented. It was shown that the XGboost model provided superior diagnostic performance. This situation of a prediction has been explained using Shapley Additive Explanations (SHAP). The reason the XGBoost machine learning model gives a certain prediction result may be deduced with the use of three statistics charts.
ER  - 

TY  - CHAP
TI  - 10 Explainable Artificial Intelligence Applications in Dentistry: A Theoretical Research
T2  - Explainable Artificial Intelligence for Biomedical Applications
SP  - 189
EP  - 212
AU  - B. Aksoy
AU  - M. Yücel
AU  - H. Sayın
AU  - O. K. M. Salman
AU  - M. Eylence
AU  - M. M. Özmen
PY  - 2023
DO  - 
PB  - River Publishers
SN  - 9788770228848
UR  - http://ieeexplore.ieee.org/document/10158463
AB  - Since its first appearance, artificial intelligence has been ensuring revolutionary outcomes in the context of real-world problems. At this point, it has strong relations with biomedical and today&#x2019;s intelligent systems compete with human capabilities in medical tasks. However, advanced use of artificial intelligence causes intelligent systems to be black-box. That situation is not good for building trustworthy intelligent systems in medical applications. For a remarkable amount of time, researchers have tried to solve the black-box issue by using modular additions, which have led to the rise of the term: interpretable artificial intelligence. As the literature matured (as a result of, in particular, deep learning), that term transformed into explainable artificial intelligence (XAI). This book provides an essential edited work regarding the latest advancements in explainable artificial intelligence (XAI) for biomedical applications. It includes not only introductive perspectives but also applied touches and discussions regarding critical problems as well as future insights. Topics discussed in the book include: &#x2022; XAI for the applications with medical images &#x2022; XAI use cases for alternative medical data/task &#x2022; Different XAI methods for biomedical applications &#x2022; Reviews for the XAI research for critical biomedical problems. Explainable Artificial Intelligence for Biomedical Applications is ideal for academicians, researchers, students, engineers, and experts from the fields of computer science, biomedical, medical, and health sciences. It also welcomes all readers of different fields to be informed about use cases of XAI in black-box artificial intelligence. In this sense, the book can be used for both teaching and reference source purposes.
ER  - 

TY  - CHAP
TI  - 2 LIME Approach in Diagnosing Diseases &#x2013; A Study on Explainable AI
T2  - Explainable Artificial Intelligence for Biomedical Applications
SP  - 17
EP  - 32
AU  - Muralikrishna Iyyanki
AU  - Muralikrishna Iyyanki
AU  - Jayanthi Prisilla
PY  - 2023
DO  - 
PB  - River Publishers
SN  - 9788770228848
UR  - http://ieeexplore.ieee.org/document/10158243
AB  - Since its first appearance, artificial intelligence has been ensuring revolutionary outcomes in the context of real-world problems. At this point, it has strong relations with biomedical and today&#x2019;s intelligent systems compete with human capabilities in medical tasks. However, advanced use of artificial intelligence causes intelligent systems to be black-box. That situation is not good for building trustworthy intelligent systems in medical applications. For a remarkable amount of time, researchers have tried to solve the black-box issue by using modular additions, which have led to the rise of the term: interpretable artificial intelligence. As the literature matured (as a result of, in particular, deep learning), that term transformed into explainable artificial intelligence (XAI). This book provides an essential edited work regarding the latest advancements in explainable artificial intelligence (XAI) for biomedical applications. It includes not only introductive perspectives but also applied touches and discussions regarding critical problems as well as future insights. Topics discussed in the book include: &#x2022; XAI for the applications with medical images &#x2022; XAI use cases for alternative medical data/task &#x2022; Different XAI methods for biomedical applications &#x2022; Reviews for the XAI research for critical biomedical problems. Explainable Artificial Intelligence for Biomedical Applications is ideal for academicians, researchers, students, engineers, and experts from the fields of computer science, biomedical, medical, and health sciences. It also welcomes all readers of different fields to be informed about use cases of XAI in black-box artificial intelligence. In this sense, the book can be used for both teaching and reference source purposes.
ER  - 

TY  - CONF
TI  - Detecting Diabetic Retinopathy using ResNet50 and Explainable AI
T2  - 2023 IEEE International Carnahan Conference on Security Technology (ICCST)
SP  - 1
EP  - 6
AU  - S. Tyagi
AU  - S. Pingulkar
AU  - A. Tiwary
PY  - 2023
KW  - Deep learning
KW  - Diabetic retinopathy
KW  - Explainable AI
KW  - Visual impairment
KW  - Medical services
KW  - Blindness
KW  - Predictive models
KW  - Diabetic Retinopathy
KW  - Deep learning
KW  - SHAP model
KW  - Explainable AI
KW  - Convolutional Neural Network (CNN)
KW  - healthcare
KW  - Vision impairment
DO  - 10.1109/ICCST59048.2023.10474275
JO  - 2023 IEEE International Carnahan Conference on Security Technology (ICCST)
IS  - 
SN  - 2153-0742
VO  - 
VL  - 
JA  - 2023 IEEE International Carnahan Conference on Security Technology (ICCST)
Y1  - 11-15 Oct. 2023
AB  - Diabetic retinopathy, a common complication of diabetes, can lead to vision impairment and blindness if not detected early. Current diagnostic methods, involving manual examination of fundus images, can be time-consuming. This paper proposes an innovative approach that combines deep learning models with Explainable AI techniques, such as the SHAP method, to enhance the interpretability of diabetic retinopathy detection. This combination allows for more accurate identification of the disease stage and provides insights into the model's decision-making process, which can guide treatment and interventions. The proposed approach aims to improve the accuracy, fairness, transparency, and outcomes of diabetic retinopathy detection, fostering trust in AI models and enabling informed decision-making in healthcare settings.
ER  - 

TY  - CONF
TI  - Graph Convolutional Networks For Disease Mapping and Classification in Healthcare
T2  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
SP  - 1
EP  - 7
AU  - R. Kumar
AU  - D. Verma
AU  - J. R. F. Raj
AU  - A. L. N. Rao
AU  - S. L. Chari
AU  - A. K. Khan
PY  - 2023
KW  - Industries
KW  - Technological innovation
KW  - Ethics
KW  - Medical treatment
KW  - Machine learning
KW  - Learning (artificial intelligence)
KW  - Planning
KW  - Graph Convolutional Networks
KW  - healthcare analytics
KW  - disease mapping
KW  - interpretability
KW  - machine learning
DO  - 10.1109/ICAIIHI57871.2023.10489220
JO  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
Y1  - 29-30 Dec. 2023
AB  - In the context of healthcare, this study investigates the use of Graph A convolutional Networks (GCNs) for disease mapping along with classification. Based on an interpretivist philosophical thought, a descriptive design alongside secondary data collection is used in a deductive manner. The research creates a strong framework for sickness mapping, assesses how well GCNs adapt to varied health information, and compares their effectiveness with more conventional machine learning techniques in order to determine how suitable they are. An investigation is conducted into the understanding of GCN-based diagnosis models, offering valuable perspectives into their decision-making procedures. The findings support improved diagnostic precision, wellinformed treatment planning, along with precision in medical treatments. The emphasis when applying research results to medical procedures is on connection into systems that provide decision support, and ongoing improvement. The importance of model interpretability, the ability to be general as well realworld integration is highlighted by a critical analysis. Developing interpretability strategies and addressing ethical issues are among the recommendations. In order to ensure responsible deployment, future work ought to concentrate on improving GCN architectures, integrating multi-modal information and advocating interdisciplinary collaboration.
ER  - 

TY  - CONF
TI  - A Novel Metric for XAI Evaluation Incorporating Pixel Analysis and Distance Measurement
T2  - 2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)
SP  - 1
EP  - 9
AU  - J. Stodt
AU  - C. Reich
AU  - N. Clarke
PY  - 2023
KW  - Measurement
KW  - Bridges
KW  - Decision making
KW  - Medical services
KW  - Distance measurement
KW  - Reliability
KW  - Artificial intelligence
KW  - XAI
KW  - XAI evaluation
KW  - evaluation metric
KW  - explainability
KW  - understandability
DO  - 10.1109/ICTAI59109.2023.00009
JO  - 2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)
IS  - 
SN  - 2375-0197
VO  - 
VL  - 
JA  - 2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)
Y1  - 6-8 Nov. 2023
AB  - Explainable Artificial Intelligence (XAI) seeks to enhance transparency and trust in AI systems. Evaluating the quality of XAI explanation methods remains challenging due to limitations in existing metrics. To address these issues, we propose a novel metric called Explanation Significance Assessment (ESA) and its extension, the Weighted Explanation Significance Assessment (WESA). These metrics offer a comprehensive evaluation of XAI explanations, considering spatial precision, focus overlap, and relevance accuracy. In this paper, we demonstrate the applicability of ESA and WESA on medical data. These metrics quantify the understandability and reliability of XAI explanations, assisting practitioners in interpreting AI-based decisions and promoting informed choices in critical domains like healthcare. Moreover, ESA and WESA can play a crucial role in AI certification, ensuring both accuracy and explainability. By evaluating the performance of XAI methods and underlying AI models, these metrics contribute to trustworthy AI systems. Incorporating ESA and WESA in AI certification efforts advances the field of XAI and bridges the gap between accuracy and interpretability. In summary, ESA and WESA provide comprehensive metrics to evaluate XAI explanations, benefiting research, critical domains, and AI certification, thereby enabling trustworthy and interpretable AI systems.
ER  - 

TY  - CONF
TI  - In-Hospital Mortality Prognosis: Unmasking Patterns using Data Science and Explainable AI
T2  - 2023 9th International Conference on Signal Processing and Communication (ICSC)
SP  - 356
EP  - 361
AU  - B. U. Maheswari
AU  - A. F
AU  - A. George
AU  - A. Jose
PY  - 2023
KW  - Support vector machines
KW  - Explainable AI
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Physiology
KW  - Classification tree analysis
KW  - Machine Learning
KW  - In-hospital Mortality
KW  - interpretability
KW  - Explainable AI
KW  - SHAP
DO  - 10.1109/ICSC60394.2023.10441356
JO  - 2023 9th International Conference on Signal Processing and Communication (ICSC)
IS  - 
SN  - 2643-444X
VO  - 
VL  - 
JA  - 2023 9th International Conference on Signal Processing and Communication (ICSC)
Y1  - 21-23 Dec. 2023
AB  - In-hospital mortality prediction is a crucial part of healthcare that can significantly impact patient care strategies and clinical decision-making. Employing Machine Learning (ML) methods in the healthcare domain to improve clinical decision-making and create more effective patient care plans, potentially saves lives. One example is the development of a trustworthy model for forecasting in-hospital mortality. In this work, we make use of a diverse set of patient attributes (features), such as demographics, medical history, and physiological metrics, in order to reveal hidden patterns in the patient data. Our goal is to suggest the best machine learning model that can effectively predict in-hospital mortality, eventually improving patient care outcomes. After investigating 7 ML models, namely; Decision Tree (DT), Random Forest Classifier (RF), Linear Regression (LR), Logistic Regression (LogR), Support Vector Machine (SVM), k-NN Classifier and Naïve-Bayes Classifier (NB Classifier), we have compared and concluded that the RF model outperforms the other models, providing us with the best accuracy of 92.662%. In the health care domain, it is very much required to explain why and how a particular ML model predicts the specific outcome, to the healthcare professionals. Hence in this work, Explainable AI (XAI) is used to better interpret the RF Classifier, by generating SHAP (SHapley Additive exPlanations) plots.
ER  - 

TY  - CONF
TI  - Permanent Neonatal Diabetes Mellitus Detection Using Machine Learning
T2  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
SP  - 1
EP  - 6
AU  - M. Garg
AU  - G. Kaur
PY  - 2023
KW  - Support vector machines
KW  - Pediatrics
KW  - Technological innovation
KW  - Logistic regression
KW  - Biological system modeling
KW  - Medical services
KW  - Genetics
KW  - Permanent Neonatal Diabetes Mellitus (PNDM)
KW  - Genetic Mutations
KW  - Clinical Parameters
KW  - Interpretability
KW  - Rare Genetic Disorders
DO  - 10.1109/ICAIIHI57871.2023.10489241
JO  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
Y1  - 29-30 Dec. 2023
AB  - This study aids in the early diagnosis and treatment of Permanent Neonatal Diabetes Mellitus (PNDM), enhancing the quality of life for those affected. In our study, we employed a variety of machine learning techniques, including “Logistic Regression,” “Random Forest,” and “Support Vector Machine,” to identify cases of PNDM from non-PNDM instances based on clinical signs and biomarkers. Our findings demonstrate that our proposed approach can reliably and specifically identify PNDM patients with the added benefit of using LIME (Local Interpretable Model-Agnostic Explanations) for interpretability, providing insights into the most influential diagnostic factors. Notably, the Random Forest model achieved an exceptional accuracy of 99.99%. This breakthrough underscores the broader applicability of machine learning in pediatric healthcare, offering a valuable tool for healthcare professionals and researchers for neonatal diabetes mellitus. Our approach can also be extended to other uncommon genetic diseases, enhancing early diagnosis and patient outcomes while expanding our understanding of these rare conditions.
ER  - 

TY  - CONF
TI  - An Interpretable Deep Learning Approach for Skin Cancer Categorization
T2  - 2023 26th International Conference on Computer and Information Technology (ICCIT)
SP  - 1
EP  - 6
AU  - F. Mahmud
AU  - M. M. Mahfiz
AU  - M. Z. I. Kabir
AU  - Y. Abdullah
PY  - 2023
KW  - Deep learning
KW  - Visualization
KW  - Explainable AI
KW  - Computational modeling
KW  - Medical services
KW  - Skin
KW  - Lesions
KW  - Skin Cancer Detection
KW  - Deep Learning
KW  - Pre-trained Models
KW  - Convolutional Neural Networks (CNN)
KW  - HAM10000
KW  - Medical Imaging
KW  - Explainable Artificial Intelligence (XAI)
DO  - 10.1109/ICCIT60459.2023.10508527
JO  - 2023 26th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 26th International Conference on Computer and Information Technology (ICCIT)
Y1  - 13-15 Dec. 2023
AB  - Skin cancer is a serious worldwide health issue, precise and early detection is essential for better patient outcomes and effective treatment. In this research, we use modern deep learning methods and explainable artificial intelligence (XAI) approaches to address the problem of skin cancer detection. To categorize skin lesions, we employ four cutting-edge pre-trained models: XceptionNet, EfficientNetV2S, InceptionResNetV2, and EfficientNetV2M. Image augmentation approaches are used to reduce class imbalance and improve the generalization capabilities of our models. Our models’ decision-making process can be clarified because of the implementation of explainable artificial intelligence (XAI). In the medical field, interpretability is essential to establish credibility and make it easier to implement AI-driven diagnostic technologies into clinical workflows. We determined the XceptionNet architecture to be the best performing model, achieving an accuracy of 88.72%. Our study shows how deep learning and explainable artificial intelligence (XAI) can improve skin cancer diagnosis, laying the groundwork for future developments in medical image analysis. These technologies’ ability to allow for early and accurate detection could enhance patient care, lower healthcare costs, and raise the survival rates for those with skin cancer. Source Code: https://github.com/Faysal-MD/An-Interpretable-Deep-Learning-Approach-for-Skin-Cancer-Categorization-IEEE2023
ER  - 

TY  - CONF
TI  - AI in Healthcare: Navigating the Ethical, Legal, and Social Implications for Improved Patient Outcomes
T2  - 2023 International Conference on Data Science and Network Security (ICDSNS)
SP  - 1
EP  - 8
AU  - A. K. Saini
AU  - R. Yadav
AU  - S. S. Shekhawat
AU  - P. Vats
AU  - S. L. Yadav
AU  - A. P. Singh
AU  - S. S. Biswas
PY  - 2023
KW  - Ethics
KW  - Technological innovation
KW  - Law
KW  - Navigation
KW  - Data security
KW  - Medical services
KW  - Network security
KW  - Artificial intelligence (AI)
KW  - Healthcare
KW  - Ethical implications
KW  - Legal implications
KW  - social implications
KW  - Data privacy
KW  - Data security
KW  - Algorithm bias
KW  - Informed consent
KW  - Transparency
DO  - 10.1109/ICDSNS58469.2023.10245763
JO  - 2023 International Conference on Data Science and Network Security (ICDSNS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Data Science and Network Security (ICDSNS)
Y1  - 28-29 July 2023
AB  - Medical services in Indian Subcontinent might be completely transformed by the application of artificial intelligence (AI) in healthcare. However, it also brings up significant ethical, legal, and societal consequences that must be addressed. The ethical ramifications of AI in Indian healthcare are examined in this research with particular attention paid to data security and privacy, computational bias, informed decision-making, and accountability. Data protection rules and regulations are covered, as well as the legal foundation for AI in healthcare. The influence of AI on healthcare workers and patient-doctor interactions is also examined, along with its societal ramifications, including concerns of equity and accessibility. In order to secure the ethical & egalitarian use of AI for medical care in the Indian subcontinent, the study emphasizes the need for strong ethical principles, current law, and involvement by stakeholders.
ER  - 

TY  - JOUR
TI  - A Clinical Decision Support System for Edge/Cloud ICU Readmission Model Based on Particle Swarm Optimization, Ensemble Machine Learning, and Explainable Artificial Intelligence
T2  - IEEE Access
SP  - 100604
EP  - 100621
AU  - M. Alabdulhafith
AU  - H. Saleh
AU  - H. Elmannai
AU  - Z. H. Ali
AU  - S. El-Sappagh
AU  - J. -W. Hu
AU  - N. El-Rashidy
PY  - 2023
KW  - Predictive models
KW  - Hospitals
KW  - Monitoring
KW  - Machine learning
KW  - Edge computing
KW  - MIMICs
KW  - Real-time systems
KW  - Ensemble learning
KW  - Stacking
KW  - Artificial intelligence
KW  - Medical information systems
KW  - Particle swarm optimization
KW  - Admittance measurement
KW  - Machine learning
KW  - ensemble learning
KW  - stacking ensemble learning
KW  - ICU readmission rate
KW  - explainable artificial intelligence
DO  - 10.1109/ACCESS.2023.3312343
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - ICU readmission is usually associated with an increased number of hospital death. Predicting readmission helps to reduce such risks by avoiding early discharge, providing appropriate intervention, and planning for patient placement after ICU discharge. Unfortunately, ICU scores such as the simplified acute physiology score (SAPS) and Acute Physiology and Chronic Health (APACHE) could help predict mortality or evaluate illness severity. Still, it is ineffective in predicting ICU readmission. This study introduces a clinical monitoring fog-computing-based system for remote prognosis and monitoring of intensive care patients. This proposed monitoring system uses the advantages of machine learning (ML) approaches for generating a real-time alert signal to doctors for supplying e-healthcare, accelerating decision-making, and monitoring and controlling health systems. The proposed system includes three main layers. First, the data acquisition layer, in which we collect the vital signs and lab tests of the patient’s health conditions in real-time. Then, the fog computing layer processes. The results are then sent to the cloud layer, which offers sizable storage space for patient healthcare. Demographic data, lab tests, and vital signs are aggregated from the MIMIC III dataset for 10,465 patients. Feature selection methods: Genetic algorithm (GA) and practical swarm optimization (PSO) are used to choose the optimal feature subset from detests. Moreover, Different traditional ML models, ensemble learning models, and the proposed stacking models are applied to full features and selected features to predict readmission after 30 days of ICU discharge. The proposed stacking models recorded the highest performance compared to other models. The proposed stacking ensemble model with selected features by POS achieved promising results (accuracy = 98.42, precision = 98.42, recall = 98.42, and F1-Score = 98.42), compared to full features and selected features. We also, provide model explanations to ensure efficiency, effectiveness, and trust in the developed model through local and global explanations.
ER  - 

TY  - CONF
TI  - Penetration of Deep Learning in Human Health Care and Pharmaceutical Industries; the Opportunities and Challenges
T2  - 2023 3rd International Conference on Innovative Practices in Technology and Management (ICIPTM)
SP  - 1
EP  - 6
AU  - R. Raman
AU  - R. H. R
AU  - T. M. Inbamalar
AU  - D. A. Subhahan
AU  - A. Kumar
AU  - S. Bathrinath
AU  - S. Sarkar
PY  - 2023
KW  - Deep learning
KW  - Drugs
KW  - Genomics
KW  - Medical services
KW  - Production
KW  - Feature extraction
KW  - Biology
KW  - Deep Learning
KW  - Health care
KW  - Neural Network
KW  - Diseases
KW  - Drug production
DO  - 10.1109/ICIPTM57143.2023.10118224
JO  - 2023 3rd International Conference on Innovative Practices in Technology and Management (ICIPTM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 3rd International Conference on Innovative Practices in Technology and Management (ICIPTM)
Y1  - 22-24 Feb. 2023
AB  - Computational medicine has emerged as a result of the advancement of medical technology, which has led to the emergence of the big data era in the biomedical area, which is supported by artificial intelligence technology. To advance the development of precision medicine, people must be able to extract the valuable information from this vast biomedical data. In the past, professionals in the field of feature engineering and domain knowledge were typically utilised to extract the features from the biological data using machine learning techniques, which took a lot of time and resources. Modern machine learning techniques like deep learning (DL) have an advantage over them in that they can automatically find strong, complex features from fresh data without the necessity for succeeding engineering. The study of DL's applications in the fields of genomics, drug development, electronic health records, and medical imaging suggests that deep learning has clear advantages in maximising the use of biomedical data. Deep learning is becoming increasingly important in the field of medicine and health due to its large range of potential applications. The lack of data, interpretability, data privacy, and heterogeneity are some of the limitations of deep learning in computational medical health. A resource for improving the use of deep learning in medical health is provided by the analysis and discussion of these difficulties.
ER  - 

TY  - CONF
TI  - Explaining Autism Diagnosis Model Through Local Interpretability Techniques – A Post-hoc Approach
T2  - 2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
SP  - 1
EP  - 5
AU  - D. S. Sujana
AU  - D. P. Augustine
PY  - 2023
KW  - Deep learning
KW  - Autism
KW  - Visualization
KW  - Predictive models
KW  - Reliability
KW  - Medical diagnostic imaging
KW  - Standards
KW  - lime
KW  - shap
KW  - anchor
KW  - explainability
KW  - xai
KW  - ml interpretability
DO  - 10.1109/ICDSAAI59313.2023.10452575
JO  - 2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI)
Y1  - 21-23 Dec. 2023
AB  - In this era of machine learning and deep learning algorithms dominating the Artificial Intelligence (AI) world, the trustworthiness of these black box models is still questionable. Life-caring sectors like healthcare and banking make use of these black box models as assistance in critical decision-making processes, but the degree of reliability of these decisions is still uncertain. This is because these black box models will not reveal the causation of the predicted outcome. However, creating an interpretable model that can explain the internal workings of these black box models can provide some reliable insights and trustable justifications for the predicted outcome. This study aimed to create an interpretable model for autism diagnosis which can give some trustable explanations for its predicted outcome. Using local interpretability methods such as LIME, SHAP, and Anchors the predicted outcome for each instance is explained well with some standard visual representations. As a result, this study developed an interpretable autism diagnosis model with an accuracy rate of 91.37% and with good local model explanations.
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence by Genetic Programming: A Survey
T2  - IEEE Transactions on Evolutionary Computation
SP  - 621
EP  - 641
AU  - Y. Mei
AU  - Q. Chen
AU  - A. Lensen
AU  - B. Xue
AU  - M. Zhang
PY  - 2023
KW  - Machine learning
KW  - Genetic programming
KW  - Task analysis
KW  - Predictive models
KW  - Adaptation models
KW  - Training
KW  - Measurement
KW  - Explainable artificial intelligence (XAI)
KW  - Genetic programming (GP)
DO  - 10.1109/TEVC.2022.3225509
JO  - IEEE Transactions on Evolutionary Computation
IS  - 3
SN  - 1941-0026
VO  - 27
VL  - 27
JA  - IEEE Transactions on Evolutionary Computation
Y1  - June 2023
AB  - Explainable artificial intelligence (XAI) has received great interest in the recent decade, due to its importance in critical application domains, such as self-driving cars, law, and healthcare. Genetic programming (GP) is a powerful evolutionary algorithm for machine learning. Compared with other standard machine learning models such as neural networks, the models evolved by GP tend to be more interpretable due to their model structure with symbolic components. However, interpretability has not been explicitly considered in GP until recently, following the surge in the popularity of XAI. This article provides a comprehensive review of the studies on GP that can potentially improve the model interpretability, both explicitly and implicitly, as a byproduct. We group the existing studies related to explainable artificial intelligence by GP into two categories. The first category considers the intrinsic interpretability, aiming to directly evolve more interpretable (and effective) models by GP. The second category focuses on post-hoc interpretability, which uses GP to explain other black-box machine learning models, or explain the models evolved by GP by simpler models such as linear models. This comprehensive survey demonstrates the strong potential of GP for improving the interpretability of machine learning models and balancing the complex tradeoff between model accuracy and interpretability.
ER  - 

TY  - CONF
TI  - Explanation of machine learning models for predicting obesity level using Shapley values
T2  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 3288
EP  - 3291
AU  - L. Bottino
AU  - M. Cannataro
PY  - 2023
KW  - Training
KW  - Obesity
KW  - Machine learning algorithms
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Genetics
KW  - Obesity
KW  - Machine Learning
KW  - Explainability
DO  - 10.1109/BIBM58861.2023.10385994
JO  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 2156-1133
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 5-8 Dec. 2023
AB  - Obesity is a multifactorial disease that includes genetic, biological and behavioral factors. Training machine learning algorithms on some of variables related to these factors can help healthcare professionals equip themselves with tools capable of predicting obesity. But for these tools to gain trust they must be understandable and explainable. SHAP and LIME are two methodologies that allow you to achieve this objective.
ER  - 

TY  - CONF
TI  - Deep Learning in Medical Image Analysis for Personalized Medicine
T2  - 2023 International Symposium ELMAR
SP  - 207
EP  - 212
AU  - I. Galić
AU  - M. Habijan
PY  - 2023
KW  - Deep learning
KW  - Image segmentation
KW  - Image registration
KW  - Recurrent neural networks
KW  - Precision medicine
KW  - Generative adversarial networks
KW  - Convolutional neural networks
KW  - Deep Learning
KW  - Medical Image Analysis
KW  - Medical Image Classification
KW  - Medical Image Segmentation
KW  - Medical Image Registration
DO  - 10.1109/ELMAR59410.2023.10253934
JO  - 2023 International Symposium ELMAR
IS  - 
SN  - 2835-3781
VO  - 
VL  - 
JA  - 2023 International Symposium ELMAR
Y1  - 11-13 Sept. 2023
AB  - Deep learning (DL) has transformed the field of medical image processing, enabling unprecedented accuracy and efficiency in various applications. It has been widely utilized for medical image analysis of different anatomical regions. In this article, we provide an overview of commonly used deep learning methods, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs). We briefly overview their applications in medical image analysis, such as image classification, object detection/localization, segmentation, generation, and registration. We also highlight the strengths and limitations of each method and identify the challenges that still need to be addressed, including the limited availability of annotated data, variability in medical images, and the interpretability issue. Finally, we discuss future research directions, including developing explainable deep learning methods and integrating multi-modal data.
ER  - 

TY  - CONF
TI  - Gradient-based Explainable Artificial Intelligence Methods for Eye Disease Classification
T2  - 2023 IV International Conference on Neural Networks and Neurotechnologies (NeuroNT)
SP  - 6
EP  - 9
AU  - E. N. Volkov
AU  - A. N. Averkin
PY  - 2023
KW  - Glaucoma
KW  - Cataracts
KW  - Training
KW  - Diabetic retinopathy
KW  - Artificial neural networks
KW  - Ophthalmology
KW  - Task analysis
KW  - artificial intelligence
KW  - explanatory artificial intelligence
KW  - hybrid artificial intelligence
KW  - cataracts
KW  - glaucoma
KW  - diabetic retinopathy
KW  - personalized medicine
DO  - 10.1109/NeuroNT58640.2023.10175855
JO  - 2023 IV International Conference on Neural Networks and Neurotechnologies (NeuroNT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IV International Conference on Neural Networks and Neurotechnologies (NeuroNT)
Y1  - 16-16 June 2023
AB  - The use of artificial neural networks in eye disease recognition holds great promise. However, with the increasing complexity of neural network architectures and the inability to trace decision-making pathways, trust in the technology is declining. The use of explainable artificial intelligence in eye disease recognition will provide confidence to ophthalmologists. Methods based on gradient calculation (gradient-based) are becoming popular for image processing tasks. Our study presents a theoretical description and practical implementation of several explanatory artificial intelligence methods (Grad-CAM, Vanilla Gradient, Guided IG, Blur IG, XRAI) on the example of explanatory artificial neural network results for the task of ophthalmic disease image classification (diabetic retinopathy, cataracts, glaucoma).
ER  - 

TY  - CONF
TI  - An Efficient and Robust Explainable Artificial Intelligence for Securing Smart Healthcare System*
T2  - 2023 Second International Conference On Smart Technologies For Smart Nation (SmartTechCon)
SP  - 1066
EP  - 1071
AU  - T. Seetharaman
AU  - V. Sharma
AU  - B. B
AU  - V. Grover
AU  - A. Agnihotri
PY  - 2023
KW  - Resistance
KW  - Medical services
KW  - Self-supervised learning
KW  - Machine learning
KW  - Real-time systems
KW  - Security
KW  - Monitoring
KW  - Smart Healthcare
KW  - IoT
KW  - Explainable Artificial Intelligence
KW  - Security
DO  - 10.1109/SmartTechCon57526.2023.10391664
JO  - 2023 Second International Conference On Smart Technologies For Smart Nation (SmartTechCon)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 Second International Conference On Smart Technologies For Smart Nation (SmartTechCon)
Y1  - 18-19 Aug. 2023
AB  - The advent of IoT technologies has a tremendous impact on the healthcare sector enabling efficient monitoring of patients and utilizing the data for better analytics. Since every activity related to a patient’s health is monitored, the focus on smart healthcare applications has significantly transferred from service provision to a security perspective. As most healthcare applications are automated security plays a vital role. The technique of machine learning has been widely used in securing smart healthcare systems. The major challenge is that these applications require high-quality labeled images, which are difficult to acquire from real-time security applications. Further, it highly time-consuming and cost-expensive process. To address these constraints, in this paper, we define an efficient and robust explainable artificial intelligence technique that takes a small quantity of labeled data to train and de-ploy the security countermeasure for targeted healthcare applications. The proposed approach enhances the security measure through the detection of drifting samples with explainability. It is observed that the proposed approach improved accuracy, high fidelity, and explanation measures. Also, this approach is proven to be considerably resistant against numerous security threats.
ER  - 

TY  - CONF
TI  - Explainability and Responsibility: Developing Trustworthy Artificial Intelligence Models
T2  - 2023 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)
SP  - 1
EP  - 1
AU  - S. Baker
PY  - 2023
KW  - Biological system modeling
KW  - Decision making
KW  - Medical services
KW  - Signal processing
KW  - Data models
KW  - Safety
KW  - Artificial intelligence
DO  - 10.1109/SPMB59478.2023.10372636
JO  - 2023 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)
IS  - 
SN  - 2473-716X
VO  - 
VL  - 
JA  - 2023 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)
Y1  - 2-2 Dec. 2023
AB  - In recent years, artificial intelligence (AI) has become pervasive for decision making in a wide range of fields, including healthcare and biomedicine. AI offers significant potential to the healthcare domain; however, it is not without risk. Without caution, AI models developed based on health data can learn to perpetuate biased, incomplete, outdated, or incorrect information. In the context of healthcare, poor decision making by AI models can have life-altering consequences. The need for trustworthy AI models has given rise to the field of responsible AI, which prioritizes characteristics including fairness, transparency, accountability, and safety. The question of how to achieve responsibility remains an active topic in the literature. Explainable AI techniques, which focus on revealing the inner workings of AI models, have been repeatedly shown to improve transparency of AI models. However, recent research shows that explainability can address much more than transparency – in fact, it can address all characteristics required to achieve responsibility. In this talk, we will first discuss responsible AI and the range of ways that responsibility has been defined. Then, we will introduce methods of achieving explainability in AI systems – both through explainability by design, and through post-hoc explanation techniques that can be implemented for any AI model. We then draw these two fields together, illustrating how explainability can be leveraged to meet the key characteristics required of responsible AI models in healthcare and related domains. This talk will illustrate that explainability is the first stepping stone to achieving truly responsible AI systems.
ER  - 

TY  - CONF
TI  - Anomaly Detection in Healthcare: A Deep Learning Approach with Autoencoders
T2  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
SP  - 1
EP  - 6
AU  - A. Badhoutiya
AU  - D. P. Singh
AU  - J. R. F. Raj
AU  - A. P. Srivastava
AU  - S. L. Chari
AU  - A. K. Khan
PY  - 2023
KW  - Deep learning
KW  - Industries
KW  - Adaptation models
KW  - Ethics
KW  - Technological innovation
KW  - Philosophical considerations
KW  - Closed box
KW  - healthcare anomaly detection
KW  - deep learning
KW  - interpretability
KW  - ethical considerations
KW  - feature importance
DO  - 10.1109/ICAIIHI57871.2023.10489790
JO  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
Y1  - 29-30 Dec. 2023
AB  - This work investigates the use of autoencoders in deep learning for anomaly detection in the healthcare domain, with a focus on ethical and interpretable issues. The study creates a strong anomaly detection system by utilizing secondary data, a deductive methodology, and an interpretivist philosophy. The results demonstrate a high degree of judgment ability, recall, and precision. By improving interpretability, feature importance analysis solves the deep learning models' “black-box” problem. Responsibly using data is prioritized by confidentiality techniques and ethical considerations. Investigating hybrid models and evaluating adaptability to various healthcare scenarios are among the recommendations. Subsequent research will focus on integrating multi-modal data, enhancing the model with arising architectures, and resolving deployment issues.
ER  - 

TY  - CONF
TI  - Thompson Sampling Algorithm for Personalized Treatment Recommendations in Healthcare
T2  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
SP  - 1
EP  - 6
AU  - K. K. Dixit
AU  - D. Verma
AU  - S. K. Muthuvel
AU  - K. Laxminarayanamma
AU  - M. Kumar
AU  - A. Srivastava
PY  - 2023
KW  - Industries
KW  - Ethics
KW  - Technological innovation
KW  - Sensitivity
KW  - Heuristic algorithms
KW  - Decision making
KW  - Medical services
KW  - Thompson sampling
KW  - personalized healthcare
KW  - Bayesian algorithms
KW  - algorithmic transparency
KW  - ethical considerations
DO  - 10.1109/ICAIIHI57871.2023.10488989
JO  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2023 International Conference on Artificial Intelligence for Innovations in Healthcare Industries (ICAIIHI)
Y1  - 29-30 Dec. 2023
AB  - The Thompson Sampling algorithm's use in individualized healthcare decision-making is examined in this study. Utilizing constantly changing patient data, the algorithm, which is rooted in Bayesian principles, dynamically modifies recommendations for treatment. The study uses a descriptive design, interpretivism, and a deductive method. Secondary data will be utilized for a thorough analysis. The performance metrics, comparative analysis with conventional methods, alongside adaptability of the algorithm are highlighted by the results. The robustness and generalizability are emphasized as we examine sensitivity to data variability. Transparency and ethical considerations are important focal points. Recommendations include developing ethical guidelines, validating in various healthcare settings, dealing with biases, and improving interpretability. The careful integration of Thompson Sampling is guided by these insights, which further the advancement of personalized healthcare.
ER  - 

TY  - CONF
TI  - Unveiling Diabetes Predictions: Bridging Complexity and Clarity through Interpretable AI and SHAP Insights
T2  - 2023 4th International Conference on Data Analytics for Business and Industry (ICDABI)
SP  - 258
EP  - 261
AU  - R. Alam
AU  - M. Atif
PY  - 2023
KW  - Bridges
KW  - Accuracy
KW  - Explainable AI
KW  - Neural networks
KW  - Collaboration
KW  - Medical services
KW  - Predictive models
KW  - diabetes prediction
KW  - machine learning
KW  - SHAP
KW  - artificial intelligence
DO  - 10.1109/ICDABI60145.2023.10629275
JO  - 2023 4th International Conference on Data Analytics for Business and Industry (ICDABI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 4th International Conference on Data Analytics for Business and Industry (ICDABI)
Y1  - 25-26 Oct. 2023
AB  - This research paper explores the realm of interpretable machine learning models for predicting diabetes, utilizing the PIMA Indian diabetes dataset (PIDD). With the increasing prevalence of diabetes, accurate prediction has gained paramount importance, and explainable AI offers a bridge between complex models and actionable insights. Various machine learning algorithms, such as decision trees, random forests, gradient boosting, and neural networks, are explored for predicting diabetes onset based on individual health attributes. To enhance model interpretability, SHapley Additive exPlanations (SHAP) values, a powerful technique, are employed to offer transparent insights into prediction rationale. Through SHAP values, the impact of individual features on model predictions is uncovered, demystifying the decision-making process. Traditional models like decision trees inherently provide interpretability, whereas complex models like neural networks benefit from SHAP values to reveal their prediction drivers. The analysis not only identifies influential features but also aids in assessing model fairness and robustness. The results highlight the delicate balance between model complexity and transparency. This study underscores the significance of interpretability in diabetes prediction models and demonstrates SHAP value’s efficacy in unraveling intricate machine learning behaviors. As healthcare relies increasingly on artificial intelligence (AI), this research contributes to informed model selection, fostering reliable and transparent medical AI applications. By shedding light on the intricate workings of predictive models, the discourse on trustworthy and effective AI deployment in critical healthcare scenarios is advanced.
ER  - 

TY  - CONF
TI  - Advances of Deep learning in Breast Cancer Modeling
T2  - 2023 IEEE 21st Jubilee International Symposium on Intelligent Systems and Informatics (SISY)
SP  - 000501
EP  - 000508
AU  - S. Ardabili
AU  - A. Mosavi
AU  - I. Felde
PY  - 2023
KW  - Deep learning
KW  - Data analysis
KW  - Precision medicine
KW  - Breast cancer
KW  - Teamwork
KW  - Convolutional neural networks
KW  - Reliability
KW  - breast cancer
KW  - deep learning
KW  - systematic review
KW  - convolutional neural network
KW  - machine learning
KW  - big data
KW  - data science
KW  - soft computing
KW  - cancer
KW  - diagnosis
KW  - modeling
KW  - mathematics
KW  - AI
KW  - XAI
KW  - survery
DO  - 10.1109/SISY60376.2023.10417961
JO  - 2023 IEEE 21st Jubilee International Symposium on Intelligent Systems and Informatics (SISY)
IS  - 
SN  - 1949-0488
VO  - 
VL  - 
JA  - 2023 IEEE 21st Jubilee International Symposium on Intelligent Systems and Informatics (SISY)
Y1  - 21-23 Sept. 2023
AB  - Deep learning (DL) has recently gained popularity in forecasting, detecting, categorizing, and diagnosing for breast cancer with promising results. Developing a review paper to assess the efficacy of DL methods in this context is essential. We've established a standardized database initially containing fundamental publications for methodical reviews. The primary objective of this review is to systematically present the current state-of-the-art, using an updated PRISMA guidelines to better review and evaluate the DL's effectiveness in breast cancer applications. The research follows three main stages: data collection, data analysis, and summarization of initial outcomes. The results highlight accuracy as the prevailing and comprehensive metric used in evaluating DL tools across varied breast cancer applications. Convolutional neural networks (CNNs) have found to have widespread utility, notably surpassing other DL methods. In contrast, collaborative teamwork and employing advanced DL techniques yield optimal performance.
ER  - 

TY  - CONF
TI  - Gen-AI Perspective in Digital Healthcare: Ownership versus Practicality
T2  - 2023 Seventh International Conference on Image Information Processing (ICIIP)
SP  - 634
EP  - 639
AU  - A. D. Gupta
AU  - S. Sharma
PY  - 2023
KW  - Industries
KW  - Privacy
KW  - Precision medicine
KW  - Magnetic resonance imaging
KW  - Generative adversarial networks
KW  - Electronic healthcare
KW  - Stakeholders
KW  - Digital Healthcare
KW  - Gen-AI
KW  - Data Ownership
KW  - Patient-as-an-Organization
KW  - Data Security
DO  - 10.1109/ICIIP61524.2023.10537747
JO  - 2023 Seventh International Conference on Image Information Processing (ICIIP)
IS  - 
SN  - 2640-074X
VO  - 
VL  - 
JA  - 2023 Seventh International Conference on Image Information Processing (ICIIP)
Y1  - 22-24 Nov. 2023
AB  - The practical implementation and data ownership implications of artificial intelligence (AI) integration in digital healthcare is investigated in this article. It suggests conceptualizing every patient as a complex data structure to facilitate the implementation of personalized medicine. This, however, poses privacy and commercialization-related ethical concerns. The article emphasizes the capacity of artificial intelligence to enhance diagnosis and treatment via the analysis of massive datasets. However, obstacles persist concerning bias, transparency, and human oversight. It provides the conversion of MRI to CT scans via generative adversarial networks as an illustration of the utility of AI. Work remains, nevertheless, to guarantee that AI advances health equity. It is concluded in the paper that continuous research and stakeholder collaboration are required to strike a balance between technological advancement and patient rights. Additional research is necessary in privacy protection, human-AI interaction, and bias mitigation as the healthcare industry undergoes digital transformation.
ER  - 

TY  - CONF
TI  - From cloud AI to embedded AI in cardiac healthcare
T2  - 2023 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)
SP  - 1
EP  - 6
AU  - B. Costa
AU  - O. Postolache
AU  - J. Araujo
PY  - 2023
KW  - Heart
KW  - Sociology
KW  - Medical services
KW  - Electrocardiography
KW  - Signal processing
KW  - Artificial intelligence
KW  - Statistics
KW  - artificial intelligence
KW  - healthcare
KW  - cardiac diseases
KW  - machine learning
KW  - embedded system
DO  - 10.1109/I2MTC53148.2023.10176077
JO  - 2023 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)
IS  - 
SN  - 2642-2077
VO  - 
VL  - 
JA  - 2023 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)
Y1  - 22-25 May 2023
AB  - The health of the population is a global concern, and cardiac health is an increasingly studied aspect. In the era of artificial intelligence (AI), there are many possibilities to improve the diagnosis and monitoring of our cardiac system in a non-invasive and unobtrusive and way and accessible to a wider population. This article addresses issues such as the diagnosis of heart diseases in which the heartbeat becomes irregular using wearable systems containing embedded artificial intelligence. This paper is mainly a review of reported studies that have recently been developed that demonstrate how technology is moving from cloud-based AI systems to embedded AI systems in cardiac healthcare.
ER  - 

TY  - CONF
TI  - AI trustworthiness in prostate cancer imaging: a look at algorithmic and system transparency*
T2  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
SP  - 79
EP  - 80
AU  - S. Colantonio
AU  - A. Berti
AU  - R. Buongiorno
AU  - G. Del Corso
AU  - E. Pachetti
AU  - M. A. Pascali
AU  - C. Kalantzopoulos
AU  - V. Kalokyri
AU  - H. Kondylakis
AU  - N. Tachos
AU  - D. Fotiadis
AU  - V. Giannini
AU  - S. Mazzetti
AU  - D. Regge
AU  - N. Papanikolaou
AU  - K. Marias
AU  - M. Tsiknakis
PY  - 2023
KW  - Medical services
KW  - Machine learning
KW  - Robustness
KW  - Prostate cancer
KW  - Monitoring
KW  - Medical diagnostic imaging
KW  - Testing
DO  - 10.1109/IEEECONF58974.2023.10404432
JO  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
Y1  - 7-9 Dec. 2023
AB  - A responsible approach to artificial intelligence and machine learning technologies, grounded in sound scientific foundations, technical robustness, rigorous testing and validation, risk-based continuous monitoring and alignment with human values is imperative to guarantee their favorable impact and prevent any adverse effects they may have on individuals and communities. An essential aspect of responsible development is transparency, which constitutes a fundamental principle of the European approach towards artificial intelligence. Transparency can be achieved at different levels, such as data origin and use, system development, operation and usage. In this paper, we present the techniques implemented and delivered in the EU H2020 ProCAncer-I project to meet the transparency requirements at the different levels required.Clinical Relevance: This paper examines the primary transparency hurdles in artificial intelligence for medical imaging diagnostics, and presents the approaches that the EU H2020 project ProCAncer-I is taking to address them.
ER  - 

TY  - CONF
TI  - Approaching Explainable Artificial Intelligence Methods in the Diagnosis of Iron Deficiency Anemia Using Blood Parameters
T2  - 2023 International Conference on Recent Advances in Information Technology for Sustainable Development (ICRAIS)
SP  - 201
EP  - 206
AU  - U. Ponnusamy
AU  - D. D. B S
AU  - N. Sampathila
PY  - 2023
KW  - Collaboration
KW  - Medical services
KW  - Machine learning
KW  - Lead
KW  - Iron
KW  - Sustainable development
KW  - Information technology
KW  - Anemia
KW  - Machine learning
KW  - Iron deficiency
KW  - Explainable artificial intelligence
KW  - SHAP
KW  - Beeswarm plot
DO  - 10.1109/ICRAIS59684.2023.10367126
JO  - 2023 International Conference on Recent Advances in Information Technology for Sustainable Development (ICRAIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Recent Advances in Information Technology for Sustainable Development (ICRAIS)
Y1  - 6-7 Nov. 2023
AB  - Anemia is a global health disorder diagnosed by observing blood parameters. It is a tedious and time-consuming method for healthcare workers to analyze the data manually and may also lead to mistakes. This paper proposes a novel method to understand the impact of blood parameters in diagnosing anemia. Machine learning methods have been used to classify the data, and the impact of the attributes was explained using explainable AI tools to bring transparency and trust to the architectures. XAI helps in ensuring fairness, accountability, and transparency. The models show a high accuracy of 80-100%• The beeswarm plot explained the impact of the various attributes present in a complete blood count in the diagnosis of iron deficiency anemia. The methods introduced help in the quick diagnosis of anemia and save time for healthcare professionals. Improvement in the current technology in collaboration with healthcare workers will lead the medical domain to new heights.
ER  - 

TY  - CONF
TI  - Local Interpretable Model-Agnostic Explanations for Online Maternal Healthcare
T2  - 2023 2nd International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN)
SP  - 1
EP  - 6
AU  - G. Marvin
AU  - D. Jjingo
AU  - J. Nakatumba-Nabende
AU  - M. G. R. Alam
PY  - 2023
KW  - Social networking (online)
KW  - Social groups
KW  - Computational modeling
KW  - Medical services
KW  - Machine learning
KW  - Information filters
KW  - Fake news
KW  - Artificial Intelligence (AI)
KW  - Trustworthy AI
KW  - Conversational AI
KW  - Natural Language Processing (NLP)
KW  - Maternal Healthcare
KW  - Digital Medicine
KW  - Mobile Health
DO  - 10.1109/ICSTSN57873.2023.10151520
JO  - 2023 2nd International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 2nd International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN)
Y1  - 21-22 April 2023
AB  - In culturally conservative communities, access to authentic sexual, reproductive, and adolescent information is scarce, particularly in low and middle-income countries. This has led to an over-reliance on social media and online communities to obtain such information, hence leading to the proliferation of fake and inappropriate healthcare advice. Moreover, there is no regulatory body to verify and validate shared healthcare information on online platforms. Individuals often disguise their identity while seeking sensitive information on sexual, reproductive and maternal health online. This has facilitated untraceable spread of incorrect information and harmful medical advice among social groups. These variations in social dynamics result in healthcare disparities, which reinforce health inequalities. In this paper, we propose the use of interpretable machine learning to evaluate online maternal medical advice for authenticity. We report on the negative results of Machine Learning Models attempt to distinguish between authentic and fake medical advice and urgently advocate for the establishment of a sexual, reproductive and maternal health corpus for machine learning models to learn, filter and detect medical imposters or misinformation. Our work highlights the insufficiency of explainable AI in medical contexts and underscores the need for establishing regulatory bodies to ensure the authenticity of sensitive healthcare information via social media and online platforms.
ER  - 

TY  - CONF
TI  - Exploring the Trade-Offs Between Blackbox and Explainable AI: A Comparative Study
T2  - 2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)
SP  - 1
EP  - 8
AU  - S. S. Makubhai
AU  - G. R. Pathak
AU  - P. R. Chandre
PY  - 2023
KW  - Industries
KW  - Ethics
KW  - Analytical models
KW  - Automation
KW  - Computational modeling
KW  - Closed box
KW  - Finance
KW  - healthcare
KW  - blackbox
KW  - explainable artificial intelligence
DO  - 10.1109/ICCUBEA58933.2023.10391996
JO  - 2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)
IS  - 
SN  - 2771-1358
VO  - 
VL  - 
JA  - 2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)
Y1  - 18-19 Aug. 2023
AB  - Healthcare, finance, and transportation are just a few of the sectors where artificial intelligence (AI) is now a necessity. AI algorithms come in two flavors: explainable and blackbox. Explainable models are intended to be more transparent and interpretable than blackbox models, which are complicated and challenging to understand. Through a comparative analysis, this article investigates the trade-offs between explainable AI and blackbox AI. It specifically looks into how explainability affects accuracy, user perception of AI models, the efficiency of explainability methods, domain-specific effects, and ethical ramifications. The study seeks to offer useful insights into the advantages and disadvantages of these models and to assist in guiding decisions regarding the use of explainable and blackbox AI in various applications.
ER  - 

TY  - CONF
TI  - Enhancing Stroke Prediction through Interpretable AI: Distinguishing Stroke Cases from Non-Stroke Cases
T2  - 2023 6th International Conference on Electrical Information and Communication Technology (EICT)
SP  - 1
EP  - 6
AU  - N. Islam
AU  - H. B. Kibria
PY  - 2023
KW  - Training
KW  - Explainable AI
KW  - Medical services
KW  - Stroke (medical condition)
KW  - Prediction algorithms
KW  - Random forests
KW  - Classification tree analysis
KW  - Stroke
KW  - Random forest
KW  - Explainable AI
KW  - SHAP
DO  - 10.1109/EICT61409.2023.10427860
JO  - 2023 6th International Conference on Electrical Information and Communication Technology (EICT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 6th International Conference on Electrical Information and Communication Technology (EICT)
Y1  - 7-9 Dec. 2023
AB  - A sudden interruption of blood supply to part of the brain can result in a stroke, causing impairment and possible fatalities due to damage to brain cells. Rapid identification and intervention are crucial for preventing strokes and minimizing severe brain damage. The integration of Artificial Intelligence (AI) and machine learning (ML) shows promise in improving healthcare practitioners' ability to predict strokes swiftly and accurately. This study focuses on creating and assessing a stroke prediction model using a Random Forest (RF) classifier, supported by various performance metrics like AUC, precision, recall, F1-score, and accuracy, achieving a commendable accuracy rate of 94.6%. To address the class imbalance in the training set, a Random Over-Sampler was employed. Moreover, interpretability is enhanced through Explainable AI (XAI) techniques, specifically Shapley Additive Values (SHAP) and Local Interpretable Modelagnostic Explanations (LIME). By establishing an automated screening framework, this research aims to provide healthcare professionals with tools to offer more personalized and efficient care, potentially transforming stroke prevention and treatment.
ER  - 

TY  - CONF
TI  - Human-Centered Explainable AI at the Edge for eHealth
T2  - 2023 IEEE International Conference on Edge Computing and Communications (EDGE)
SP  - 227
EP  - 232
AU  - J. Dutta
AU  - D. Puthal
PY  - 2023
KW  - Industries
KW  - Medical conditions
KW  - Databases
KW  - Computational modeling
KW  - Cardiac arrest
KW  - Machine learning
KW  - Predictive models
KW  - XAI
KW  - IoMT
KW  - Edge
KW  - Machine Learning
KW  - Interpretability
KW  - eHealth
DO  - 10.1109/EDGE60047.2023.00044
JO  - 2023 IEEE International Conference on Edge Computing and Communications (EDGE)
IS  - 
SN  - 2767-9918
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Edge Computing and Communications (EDGE)
Y1  - 2-8 July 2023
AB  - Explainable Artificial Intelligence (XAI) is a new paradigm of Artificial Intelligence (AI) that is giving different AI/ Machine Learning (ML) models a boost to penetrate sectors where people are thinking about adopting AI. This work focuses on the adoption of XAI in the health sector. It portrays that careful integration of XAI in both cloud and edge could change the whole healthcare industry and make humans more aware of their present health conditions, which is the need of the hour. To demonstrate the same, we have done an experiment based on the prediction of a particular medical condition called "cardiac arrest" in a specific subject group (patients who are 70 years old). Here, based on the explanation provided by the XAI model (e.g., SHAP, LIME) at Cloud and Edge, our system can predict the chances of a "cardiac arrest" for the subject with a valid explanation. This type of model will be the next big upgrade in the healthcare industry in terms of automation and a self-explanatory system that works as a personal health assistant for individuals.
ER  - 

TY  - CONF
TI  - Decoding the Black Box: A Comprehensive Review of Explainable Artificial Intelligence
T2  - 2023 9th International Conference on Information Technology Trends (ITT)
SP  - 108
EP  - 113
AU  - O. Embarak
PY  - 2023
KW  - Ethics
KW  - Finance
KW  - Medical services
KW  - Market research
KW  - Feature extraction
KW  - History
KW  - Artificial intelligence
KW  - Explainable Artificial Intelligence (XAI)
KW  - Interpretability
KW  - literature review
KW  - Explainable models Human-AI interaction
KW  - XAI challenges
KW  - future developments
DO  - 10.1109/ITT59889.2023.10184238
JO  - 2023 9th International Conference on Information Technology Trends (ITT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 9th International Conference on Information Technology Trends (ITT)
Y1  - 24-25 May 2023
AB  - This review explores the current state of Explainable Artificial Intelligence (XAI). This study looks at current advances in XAI research, as well as challenges and the future. To accomplish this, the review will explore XAI literature on a wide range of topics, including current techniques, strategies, and applications. The review begins with an overview of XAI, including its definition, history, and motivations. It will then investigate XAI's current situation as well as explainability approaches. This will entail a look at model-based, post-hoc, and interactive explainability, as well as the methods used to get there, such as feature attribution, rule extraction, and counterfactual analysis. The application of XAI in healthcare, finance, and autonomous systems will be examined. The review will look at the ethical and social consequences of XAI, such as bias, accountability, and transparency. Among the challenges that XAI faces include a lack of uniform measurements and evaluation procedures, interpretable data, and domain-specific expertise. The review will include important trends, patterns, and avenues for improvement. The review will also recommend XAI research and development. This review will assist XAI researchers, practitioners, and policymakers in understanding the field's current state, limitations and challenges, and future direction.
ER  - 

TY  - CONF
TI  - XCardio-Twin: An explainable framework to aid in monitoring and analysis of cardiovascular status
T2  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
SP  - 1
EP  - 6
AU  - R. Krzysiak
AU  - D. An
AU  - Y. Chen
PY  - 2023
KW  - Wearable computers
KW  - Decision making
KW  - Machine learning
KW  - Electrocardiography
KW  - Predictive models
KW  - Feature extraction
KW  - Real-time systems
KW  - healthcare
KW  - ECG
KW  - digital-twin
KW  - machine learning
KW  - cardiovascular system
KW  - XAI
DO  - 10.1109/DTPI59677.2023.10365417
JO  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence (DTPI)
Y1  - 7-9 Nov. 2023
AB  - We present an explainable digital-twin framework designed for cardiovascular health monitoring by integrating feature extraction using the NeuroKit2 and Heartpy python libraries and predictive modeling through machine learning. The digital-twin extracts key physiological features from ECG data, such as heart rate variability, and utilized machine learning algorithms to predict individual cardiovascular status. To improve trustworthiness, interpretability and transparency behind the decision making process, SHAP (Shapley Additive Explanations) was paired to help describe the underlying contributions of each feature to the model’s decision-making process. The performance of the machine learning model scored a 90.08 % at accurately predicting abnormal or normal ECG signal. The combination of extraction of key features from ECG signals and a trained explainable machine learning model offers transparent, personalized insights into cardiovascular health.
ER  - 

TY  - CONF
TI  - An Explainable AI based Clinical Assistance Model for Identifying Patients with the Onset of Sepsis
T2  - 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)
SP  - 297
EP  - 302
AU  - S. Chakraborty
AU  - K. Kumar
AU  - B. P. Reddy
AU  - T. Meena
AU  - S. Roy
PY  - 2023
KW  - Sensitivity
KW  - Biological system modeling
KW  - Predictive models
KW  - Data science
KW  - Sepsis
KW  - Feature extraction
KW  - Rough surfaces
KW  - Healthcare
KW  - XAI
KW  - Sepsis Prediction
KW  - Autoencoders
DO  - 10.1109/IRI58017.2023.00059
JO  - 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)
IS  - 
SN  - 2835-5776
VO  - 
VL  - 
JA  - 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)
Y1  - 4-6 Aug. 2023
AB  - The high mortality rate of sepsis, especially in Intensive Care Unit (ICU) makes it third-highest mortality disease globally. The treatment of sepsis is also time consuming and depends on multi-parametric tests, hence early identification of patients with sepsis becomes crucial. The recent rise in the development of Artificial Intelligence (AI) based models, especially in early prediction of sepsis, have improved the patient outcome. However, drawbacks like low sensitivity, use of excess features that leads to overfitting, and lack of interpretability limit their ability to be used in a clinical setting. So, in this research we have developed a smart, explainable and a highly accurate AI based model (called XAutoNet) that provides quick and early prediction of sepsis with a minimal number of features as input. An application based novel convolutional neural network (CNN) based autoencoder is also implemented that improves the performance of XAutoNet by dimensional reduction. Finally, to unbox the “Black Box” nature of these models, Gradient based Class Activation Map (GradCAM) and SHapley Additive exPlanations (SHAP) are implemented to provide interpretability of autoencoder and XAutoNet in the form of visualization graphs to assist clinicians in diagnosis and treatment.
ER  - 

TY  - CONF
TI  - XAIA: An Explainable AI Approach for Classification and Analysis of Blood Anemia
T2  - 2023 OITS International Conference on Information Technology (OCIT)
SP  - 88
EP  - 93
AU  - J. Prajapati
AU  - V. Uduthalapally
AU  - D. Das
AU  - R. Mahapatra
AU  - P. N. Wasnik
PY  - 2023
KW  - Explainable AI
KW  - Decision making
KW  - Medical diagnosis
KW  - Task analysis
KW  - Information technology
KW  - Blood
KW  - Diseases
KW  - Anemia
KW  - Classification
KW  - Explainable AI
KW  - Smart Healthcare
DO  - 10.1109/OCIT59427.2023.10430938
JO  - 2023 OITS International Conference on Information Technology (OCIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 OITS International Conference on Information Technology (OCIT)
Y1  - 13-15 Dec. 2023
AB  - Anemia like blood-borne disease diagnosis is a significant challenge in hematology. Generally, healthcare workers diagnose the Anemia type using a Complete Blood Count (CBC) report but finding the Anemia types and the root cause in a large number of patients is an exhausting task in a resource-constrained setup. The existing artificial intelligence models used to classify these types of anemia are black-box in nature. This paper presents a method for automated classification of anemia with explainable AI (XAIA) model-derived-atlas to unbox the decision-making. XGBClassifier is applied on a collected dataset with SHAP value to get interpretability and causability of decision. XGBClassifier model had reported an accuracy of 96.95%. Integrating explainability for anemia classification improves the clarity of predictions and helps patients understand the underlying cause of their anemia problem. The proposed solution will build trust among healthcare workers to accept AI-based automatic diagnosis the anemia diseases with their root-causes.
ER  - 

TY  - CONF
TI  - Probability of Heart Disease using various Machine Learning Algorithms
T2  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
SP  - 1
EP  - 6
AU  - S. Pillai
AU  - S. Mogra
AU  - N. Shriman
AU  - K. Bakade
PY  - 2023
KW  - Heart
KW  - Support vector machines
KW  - Computers
KW  - Machine learning algorithms
KW  - Costs
KW  - Medical services
KW  - Predictive models
KW  - Predictive modeling
KW  - data mining
KW  - machine learning
KW  - healthcare
KW  - accuracy
DO  - 10.1109/ICACTA58201.2023.10393129
JO  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
Y1  - 6-7 Oct. 2023
AB  - Heart disease is the leading cause of death globally, requiring advanced predictive knowledge and skills. Healthcare systems lack effective analysis tools to uncover hidden patterns. Using Kaggle’s dataset, an automated method aims to predict disease occurrences using data mining techniques, reducing costs and waiting times. Heart disease risk demands a sizable dataset that is excessively large and complex toprocess and analyze using traditional methods. Our goal is to identify an effective and precise method for estimating the probability of heart illness. The experimental results show the highest accuracy, Area under curve is best achieved by the random forest classifier which is about 83.96%, 92.35%, and the highest F1 score is achieved by SVM which is about 80.7%.
ER  - 

TY  - CONF
TI  - Next Generation Healthcare with Explainable AI: IoMT-Edge-Cloud Based Advanced eHealth
T2  - GLOBECOM 2023 - 2023 IEEE Global Communications Conference
SP  - 7327
EP  - 7332
AU  - J. Dutta
AU  - D. Puthal
AU  - C. Y. Yeun
PY  - 2023
KW  - Explainable AI
KW  - Databases
KW  - Medical services
KW  - Transforms
KW  - Predictive models
KW  - Real-time systems
KW  - Next generation networking
KW  - IoMT
KW  - XAI
KW  - Interpretability
KW  - counterfactuals
KW  - Edge Computing
KW  - Cloud Computing
KW  - eHealth
DO  - 10.1109/GLOBECOM54140.2023.10436967
JO  - GLOBECOM 2023 - 2023 IEEE Global Communications Conference
IS  - 
SN  - 2576-6813
VO  - 
VL  - 
JA  - GLOBECOM 2023 - 2023 IEEE Global Communications Conference
Y1  - 4-8 Dec. 2023
AB  - This article provides in-depth experimental studies of XAI (EXplainable Artificial Intelligence) in the IoT-Edge-Cloud continuum. Within the different available XAI frameworks, such as Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) frameworks are utilized here as they are the most suitable feature map-based, model-agnostic, posthoc frameworks that match our requirements for getting real-time prediction explanations in the healthcare domain. In order to evaluate LIME and SHAP in this continuum and to make black box AI (BBAI)-based decisions interpretable, we have considered the real-world electronic health record (EHR)-based large cloud database (which could be a very large database–VLDB) and IoMT based real-time streams as edge databases for the prediction of cardiac arrest in the real-world. We have also verified the effectiveness of automated counterfactual explanations in this context for taking remedial actions. Thus, our proposed model is capable of making significant advancements in the healthcare industry by offering conscious healthcare monitoring automation along with an AI-based self-explanatory system that serves as a personalized health assistant for individuals, paving the way for the next major upgrade in healthcare.
ER  - 

TY  - JOUR
TI  - eXplainable AI Allows Predicting Upper Limb Rehabilitation Outcomes in Sub-Acute Stroke Patients
T2  - IEEE Journal of Biomedical and Health Informatics
SP  - 263
EP  - 273
AU  - M. Gandolfi
AU  - I. Boscolo Galazzo
AU  - R. Gasparin Pavan
AU  - F. Cruciani
AU  - N. Valè
AU  - A. Picelli
AU  - S. F. Storti
AU  - N. Smania
AU  - G. Menegaz
PY  - 2023
KW  - Stroke (medical condition)
KW  - Predictive models
KW  - Artificial intelligence
KW  - Radio frequency
KW  - Indexes
KW  - Feature extraction
KW  - Task analysis
KW  - Explainable artificial intelligence
KW  - machine learning
KW  - prediction
KW  - rehabilitation
KW  - stroke
DO  - 10.1109/JBHI.2022.3220179
JO  - IEEE Journal of Biomedical and Health Informatics
IS  - 1
SN  - 2168-2208
VO  - 27
VL  - 27
JA  - IEEE Journal of Biomedical and Health Informatics
Y1  - Jan. 2023
AB  - While stroke is one of the leading causes of disability, the prediction of upper limb (UL) functional recovery following rehabilitation is still unsatisfactory, hampered by the clinical complexity of post-stroke impairment. Predictive models leading to accurate estimates while revealing which features contribute most to the predictions are the key to unveil the mechanisms subserving the post-intervention recovery, prompting a new focus on individualized treatments and precision medicine in stroke. Machine learning (ML) and explainable artificial intelligence (XAI) are emerging as the enabling technology in different fields, being promising tools also in clinics. In this study, we had the twofold goal of evaluating whether ML can allow deriving accurate predictions of UL recovery in sub-acute patients, and disentangling the contribution of the variables shaping the outcomes. To do so, Random Forest equipped with four XAI methods was applied to interpret the results and assess the feature relevance and their consensus. Our results revealed increased performance when using ML compared to conventional statistical approaches. Moreover, the features deemed as the most relevant were concordant across the XAI methods, suggesting good stability of the results. In particular, the baseline motor impairment as measured by simple clinical scales had the largest impact, as expected. Our findings highlight the core role of ML not only for accurately predicting the individual outcome scores after rehabilitation, but also for making ML results interpretable when associated to XAI methods. This provides clinicians with robust predictions and reliable explanations that are key factors in therapeutic planning/monitoring of stroke patients.
ER  - 

TY  - CONF
TI  - Anemia Identification from Blood Smear Images Using Deep Learning: An XAI Approach
T2  - 2023 International Conference on Recent Advances in Information Technology for Sustainable Development (ICRAIS)
SP  - 147
EP  - 152
AU  - S. Sajith
AU  - P. A
AU  - T. Ramesh
AU  - P. Rajpal
AU  - R. A. R
AU  - J. Ahammad
AU  - A. N. P
PY  - 2023
KW  - Deep learning
KW  - Training
KW  - Measurement
KW  - Red blood cells
KW  - Computational modeling
KW  - Training data
KW  - Data augmentation
KW  - Anemia
KW  - CNN
KW  - DenseNet
KW  - Deep learning
DO  - 10.1109/ICRAIS59684.2023.10367078
JO  - 2023 International Conference on Recent Advances in Information Technology for Sustainable Development (ICRAIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Recent Advances in Information Technology for Sustainable Development (ICRAIS)
Y1  - 6-7 Nov. 2023
AB  - Anemia is a prevalent hematologic disorder characterized by a reduction in the number of red blood cells or a decrease in their oxygen-carrying capacity. Accurate and timely diagnosis of anemia is crucial for effective patient management and improved health outcomes. In this proposed work, we developed a CNN-based model to classify individuals as having symptoms of Iron-deficiency Anemia (IDA) or not, using blood smear images. The CNN model will be trained and fine-tuned using this dataset to maximize the classification accuracy. This helps healthcare professionals to understand the system's decisions and assist in the diagnosis and management of anemia. Despite a relatively limited number of data samples, our proposed work showed promising results by leveraging augmentation techniques. We achieved an accuracy of 0.95, with precision and recall values of 0.9 and 0.89, respectively. The area under the Receiver Operating Characteristic (ROC) curve is reported as 0.98.
ER  - 

TY  - JOUR
TI  - Assessing Bias in Skin Lesion Classifiers With Contemporary Deep Learning and Post-Hoc Explainability Techniques
T2  - IEEE Access
SP  - 78339
EP  - 78352
AU  - A. Corbin
AU  - O. Marques
PY  - 2023
KW  - Skin
KW  - Lesions
KW  - Deep learning
KW  - Computational modeling
KW  - Image color analysis
KW  - Dermatology
KW  - Bias
KW  - deep learning
KW  - explainable AI
KW  - fairness
KW  - skin lesion classification
DO  - 10.1109/ACCESS.2023.3289320
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - As Artificial Intelligence (AI) is increasingly utilized in dermatology, ensuring fairness in the development of Machine Learning models is crucial, particularly in skin lesion classification, where decisions can significantly impact people’s lives. This study investigates the presence of biases between different Fitzpatrick Skin Types in baseline pretrained models and evaluates various training techniques to mitigate these disparities. An unsupervised skin transformer is developed to adjust an image’s Fitzpatrick Skin Type (FST), and joint regularization and synthetic image blending methods are employed to address bias concerns. Additionally, eXplainable Artificial Intelligence (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), are utilized to identify any underlying reasons for bias in the models. The results indicate that joint regularization and synthetic blending methods enhance the area under the curve performance and fairness. Meanwhile, XAI was found to be a valuable tool for fine-tuning Deep Learning models and uncovering problems. These findings can aid in developing accurate and unbiased skin lesion classification models, promoting equitable healthcare, and improving patient outcomes.
ER  - 

TY  - CONF
TI  - Opening the Black Box: Explainable Machine Learning for Heart Disease Patients
T2  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
SP  - 1
EP  - 5
AU  - D. Srivastava
AU  - H. Pandey
AU  - A. K. Agarwal
AU  - R. Sharma
PY  - 2023
KW  - Heart
KW  - Industries
KW  - Closed box
KW  - Machine learning
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Heart disease
KW  - Machine learning
KW  - Explainable Machine Learning
DO  - 10.1109/ICACTA58201.2023.10392874
JO  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)
Y1  - 6-7 Oct. 2023
AB  - Unfortunately, the “black box” stigma that has long plagued machine learning has caused many doctors to remain leery of its applications. A “black box” is a model that is so complicated that it is difficult, if not impossible, for a person to understand. Especially in the field of medicine, where many choices have profound consequences for patients’ lives, the inability to understand the reasoning behind prediction models might erode their credibility. New studies in the area of explainable machine learning try to allay these fears. While the benefits of explainable machine learning are vast, they are especially pertinent when deciding whether or not to admit a patient to an intensive care unit. In this article, we take a look at the fundamentals of explainable machine learning and how they might be applied to the healthcare industry. In the first step, we use four well-known boosting methods for heart disease detection and prognosis. Finally, we provide an explanation of the aspects that contribute to the best model judgments, improving clinical reasoning by enhancing our capacity for better predictions and our comprehension of the reasons behind them
ER  - 

TY  - CONF
TI  - A Review of Machine Learning Algorithms and Feature Selection Techniques for Cardiovascular Disease Prediction: Insights and Implications
T2  - 2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)
SP  - 1
EP  - 5
AU  - A. Mahajan
AU  - B. Kaushik
PY  - 2023
KW  - Deep learning
KW  - Machine learning algorithms
KW  - Costs
KW  - Automation
KW  - Predictive models
KW  - Feature extraction
KW  - Prediction algorithms
KW  - chi-square
KW  - feature selection
KW  - machine learning
KW  - filter method
KW  - wrapper method
DO  - 10.1109/ICCUBEA58933.2023.10392135
JO  - 2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)
IS  - 
SN  - 2771-1358
VO  - 
VL  - 
JA  - 2023 7th International Conference On Computing, Communication, Control And Automation (ICCUBEA)
Y1  - 18-19 Aug. 2023
AB  - Cardiovascular disease (CVD) is a formidable public health challenge across the globe and is the most prevalent cause of mortality. Early detection and accurate prediction of CVD can help prevent disease progression and reduce the risk of complications. Machine learning (ML) techniques show promising results in improving the accuracy and efficiency of CVD prediction to precision. However, the effectiveness of machine learning algorithms in CVD prediction largely depends on the selection of relevant features from complex datasets. The performance and interpretability of ML models are improved by feature selection strategies, which attempt to identify significant attributes while eliminating duplicate or irrelevant features. The feature selection and ML algorithms for CVD are thoroughly reviewed in this publication. The review provides insight into the selection of appropriate feature selection techniques and machine learning algorithms for accurate CVD prediction and evaluates the effectiveness and performance of these methods on cardiovascular datasets. Insights from the findings of this study can be used for interpreting the selection of optimal feature selection methods and ML algorithms for the precise prediction of cardiovascular disease, thereby improving patient outcomes and reducing healthcare costs.
ER  - 

TY  - CONF
TI  - LIME-based Explainable AI Models for Predicting Disease from Patient’s Symptoms
T2  - 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)
SP  - 1
EP  - 6
AU  - Ferdib-Al-Islam
AU  - A. Saha
AU  - E. J. Bristy
AU  - M. Rahatul Islam
AU  - R. Afzal
AU  - S. A. Ridita
PY  - 2023
KW  - Logistic regression
KW  - Computational modeling
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Reliability
KW  - Decision trees
KW  - Disease Prediction
KW  - Explainable AI Model
KW  - ML Classifier
KW  - LIME
KW  - Recursive Feature Elimination
DO  - 10.1109/ICCCNT56998.2023.10307223
JO  - 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)
IS  - 
SN  - 2473-7674
VO  - 
VL  - 
JA  - 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)
Y1  - 6-8 July 2023
AB  - In recent years, there has been a significant increase in the use of artificial intelligence (AI) models for predicting disease from patient symptoms. However, these models are often considered black boxes, as they lack transparency in how they make their predictions. This lack of transparency raises concerns about the reliability and trustworthiness of these models. To address this issue, explainable AI (XAI) techniques have been developed to provide insights into how these models work. One such technique is LIME (Local Interpretable Model-agnostic Explanations), which generates explanations for individual predictions by approximating the behavior of the model locally. In this paper, we proposed a novel approach that combines LIME with AI models for predicting disease from patient symptoms. We also applied Recursive Feature Elimination with Cross Validation (RFECV) to diagnose disease from less features. We have shown that this approach provides almost accurate predictions and interpretable explanations for those predictions. The prediction accuracy of 91.57%, 99.59%, 99.59%, 99.59%, 99.59%, and 99.59% have been achieved for Logistic regression, Decision tree, Random forest, Adaboost classifier, Gradient boosting, and Light gradient boosted machine models respectively. Our results suggest that the proposed approach has the potential to improve the trustworthiness and reliability of AI models for predicting disease from patient symptoms.
ER  - 

TY  - CONF
TI  - Integrating Ontology and Machine Learning for Enhanced Decision Support in Thyroid Disease Prediction
T2  - 2023 International Conference on Decision Aid Sciences and Applications (DASA)
SP  - 208
EP  - 212
AU  - S. Ouartani
AU  - N. Taleb
AU  - B. Ayoub
PY  - 2023
KW  - Decision support systems
KW  - Closed box
KW  - Machine learning
KW  - Ontologies
KW  - Predictive models
KW  - Decision trees
KW  - Medical diagnosis
KW  - Medical Decision Support Systems
KW  - Thyroid Prediction
KW  - Machine Learning
KW  - Decision Tree
KW  - Ontology
DO  - 10.1109/DASA59624.2023.10286675
JO  - 2023 International Conference on Decision Aid Sciences and Applications (DASA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Decision Aid Sciences and Applications (DASA)
Y1  - 16-17 Sept. 2023
AB  - In recent years, decision support systems have emerged as valuable tools in the field of medical diagnosis and prediction. With the increasing prevalence of thyroid diseases, accurate and timely prediction plays a crucial role in improving patient outcomes. However, the adoption of traditional black-box machine learning models for thyroid disease prediction is hindered by their lack of interpretability, making it challenging for clinicians to trust and understand the decision-making process. Our innovative approach combines machine learning, specifically decision trees, with ontology integration for interpretability. The ontology classifier provides insights into the model's decision-making process. The dataset used comprises 9172 observations, each represented by 31 features, accessible for download on Kaggle. According to the results The ontology model surpassed Decision tree model.
ER  - 

TY  - CONF
TI  - Clinical Research Protocol, Data Management, and Analysis Model for Epidermolysis bullosa (EB) Biomedical Data Acquisitions Towards Precision Medicine of Indonesian Patients
T2  - 2023 3rd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)
SP  - 28
EP  - 33
AU  - I. Syafarina
AU  - M. Mazaya
AU  - A. Indrawati
AU  - S. Widhiati
AU  - I. W. Aditya Swardiana
AU  - D. Delima
PY  - 2023
KW  - Ethics
KW  - Protocols
KW  - Microorganisms
KW  - Precision medicine
KW  - Genetics
KW  - Skin
KW  - Bioinformatics
KW  - Epidermolysis bullosa
KW  - Clinical protocol design
KW  - Clinical data management
KW  - Clinical data analysis
KW  - bioinformatics
DO  - 10.1109/ICICyTA60173.2023.10428699
JO  - 2023 3rd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 3rd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)
Y1  - 13-15 Dec. 2023
AB  - Epidermolysis bullosa (EB), a rare genetic skin disorder, poses significant challenges in treatment management due to its complexity and rarity. In response to the pressing need for more precise treatment approaches, this study presents a robust research protocol, comprehensive data management, and an analytical model for collecting biomedical data from Indonesian EB patients and managing it with the SRIKANDI and InNA. The study, ethically approved by the BRIN Ethical Clearance Commission, aims to fill a critical gap in the skin microbiome data specific to Indonesian EB patients. The proposed methodology for data analysis involves a multidisciplinary approach, integrating bioinformatics and metagenomic analysis to explore the microbiome profile in EB patients. The anticipated results are expected to provide insights into a complex interplay between genetics, microbiome, and clinical manifestations, paving the way for advancements in precision medicine for EB patients. This paper outlines the regulatory framework, research design, population, and sample criteria, and details of data management and analysis. By presenting this research protocol, we aim to contribute to more appropriate treatment toward precision medicine for EB patients in the near future and can be the pilot study of similar research.
ER  - 

TY  - CONF
TI  - Implementing Deep Learning Techniques into Healthcare System Improvement
T2  - 2023 Second International Conference on Electronics and Renewable Systems (ICEARS)
SP  - 1415
EP  - 1419
AU  - H. K. Kochhar
AU  - M. Kilaru
AU  - N. Aljohani
AU  - H. Rai Goyal
AU  - D. Singh
AU  - D. J. Bahadur Saini
PY  - 2023
KW  - Technological innovation
KW  - Renewable energy sources
KW  - Medical services
KW  - Transforms
KW  - System improvement
KW  - Writing
KW  - Software
KW  - Keywords: Deep learning
KW  - techniques
KW  - Technology
KW  - Healthcare
KW  - AutoML
DO  - 10.1109/ICEARS56392.2023.10085499
JO  - 2023 Second International Conference on Electronics and Renewable Systems (ICEARS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 Second International Conference on Electronics and Renewable Systems (ICEARS)
Y1  - 2-4 March 2023
AB  - Artificial intelligence (AI) has the potential to transform clinical decision-making and assistance. In any case, the utilization of current AI methods remains only as a supportive solution to make clinical decisions. The existing methodologies fail to perform in-depth analysis to deliver accurate predictions. This study reviews the integration of advanced technologies AutoML in healthcare sector by considering the data privacy, heterogeneity and interpretability. This review will be a useful resource for medical practitioners interested in applying information science methods to their field of study.
ER  - 

TY  - CONF
TI  - Capabilities of Distributed Artificial Intelligence in Medicine
T2  - 2023 5th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)
SP  - 424
EP  - 429
AU  - S. Rumovskaya
AU  - A. Litvin
PY  - 2023
KW  - Automation
KW  - Distributed ledger
KW  - Decision making
KW  - Control systems
KW  - Mathematical models
KW  - Energy efficiency
KW  - Blockchains
KW  - blockchain
KW  - artificial intelligence
KW  - healthcare
KW  - medicine
KW  - decentralized decision-making
DO  - 10.1109/SUMMA60232.2023.10349394
JO  - 2023 5th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 5th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)
Y1  - 8-10 Nov. 2023
AB  - Blockchain and artificial intelligence (AI) are two of the most disruptive modern technologies. A blockchain is a distributed ledger capable of storing data in blocks. The data stored on a blockchain is immutable and secured with cryptography. The blockchain can guarantee data security without the involvement of a third party. The integration of it with AI to create distributed artificial intelligence (DAI) is increasingly being used in various areas of human activity. AI can increase the efficiency of blockchains by streamlining computations and processes, reducing the burden on miners and decreasing latency. The latter results in faster transactions and reduced carbon footprint of the blockchain technology. Blockchain can help develop explainable AI by means of accessing an immutable record of unstructured medical data and processes used by the system in its decision-making process. The purpose of this narrative review is to analyze the possibilities of using a decentralized AI in medicine.
ER  - 

TY  - CONF
TI  - AI Ethics in Healthcare - A Survey
T2  - 2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)
SP  - 826
EP  - 833
AU  - B. Singh
AU  - M. A. B. U. Rahim
AU  - S. Hussain
AU  - M. A. Rizwan
AU  - J. Zhao
PY  - 2023
KW  - Surveys
KW  - Ethics
KW  - Collaboration
KW  - Medical services
KW  - Weaving
KW  - Trajectory
KW  - Artificial intelligence
KW  - AI ethics
KW  - healthcare
KW  - ethical considerations
KW  - data privacy
KW  - bias mitigation
KW  - human-AI collaboration
DO  - 10.1109/QRS-C60940.2023.00086
JO  - 2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)
IS  - 
SN  - 2693-9371
VO  - 
VL  - 
JA  - 2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security Companion (QRS-C)
Y1  - 22-26 Oct. 2023
AB  - In the realm of healthcare, the exponential growth of Artificial Intelligence has precipitated a need to scrutinize its ethical implications. This research undertakes a comprehensive survey to unravel the intricate tapestry of AI ethics within the healthcare landscape. Objective is to delineate the multifaceted challenges that arise from the symbiotic relationship between AI and healthcare and proposing viable solutions for mitigation. A pivotal focus of this study is to bridge the divide between medical practitioners and AI developers, thus addressing a conspicuous research gap. This gap pertains to fostering seamless collaboration between these stakeholders, ensuring that AI systems align with the actual requirements of healthcare providers. The paper explores strategies to establish an effective dialogue, facilitating the design and implementation of ethically sound AI applications. The paper also delves into the moral conundrums engendered by AI's lack of emotional intelligence in sensitive healthcare contexts. The absence of human emotional comprehension has, in certain instances, led to grievous outcomes, necessitating a nuanced approach to machine autonomy. This study advocates for an equilibrium where intelligent machines operate under prudent human oversight, striking a harmonious balance between precision and compassion. Furthermore, the research evaluates prevailing systems and their attendant challenges, emphasizing the advantages of integrating ethically guided, intelligent systems. The paper contemplates governance structures, protocols and strategies to counteract biases inherent in AI algorithms. By dissecting the principles of fairness, accountability and transparency, this study paves the way for a cogent framework that governs AI deployment within healthcare. In essence, this paper charts an uncharted course through the unexplored terrain of AI ethics in healthcare. It not only recognizes the inherent challenges but also underscores the imperative for ethical introspection. The insights herein have far reaching implications for shaping a future where AI and healthcare coalesce ethically, ultimately benefiting both patients and practitioners.
ER  - 

TY  - CONF
TI  - Employing Nudge Theory and Persuasive Principles with Explainable AI in Clinical Decision Support
T2  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 2983
EP  - 2989
AU  - S. P. Erdeniz
AU  - T. N. Trang Tran
AU  - A. Felfernig
AU  - S. Lubos
AU  - M. Schrempf
AU  - D. Kramer
AU  - P. P. Rainer
PY  - 2023
KW  - Decision support systems
KW  - Costs
KW  - Biological system modeling
KW  - Medical services
KW  - Biomedical measurement
KW  - Bioinformatics
KW  - Faces
KW  - Clinical Decision Support Systems
KW  - Machine-Learning
KW  - Predictive Modelling
KW  - Recommender Systems
KW  - Explanations
KW  - Explainable AI
KW  - Persuasive Techniques
KW  - Influential Principles
DO  - 10.1109/BIBM58861.2023.10385315
JO  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 2156-1133
VO  - 
VL  - 
JA  - 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 5-8 Dec. 2023
AB  - Persuasive XAI, also known as Persuasive Explainable Artificial Intelligence, refers to the combination of two concepts: explainable artificial intelligence (XAI) and persuasive technology. Persuasive XAI can be applied in decision-support systems in the healthcare domain in various ways to enhance the perception of patients about the outcomes, encourage adherence to treatment plans, and improve overall patient or clinician engagement. In this paper, we aim to analyze state-of-the-art persuasion strategies used in XAI and discuss how to apply them in Clinical Decision Support Systems (CDSS) to support clinicians and patients in making healthcare-related decisions.
ER  - 

TY  - CONF
TI  - An explainable and trustworthy AI framework for federated learning: a case study in rare autoimmune diseases*
T2  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
SP  - 35
EP  - 36
AU  - V. C. Pezoulas
AU  - A. Goules
AU  - A. G. Tzioufas
AU  - D. I. Fotiadis
PY  - 2023
KW  - Training
KW  - Law
KW  - Europe
KW  - Medical services
KW  - Boosting
KW  - Artificial intelligence
KW  - Task analysis
DO  - 10.1109/IEEECONF58974.2023.10404160
JO  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology
Y1  - 7-9 Dec. 2023
AB  - Nowadays, the existence of data silos obscures any advancements in healthcare. On the other hand, conventional centralized data analysis faces numerous obstacles, including legal issues, reduced levels of trustworthiness and lack of data interoperability. Federated artificial intelligence (AI) is an emerging strategy which enables collaborative model training across multiple centers without the need to centralize sensitive patient data. However, the biases which are introduced during the training process combined with the need to install equipment on premises remain underexplored. In this work, we propose a trustworthy, cloud-based AI framework, where federated implementations of high-performance boosting classifiers with hybrid loss functions were developed to solve supervised learning tasks and to provide interpretable risk factors. A case study was conducted to solve an unmet mucosa-associated lymphoid tissue (MALT) lymphoma classification problem by utilizing a PanEuropean data hub with 4805 patients with primary Sjogren’s Syndrome (pSS) (21 European cohorts). Our results highlight the performance of the federated boosting classifiers (0.9 AUC) along with explainable risk factors.Clinical Relevance: The proposed framework was utilized to develop robust MALT lymphoma classifiers for pSS patients by utilizing a PanEuropean data hub with 21 European cohorts across diverse geographical sites yielding favorable classification accuracy along with explainable risk factors for prevention.
ER  - 

TY  - CONF
TI  - Systems Analysis of Bias and Risk in AI-Enabled Medical Diagnosis
T2  - 2023 IEEE Symposium Series on Computational Intelligence (SSCI)
SP  - 1800
EP  - 1807
AU  - N. Moghadasi
AU  - M. Piran
AU  - S. Baek
AU  - R. S. Valdez
AU  - M. D. Porter
AU  - D. Johnson
AU  - J. H. Lambert
PY  - 2023
KW  - Ethics
KW  - Databases
KW  - NIST
KW  - Prediction algorithms
KW  - Risk management
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - Algorithm ethics
KW  - computational intelligence
KW  - social impacts
KW  - legal implications
KW  - risk management
KW  - scenario-based preferences
KW  - cardiac sarcoidosis
KW  - disease progression
DO  - 10.1109/SSCI52147.2023.10371919
JO  - 2023 IEEE Symposium Series on Computational Intelligence (SSCI)
IS  - 
SN  - 2472-8322
VO  - 
VL  - 
JA  - 2023 IEEE Symposium Series on Computational Intelligence (SSCI)
Y1  - 5-8 Dec. 2023
AB  - AI technologies have made significant advancements across various sectors, especially healthcare. Although AI algorithms in healthcare showcase remarkable predictive capabilities, apprehensions have emerged owing to errors, biases, and a lack of transparency. These concerns have led to a decline in trust among clinicians and patients, while also posing the risk of further accentuating pre-existing biases against marginalized groups and exacerbating inequities. This paper presents a scenario-based preferences risk register11Denotes a methodically arranged document or database detailing potential risks linked to particular scenarios or situations. framework for identifying and accounting AI algorithm biases in diagnosing diseases. The framework is demonstrated with a realistic case study on cardiac sarcoidosis. The framework identifies success criteria, initiatives, emergent conditions and the most and least disruptive scenarios. The success criteria align with the National Institute of Standards and Technology AI Risk Management Framework (NIST AI RMF) trustworthy AI characteristics, and the scenarios are based on various statistical/computational bias that causes algorithmic bias. The framework provides valuable guidance for leveraging AI in healthcare, enhancing objective designs, and mitigating risks by adopting a figure of merit to score the initiatives and measuring the disruptive order. By prioritizing transparency, trustworthy AI, and identifying the most and least disruptive scenarios/biases, the framework promotes responsible and effective use of AI technologies in healthcare.
ER  - 

TY  - CONF
TI  - A Review on Blockchain Technology with Artificial Intelligence
T2  - 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)
SP  - 364
EP  - 369
AU  - T. K. Bhatia
AU  - B. Naha
AU  - Y. Jain
AU  - S. Gupta
PY  - 2023
KW  - Industries
KW  - Supply chains
KW  - Blockchains
KW  - Internet of Things
KW  - Time factors
KW  - Artificial intelligence
KW  - Convergence
KW  - Blockchain
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Security
DO  - 10.1109/UPCON59197.2023.10434660
JO  - 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)
IS  - 
SN  - 2687-7767
VO  - 10
VL  - 10
JA  - 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)
Y1  - 1-3 Dec. 2023
AB  - An analysis of the convergence of blockchain and artificial intelligence (AI) technology demonstrates how these technologies can work together to revolutionize data management across a wide range of industries with their synergistic potential. To begin with, the paper discusses blockchain and artificial intelligence individually, emphasizing their respective advantages in decentralized data storage and intelligent decision-making. Blockchain-AI convergence is inevitable as both deal with data and value. After discussing the integration of blockchain and artificial intelligence, the authors present an innovative framework that takes advantage of their strengths. As a result of blockchain's immutability and transparency, data can be securely stored and shared within this framework, making it ideal for sectors such as healthcare, finance, and supply chain. As a result, the research paper highlights how blockchain and AI technologies can be transformed into transformative technologies. Using the synergistic framework presented in this paper, data management can be made more secure, transparent, and intelligent, with implications that go beyond traditional industries into emerging fields like the Internet of Things (IoT) and smart cities.
ER  - 

TY  - CONF
TI  - Extraction of Unstructured Electronic Healthcare Records using Natural Language Processing
T2  - 2023 International Conference on Networking and Communications (ICNWC)
SP  - 1
EP  - 6
AU  - S. S. Patil
AU  - V. Moorthy
PY  - 2023
KW  - Deep learning
KW  - Drugs
KW  - Measurement
KW  - Text recognition
KW  - Decision making
KW  - Text categorization
KW  - Natural language processing
KW  - Electronic Healthcare Records (EHR)
KW  - Unstructured Data
KW  - Natural Language Processing
KW  - Word Sense Disambiguation
KW  - Segmentation
KW  - word Embedding
DO  - 10.1109/ICNWC57852.2023.10127351
JO  - 2023 International Conference on Networking and Communications (ICNWC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 International Conference on Networking and Communications (ICNWC)
Y1  - 5-6 April 2023
AB  - Artificial Intelligence in the healthcare sector is becoming increasingly essential to extract huge texts for decision-making. Extraction of clinical data is a fundamental task in Medical Natural language processing. This process is still challenging through deep learning due to critical medical data, lack of interpretability, and limited availability. Text extraction from Electronic Healthcare records is crucial for improving patient care and understanding clinical decision-making. It also supports analysing the patients’ feedback and physician notes to identify areas for improvement in patients’ satisfaction and care quality. This helps in drug discovery and development through clinical data patterns. The proposed research focuses on implementing Natural language processing methods for data processing like classification and prediction, Word Sense Disambiguation, Segmentation, and word Embedding. These methods can process vast amounts of medical text data for decision support, research, and drug discovery. It can increase the possibility of identifying the patients who may at risk for certain conditions and diseases related to cancer and comparing it with their medical history. The chief aim is to provide improvised data analyses that could further improve their treatment.
ER  - 

TY  - CONF
TI  - Unleashing the Power of Machine Learning: A Precision Paradigm for Breast Cancer Subtype Classification Using Open-Source Data, with Caution on Dataset Size and Interpretability
T2  - 2023 6th International Conference on Contemporary Computing and Informatics (IC3I)
SP  - 1004
EP  - 1008
AU  - R. P. Tripathi
AU  - S. K. Khatri
AU  - D. Van Greunen
AU  - D. Ather
PY  - 2023
KW  - Measurement
KW  - Support vector machines
KW  - Logistic regression
KW  - Machine learning algorithms
KW  - Breast cancer
KW  - Medical diagnostic imaging
KW  - Random forests
KW  - Breast Cancer
KW  - Open Source
KW  - Machine learning algorithm
KW  - Precision treatment
DO  - 10.1109/IC3I59117.2023.10397969
JO  - 2023 6th International Conference on Contemporary Computing and Informatics (IC3I)
IS  - 
SN  - 
VO  - 6
VL  - 6
JA  - 2023 6th International Conference on Contemporary Computing and Informatics (IC3I)
Y1  - 14-16 Sept. 2023
AB  - This research paper introduces an innovative approach that harnesses the power of machine learning algorithms and open-source data to enhance the precision and personalization of breast cancer treatment decisions. Leveraging the publicly available Breast Cancer Wisconsin (Diagnostic) Dataset. we explore the potential of machine learning in accurately classifying breast cancer subtypes. A wide range of machine learning algorithms, including Logistic Regression, Support Vector Machines, Decision Trees, Random Forest, and Gradient Boosting, are employed to classify breast cancer subtypes. Their performance is evaluated using key metrics such as accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC). These metrics provide a comprehensive assessment of the algorithms' effectiveness in subtype classification. The results demonstrate the exceptional performance of the machine learning models, with Logistic Regression emerging as the top-performing algorithm, achieving an accuracy of 99.12%, precision of 98.61 %, recall of 100%, F1-score of 99.3%, and AUC-ROC of 99.87%. These remarkable metrics highlight the potential of machine learning in accurately identifying and distinguishing between malignant and benign breast cancer subtypes. The identified best-performing model holds promise for improving precision and personalized treatment decisions in breast cancer management.
ER  - 

TY  - CONF
TI  - Transforming Work: The Impact of Artificial Intelligence (AI) on Modern Workplace
T2  - 2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS)
SP  - 602
EP  - 607
AU  - Anurag
AU  - N. Vyas
AU  - U. Kumar Lilhore
PY  - 2023
KW  - Productivity
KW  - Ethics
KW  - Virtual assistants
KW  - Employment
KW  - Decision making
KW  - Organizations
KW  - Transforms
KW  - Artificial Intelligence (AI)
KW  - Marketplace
KW  - Revolution
KW  - Workplace
KW  - Enhancement
KW  - Modern Workplace
DO  - 10.1109/ICTACS59847.2023.10390258
JO  - 2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2023 3rd International Conference on Technological Advancements in Computational Sciences (ICTACS)
Y1  - 1-3 Nov. 2023
AB  - The impact of AI on the workplace is wide-ranging and significant. While AI can increase productivity and create new job opportunities, it also raises concerns about the displacement of human labor and the potential for unintentional bias and discrimination. The development of new skills, the assurance of transparency and accountability in the decision-making processes, and the development of rules to ensure that all facets of society share the benefits of AI are all necessary to address these issues. Modern workplace applications of Artificial Intelligence (AI) have ushered in a disruptive era that has revolutionized business practices and altered the nature of employment. This research looks at the multifaceted impact of AI on the modern workplace, as well as the promise and issues. Robotics, machine learning, and other AI-related technologies have improved quickly, enabling organizations to automate repetitive jobs, speed up workflows, and augment human capacity. Consequently, organizations have seen enhanced efficiency, production, and cost savings. Customer service has improved thanks to AI-powered chatbots and virtual assistants, while predictive analytics has enhanced decision-making by extracting insights from enormous amounts of data.
ER  - 

TY  - JOUR
TI  - EXplainable AI for Decision Support to Obesity Comorbidities Diagnosis
T2  - IEEE Access
SP  - 107767
EP  - 107782
AU  - G. V. Aiosa
AU  - M. Palesi
AU  - F. Sapuppo
PY  - 2023
KW  - Diseases
KW  - Diabetes
KW  - Heart
KW  - Artificial intelligence
KW  - Pathology
KW  - Obesity
KW  - Predictive models
KW  - Decision support systems
KW  - Clinical diagnosis
KW  - Machine learning
KW  - Clinical decision support system
KW  - explainable artificial intelligence
KW  - multi-node graph
KW  - machine learning
KW  - obesity comorbidity
KW  - predictive models
DO  - 10.1109/ACCESS.2023.3320057
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 11
VL  - 11
JA  - IEEE Access
Y1  - 2023
AB  - This paper describes the implementation of a comprehensive clinical decision support system (CDSS) for the risk factors prediction of comorbidities related to obesity and for the characterization of indirect connections between such comorbidities and non-communicable diseases. In particular, the direct correlation between obesity, diabetes, cardiovascular, and heart disease is analyzed by using machine learning (ML) predictive models, while the connection of the co-occurring disorders to the numerous additional non-communicable diseases is analyzed via a graph-based user interface. The CDSS here proposed is, therefore, structured with three main components: ML predictive models based on publicly available datasets, explainable artificial intelligence (XAI) local and global model interpretation, and graph-based representation of non-communicable disease connections. Multiple ML models are presented for risk assessment and a comparison is carried out based on performance key performance indicators. The best-performing model for each disease was proved to be: the multi-layer perceptron for diabetes and heart disease, and extreme gradient boosting for cardiovascular disease. Comorbidities risk factor prediction and a XAI local model explanation is performed on significant case studies. In addition, XAI global model interpretation is given for the entire dataset providing insights on the features’ contribution to the models’ implementation. Moreover, the graph-based visualization of indirect disease co-occurrence is performed by filtering connections according to different relative risk factor thresholds. This interface can be exploited by healthcare professionals to obtain, according to the needs and the clinical approach, a global perspective on obesity and its associated pathologies prevention as well as long-term treatment and care provision.
ER  - 

