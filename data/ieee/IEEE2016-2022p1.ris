TY  - JOUR
TI  - A Systematic Review of Human–Computer Interaction and Explainable Artificial Intelligence in Healthcare With Artificial Intelligence Techniques
T2  - IEEE Access
SP  - 153316
EP  - 153348
AU  - M. Nazar
AU  - M. M. Alam
AU  - E. Yafi
AU  - M. M. Su’ud
PY  - 2021
KW  - Artificial intelligence
KW  - Medical services
KW  - Human computer interaction
KW  - Usability
KW  - Security
KW  - Data models
KW  - Systematics
KW  - Artificial intelligence
KW  - deep learning
KW  - explainable artificial intelligence
KW  - healthcare
KW  - human-computer interaction
KW  - human-centered design
KW  - machine learning
KW  - usability
KW  - user-centered design
DO  - 10.1109/ACCESS.2021.3127881
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - Artificial intelligence (AI) is one of the emerging technologies. In recent decades, artificial intelligence (AI) has gained widespread acceptance in a variety of fields, including virtual support, healthcare, and security. Human-Computer Interaction (HCI) is a field that has been combining AI and human-computer engagement over the past several years in order to create an interactive intelligent system for user interaction. AI, in conjunction with HCI, is being used in a variety of fields by employing various algorithms and employing HCI to provide transparency to the user, allowing them to trust the machine. The comprehensive examination of both the areas of AI and HCI, as well as their subfields, has been explored in this work. The main goal of this article was to discover a point of intersection between the two fields. The understanding of Explainable Artificial Intelligence (XAI), which is a linking point of HCI and XAI, was gained through a literature review conducted in this research. The literature survey encompassed themes identified in the literature (such as XAI and its areas, major XAI aims, and XAI problems and challenges). The study’s other major focus was on the use of AI, HCI, and XAI in healthcare. The poll also addressed the shortcomings in XAI in healthcare, as well as the field’s future potential. As a result, the literature indicates that XAI in healthcare is still a novel subject that has to be explored more in the future.
ER  - 

TY  - CONF
TI  - Interpretable Machine Learning in Healthcare
T2  - 2018 IEEE International Conference on Healthcare Informatics (ICHI)
SP  - 447
EP  - 447
AU  - M. A. Ahmad
AU  - A. Teredesai
AU  - C. Eckert
PY  - 2018
KW  - Machine learning
KW  - Machine learning algorithms
KW  - Prediction algorithms
KW  - Tutorials
KW  - Predictive models
KW  - Cancer
KW  - interpretable machine learning
KW  - explainable artificial intelligence
DO  - 10.1109/ICHI.2018.00095
JO  - 2018 IEEE International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2018 IEEE International Conference on Healthcare Informatics (ICHI)
Y1  - 4-7 June 2018
AB  - This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.
ER  - 

TY  - CONF
TI  - Explainable AI in Healthcare
T2  - 2020 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)
SP  - 1
EP  - 2
AU  - U. Pawar
AU  - D. O’Shea
AU  - S. Rea
AU  - R. O’Reilly
PY  - 2020
KW  - Medical services
KW  - Predictive models
KW  - Analytical models
KW  - Biomedical monitoring
KW  - Computational modeling
KW  - Data models
KW  - Explainable AI
KW  - Smart healthcare
KW  - Personalised Connected Healthcare
DO  - 10.1109/CyberSA49311.2020.9139655
JO  - 2020 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)
Y1  - 15-19 June 2020
AB  - Artificial Intelligence (AI) is an enabling technology that when integrated into healthcare applications and smart wearable devices such as Fitbits etc. can predict the occurrence of health conditions in users by capturing and analysing their health data. The integration of AI and smart wearable devices has a range of potential applications in the area of smart healthcare but there is a challenge in the black box operation of decisions made by AI models which have resulted in a lack of accountability and trust in the decisions made. Explainable AI (XAI) is a domain in which techniques are developed to explain predictions made by AI systems. In this paper, XAI is discussed as a technique that can used in the analysis and diagnosis of health data by AI-based systems and a proposed approach presented with the aim of achieving accountability. transparency, result tracing, and model improvement in the domain of healthcare.
ER  - 

TY  - CONF
TI  - Explainable AI in Drug Sensitivity Prediction on Cancer Cell Lines
T2  - 2022 International Conference on Emerging Trends in Smart Technologies (ICETST)
SP  - 1
EP  - 5
AU  - I. S. Gillani
AU  - M. Shahzad
AU  - A. Mobin
AU  - M. R. Munawar
AU  - M. U. Awan
AU  - M. Asif
PY  - 2022
KW  - Drugs
KW  - Deep learning
KW  - Sensitivity
KW  - Precision medicine
KW  - Genomics
KW  - Predictive models
KW  - Market research
KW  - Drug sensitivity
KW  - Drug similarity
KW  - Cell lines
KW  - Explainable AI
KW  - Personalized drugs
DO  - 10.1109/ICETST55735.2022.9922931
JO  - 2022 International Conference on Emerging Trends in Smart Technologies (ICETST)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Emerging Trends in Smart Technologies (ICETST)
Y1  - 23-24 Sept. 2022
AB  - Explainable Artificial Intelligence (XAI) is a field that develops ways to explain predictions made by AI models. In this paper XAI which is a multifaceted approach is discussed which is capable of defining the value of features while producing predictions. Precision medicine and the forecast of cancer’s reaction to a specific treatment or drug efficiency is an area of active research. Drug sensitivity forecasting on massive genomics data is a strenuous process in drug discovery. However, drug personalization on the other hand is a tedious and arduous matter. Explainable AI is one of the many properties that instills confidence and dependency in AI systems which is why more attention needs to be paid to XAI. This research is a step toward a more profound understanding of deep learning techniques [1] on gene expressions and drug chemical structures.
ER  - 

TY  - CONF
TI  - Diabetes prognosis using white-box machine learning framework for interpretability of results
T2  - 2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)
SP  - 1501
EP  - 1506
AU  - P. F. Khan
AU  - K. Meehan
PY  - 2021
KW  - Machine learning
KW  - Medical services
KW  - Predictive models
KW  - Diabetes
KW  - Glucose
KW  - Prognostics and health management
KW  - Diseases
KW  - White box
KW  - Diabetes
KW  - Machine Learning
KW  - LIME
KW  - Explainable AI
KW  - Pima Indians
DO  - 10.1109/CCWC51732.2021.9375927
JO  - 2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)
Y1  - 27-30 Jan. 2021
AB  - Artificial intelligence solutions in the healthcare sector are a fundamental phenomenon. It has enabled medical practitioners to perform high quality and precision treatments to prevent diseases or cure a patient. While it is essential to use such solutions, it is also more important to make these solutions transparent to medical professionals. Doctors rely on the cause behind a prognosis rather than just the binary result. This study provides an insight into the feasibility and importance of explainable artificial intelligence solutions for the healthcare sector. A case-study on diabetes in Pima Indian females aids this research motive. The study has maintained good explainability of the predictions and high accuracy by the machine learning models used. This study used a white-box machine learning framework, local interpretable model-agnostic explanations, to prove the cause. The framework successfully interpreted case-by-case predictions of some machine learning models. The machine learning models, while being interpretable, also provided high accuracy in prediction. The highest accuracy, 80.5%, was shown by a random forest model. The study found out glucose levels as the most contributing factors for the outcome of diabetes. The results from this study can be used by researchers to reevaluate their position on white-box machine-learning solutions in the healthcare sector.
ER  - 

TY  - JOUR
TI  - Explainable AI for Healthcare 5.0: Opportunities and Challenges
T2  - IEEE Access
SP  - 84486
EP  - 84517
AU  - D. Saraswat
AU  - P. Bhattacharya
AU  - A. Verma
AU  - V. K. Prasad
AU  - S. Tanwar
AU  - G. Sharma
AU  - P. N. Bokoro
AU  - R. Sharma
PY  - 2022
KW  - Medical services
KW  - Artificial intelligence
KW  - Predictive models
KW  - Analytical models
KW  - Prediction algorithms
KW  - Medical diagnostic imaging
KW  - Deep learning
KW  - Explainable AI
KW  - healthcare 50
KW  - metrics
KW  - deep learning
DO  - 10.1109/ACCESS.2022.3197671
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 10
VL  - 10
JA  - IEEE Access
Y1  - 2022
AB  - In the healthcare domain, a transformative shift is envisioned towards Healthcare 5.0. It expands the operational boundaries of Healthcare 4.0 and leverages patient-centric digital wellness. Healthcare 5.0 focuses on real-time patient monitoring, ambient control and wellness, and privacy compliance through assisted technologies like artificial intelligence (AI), Internet-of-Things (IoT), big data, and assisted networking channels. However, healthcare operational procedures, verifiability of prediction models, resilience, and lack of ethical and regulatory frameworks are potential hindrances to the realization of Healthcare 5.0. Recently, explainable AI (EXAI) has been a disruptive trend in AI that focuses on the explainability of traditional AI models by leveraging the decision-making of the models and prediction outputs. The explainability factor opens new opportunities to the black-box models and brings confidence in healthcare stakeholders to interpret the machine learning (ML) and deep learning (DL) models. EXAI is focused on improving clinical health practices and brings transparency to the predictive analysis, which is crucial in the healthcare domain. Recent surveys on EXAI in healthcare have not significantly focused on the data analysis and interpretation of models, which lowers its practical deployment opportunities. Owing to the gap, the proposed survey explicitly details the requirements of EXAI in Healthcare 5.0, the operational and data collection process. Based on the review method and presented research questions, systematically, the article unfolds a proposed architecture that presents an EXAI ensemble on the computerized tomography (CT) image classification and segmentation process. A solution taxonomy of EXAI in Healthcare 5.0 is proposed, and operational challenges are presented. A supported case study on electrocardiogram (ECG) monitoring is presented that preserves the privacy of local models via federated learning (FL) and EXAI for metric validation. The case-study is supported through experimental validation. The analysis proves the efficacy of EXAI in health setups that envisions real-life model deployments in a wide range of clinical applications.
ER  - 

TY  - CONF
TI  - Explainable AI for Breast Cancer Diagnosis: Application and User’s Understandability Perception
T2  - 2022 International Conference on Electrical, Computer and Energy Technologies (ICECET)
SP  - 1
EP  - 6
AU  - R. Larasati
PY  - 2022
KW  - Medical services
KW  - Machine learning
KW  - Breast cancer
KW  - Artificial intelligence
DO  - 10.1109/ICECET55527.2022.9872950
JO  - 2022 International Conference on Electrical, Computer and Energy Technologies (ICECET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Electrical, Computer and Energy Technologies (ICECET)
Y1  - 20-22 July 2022
AB  - With the current progress of how Artificial Intelligence (AI) is implemented in different fields, the concerns on AI technologies’ accountability, transparency, trust, and social acceptability are also raised. These concerns become even bigger when people’s well being is at stake, as in the case of AI applications implemented in healthcare. Explainable AI (XAI) or AI explanation algorithms are proposed to solve the accountability and transparency problems. However, how users perceived algorithm explanation have not yet been explored extensively. In this paper, Explainable AI approaches were implemented to the specific case in healthcare: Breast Cancer. An online survey was conducted to investigate users’ perception and understanding of the AI explanation algorithms: LIME and Anchors. We were looking at the users’ perception of Explainable AI, specifically non-expert users, and found that users’ perceived understanding was high even though the majority did not understand the explanation.
ER  - 

TY  - CONF
TI  - Explainable A.I. in Healthcare
T2  - 2022 International Conference on Computational Science and Computational Intelligence (CSCI)
SP  - 1725
EP  - 1729
AU  - H. Rao
AU  - A. Khan
PY  - 2022
KW  - Industries
KW  - Automation
KW  - Scientific computing
KW  - Neural networks
KW  - Decision making
KW  - Closed box
KW  - Medical services
KW  - XAI
KW  - Explainable AI
KW  - XAI in Healthcare
KW  - LIME
KW  - SHaP
KW  - Anchor
DO  - 10.1109/CSCI58124.2022.00306
JO  - 2022 International Conference on Computational Science and Computational Intelligence (CSCI)
IS  - 
SN  - 2769-5654
VO  - 
VL  - 
JA  - 2022 International Conference on Computational Science and Computational Intelligence (CSCI)
Y1  - 14-16 Dec. 2022
AB  - Explainable AI in healthcare is an emerging field with enormous potential to change the structure of the modern healthcare industry. The automation of assistive healthcare and its evolution with the help of ML algorithms and Deep neural nets will play an essential role in advancing the current healthcare infrastructure in the future. Explainable AI can help solve the lack of trust in these near blackbox neural networks in an industry like healthcare where the decision making process to achieve the result holds as much importance as the results themselves. Explainable AI models like LIME, SHAM, decision sets, etc plays a vital role in developing in depth understanding of a prediction model and the factors that lead to a predictions made by it, with this constant learning and feedback not only is it possible to create a transparent model but also improve on the results with human feedback to fine tune the model for a higher degree of accuracy than before.
ER  - 

TY  - STD
TI  - IEEE Approved Draft Standard for Transparency of Autonomous Systems
T2  - IEEE P7001/D4, October 2021
SP  - 1
EP  - 75
PY  - 2021
KW  - IEEE Standards
KW  - Autonomous systems
KW  - Artificial intelligence
KW  - Ethics
KW  - autonomous systems
KW  - artificial intelligence
KW  - ethics
KW  - IEEE 7001
KW  - transparency
DO  - 
JO  - IEEE P7001/D4, October 2021
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE P7001/D4, October 2021
Y1  - 13 Dec. 2021
AB  - Measurable, testable levels of transparency, so that autonomous systems can be objectively assessed, and levels of compliance determined, are described in this standard.
ER  - 

TY  - STD
TI  - IEEE Draft Standard for Transparency of Autonomous Systems
T2  - IEEE P7001/D2, June 2021
SP  - 1
EP  - 73
PY  - 2021
KW  - IEEE Standards
KW  - Autonomous systems
KW  - Ethics
KW  - Artificial intelligence
KW  - autonomous systems
KW  - artificial intelligence
KW  - ethics
KW  - IEEE 7001
KW  - transparency
DO  - 
JO  - IEEE P7001/D2, June 2021
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE P7001/D2, June 2021
Y1  - 9 June 2021
AB  - Measurable, testable levels of transparency, so that autonomous systems can be objectively assessed, and levels of compliance determined, are described in this standard.
ER  - 

TY  - JOUR
TI  - From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where
T2  - IEEE Transactions on Industrial Informatics
SP  - 5031
EP  - 5042
AU  - I. Ahmed
AU  - G. Jeon
AU  - F. Piccialli
PY  - 2022
KW  - Fourth Industrial Revolution
KW  - Artificial intelligence
KW  - Industries
KW  - Hidden Markov models
KW  - Manufacturing
KW  - Service robots
KW  - Robots
KW  - Artificial intelligence (AI)
KW  - cloud computing
KW  - cyber-physical system
KW  - explainable artificial intelligence (XAI)
KW  - Industry 4.0
KW  - Internet of Things (IoT)
DO  - 10.1109/TII.2022.3146552
JO  - IEEE Transactions on Industrial Informatics
IS  - 8
SN  - 1941-0050
VO  - 18
VL  - 18
JA  - IEEE Transactions on Industrial Informatics
Y1  - Aug. 2022
AB  - Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications.
ER  - 

TY  - CONF
TI  - Role of Artificial Intelligence in Medical Predictions, Interventions and Quality of Life
T2  - 2021 7th International Conference on Systems and Informatics (ICSAI)
SP  - 1
EP  - 4
AU  - M. Ivanovic
PY  - 2021
KW  - Analytical models
KW  - Medical treatment
KW  - Data visualization
KW  - Machine learning
KW  - Predictive models
KW  - Data models
KW  - Informatics
KW  - Artificial intelligence
KW  - Personalization
KW  - Health related quality of life indicators
DO  - 10.1109/ICSAI53574.2021.9664199
JO  - 2021 7th International Conference on Systems and Informatics (ICSAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 7th International Conference on Systems and Informatics (ICSAI)
Y1  - 13-15 Nov. 2021
AB  - This short paper is focused on giving a brief summary on recent methodologies in assembling, managing, and utilizing massive data in medicine and health that are highly supported by robust practices and advances that Artificial Intelligence offers in contemporary research. The presented considerations are oriented towards facilitating improvements of patients' health parameters and quality of life indicators. Two typical cases are presented inspired by authors' enduring approaches, applications and investigations in using general Artificial Intelligence methods and particularly Machine Learning, and Agent Technologies. In last decade the author and his research group have been developing several high quality solutions with particular attention towards medical areas.
ER  - 

TY  - STD
TI  - IEEE Draft Standard for Transparency of Autonomous Systems
T2  - IEEE P7001/D3, September 2021
SP  - 1
EP  - 75
PY  - 2021
KW  - IEEE Standards
KW  - Autonomous systems
KW  - Artificial intelligence
KW  - Intelligent transportation systems
KW  - autonomous systems
KW  - artificial intelligence
KW  - ethics
KW  - IEEE 7001
KW  - transparency
DO  - 
JO  - IEEE P7001/D3, September 2021
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE P7001/D3, September 2021
Y1  - 27 Sept. 2021
AB  - Measurable, testable levels of transparency, so that autonomous systems can be objectively assessed, and levels of compliance determined, are described in this standard.
ER  - 

TY  - CONF
TI  - Automated and interpretable m-health discrimination of vocal cord pathology enabled by machine learning
T2  - 2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
SP  - 1
EP  - 6
AU  - N. Seedat
AU  - V. Aharonson
AU  - Y. Hamzany
PY  - 2020
KW  - Performance evaluation
KW  - Pathology
KW  - Endoscopes
KW  - Machine learning
KW  - Medical services
KW  - Feature extraction
KW  - Acoustics
KW  - explainable AI (XAI)
KW  - machine learning
KW  - m-health
KW  - vocal pathology
DO  - 10.1109/CSDE50874.2020.9411529
JO  - 2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
Y1  - 16-18 Dec. 2020
AB  - Clinical methods that assess voice pathologies are typically based on laryngeal endoscopy or audio-perceptual assessment. Both methods have limited accessibility in low-resourced healthcare settings. M-health systems can provide a quantitative assessment and improve early detection in a patient centered care. Automated methods for voice pathologies assessment apply machine learning methods to acoustic, frequency and noise features extracted from sustained phonation recordings and aim to discriminate pathological voices from controls. The machine learning methods in this study are applied to a discriminating between two prevalent vocal pathologies: vocal cord polyp and vocal cord paralysis. The data was acquired by a low-cost recording device in an experiment at a tertiary medical center and the pathologies were clinically labeled. Acoustic and spectral features were extracted and multiple classifiers compared using batched cross validation. The best classifiers were tree-based classifiers, with the Extra Trees classifier providing the best performance with an accuracy of 0.9565 and F1-score of 0.9130. Explainable AI (XAI) and feature interpretability analysis was carried out to allow clinicians to use the features marked as important to clinical care and planning. The most important features were octave-based spectral contrast and MFCCs 0 to 3. The results indicate a feasibility of machine learning to accurately discriminate between different types of vocal cord pathologies.
ER  - 

TY  - CONF
TI  - A Comparison of Explanations Given by Explainable Artificial Intelligence Methods on Analysing Electronic Health Records
T2  - 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)
SP  - 1
EP  - 4
AU  - J. Duell
AU  - X. Fan
AU  - B. Burnett
AU  - G. Aarts
AU  - S. -M. Zhou
PY  - 2021
KW  - Machine learning algorithms
KW  - Conferences
KW  - Biological system modeling
KW  - Decision making
KW  - Medical services
KW  - Machine learning
KW  - Predictive models
KW  - Explainable AI
KW  - Black-box
KW  - Glass-box
KW  - Machine Learning
KW  - Electronic Health Records
DO  - 10.1109/BHI50953.2021.9508618
JO  - 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)
IS  - 
SN  - 2641-3604
VO  - 
VL  - 
JA  - 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)
Y1  - 27-30 July 2021
AB  - eXplainable Artificial Intelligence (XAI) aims to provide intelligible explanations to users. XAI algorithms such as SHAP, LIME and Scoped Rules compute feature importance for machine learning predictions. Although XAI has attracted much research attention, applying XAI techniques in healthcare to inform clinical decision making is challenging. In this paper, we provide a comparison of explanations given by XAI methods as a tertiary extension in analysing complex Electronic Health Records (EHRs). With a large-scale EHR dataset, we compare features of EHRs in terms of their prediction importance estimated by XAI models. Our experimental results show that the studied XAI methods circumstantially generate different top features; their aberrations in shared feature importance merit further exploration from domain-experts to evaluate human trust towards XAI.
ER  - 

TY  - CONF
TI  - Demystifying Deep Learning Models for Retinal OCT Disease Classification using Explainable AI
T2  - 2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
SP  - 1
EP  - 6
AU  - T. S. Apon
AU  - M. M. Hasan
AU  - A. Islam
AU  - M. G. R. Alam
PY  - 2021
KW  - Deep learning
KW  - Computational modeling
KW  - Atmospheric modeling
KW  - Memory management
KW  - Predictive models
KW  - Retina
KW  - Real-time systems
KW  - Medical Image Processing
KW  - Explainable AI
KW  - Retinal OCT
KW  - Lime
KW  - Image Classification
KW  - Deep Neural Network
KW  - AI in Healthcare
DO  - 10.1109/CSDE53843.2021.9718400
JO  - 2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
Y1  - 8-10 Dec. 2021
AB  - In the world of medical diagnostics, the adoption of various deep learning techniques is quite common as well as effective, and its statement is equally true when it comes to implementing it into the retina Optical Coherence Tomography (OCT) sector. However, firstly, these techniques have the black box characteristics that prevent the medical professionals from completely trusting the results generated from them. Secondly, the lack of precision of these methods restricts their implementation in clinical and complex cases, and finally, the existing works and models on the OCT classification are substantially large and complicated and they require a considerable amount of memory and computational power, reducing the quality of classifiers in real-time applications. To meet these problems, in this paper a self-developed CNN model has been proposed which is comparatively smaller and simpler along with the use of Lime that introduces Explainable AI to the study and helps to increase the interpretability of the model. This addition will be an asset to the medical experts for getting major and detailed information and will help them in making final decisions and will also reduce the opacity and vulnerability of the conventional deep learning models.
ER  - 

TY  - CONF
TI  - X-ECGNet: An Interpretable DL model for Stress Detection using ECG in COVID-19 Healthcare Workers
T2  - 2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART)
SP  - 1
EP  - 5
AU  - A. Gupta
AU  - D. Kansal
AU  - V. Gupta
AU  - M. K. Shetty
AU  - M. P. Girish
AU  - M. D. Gupta
PY  - 2021
KW  - COVID-19
KW  - Heart
KW  - Deep learning
KW  - Pandemics
KW  - Hospitals
KW  - Electrocardiography
KW  - Data models
KW  - Stress Detection
KW  - Burnout
KW  - Healthcare Workers
KW  - COVID-19
KW  - ECG
KW  - Deep Learning
KW  - ECGNET
DO  - 10.1109/BioSMART54244.2021.9677750
JO  - 2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 4th International Conference on Bio-Engineering for Smart Technologies (BioSMART)
Y1  - 8-10 Dec. 2021
AB  - COVID-19 pandemic erupted in December 2019, spreading extremely fast and stretching the healthcare infras-tructure of most countries beyond their capacities. This impacted the healthcare workers (HCW) adversely because 1) they were pressured to work almost round the clock without a break; 2) they were in close contact with the COVID-19 patients and hence, were at high risk; and 3) they suffered from the fear of spreading COVID to their families. Hence, many HCWs were stressed and burnout. It is known that stress directly affects the heart and can lead to serious cardiovascular problems. Currently, stress is measured subjectively via self-declared questionnaires. Objective markers of stress are required to ascertain the quantitative impact of stress on the heart. Thus, this paper aims to detect stress contributing factors in HCWs and determine the changes in the ECG of stressed HCWs. We collected data from multiple hospitals in Northern India and developed a deep learning model, namely X-ECGNet, to detect stress. We also tried to add interpretability to the model using the recent method of SHAP analysis. Deployment of such models can help the government and hospital administrations timely detect stress in HCWs and make informed decisions to save systems from collapse during such calamities.
ER  - 

TY  - CONF
TI  - A Conceptual Review on Artificial Intelligence in Biomedical Applications
T2  - 2022 International Conference on Artificial Intelligence of Things and Crowdsensing (AIoTCs)
SP  - 16
EP  - 21
AU  - I. Muazu
AU  - F. Al-Turjman
AU  - İ. Etikan
PY  - 2022
KW  - Industries
KW  - Protocols
KW  - Crowdsensing
KW  - Organizations
KW  - Medical services
KW  - Internet of Things
KW  - History
KW  - Artificial Intelligence
KW  - Healthcare data
DO  - 10.1109/AIoTCs58181.2022.00010
JO  - 2022 International Conference on Artificial Intelligence of Things and Crowdsensing (AIoTCs)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Artificial Intelligence of Things and Crowdsensing (AIoTCs)
Y1  - 26-28 Oct. 2022
AB  - A wide range of human efforts are being impacted by AI in our daily lives. Numerous organizations, including the health sector, are also taking steps to adapt to AI technology, which can help them find new ways to complete jobs and comprehend data patterns for maximum efficiency. The main goal of this research is to assist the AI research community in fully comprehending the most well-liked AI Techniques, which are primarily utilized in the healthcare industry for effective data administration. This study focuses in particular on describing the AI Techniques on health data and expanding on their advantages and applications. Additionally, this study reviews, summarizes, and compares comparable AI technique-based protocols from the perspectives of prediction accuracy and ability. Not only to this extent, but rather based on AI performance between the years of 2020 to 2022, the history of AI research in healthcare-related data is professionally studied. We also outlined the numerous contributions to knowledge that each of the writers whose papers were thoroughly examined for this study made.
ER  - 

TY  - CONF
TI  - XAIoT - The Future of Wearable Internet of Things
T2  - 2022 18th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA)
SP  - 1
EP  - 6
AU  - R. Krzysiak
AU  - S. Nguyen
AU  - Y. Chen
PY  - 2022
KW  - Mechatronics
KW  - Machine learning algorithms
KW  - Predictive models
KW  - Internet of Things
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - Biomedical monitoring
KW  - Artificial Intelligence
KW  - XAI
KW  - IoT
KW  - Machine Learning
KW  - HMI
KW  - explainability
KW  - transparency
KW  - medicine
KW  - wearable devices
DO  - 10.1109/MESA55290.2022.10004460
JO  - 2022 18th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 18th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA)
Y1  - 28-30 Nov. 2022
AB  - The addition of Machine Learning and other Artificial Intelligent (AI) algorithms has expanded the capabilities of the Internet of Things (IoT) framework. However, the Artificial Internet of Things (AIoT), has also brought forward many issues with the combination, mainly lack of explanations and transparency. This lack of explanations of what AI models are doing is problematic in many fields, especially in the medical field. Explainable Artificial Intelligence (XAI) allows users to be given more in depth knowledge with the background processes of the model prediction. Enabling XAI into the IoT framework would develop a system that would not only capture Big Data more effectively, but also allow the system to be more transparent and explainable, causing wider adoption of the framework. This review focuses on current work done in AIoT and how it can be vastly improved with XAIoT. We propose an Explainable Artificial Intelligent Internet of Things (XAIoT) framework for monitoring physiological health using a smart watch.
ER  - 

TY  - JOUR
TI  - Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)
T2  - IEEE Access
SP  - 52138
EP  - 52160
AU  - A. Adadi
AU  - M. Berrada
PY  - 2018
KW  - Conferences
KW  - Machine learning
KW  - Market research
KW  - Prediction algorithms
KW  - Machine learning algorithms
KW  - Biological system modeling
KW  - Explainable artificial intelligence
KW  - interpretable machine learning
KW  - black-box models
DO  - 10.1109/ACCESS.2018.2870052
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 6
VL  - 6
JA  - IEEE Access
Y1  - 2018
AB  - At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.
ER  - 

TY  - CONF
TI  - Towards Trustworthy Artificial Intelligence in Healthcare
T2  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
SP  - 626
EP  - 632
AU  - C. K. Leung
AU  - E. W. R. Madill
AU  - J. Souza
AU  - C. Y. Zhang
PY  - 2022
KW  - Computer science
KW  - Computational modeling
KW  - Medical services
KW  - Predictive models
KW  - Data science
KW  - Data models
KW  - Artificial intelligence
KW  - healthcare informatics
KW  - data science
KW  - inter-pretability
KW  - explainability
KW  - explainable artificial intelligence (XAI)
KW  - trustworthy AI
KW  - human-computer interaction (HCI)
KW  - visual ana-lytics
KW  - healthcare analytics
KW  - COVID-19
DO  - 10.1109/ICHI54592.2022.00127
JO  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
Y1  - 11-14 June 2022
AB  - Healthcare informatics is an interdisciplinary area where computer science, data science, cognitive science, informatics principles, and information technology meet to address problems and support healthcare, medicine, public health, and/or everyday wellness. In many healthcare and medical applications, it is helpful to have models that can learn from historical healthcare data or instances to make predictions on future instances. For human to trust these models or to perceive these models to be trustworthy, it is equally important to build a trustworthy artificial intelligence (AI) solution. Hence, in this paper, towards trustworthy AI in healthcare, we present an explainable AI (XAI) solution that makes accurate predictions and explains the predictions. Evaluation results on real-life datasets demonstrates the effectiveness of our XAI solution towards trustworthy AI in healthcare.
ER  - 

TY  - CONF
TI  - Augmenting Decision Competence in Healthcare Using AI-based Cognitive Models
T2  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
SP  - 1
EP  - 4
AU  - N. Keller
AU  - M. A. Jenny
AU  - C. A. Spies
AU  - S. M. Herzog
PY  - 2020
KW  - Analytical models
KW  - Machine learning algorithms
KW  - Medical services
KW  - Machine learning
KW  - Tools
KW  - Predictive models
KW  - Prediction algorithms
KW  - augmented intelligence
KW  - post-operative risk stratification
KW  - decision support
KW  - fast-and-frugal trees
KW  - explainable artificial intelligence
DO  - 10.1109/ICHI48887.2020.9374376
JO  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
Y1  - 30 Nov.-3 Dec. 2020
AB  - In many critical decisions, such as medicine, transparency of the underlying decision process is critical. This extends to decision processes that are supported by artificial intelligence. Rather than using a post-hoc explainability approach from explainable AI research (SHAP or LIME), we develop and test an intrinsically transparent and intuitively interpretable model developed from cognitive science, fast-and-frugal trees, in a comparative analysis with state-of-the-art machine learning models. The resultant decision support can be easily implemented as laminated pocket card, augmenting the decision competence of physicians rather than replacing it.
ER  - 

TY  - CONF
TI  - Integrating Machine Learning with Symbolic Reasoning to Build an Explainable AI Model for Stroke Prediction
T2  - 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE)
SP  - 817
EP  - 821
AU  - N. Prentzas
AU  - A. Nicolaides
AU  - E. Kyriacou
AU  - A. Kakas
AU  - C. Pattichis
PY  - 2019
KW  - Germanium
KW  - Conferences
KW  - Bioinformatics
KW  - Biomedical engineering
KW  - argumentation, explainability, inTrees, random forests, XAI
DO  - 10.1109/BIBE.2019.00152
JO  - 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE)
IS  - 
SN  - 2471-7819
VO  - 
VL  - 
JA  - 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE)
Y1  - 28-30 Oct. 2019
AB  - Despite the recent recognition of the value of Artificial Intelligence and Machine Learning in healthcare, barriers to further adoption remain, mainly due to their "black box" nature and the algorithm's inability to explain its results. In this paper we present and propose a methodology of applying argumentation on top of machine learning to build explainable AI (XAI) models. We compare our results with Random Forests and an SVM classifier that was considered best for the same dataset in [1].
ER  - 

TY  - CONF
TI  - Recent and Future Innovative Artificial Intelligence Services and Fields
T2  - 2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)
SP  - 29
EP  - 32
AU  - M. S. Jawad
AU  - H. Mahdin
AU  - N. A. Mohammed Alduais
AU  - M. Hlayel
AU  - S. A. Mostafa
AU  - M. H. Abd Wahab
PY  - 2021
KW  - Industries
KW  - Technological innovation
KW  - Digital transformation
KW  - Finance
KW  - Medical services
KW  - Banking
KW  - Real-time systems
KW  - Artificial Intelligence (AI)
KW  - Innovative Services
KW  - Creative AI
KW  - Responsible AI
KW  - Explainable AI
KW  - emotion AI
DO  - 10.1109/ISAMSR53229.2021.9567891
JO  - 2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)
Y1  - 6-8 Sept. 2021
AB  - Recent innovative Artificial Intelligence (AI) solutions accelerate digital transformations in different fields. It is important to highlight and explore this innovative AI service in different domains so that digital transformation can be planned, designed, and implemented for maximum society benefits. This paper investigates the different fields of AI services that can be utilized towards achieving highly beneficial digital transformations in different societal domains. This includes marketing, finance and banking, healthcare industry, emotion, and creative AI, as well as recent AI fields as explainable and responsible AI. Exploring and understanding the innovative AI services in these domains widen researcher capabilities to achieve effective and highly beneficial digital transformations.
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence Applications in Cyber Security: State-of-the-Art in Research
T2  - IEEE Access
SP  - 93104
EP  - 93139
AU  - Z. Zhang
AU  - H. A. Hamadi
AU  - E. Damiani
AU  - C. Y. Yeun
AU  - F. Taher
PY  - 2022
KW  - Computer crime
KW  - Cyberattack
KW  - Computer security
KW  - Deep learning
KW  - Medical services
KW  - Malware
KW  - Intrusion detection
KW  - Artificial intelligence
KW  - Unsolicited e-mail
KW  - Information filters
KW  - Artificial intelligence
KW  - cyber security
KW  - deep learning
KW  - explanation artificial intelligence
KW  - intrusion detection
KW  - machine learning
KW  - malware detection
KW  - spam filtering
DO  - 10.1109/ACCESS.2022.3204051
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 10
VL  - 10
JA  - IEEE Access
Y1  - 2022
AB  - This survey presents a comprehensive review of current literature on Explainable Artificial Intelligence (XAI) methods for cyber security applications. Due to the rapid development of Internet-connected systems and Artificial Intelligence in recent years, Artificial Intelligence including Machine Learning (ML) and Deep Learning (DL) has been widely utilized in the fields of cyber security including intrusion detection, malware detection, and spam filtering. However, although Artificial Intelligence-based approaches for the detection and defense of cyber attacks and threats are more advanced and efficient compared to the conventional signature-based and rule-based cyber security strategies, most ML-based techniques and DL-based techniques are deployed in the “black-box” manner, meaning that security experts and customers are unable to explain how such procedures reach particular conclusions. The deficiencies of transparencies and interpretability of existing Artificial Intelligence techniques would decrease human users’ confidence in the models utilized for the defense against cyber attacks, especially in current situations where cyber attacks become increasingly diverse and complicated. Therefore, it is essential to apply XAI in the establishment of cyber security models to create more explainable models while maintaining high accuracy and allowing human users to comprehend, trust, and manage the next generation of cyber defense mechanisms. Although there are papers reviewing Artificial Intelligence applications in cyber security areas and the vast literature on applying XAI in many fields including healthcare, financial services, and criminal justice, the surprising fact is that there are currently no survey research articles that concentrate on XAI applications in cyber security. Therefore, the motivation behind the survey is to bridge the research gap by presenting a detailed and up-to-date survey of XAI approaches applicable to issues in the cyber security field. Our work is the first to propose a clear roadmap for navigating the XAI literature in the context of applications in cyber security.
ER  - 

TY  - CONF
TI  - Class Specific Interpretability in CNN Using Causal Analysis
T2  - 2021 IEEE International Conference on Image Processing (ICIP)
SP  - 3702
EP  - 3706
AU  - A. Yadu
AU  - P. K. Suhas
AU  - N. Sinha
PY  - 2021
KW  - Measurement
KW  - Location awareness
KW  - Visualization
KW  - Image color analysis
KW  - Computational modeling
KW  - Conferences
KW  - Machine learning
KW  - Causal Inference
KW  - Interpretability
KW  - CNN
KW  - Explainability
KW  - Machine Learning
DO  - 10.1109/ICIP42928.2021.9506118
JO  - 2021 IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 2381-8549
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Image Processing (ICIP)
Y1  - 19-22 Sept. 2021
AB  - A singular problem that mars the wide applicability of machine learning (ML) models is the lack of generalizability and interpretability. The ML community is increasingly working on bridging this gap. Prominent among them are methods that study causal significance of features, with techniques such as Average Causal Effect (ACE). In this paper, our objective is to utilize the causal analysis framework to measure the significance level of the features in binary classification task. Towards this, we propose a novel ACE-based metric called “Absolute area under ACE (A-ACE)” which computes the area of the absolute value of the ACE across different permissible levels of intervention. The performance of the proposed metric is illustrated on (i) ILSVRC (Imagenet) dataset and (ii) MNIST data set $(\sim 42000$ images) by considering pair-wise binary classification problem. Encouraging results have been observed on these two datasets. The computed metric values are found to be higher - peak performance of 10x higher than other for ILSVRC dataset and 50% higher than others for MNIST dataset - at precisely those locations that human intuition would mark as distinguishing regions. The method helps to capture the quantifiable metric which represents the distinction between the classes learnt by the model. This metric aids in visual explanation of the model’s prediction and thus, makes the model more trustworthy.
ER  - 

TY  - CONF
TI  - Interpretability in HealthCare A Comparative Study of Local Machine Learning Interpretability Techniques
T2  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
SP  - 275
EP  - 280
AU  - R. El Shawi
AU  - Y. Sherif
AU  - M. Al-Mallah
AU  - S. Sakr
PY  - 2019
KW  - Measurement
KW  - Predictive models
KW  - Machine learning
KW  - Testing
KW  - Diabetes
KW  - Data models
KW  - Training
KW  - Machine Learning
KW  - Black-Box Model
KW  - Machine Learning Interpretability
KW  - Model-Agnostic Interpretability
DO  - 10.1109/CBMS.2019.00065
JO  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
IS  - 
SN  - 2372-9198
VO  - 
VL  - 
JA  - 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
Y1  - 5-7 June 2019
AB  - Although complex machine learning models (e.g., Random Forest, Neural Networks) are commonly outperforming the traditional simple interpretable models (e.g., Linear Regression, Decision Tree), in the healthcare domain, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. With the new General Data Protection Regulation (GDPR), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. To tackle this challenge, recently, several machine learning interpretability techniques have been developed and introduced. In general, the main aim of these interpretability techniques is to shed light and provide insights into the predictions process of the machine learning models and explain how the model predictions have resulted. However, in practice, assessing the quality of the explanations provided by the various interpretability techniques is still questionable. In this paper, we present a comprehensive experimental evaluation of three recent and popular local model agnostic interpretability techniques, namely, LIME, SHAP and Anchors on different types of real-world healthcare data. Our experimental evaluation covers different aspects for its comparison including identity, stability, separability, similarity, execution time and bias detection. The results of our experiments show that LIME achieves the lowest performance for the identity metric and the highest performance for the separability metric across all datasets included in this study. On average, SHAP has the smallest average time to output explanation across all datasets included in this study. For detecting the bias, SHAP enables the participants to better detect the bias.
ER  - 

TY  - CONF
TI  - Interpreting and Evaluating Black Box Models in a Customizable Way
T2  - 2020 IEEE International Conference on Big Data (Big Data)
SP  - 5435
EP  - 5440
AU  - L. Fan
AU  - C. Liu
AU  - Y. Zhou
AU  - T. Zhang
AU  - Q. Yang
PY  - 2020
KW  - Deep learning
KW  - Learning (artificial intelligence)
KW  - Predictive models
KW  - Big Data
KW  - Data models
KW  - Medical diagnosis
KW  - Medical diagnostic imaging
KW  - Explainable Artificial Intelligence
KW  - Deep Learning
DO  - 10.1109/BigData50022.2020.9378081
JO  - 2020 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Big Data (Big Data)
Y1  - 10-13 Dec. 2020
AB  - A vast majority of complex deep learning models currently remain black boxes, which means that their internal working process and the logical relationships between the input data and the output predictions are hidden to human users. For financial and medical use cases, this characteristic greatly hinders the application of deep learning models because people cannot know the logic behind model decisions such as stock forecast and disease diagnosis. In this paper, we propose CMIE, namely, Customizable Model Interpretation Evaluation, which is a set of customizable evaluation methods of using the in-model and post-model information to generate multi-dimensional interpretability evaluation of the convolutional neural networks (CNNs) with different structures. The evaluation report includes the results of several interpretability criteria of the CNNs so that the user can know how well the model learns from the features in a comprehensible way. Experiments on LeNet, AlexNet and VGG-16 with MNIST and CIFAR-10 have demonstrated the practicality and effectiveness of the adopted Explainable Artificial Intelligence (XAI) method.
ER  - 

TY  - CONF
TI  - Towards Fairness and Interpretability: Clinical Decision Support for Acute Coronary Syndrome
T2  - 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)
SP  - 882
EP  - 886
AU  - H. S. Sahoo
AU  - N. E. Ingraham
AU  - G. M. Silverman
AU  - J. M. Sartori
PY  - 2022
KW  - Measurement
KW  - Machine learning algorithms
KW  - Sociology
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Boosting
KW  - machine learning
KW  - fairness
KW  - interpretability
KW  - acute coronary syndrome
KW  - healthcare
DO  - 10.1109/ICMLA55696.2022.00146
JO  - 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)
Y1  - 12-14 Dec. 2022
AB  - Machine learning based prediction of acute coronary syndrome positive patients is becoming increasingly common. However, there is little to no research in terms of assessing fairness and interpretability of these methods. This study discusses methodologies to reduce fairness related disparities in acute coronary syndrome predictions made by machine learning algorithms for different demographics sections of the population. Additionally, existing interpretability technique called LIME is used to understand predictions made using fairness methodologies. Our analysis show that discussed methods have: (1) potential to reduce treatment disparities; (2) generalize well to different healthcare setting in the United States; and (3) reduce overall false negative rates by 0.34 and improve AUROC score by 0.11 when compared to algorithms like XGBoost.
ER  - 

TY  - CONF
TI  - Explainable Deep Learning-Based Epiretinal Membrane Classification - An Empirical Comparison of Seven Interpretation Methods
T2  - 2022 IEEE Sixth Ecuador Technical Chapters Meeting (ETCM)
SP  - 1
EP  - 6
AU  - E. Parra-Mora
AU  - L. A. da Silva Cruz
PY  - 2022
KW  - Deep learning
KW  - Visualization
KW  - Retina
KW  - Prediction algorithms
KW  - Classification algorithms
KW  - Lesions
KW  - Convolutional neural networks
KW  - deep learning
KW  - convolutional neural networks
KW  - medical images
KW  - optical coherence tomography
KW  - epiretinal membrane
KW  - explainable learning
KW  - XAI
DO  - 10.1109/ETCM56276.2022.9935750
JO  - 2022 IEEE Sixth Ecuador Technical Chapters Meeting (ETCM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE Sixth Ecuador Technical Chapters Meeting (ETCM)
Y1  - 11-14 Oct. 2022
AB  - Deep learning (DL) methods have been widely applied in medical imaging screening, semantic segmentation, and lesion localization. Several such methods have reported human-level performance for the aforementioned tasks but the black-box nature of DL models hinders their acceptance in the field of health care, where reliability and transparency are important requirements to be fulfilled by computer-aided diagnostic systems (CADs). Explainable artificial learning (XAI) techniques address these accountability issues by providing human-understandable information about the inferences and computations leading to the predictions outputted by the algorithms. Epiretinal membrane (ERM) is an ocular disease that mainly affects the macular region of the retina at the back of the eye. Some studies have reported on CNN-based classifiers reaching ophthalmologist-level accuracy rates in discriminating between ERM and normal retinas. However, only two studies included any type of explainability functionality in their methods. In this work, we present a comparison of seven XAI methods for convolutional neural networks (CNNs) applied to the screening of ERM using optical coherence tomography retinal 2-D images.
ER  - 

TY  - CONF
TI  - Interpretable Machine Learning in Healthcare through Generalized Additive Model with Pairwise Interactions (GA2M): Predicting Severe Retinopathy of Prematurity
T2  - 2019 International Conference on Deep Learning and Machine Learning in Emerging Applications (Deep-ML)
SP  - 61
EP  - 66
AU  - T. Karatekin
AU  - S. Sancak
AU  - G. Celik
AU  - S. Topcuoglu
AU  - G. Karatekin
AU  - P. Kirci
AU  - A. Okatan
PY  - 2019
KW  - Pediatrics
KW  - Machine learning
KW  - Retinopathy
KW  - Logistics
KW  - Additives
KW  - interpretability of machine learning in healthcare
KW  - generalized additive model
KW  - logistic regression
KW  - GAM
KW  - GA2M
KW  - Retinopathy of Prematurity (RoP)
KW  - neonatology
DO  - 10.1109/Deep-ML.2019.00020
JO  - 2019 International Conference on Deep Learning and Machine Learning in Emerging Applications (Deep-ML)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 International Conference on Deep Learning and Machine Learning in Emerging Applications (Deep-ML)
Y1  - 26-28 Aug. 2019
AB  - We have investigated the risk factors that lead to severe retinopathy of prematurity using statistical analysis and logistic regression as a form of generalized additive model (GAM) with pairwise interaction terms (GA2M). In this process, we discuss the trade-off between accuracy and interpretability of these machine learning techniques on clinical data. We also confirm the intuition of expert neonatologists on a few risk factors, such as gender, that were previously deemed as clinically not significant in RoP prediction.
ER  - 

TY  - STD
TI  - IEEE Draft Standard for Transparency of Autonomous Systems
T2  - IEEE P7001/D1, June 2020
SP  - 1
EP  - 70
PY  - 2020
KW  - IEEE Standards
KW  - Artificial intelligence
KW  - Ethics
KW  - Autonomous systems
KW  - Autonomous vehicles
KW  - autonomous systems
KW  - artificial intelligence
KW  - ethics
KW  - IEEE 7001
KW  - transparency
DO  - 
JO  - IEEE P7001/D1, June 2020
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE P7001/D1, June 2020
Y1  - 17 Sept. 2020
AB  - Measurable, testable levels of transparency, so that autonomous systems can be objectively assessed, and levels of compliance determined, are described in this standard.
ER  - 

TY  - CONF
TI  - An xAI Thick Data Assisted Caption Generation for Labeling Severity of Ulcerative Colitis Video Colonoscopy
T2  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
SP  - 647
EP  - 652
AU  - J. Fiaidhi
AU  - S. Mohammed
AU  - P. Zezos
PY  - 2022
KW  - Training
KW  - Image analysis
KW  - Neural networks
KW  - Colonoscopy
KW  - Medical services
KW  - Machine learning
KW  - Predictive models
KW  - Explainable AI (xAI)
KW  - Thick Data
KW  - Ulcerative Colitis
KW  - Colonoscopy
KW  - Captioning
KW  - Siamese Neural Network
KW  - LSTM
DO  - 10.1109/ICHI54592.2022.00131
JO  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)
Y1  - 11-14 June 2022
AB  - Deep-learning convolutional neural networks (DCNNs) has made significant success in the area of medical image analysis and in particular in the area of colonoscopy. However, DCNNs are largely black-box predictors with no power to provide explanation for the underlying reasons of classification which is so important for evidence-based care practice. Providing thick data as additional heuristics in the form of generating relevant captions according to the features predicted by the machine learning can provide the explainable artificial intelligence (xAI) component to transfer those black boxes into more explainable components. This paper presented an approach that uses Siamese neural network for identifying the ulcerative colitis features from small training samples as well as to use an LSTM model to combine and embed relevant captions for providing that power of explainability to the Siamese classifier. Our modeling uses the Glove as an embedding model but did not use a copra that are dedicated for clinical practice. In our next research we are going to add this Glove based clinical copra to enhance our caption prediction accuracy.
ER  - 

TY  - CONF
TI  - Explainable AI in Decision Support Systems : A Case Study: Predicting Hospital Readmission Within 30 Days of Discharge
T2  - 2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)
SP  - 1
EP  - 4
AU  - A. Vucenovic
AU  - O. Ali-Ozkan
AU  - C. Ekwempe
AU  - O. Eren
PY  - 2020
KW  - Predictive models
KW  - Data models
KW  - Medical services
KW  - Hospitals
KW  - Computational modeling
KW  - Conferences
KW  - Logistics
KW  - clinical decision support systems
KW  - cross-validation
KW  - explainable AI
KW  - feature selection
KW  - healthcare
KW  - lasso
KW  - machine learning
KW  - model selection
KW  - regularization
KW  - shrinkage
DO  - 10.1109/CCECE47787.2020.9255721
JO  - 2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)
IS  - 
SN  - 2576-7046
VO  - 
VL  - 
JA  - 2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)
Y1  - 30 Aug.-2 Sept. 2020
AB  - Explainable models are a critical requirement for predictive analytics applications in the healthcare domain. In this work we develop a hypothetical clinical decision support system for the classification task of predicting hospital readmission within 30 days of discharge. We compare a baseline logistic regression model with an implementation of the coordinate descent algorithm known as lasso. We choose lasso because it inherently performs variable selection during optimization which leads to an explainable model. Using model evaluation data we achieve an area under the ROC curve score of 0.795 improving on the baseline score of 0.683 without inflating the feature space.
ER  - 

TY  - CONF
TI  - XAI-AV: Explainable Artificial Intelligence for Trust Management in Autonomous Vehicles
T2  - 2021 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI)
SP  - 1
EP  - 5
AU  - H. Mankodiya
AU  - M. S. Obaidat
AU  - R. Gupta
AU  - S. Tanwar
PY  - 2021
KW  - Computational modeling
KW  - Stacking
KW  - Vehicular ad hoc networks
KW  - Medical services
KW  - Prediction algorithms
KW  - Telecommunication computing
KW  - Artificial intelligence
KW  - Explainable AI
KW  - ML
KW  - Trust Management
KW  - Autonomous Vehicles
KW  - VANETs
KW  - VeRiMi dataset
DO  - 10.1109/CCCI52664.2021.9583190
JO  - 2021 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI)
Y1  - 15-17 Oct. 2021
AB  - Artificial intelligence (AI) is the most looked up technology with a diverse range of applications across all the fields, whether it is intelligent transportation systems (ITS), medicine, healthcare, military operations, or others. One such application is autonomous vehicles (AVs), which comes under the category of AI in ITS. Vehicular Adhoc Networks (VANET) makes communication possible between AVs in the system. The performance of each vehicle depends upon the information exchanged between AVs. False or malicious information can perturb the whole system leading to severe consequences. Hence, the detection of malicious vehicles is of utmost importance. We use machine learning (ML) algorithms to predict the flaw in the data transmitted. Recent papers that used the stacking ML approach gave an accuracy of 98.44%. Decision tree-based random forest is used to solve the problem in this paper. We achieved accuracy and F1 score of 98.43% and 98.5% respectively on the VeRiMi dataset in this paper. Explainable AI (XAI) is the method and technique to make the complex black-box ML and deep learning (DL) models more interpretable and understandable. We use a particular model interface of the evaluation metrics to explain and measure the model’s performance. Applying XAI to these complex AI models can ensure a cautious use of AI for AVs.
ER  - 

TY  - CONF
TI  - Interpretable Machine Learning: A Case Study of Healthcare
T2  - 2021 International Symposium on Networks, Computers and Communications (ISNCC)
SP  - 1
EP  - 6
AU  - F. Y. Okay
AU  - M. Yıldırım
AU  - S. Özdemir
PY  - 2021
KW  - Radio frequency
KW  - Machine learning algorithms
KW  - Decision making
KW  - Focusing
KW  - Motion pictures
KW  - Data models
KW  - Diabetes
KW  - IML
KW  - interpretability
KW  - SHAP
KW  - LIME
KW  - healthcare
DO  - 10.1109/ISNCC52172.2021.9615727
JO  - 2021 International Symposium on Networks, Computers and Communications (ISNCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Symposium on Networks, Computers and Communications (ISNCC)
Y1  - 31 Oct.-2 Nov. 2021
AB  - With the evolution of artificial intelligence, Machine Learning (ML) techniques have become more powerful predictors, and accordingly, the use of ML techniques has become a part of our daily life in different application scenarios such as disease diagnosis, movie recommendation, monitoring system, or detection of malicious attacks. Although ML provides high accurate predictions, it suffers from opacity. By behaving like a black box it excluded users about how to reach particular decisions. Interpretable Machine Learning (IML) is a recent technology that offers a promising solution to the opaqueness problem of complex ML techniques. It provides transparency of how the inner workings of ML lead to certain decisions and allows users to be aware of the decision-making process. Especially, in critical scenarios such as healthcare, it may become extremely important to know the reasons that affect the decision as well as the result. In this study, we aim to show the benefits of IML over a healthcare case study. In experiments, we employ SHAP and LIME IML models for the Random Forest (RF) and Gradient Boosting (GB) algorithms for the problem of diagnosing diabetes and its explanations. Overall results exhibit that applying IML models to complex and hard-to-interpret ML techniques ensures detailed interpretability while maintaining accuracy. We also perform experiments for local interpretability by focusing on an instance, which is another advantage of IML.
ER  - 

TY  - CONF
TI  - ECG Beats Classification with Interpretability
T2  - 2022 International Conference of Advanced Technology in Electronic and Electrical Engineering (ICATEEE)
SP  - 1
EP  - 5
AU  - R. Hammachi
AU  - N. Messaoudi
AU  - S. Belkacem
PY  - 2022
KW  - Location awareness
KW  - Industries
KW  - Electrical engineering
KW  - Machine learning algorithms
KW  - Neural networks
KW  - Closed box
KW  - Electrocardiography
KW  - Arrhythmia
KW  - Electrocardiogram
KW  - Healthcare
KW  - Interpretability
KW  - Machine Learning
DO  - 10.1109/ICATEEE57445.2022.10093744
JO  - 2022 International Conference of Advanced Technology in Electronic and Electrical Engineering (ICATEEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference of Advanced Technology in Electronic and Electrical Engineering (ICATEEE)
Y1  - 26-27 Nov. 2022
AB  - Recently, a lot of emphasis has been placed on Artificial Intelligence (AI) and Machine Learning (ML) algorithms in medicine and the healthcare industry. Cardiovascular disease (CVD), is one of the most common causes of death globally, and Electrocardiogram (ECG) is the most widely used diagnostic tool to investigate this disease. However, the analysis of ECG signals is a very difficult process. Therefore, in this work, automated classification of ECG data into five different arrhythmia classes is proposed, based on MIT-BIH dataset. Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) Deep Learning (DL) models were used. The black-box nature of these complex models imposes the need to explain their outcomes. Hence, both Permutation Feature Importance (PFI) with Gradient-Weighted Class Activation Maps (Grad-CAM) interpretability techniques were investigated. Using the K-Fold cross-validation method, the models achieved an accuracy of 97.1% and 98.5% for CNN and LSTM, respectively.
ER  - 

TY  - JOUR
TI  - Digital Health and Artificial Intelligence: Advancing Healthcare Provision in Latin America
T2  - IT Professional
SP  - 62
EP  - 68
AU  - R. García Alonso
AU  - U. Thoene
AU  - D. Dávila Benavides
PY  - 2022
KW  - Training data
KW  - Ethics
KW  - Telemedicine
KW  - Artificial intelligence
KW  - Collaboration
KW  - Medical services
KW  - Electronic healthcare
DO  - 10.1109/MITP.2022.3143530
JO  - IT Professional
IS  - 2
SN  - 1941-045X
VO  - 24
VL  - 24
JA  - IT Professional
Y1  - 1 March-April 2022
AB  - Artificial intelligence (AI) has shown enormous potential for transforming healthcare delivery and accessibility in low- and middle-income countries such as those in Latin America, which are among the least equitable regions of the world. While healthcare analytics is already a global trend, in Latin America, it is gaining relevance given its demographic and socio-economic specificity and its particular healthcare systems. Hence, we first explore the linkage between the concepts of AI and digital health. Second, we analyze various types of AI technology in the healthcare sector, and finally, we discuss AI health-related services in Latin American countries and classify them according to applications and digital health interventions. We highlight pertinent issues of privacy and transparency in the use of patient data and records, as well as the technological and regulatory difficulties Latin American countries encounter in implementing AI-based services in the healthcare sector, a provision that also contributes to advancing United Nations Sustainable Development Goals.
ER  - 

TY  - JOUR
TI  - Explainable AI and Mass Surveillance System-Based Healthcare Framework to Combat COVID-I9 Like Pandemics
T2  - IEEE Network
SP  - 126
EP  - 132
AU  - M. S. Hossain
AU  - G. Muhammad
AU  - N. Guizani
PY  - 2020
KW  - COVID-19
KW  - 5G mobile communication
KW  - Computed tomography
KW  - Hospitals
KW  - Medical diagnostic imaging
KW  - Artificial intelligence
KW  - Edge computing
KW  - Epidemics
KW  - Deep learning
KW  - Infectious diseases
DO  - 10.1109/MNET.011.2000458
JO  - IEEE Network
IS  - 4
SN  - 1558-156X
VO  - 34
VL  - 34
JA  - IEEE Network
Y1  - July/August 2020
AB  - Tactile edge technology that focuses on 5G or beyond 5G reveals an exciting approach to control infectious diseases such as COVID-19 internationally. The control of epidemics such as COVID-19 can be managed effectively by exploiting edge computation through the 5G wireless connectivity network. The implementation of a hierarchical edge computing system provides many advantages, such as low latency, scalability, and the protection of application and training model data, enabling COVID-19 to be evaluated by a dependable local edge server. In addition, many deep learning (DL) algorithms suffer from two crucial disadvantages: first, training requires a large COVID-19 dataset consisting of various aspects, which will pose challenges for local councils; second, to acknowledge the outcome, the findings of deep learning require ethical acceptance and clarification by the health care sector, as well as other contributors. In this article, we propose a B5G framework that utilizes the 5G network's low-latency, high-bandwidth functionality to detect COVID-19 using chest X-ray or CT scan images, and to develop a mass surveillance system to monitor social distancing, mask wearing, and body temperature. Three DL models, ResNet50, Deep tree, and Inception v3, are investigated in the proposed framework. Furthermore, blockchain technology is also used to ensure the security of healthcare data.
ER  - 

TY  - CHAP
TI  - Deep learning in patient management and clinical decision making
T2  - Deep Learning for Personalized Healthcare Services
SP  - 115
EP  - 140
AU  - D. A. Janeera
AU  - G. Jims John Wesley
AU  - P. Rajalakshmy
AU  - S. Shalini Packiam Kamala
AU  - P. Subha Hency Jose
AU  - T. M. Yunushkhan
PY  - 2021
KW  - Decision making
KW  - Medical services
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - Deep learning
KW  - Predictive models
KW  - Surgery
KW  - Data models
KW  - Automation
KW  - Telemedicine
DO  - 
PB  - De Gruyter
SN  - 9783110708172
UR  - http://ieeexplore.ieee.org/document/10789127
AB  - : With the rapid progression of technology, several hospitals are directed toward digital maintenance of patient information and records. The patient outcomes are significantly affected by the time-constrained, complex, uncertain, and high-stake decisions made by surgeons. Heuristics, individual judgement, and hypothetical deductive reasoning generally dominate clinical decisions. The traditional decision-support systems and predictive analysis faces certain challenges leading to suboptimal accuracy and manual management of information that is tedious and time consuming. This may lead to preventable harm, error, or bias. The challenges of the traditional systems can be overcome by implementation of artificial intelligence models that use deep learning for automation, where the outputs of mobile healthcare devices can be fed to electronic health records as a live-stream of patient data. In such systems, the decision-making process involves human intuition, bedside assessment preservation, detailed monitoring and implementation, model interpretability, data standardization, and consideration toward ethical challenges in error accountability and algorithm bias. Right from patient appointment to surgery and recovery, this online platform can be instigated and optimized. During the process of treatment, medication, and surgery, deep learning technology can guide the surgeon as well as the patient. In complex and debilitating cases, this technology uses statistical and quantitative models, enabling better decision making. Infections can be detected, tracked, investigated, and controlled using this model. In this chapter, we discuss the analysis of clinical images with deep learning techniques for clinical decision making. We also review the patient management system, wearable technology, standardization, and processing of information from the sensor nodes using deep learning approaches. This system offers improved solutions, explanation, and transparency when compared to the traditional clinical schemes.
ER  - 

TY  - CONF
TI  - Explainable AI-based early detection of diabetes for smart healthcare
T2  - 7th International Conference on Computing in Engineering & Technology (ICCET 2022)
SP  - 150
EP  - 154
AU  - A. Ahmad
AU  - S. Mehfuz
PY  - 2022
DO  - 10.1049/icp.2022.0608
JO  - 7th International Conference on Computing in Engineering & Technology (ICCET 2022)
IS  - 
SN  - 
VO  - 2022
VL  - 2022
JA  - 7th International Conference on Computing in Engineering & Technology (ICCET 2022)
Y1  - 12-13 Feb. 2022
AB  - Diabetes has become a worldwide epidemic as a chronic disease. Early detection of diabetes without specialized medical instruments may positively impact people's lives. This research aims to construct an intelligent system that uses a machine learning approach to classify diabetic illness. ALTHOUGH, most AI methods are based on mathematical modelling, which is difficult for the common person to comprehend. This research proposes a novel approach within the Explainable AI to explain the knowledge learned by the Machine learning model from Smart Healthcare. Innovative healthcare uses the current generation of digital technologies such as artificial intelligence and cloud computing to completely revolutionize the medical system, making it more efficient and personalized. This work proposes three techniques based on the analysis weights of artificial neurons or nearest neighbors and explaining estimation based on the analysis of training cases. Here, a model is developed with three classifiers to detect diabetes early and explain the most significant input and combination of inputs. The results show remarkable accuracy compared to previous methods, and the explanation makes sense. The hybrid intelligent system can assist medical practitioners in healthcare as a decision support system.
ER  - 

TY  - CONF
TI  - Explainable artificial intelligence: A survey
T2  - 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)
SP  - 0210
EP  - 0215
AU  - F. K. Došilović
AU  - M. Brčić
AU  - N. Hlupić
PY  - 2018
KW  - Predictive models
KW  - Machine learning
KW  - Support vector machines
KW  - Decision trees
KW  - Supervised learning
KW  - Optimization
KW  - explainable artificial intelligence
KW  - interpretability
KW  - explainability
KW  - comprehensibility
DO  - 10.23919/MIPRO.2018.8400040
JO  - 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)
Y1  - 21-25 May 2018
AB  - In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.
ER  - 

TY  - CONF
TI  - Lung Cancer Classification and Model Interpretation with Novel Gene Biomarkers using Explainable Artificial Intelligence
T2  - 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
SP  - 408
EP  - 412
AU  - K. Sekaran
AU  - A. Karthik
AU  - S. Sundaramurthy
PY  - 2022
KW  - Machine learning algorithms
KW  - Biological system modeling
KW  - Pipelines
KW  - Lung cancer
KW  - Biomarkers
KW  - Benchmark testing
KW  - Prediction algorithms
KW  - Lung Cancer
KW  - Machine Learning
KW  - Healthcare
KW  - Gene Expression
KW  - Biomarkers
KW  - Neural Networks
KW  - Wolf Search Algorithm
DO  - 10.1109/3ICT56508.2022.9990843
JO  - 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
IS  - 
SN  - 2770-7466
VO  - 
VL  - 
JA  - 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)
Y1  - 20-21 Nov. 2022
AB  - This paper proposes a machine learning pipeline to find novel genetic markers of lung cancer. The gene expression profiles of the individuals with regular and Small Cell Lung Cancer (SCLC) tumor tissues are statistically analyzed. The accession number of the dataset is GSE50412, fetched from the gene expression omnibus repository. The differentially expressed genes (DEGs) are identified based on the significance score calculated with the p-value ¡0.05. The candidate gene subset from the top 100 DEGs using Wolf Search Algorithm. The biomarkers are trained with supervised machine learning classification algorithms, and the model performance is validated using 10-fold cross-validation. The proposed model attained 92.7% accuracy, 92.6% precision, and 92.7% recall on a multilayered perceptron neural network classifier, exhibiting the efficacy of the study. The scores of the model outperformed the state-of-the-art machine learning algorithms benchmarked with the study. Furthermore, interpretation of the model prediction results is analyzed using SHapley Additive exPlanations (SHAP) after training the data with an extreme gradient boosting (XGBoost) algorithm. The result shows that gene probe cg00565075, a down-regulated gene identified from the logFC score act as a potent marker in discriminating the lung cancer samples.
ER  - 

TY  - CONF
TI  - Keynote: Explainable AI in Pervasive Healthcare: Open Challenges and Research Directions
T2  - 2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
SP  - 1
EP  - 1
AU  - D. Riboni
PY  - 2021
KW  - Pervasive computing
KW  - Infectious diseases
KW  - Senior citizens
KW  - Sociology
KW  - Smart homes
KW  - Medical services
DO  - 10.1109/PerComWorkshops51409.2021.9431134
JO  - 2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)
Y1  - 22-26 March 2021
AB  - Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. In our ageing society, there is increasing need for innovative tools to early detect fragility and non communicable diseases of the elderly population. To this aim, in the last few years, several research efforts have been made to exploit sensorized smart-homes and artificial intelligence (AI) methods to detect health issues of the elderly. However, most of these AI systems act as black-boxes, leading to a low level of trust by the clinicians and by the final users. Moreover, they provide limited support to clinicians in making a diagnosis, since they do not provide any explanation of the reason why a given prediction was computed. This talk addresses this challenging problem and our research efforts on this topic. The talk will present a novel AI system to detect cognitive decline symptoms in smart homes, which is supported by explainable AI capabilities. The system relies on clinical indicators of abnormal behaviors, spatial disorientation, and wandering. An AI-fueled dashboard allows clinicians to inspect anomalies together with the explanations of predictions. The system was experimented with a large set of real-world subjects, including people with MCI and people with dementia.
ER  - 

TY  - CONF
TI  - Decentralized Federated Learning and Deep Learning Leveraging XAI-Based Approach to Classify Colorectal Cancer
T2  - 2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
SP  - 1
EP  - 6
AU  - N. T. Arthi
AU  - K. E. Mubin
AU  - J. Rahman
AU  - G. M. Rafi
AU  - T. T. Sheja
AU  - M. T. Reza
AU  - M. A. Alam
PY  - 2022
KW  - Deep learning
KW  - Data privacy
KW  - Costs
KW  - Statistical analysis
KW  - Federated learning
KW  - Medical services
KW  - Probabilistic logic
KW  - Federated Learning
KW  - XAI
KW  - Deep Learning
KW  - Col-orectal Cancer
KW  - CNN
KW  - Image Classification
KW  - ResNeXt50
DO  - 10.1109/CSDE56538.2022.10089344
JO  - 2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
Y1  - 18-20 Dec. 2022
AB  - Convolutional Neural Networks based automated approaches are vastly utilised to anticipate and diagnose cancer, saving time and reducing mistakes. Deep Learning CNN methods use a variety of probabilistic and statistical methodologies to make computers understand and identify patterns in datasets based on previous experiences. We proposed federated learning(FL) based model to classify histopathological images for detecting colorectal cancer efficiently while providing high pre-diction accuracy. FL solves the problem of retaining privacy while utilizing vast and heterogeneous private datasets collected from numerous healthcare facilities. As the amount of patient data obtained is significantly responsible for the success of enhancing the accuracy of the system, the experiment was performed on a large dataset including cancerous and non-cancerous colorectal tissue images. FL is also capable of mitigating costs resulting from traditional ML approaches. Moreover, we have applied XAI method, a model-agnostic approach to acquire an explicit demonstration of the applied machine learning models. With XAI, we can visualize the super pixels of our colorectal tissue images through accepting and rejecting features. Applying vari-ous CNN models such as VGG, InceptionV3, ResNet, ResNeXt, and comparing their precision, we ascertained that ResNeXt50 bears the highest accuracy of 99.53%. Hence, we have applied ResNeXt50 on FL that brings forth the accuracy of 96.045% and F1 Score of 0.96.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence for Data Science on Customer Churn
T2  - 2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA)
SP  - 1
EP  - 10
AU  - C. K. Leung
AU  - A. G. M. Pazdor
AU  - J. Souza
PY  - 2021
KW  - Decision making
KW  - Transportation
KW  - Finance
KW  - Medical services
KW  - Data science
KW  - Predictive models
KW  - Tools
KW  - advanced analytics
KW  - customer churn
KW  - customer turnover
KW  - data science
KW  - explainable artificial intelligence (XAI)
KW  - interpretability
KW  - machine learning
KW  - practical applications
KW  - predictive analytics
DO  - 10.1109/DSAA53316.2021.9564166
JO  - 2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA)
Y1  - 6-9 Oct. 2021
AB  - Machine learning, as a tool, has become critical for decision-making mechanisms in the modern world. It has applications in a wide range of areas, including finance, healthcare, justice, and transportation. Unfortunately, machine learning is often considered as a “black box”. As such, recommendations made by machine learning techniques, as well as the reasoning behind those recommendations, are not easily understood by humans. In this paper, we present an explainable artificial intelligence (XAI) solution that integrates and enhances state-of-the-art techniques to produce understandable and practical explanations to end-users. To evaluate the effectiveness of our XAI solution for data science, we conduct a case study on applying our solution to explaining a random forest-based predictive model on customer churn. Results show the practicality and usefulness of our XAI solution in practical applications such as data science on customer churn.
ER  - 

TY  - CONF
TI  - Early Esophageal Malignancy Detection Using Deep Transfer Learning and Explainable AI
T2  - 2022 6th International Conference on Communication and Information Systems (ICCIS)
SP  - 129
EP  - 135
AU  - P. Shaw
AU  - S. Sankaranarayanan
AU  - P. Lorenz
PY  - 2022
KW  - Deep learning
KW  - Stomach
KW  - Transfer learning
KW  - Lung
KW  - Predictive models
KW  - Reliability
KW  - Artificial intelligence
KW  - Esophageal malignancy
KW  - healthcare
KW  - CNN
KW  - Deep Learning
KW  - transfer learning
KW  - Explainable AI
KW  - LIME
DO  - 10.1109/ICCIS56375.2022.9998162
JO  - 2022 6th International Conference on Communication and Information Systems (ICCIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 6th International Conference on Communication and Information Systems (ICCIS)
Y1  - 14-16 Oct. 2022
AB  - Esophageal malignancy is a rare form of cancer that starts in the esophagus and spreads to the other parts of the body, impacting a severe risk on the liver, lungs, lymph nodes, and stomach. Studies have shown that esophageal cancer is one of the most prevalent causes of cancer mortality. In 2020, 604100 individuals have been diagnosed with this deadly disease. There are a good number of medical studies, carried out on this topic, every year. A similar focus is also imparted on the AI-based deep learning models for the classification of malignancy. But the challenge is that the AI models are all complex and lack transparency. There is no available information to explain the opacity of such models. And as AI-based medical research seeks reliability, it becomes very important to bring in explainability. So we, through this research, have used Explainable AI(XAI) entitled LIME for creating trust-based models for the early detection of esophageal malignancy. We have used a simple CNN model and several transfer learning-based models, for this study. We have taken the actual endoscopic images from the Kvasir-v2 dataset resulting in an accuracy of 88.75%. with the DenseNet-201 model followed by the usage of an Explainable AI model, Lime, for giving an explanation for the images classified. The deep learning model, combined with explainable AI, helps in getting a clear picture of the regions contributing toward the malignancy prediction and promotes confidence in the model, without the intervention of a domain expert.
ER  - 

TY  - CONF
TI  - An automated feature selection and classification pipeline to improve explainability of clinical prediction models
T2  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
SP  - 527
EP  - 534
AU  - P. A. Moreno-Sanchez
PY  - 2021
KW  - Measurement
KW  - Pipelines
KW  - Medical services
KW  - Predictive models
KW  - Tools
KW  - Feature extraction
KW  - Data models
KW  - Feature selection
KW  - explainable AI
KW  - Classification
KW  - clinical prediction models
KW  - data pipeline
DO  - 10.1109/ICHI52183.2021.00100
JO  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
Y1  - 9-12 Aug. 2021
AB  - Artificial Intelligence is becoming recently a promising tool to achieve the deployment of personalized medicine in clinical practice. However, healthcare professionals are demanding clinical prediction models with better interpretability of the results in order to achieve an actual adoption and use of these solutions. The eXplainable Artificial Intelligence tackle this issue by offering feature relevance explanations of the model, among other techniques, where the selection of the important features and elimination of the redundant are cornerstones. This work presents a data management pipeline that allows automating the selection of those relevant features as well as the classifier technique that provides the best performance in terms of classification. The pipeline developed, named SCI-XAI (feature Selection and Classification for Improving eXplainable Artificial Intelligence) has been evaluated with 6 clinical datasets in a cross-validation approach as well as in a test set with unseen data. Next, an explainability evaluation has been carried out of the best models obtained by applying the SCI-XAI pipeline. Results obtained show that SCI-XAI achieves the best classification performance by applying different feature selection techniques depending on the variable type of the feature which reduces significantly the features processed by the model. Thus, feature reduction allows increasing the explainability of the models.
ER  - 

TY  - CONF
TI  - Detection of COVID-19 from Chest X-ray Images: A Deep Learning Approach
T2  - 2021 Ethics and Explainability for Responsible Data Science (EE-RDS)
SP  - 1
EP  - 7
AU  - V. Mohan
PY  - 2021
KW  - Deep learning
KW  - COVID-19
KW  - Learning systems
KW  - Ethics
KW  - Pandemics
KW  - Data science
KW  - Medical diagnosis
KW  - covid
KW  - deep learning
KW  - siamese network
KW  - machine learning
DO  - 10.1109/EE-RDS53766.2021.9708594
JO  - 2021 Ethics and Explainability for Responsible Data Science (EE-RDS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 Ethics and Explainability for Responsible Data Science (EE-RDS)
Y1  - 27-28 Oct. 2021
AB  - In the current COVID-19 pandemic situation, there is an urgent need to properly diagnose whether people are infected by COVID-19 or not. Fast and accurate methods should be there to improve the efficiency of the health care system so that the infected people can be given priority treatment. Deep learning methods are largely incorporated in many medical fields, especially in medical diagnosis. Deep learning techniques could find patterns that can be attributed to various diseases. The main challenge in applying deep learning techniques in the medical field is the lack of availability of quality labelled data. Machine learning approaches, such as One-shot learning methods, are becoming increasingly popular in the medical community because they perform better with limited data. We compared several deep learning models for COVID image classification in this paper. State-of-the-art architectures like ResNet, EfficientNet were compared. We also present a technique that combines the triplet loss and cross-entropy loss functions. This technique enables the model to learn weights in such a way that it attempts to cluster different classes during classification. It will increase the model’s interpretability and group together similar data. The dataset we used was made available as part of the Chest XR COVID 19 detection challenge. EfficientNet B7 model got the best result on the test set with 95.67% accuracy. Using Siamese Network, we were able to embed the images into a lower dimension in such a way that they can be clustered into different groups. Classification based on this embedded space obtained an accuracy of 93.76% in the test set.
ER  - 

TY  - CONF
TI  - An Explainable Deep Learning Model for Vision-based Human Fall Detection System
T2  - 2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT)
SP  - 1223
EP  - 1229
AU  - J. G
AU  - P. S
AU  - S. D
PY  - 2022
KW  - Deep learning
KW  - Training
KW  - Computational modeling
KW  - Medical services
KW  - Predictive models
KW  - Convolutional neural networks
KW  - Fall detection
KW  - Fall Detection
KW  - Convolutional Neural Network (CNN)
KW  - Explainable AI (Ex-AI)
KW  - Local Interpretable Model-agnostic Explanations (LIME)
KW  - Internet of Things (IoT)
KW  - Deep Learning (DL)
DO  - 10.1109/ICICICT54557.2022.9917979
JO  - 2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT)
Y1  - 11-12 Aug. 2022
AB  - Human Fall is one of the major life-risking problems among elderly people. Accidental falls among elder persons may lead to many irreversible disabilities or even deaths. Fall detection has become a critical research problem in the Healthcare domain, which needs more reliable and efficient solutions to intelligently classify fall activities. With the development of the Internet of Things, such as wearable sensors, ambient sensors, and cameras, monitoring elderly people continuously has become feasible. The proposed work ensures Fall detection through Deep Learning classification using a Convolutional Neural Network with three hidden layers. The performance of the Deep Learning model is evaluated with the Fall Detection Dataset and the classification results have achieved an accuracy of 96.5%. However, the intelligent classification methods need to be trustworthy to be able to be accepted by healthcare professionals. Since the Deep Learning models are ’black-box’, the process behind the classification is not known and accuracy alone will not be sufficient for evaluating the performance of the model. Hence, in this paper, we also propose an Explainable AI model, called LIME to interpret the classification of fall activity. the results of LIME show the feature responsible for prediction by marking the boundaries of the input image.
ER  - 

TY  - CONF
TI  - A Machine Learning Model Selection considering Tradeoffs between Accuracy and Interpretability
T2  - 2021 13th International Conference on Information Technology and Electrical Engineering (ICITEE)
SP  - 63
EP  - 68
AU  - Z. Nazir
AU  - D. Kaldykhanov
AU  - K. -K. Tolep
AU  - J. -G. Park
PY  - 2021
KW  - Measurement
KW  - Electrical engineering
KW  - Deep learning
KW  - Machine learning algorithms
KW  - Linear regression
KW  - Medical services
KW  - Forestry
KW  - Interpretable Machine Learning (IML)
KW  - Explainable ML
KW  - Model Selection techniques
DO  - 10.1109/ICITEE53064.2021.9611872
JO  - 2021 13th International Conference on Information Technology and Electrical Engineering (ICITEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 13th International Conference on Information Technology and Electrical Engineering (ICITEE)
Y1  - 14-15 Oct. 2021
AB  - Using black box machine learning models (e.g., Deep Neural Networks) in high-stakes domains such as healthcare, criminal justice and real-time systems can cause serious problems due to their complexity and poor interpretability. Moreover, model selection with interpretability in addition to accuracy is one of emerging research areas with lack of model agnostic and quantitative interpretability metrics. In this work, we adopt a quantitative interpretability metric, and then, introduce a trade-offs methodology between accuracy and interpretability, which can be demonstrated by increasing interpretability of ML models while allowing accuracy to drop up to given thresholds. In our experimental results, interpretability in terms of simulatability operation count (SOC) is improved up to 76.2&#x0025; with minimal 2.3&#x0025; accuracy drop in a SVR estimator of the Auto MPG dataset (up to 64.3&#x0025; with minimal 1.9&#x0025; accuracy drop in the Forest Fire dataset of an MLP estimator).
ER  - 

TY  - CONF
TI  - On the Need of Interpretability for Biomedical Applications: Using Fuzzy Models for Lung Cancer Prediction with Liquid Biopsy
T2  - 2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
SP  - 1
EP  - 6
AU  - N. Potie
AU  - S. Giannoukakos
AU  - M. Hackenberg
AU  - A. Fernandez
PY  - 2019
KW  - Cancer
KW  - Lung
KW  - Biopsy
KW  - Liquids
KW  - Blood
KW  - Tumors
KW  - Data mining
KW  - eXplainable Artificial Intelligence
KW  - Evolutionary Fuzzy Systems
KW  - Lung Cancer
KW  - Liquid Biopsy
KW  - Interpretability
DO  - 10.1109/FUZZ-IEEE.2019.8858976
JO  - 2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
IS  - 
SN  - 1558-4739
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
Y1  - 23-26 June 2019
AB  - In the latter years, we are witnessing a movement from the standard Data Mining towards a more profitable and challenging scenario known as Data Science. It can be defined as a set of quantitative and qualitative approaches that are applied to current relevant problems. In order to be able to "dig" to the deepest level considering the whole information available, the knowledge domain and the analysis of the data must have a strong synergy.There are many fields of application where it is necessary, if not essential, to give an explanation of the phenomenon under study. It is no longer enough to simply apply a Machine Learning model, but it must be comprehensible in order to provide a real decision support system. For this reason, a strong movement has emerged in favour of the eXplainable Artificial Intelligence that aims to respond to the "how" and "why" of the operation of automatic models.In this work, our objective is to show the benefits of one of the learning paradigms of Computational Intelligence: Fuzzy Rule Based Systems and Evolutionary Fuzzy Systems. To this end, we focus on biomedical applications by presenting a case study based on lung cancer prediction from samples taken by liquid biopsy. Liquid biopsy enable us to study genomic alterations for each individual independently, a step towards personalised medicine. The results show the goodness of the solution based on Evolutionary Fuzzy Systems in terms of interpretability and comprehensibility, obtaining a low number of rules with less than 3 fuzzy linguistic labels per antecedent.
ER  - 

TY  - CONF
TI  - Explainable AI for COVID-19 CT Classifiers: An Initial Comparison Study
T2  - 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)
SP  - 521
EP  - 526
AU  - Q. Ye
AU  - J. Xia
AU  - G. Yang
PY  - 2021
KW  - COVID-19
KW  - Deep learning
KW  - Computed tomography
KW  - Pulmonary diseases
KW  - Decision making
KW  - Neural networks
KW  - Tools
KW  - COVID-19
KW  - Explainable AI
KW  - Deep Learning
KW  - Classification
KW  - CT
DO  - 10.1109/CBMS52027.2021.00103
JO  - 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)
IS  - 
SN  - 2372-9198
VO  - 
VL  - 
JA  - 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)
Y1  - 7-9 June 2021
AB  - Artificial Intelligence (AI) has made leapfrogs in development across all the industrial sectors especially when deep learning has been introduced. Deep learning helps to learn the behaviour of an entity through methods of recognising and interpreting patterns. Despite its limitless potential, the mystery is how deep learning algorithms make a decision in the first place. Explainable AI (XAI) is the key to unlocking AI and the black-box for deep learning. XAI is an AI model that is programmed to explain its goals, logic, and decision making so that the end users can understand. The end users can be domain experts, regulatory agencies, managers and executive board members, data scientists, users that use AI, with or without awareness, or someone who is affected by the decisions of an AI model. Chest CT has emerged as a valuable tool for the clinical diagnostic and treatment management of the lung diseases associated with COVID-19. AI can support rapid evaluation of CT scans to differentiate COVID-19 findings from other lung diseases. However, how these AI tools or deep learning algorithms reach such a decision and which are the most influential features derived from these neural networks with typically deep layers are not clear. The aim of this study is to propose and develop XAI strategies for COVID-19 classification models with an investigation of comparison. The results demonstrate promising quantification and qualitative visualisations that can further enhance the clinician's understanding and decision making with more granular information from the results given by the learned XAI models.
ER  - 

TY  - CONF
TI  - Multi Disease Diagnosis Model for Chest X-ray Images with Explainable AI – Grad-Cam Feature Map Visualization
T2  - 2022 International Conference on Futuristic Technologies (INCOFT)
SP  - 1
EP  - 5
AU  - N. S. Rani
AU  - C. H. Nachappa
AU  - A. S. Krishna
AU  - B. J. Bipin Nair
PY  - 2022
KW  - Training
KW  - Lung
KW  - Lung cancer
KW  - Predictive models
KW  - Feature extraction
KW  - Proposals
KW  - Medical diagnosis
KW  - X-ray images
KW  - lung cancer detection
KW  - deep learning models
KW  - classification.
DO  - 10.1109/INCOFT55651.2022.10094451
JO  - 2022 International Conference on Futuristic Technologies (INCOFT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Futuristic Technologies (INCOFT)
Y1  - 25-27 Nov. 2022
AB  - In this work, we have proposed a deep convolutional neural network model with the background of a network as ResNet and a classification block as a faster RCNN model to predict multiple lung cancer abnormalities from X-ray images. Experimentations are conducted on Kaggle image dataset repositories with X-ray images of about 55000 images comprising of 15 different classes. We have proposed a region proposal network capable of autonomously learning from the pre-trained data specifications and applying a bounding box to cancer-affected regions to detect the affected areas. About 80% of samples were considered to conduct training from each class, and 20% were used for testing. The results obtained have an accuracy of about 96% for classification.
ER  - 

TY  - CONF
TI  - Explainability Of Artificial Intelligence For Diagnosing COVID-19 From Chest X-Rays
T2  - 2021 International Conference on Computational Performance Evaluation (ComPE)
SP  - 598
EP  - 603
AU  - A. Goel
AU  - S. Panwar Jogi
PY  - 2021
KW  - COVID-19
KW  - Visualization
KW  - Pandemics
KW  - Computational modeling
KW  - Pulmonary diseases
KW  - Transfer learning
KW  - Artificial intelligence
KW  - COVID-19
KW  - coronavirus
KW  - X-ray
KW  - transfer learning
KW  - diagnosis
KW  - artificial intelligence
KW  - explainability
DO  - 10.1109/ComPE53109.2021.9751844
JO  - 2021 International Conference on Computational Performance Evaluation (ComPE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Computational Performance Evaluation (ComPE)
Y1  - 1-3 Dec. 2021
AB  - This COVID-19 pandemic has overburdened the government and the healthcare system of many countries around the world. It has brought up the need for a fast and accurate diagnosing method. Artificial intelligence (AI) is having a notable role in different aspects of the pandemic- contact tracing, epidemiology, medical diagnosis and prognosis, and drug development. Deep learning has found its application in the diagnosis of COVID-19 chest X-rays (CXR) using convolution neural nets. Many architectures have been used and transfer learning is the most preferred approach. These models have proven to be fast and accurate in COVID-19 diagnosis. However, one key element that has prevented the use of AI in clinical practice is its lack of transparency and explainability. In this paper, we use the ResNet-50 pre-trained model for classifying the CXR of COVID-19 patients from pneumonia and normal patients. We then use explainability algorithms to visualize the model features and verify the explainability of the model.
ER  - 

TY  - CONF
TI  - Development and Evaluation of Machine Learning Models for Recovery Prediction after Treatment for Traumatic Brain Injury
T2  - 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)
SP  - 2416
EP  - 2420
AU  - H. L. Radabaugh
AU  - J. Bonnell
AU  - W. D. Dietrich
AU  - H. M. Bramlett
AU  - O. Schwartz
AU  - D. Sarkar
PY  - 2020
KW  - Principal component analysis
KW  - Predictive models
KW  - Machine learning
KW  - Injuries
KW  - Measurement
KW  - Medical treatment
KW  - Animals
DO  - 10.1109/EMBC44109.2020.9175658
JO  - 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)
IS  - 
SN  - 2694-0604
VO  - 
VL  - 
JA  - 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)
Y1  - 20-24 July 2020
AB  - Traumatic brain injury (TBI) is a leading cause of death and disability yet treatment strategies remain elusive. Advances in machine learning present exciting opportunities for developing personalized medicine and informing laboratory research. However, their feasibility has yet to be widely assessed in animal research where data are typically limited or in the TBI field where each patient presents with a unique injury. The Operation Brain Trauma Therapy (OBTT) has amassed an animal dataset that spans multiple types of injury, treatment strategies, behavioral assessments, histological measures, and biomarker screenings. This paper aims to analyze these data using supervised learning techniques for the first time by partitioning the dataset into acute input metrics (i.e. 7 days post-injury) and a defined recovery outcome (i.e. memory retention). Preprocessing is then applied to transform the raw OBTT dataset, e.g. developing a class attribute by histogram binning, eliminating borderline cases, and applying principal component analysis (PCA). We find that these steps are also useful in establishing a treatment ranking; Minocycline, a therapy with no significant findings in the OBTT analyses, yields the highest percentage recovery in our ranking. Furthermore, of the seven classifiers we have evaluated, Naïve Bayes achieves the best performance (67%) and yields significant improvement over our baseline model on the preprocessed dataset with borderline elimination. We also investigate the effect of testing on individual treatment groups to evaluate which groups are difficult to classify, and note the interpretive qualities of our model that can be clinically relevant.Clinical Relevance- These studies establish methods for better analyzing multivariate functional recovery and understanding which measures affect prognosis following traumatic brain injury.
ER  - 

TY  - JOUR
TI  - Explainable AI Over the Internet of Things (IoT): Overview, State-of-the-Art and Future Directions
T2  - IEEE Open Journal of the Communications Society
SP  - 2106
EP  - 2136
AU  - S. K. Jagatheesaperumal
AU  - Q. -V. Pham
AU  - R. Ruby
AU  - Z. Yang
AU  - C. Xu
AU  - Z. Zhang
PY  - 2022
KW  - Internet of Things
KW  - Artificial intelligence
KW  - Data models
KW  - Medical services
KW  - Industrial Internet of Things
KW  - Predictive models
KW  - Ethics
KW  - Artificial intelligence
KW  - deep learning
KW  - explainability
KW  - Internet of Things
KW  - machine learnin
DO  - 10.1109/OJCOMS.2022.3215676
JO  - IEEE Open Journal of the Communications Society
IS  - 
SN  - 2644-125X
VO  - 3
VL  - 3
JA  - IEEE Open Journal of the Communications Society
Y1  - 2022
AB  - Explainable Artificial Intelligence (XAI) is transforming the field of Artificial Intelligence (AI) by enhancing the trust of end-users in machines. As the number of connected devices keeps on growing, the Internet of Things (IoT) market needs to be trustworthy for the end-users. However, existing literature still lacks a systematic and comprehensive survey work on the use of XAI for IoT. To bridge this lacking, in this paper, we address the XAI frameworks with a focus on their characteristics and support for IoT. We illustrate the widely-used XAI services for IoT applications, such as security enhancement, Internet of Medical Things (IoMT), Industrial IoT (IIoT), and Internet of City Things (IoCT). We also suggest the implementation choice of XAI models over IoT systems in these applications with appropriate examples and summarize the key inferences for future works. Moreover, we present the cutting-edge development in edge XAI structures and the support of sixth-generation (6G) communication services for IoT applications, along with key inferences. In a nutshell, this paper constitutes the first holistic compilation on the development of XAI-based frameworks tailored for the demands of future IoT use cases.
ER  - 

TY  - JOUR
TI  - Oriented Feature Selection SVM Applied to Cancer Prediction in Precision Medicine
T2  - IEEE Access
SP  - 48510
EP  - 48521
AU  - Y. Shen
AU  - C. Wu
AU  - C. Liu
AU  - Y. Wu
AU  - N. Xiong
PY  - 2018
KW  - Cancer
KW  - Support vector machines
KW  - Feature extraction
KW  - Gene expression
KW  - Tumors
KW  - Sequential analysis
KW  - Cancer prediction
KW  - elastic net
KW  - feature selection
KW  - fused lasso
KW  - gene expression data
KW  - machine learning
KW  - precision medicine
KW  - SVM
DO  - 10.1109/ACCESS.2018.2868098
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 6
VL  - 6
JA  - IEEE Access
Y1  - 2018
AB  - Advances in the gene sequencing technology and the outbreak of artificial intelligence have made precision medicine a reality recently. Applying machine learning algorithms to cancer prediction using gene expression data helps to discover the link between genetic data and cancer, which will promote the development and application of precision medicine. Considering the natural order of genes, a new classification method that combines fused lasso and elastic net as regularization for linear support vector machine (SVM), which uses huberized hinge loss as the loss function, is proposed in this paper, which we name it oriented feature selection SVM (OFSSVM). Due to the characteristics of the elastic net and fused lasso, the OFSSVM can not only provide automatic feature selection, but also average the adjacent coefficients, resulting in a sparse and smooth solution. We demonstrate its effectiveness in both binary classification and multiclass classification in the sense of comprehensive evaluation that not only the classification accuracy but also the interpretability are considered. The experiments show that the OFSSVM is an appealing compromise between interpretability and classification accuracy, and is superior to other traditional methods in the sense of comprehensive evaluation.
ER  - 

TY  - CONF
TI  - Interpretability Analysis of Pre-trained Convolutional Neural Networks for Medical Diagnosis
T2  - CAIBDA 2022; 2nd International Conference on Artificial Intelligence, Big Data and Algorithms
SP  - 1
EP  - 8
AU  - Q. Hu
AU  - W. Liu
AU  - Y. Liu
AU  - Z. Liu
PY  - 2022
DO  - 
JO  - CAIBDA 2022; 2nd International Conference on Artificial Intelligence, Big Data and Algorithms
IS  - 
SN  - 
VO  - 
VL  - 
JA  - CAIBDA 2022; 2nd International Conference on Artificial Intelligence, Big Data and Algorithms
Y1  - 17-19 June 2022
AB  - Pre-trained Convolutional Neural Network model aims to improve the prediction efficiency and accuracy, while it is short of interpretability, leading to a lack of trust in the model. Sufficient interpretability analysis is necessary before adopting the pre-trained model in the medical field, due to the serious consequences of misdiagnosis. However, recent studies of interpretability mainly concern the theory, rather than interpretability analysis of specific pre-trained models. This paper illustrates interpretability with VGG16 and AlexNet in medical diagnosis, which includes brain, breast, and lung tumors. Firstly, we enlarge the scale of data through rotating images at different angles. Secondly, VGG16 and AlexNet neural networks are rebuilt to compare with the official pre-trained model. Thirdly, fully connected layers and specific parameters, like dropout and number of neurons in convolution layer, are reset to verify discrepancies between the pretrained model and our model. Moreover, batch normalization is added to fully connected layers to prevent gradient explosion. Finally, we compare the pre-trained model with self-training model on accuracy, loss, confusion matrix, and saliency map. The experimental results show that the pre-trained model is slightly better. The official pre-trained model gets higher accuracy and much lower loss. Our analyses illustrate that the pre-trained model can be used in cancer diagnosis directly, while the model without pre-training could also be used in cancer diagnosis.
ER  - 

TY  - JOUR
TI  - Toward Explainable Artificial Intelligence for Regression Models: A methodological perspective
T2  - IEEE Signal Processing Magazine
SP  - 40
EP  - 58
AU  - S. Letzgus
AU  - P. Wagner
AU  - J. Lederer
AU  - W. Samek
AU  - K. -R. Müller
AU  - G. Montavon
PY  - 2022
KW  - Deep learning
KW  - Neural networks
KW  - Predictive models
KW  - Medical diagnosis
KW  - Task analysis
DO  - 10.1109/MSP.2022.3153277
JO  - IEEE Signal Processing Magazine
IS  - 4
SN  - 1558-0792
VO  - 39
VL  - 39
JA  - IEEE Signal Processing Magazine
Y1  - July 2022
AB  - In addition to the impressive predictive power of machine learning (ML) models, more recently, explanation methods have emerged that enable an interpretation of complex nonlinear learning models, such as deep neural networks. Gaining a better understanding is especially important, e.g., for safety-critical ML applications or medical diagnostics and so on. Although such explainable artificial intelligence (XAI) techniques have reached significant popularity for classifiers, thus far, little attention has been devoted to XAI for regression models (XAIR). In this review, we clarify the fundamental conceptual differences of XAI for regression and classification tasks, establish novel theoretical insights and analysis for XAIR, provide demonstrations of XAIR on genuine practical regression problems, and finally, discuss challenges remaining for the field.
ER  - 

TY  - CONF
TI  - A Machine Learning Early Warning System: Multicenter Validation in Brazilian Hospitals
T2  - 2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS)
SP  - 321
EP  - 326
AU  - J. Kobylarz Ribeiro
AU  - H. D. P. dos Santos
AU  - F. Barletta
AU  - M. Cichelero da Silva
AU  - R. Vieira
AU  - H. M. P. Morales
AU  - C. da Costa Rocha
PY  - 2020
KW  - Hospitals
KW  - Machine learning
KW  - Machine learning algorithms
KW  - Protocols
KW  - Prediction algorithms
KW  - Benchmark testing
KW  - Early Warning System
KW  - Predictive medicine
KW  - Machine Learning
KW  - Explainable AI
KW  - Healthcare
KW  - Vital Signs
DO  - 10.1109/CBMS49503.2020.00067
JO  - 2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS)
IS  - 
SN  - 2372-9198
VO  - 
VL  - 
JA  - 2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS)
Y1  - 28-30 July 2020
AB  - Early recognition of clinical deterioration is one of the main steps for reducing inpatient morbidity and mortality. The challenging task of clinical deterioration identification in hospitals lies in the intense daily routines of healthcare practitioners, in the unconnected patient data stored in the Electronic Health Records (EHRs) and in the usage of low accuracy scores. Since hospital wards are given less attention compared to the Intensive Care Unit, ICU, we hypothesized that when a platform is connected to a stream of EHR, there would be a drastic improvement in dangerous situations awareness and could thus assist the healthcare team. With the application of machine learning, the system is capable to consider all patient's history and through the use of high-performing predictive models, an intelligent early warning system is enabled. In this work we used 121,089 medical encounters from six different hospitals and 7,540,389 data points, and we compared popular ward protocols with six different scalable machine learning methods (three are classic machine learning models, logistic and probabilistic-based models, and three gradient boosted models). The results showed an advantage in AUC (Area Under the Receiver Operating Characteristic Curve) of 25 percentage points in the best Machine Learning model result compared to the current state-of-the-art protocols. This is shown by the generalization of the algorithm with leave-one-group-out (AUC of 0.949) and the robustness through cross-validation (AUC of 0.961). We also perform experiments to compare several window sizes to justify the use of five patient timestamps. A sample dataset, experiments, and code are available for replicability purposes.
ER  - 

TY  - CONF
TI  - Care Robot Transparency Isn't Enough for Trust
T2  - 2018 IEEE Region Ten Symposium (Tensymp)
SP  - 293
EP  - 297
AU  - A. Poulsen
AU  - O. K. Burmeister
AU  - D. Tien
PY  - 2018
KW  - Decision making
KW  - Ethics
KW  - Medical services
KW  - Cognitive science
KW  - IEEE Regions
KW  - Robot sensing systems
KW  - machine transparency
KW  - healthcare robotics
KW  - robot ethics
KW  - human robot interaction
DO  - 10.1109/TENCONSpring.2018.8692047
JO  - 2018 IEEE Region Ten Symposium (Tensymp)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 IEEE Region Ten Symposium (Tensymp)
Y1  - 4-6 July 2018
AB  - A recent study featuring a new kind of care robot indicated that participants expect a robot's ethical decision-making to be transparent to develop trust, even though the same type of `inspection of thoughts' isn't expected of a human carer. At first glance, this might suggest that robot transparency mechanisms are required for users to develop trust in robot-made ethical decisions. But the participants were found to desire transparency only when they didn't know the specifics of a human-robot social interaction. Humans trust others without observing their thoughts, which implies other means of determining trustworthiness. The study reported here suggests that the method is social interaction and observation, signifying that trust is a social construct. Moreover, that `social determinants of trust' are the transparent elements. This socially determined behaviour draws on notions of virtue ethics. If a caregiver (nurse or robot) consistently provides good, ethical care, then patients can trust that caregiver to do so often. The same social determinants may apply to care robots and thus it ought to be possible to trust them without the ability to see their thoughts. This study suggests why transparency mechanisms may not be effective in helping to develop trust in care robot ethical decision-making. It suggests that roboticists need to build sociable elements into care robots to help patients to develop patient trust in the care robot's ethical decision-making.
ER  - 

TY  - CONF
TI  - Explainable Machine Learning to Identify Patient-specific Biomarkers for Lung Cancer
T2  - 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 3152
EP  - 3159
AU  - M. Sobhan
AU  - A. M. Mondal
PY  - 2022
KW  - Pipelines
KW  - Sociology
KW  - Lung cancer
KW  - Lung
KW  - Genomics
KW  - Biology
KW  - Convolutional neural networks
KW  - explainable machine learning
KW  - lung cancer
KW  - patient-specific biomarkers
KW  - precision medicine
DO  - 10.1109/BIBM55620.2022.9995516
JO  - 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 6-8 Dec. 2022
AB  - Background: Lung cancer is the leading cause of death compared to other cancers in the USA. The overall survival rate of lung cancer is not satisfactory even though there are cutting-edge treatment methods for cancers. Genomic profiling and biomarker gene identification of lung cancer patients may play a role in the therapeutics of lung cancer patients. The biomarker genes identified by most of the existing methods (statistical and machine learning based) belong to the whole cohort or population. That is why different people with the same disease get the same kind of treatment, but results in different outcomes in terms of success and side effects. So, the identification of biomarker genes for individual patients is very crucial for finding efficacious therapeutics leading to precision medicine. Methods: In this study, we propose a pipeline to identify lung cancer class-specific and patient-specific key genes which may help formulate effective therapies for lung cancer patients. We have used expression profiles of two types of lung cancers, lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC), and Healthy lung tissues to identify LUADand LUSC-specific (class-specific) and individual patient-specific key genes using an explainable machine learning approach, SHaphley Additive ExPlanations (SHAP). This approach provides scores for each of the genes for individual patients which tells us the attribution of each feature (gene) for each sample (patient). Result: In this study, we applied two variations of SHAP- tree explainer and gradient explainer for which tree-based classifier, XGBoost, and deep learning-based classifier, convolutional neural network (CNN) were used as classification algorithms, respectively. Our results showed that the proposed approach successfully identified class-specific (LUAD, LUSC, and Healthy) and patient-specific key genes based on the SHAP scores. Conclusion: This study demonstrated a pipeline to identify cohort-based and patient-specific biomarker genes by incorporating an explainable machine learning technique, SHAP. The patient-specific genes identified using SHAP scores may provide biological and clinical insights into the patient’s diagnosis.
ER  - 

TY  - CONF
TI  - Enhanced Deep Type-2 Fuzzy Logic System For Global Interpretability
T2  - 2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
SP  - 1
EP  - 8
AU  - R. Chimatapu
AU  - H. Hagras
AU  - M. Kern
AU  - G. Owusu
PY  - 2021
KW  - Fuzzy logic
KW  - Deep learning
KW  - Conferences
KW  - Education
KW  - Medical services
KW  - Learning (artificial intelligence)
KW  - Data models
KW  - Explainable Artificial Intelligence
KW  - Interval Type-2 Fuzzy Logic System
KW  - Deep Learning
DO  - 10.1109/FUZZ45933.2021.9494569
JO  - 2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
IS  - 
SN  - 1558-4739
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
Y1  - 11-14 July 2021
AB  - The recent advances in the field of Artificial Intelligence (AI) have led to the rapid deployment of AI systems in a variety of fields such as healthcare, financial, education etc. However, many of the AI systems are black boxes which restricts the use of these AI in applications that are highly regulated (such as financial, justice, medical, autonomous vehicles etc.) where it is necessary to provide satisfactory explanations for the decisions taken. A variety of approaches that have been proposed to tackle this problem, but these approaches generally emphasize providing satisfactory explanations for individual predictions at the cost of providing explanations at the global level. Hence, to solve these problems, in this paper, we present a hybrid deep learning type-2 fuzzy logic system which addresses these challenges by providing a highly interpretable model that can be trained using both labelled and unlabeled data. We also present a method to extract global and local explanations for this model. We also show that the presented model has reasonable performance when compared to stacked autoencoders deep neural networks.
ER  - 

TY  - JOUR
TI  - Explainable Artificial Intelligence for Tabular Data: A Survey
T2  - IEEE Access
SP  - 135392
EP  - 135422
AU  - M. Sahakyan
AU  - Z. Aung
AU  - T. Rahwan
PY  - 2021
KW  - Data models
KW  - Solid modeling
KW  - Numerical models
KW  - Neural networks
KW  - Medical services
KW  - Inspection
KW  - Black-box models
KW  - explainable artificial intelligence
KW  - machine learning
KW  - model interpretability
DO  - 10.1109/ACCESS.2021.3116481
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 9
VL  - 9
JA  - IEEE Access
Y1  - 2021
AB  - Machine learning techniques are increasingly gaining attention due to their widespread use in various disciplines across academia and industry. Despite their tremendous success, many such techniques suffer from the “black-box” problem, which refers to situations where the data analyst is unable to explain why such techniques arrive at certain decisions. This problem has fuelled interest in Explainable Artificial Intelligence (XAI), which refers to techniques that can easily be interpreted by humans. Unfortunately, many of these techniques are not suitable for tabular data, which is surprising given the importance and widespread use of tabular data in critical applications such as finance, healthcare, and criminal justice. Also surprising is the fact that, despite the vast literature on XAI, there are still no survey articles to date that focus on tabular data. Consequently, despite the existing survey articles that cover a wide range of XAI techniques, it remains challenging for researchers working on tabular data to go through all of these surveys and extract the techniques that are suitable for their analysis. Our article fills this gap by providing a comprehensive and up-to-date survey of the XAI techniques that are relevant to tabular data. Furthermore, we categorize the references covered in our survey, indicating the type of the model being explained, the approach being used to provide the explanation, and the XAI problem being addressed. Our article is the first to provide researchers with a map that helps them navigate the XAI literature in the context of tabular data.
ER  - 

TY  - CONF
TI  - Explainable Machine Learning on Classification of Healthy and Unhealthy Hair
T2  - 2022 2nd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)
SP  - 162
EP  - 167
AU  - W. Y. Chow
AU  - W. W. Heng
AU  - N. A. Abdul-Kadir
AU  - H. Nadaraj
PY  - 2022
KW  - Hair
KW  - Scalp
KW  - Predictive models
KW  - Prediction algorithms
KW  - Classification algorithms
KW  - Medical diagnosis
KW  - Convolutional neural networks
KW  - cnn
KW  - explainable machine learning
KW  - lime explanation
KW  - hair loss
KW  - scalp problem
DO  - 10.1109/ICICyTA57421.2022.10037969
JO  - 2022 2nd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 2nd International Conference on Intelligent Cybernetics Technology & Applications (ICICyTA)
Y1  - 15-16 Dec. 2022
AB  - Early scalp disease diagnosis is made easier with the aid of Deep Learning (DL) algorithms. Utilizing DL algorithms eliminates the requirement for manual data reconstruction and feature extraction for classification purposes. Moreover, the growing usage of DL for essential applications such as medical diagnostics raises reliability issues and there is an urgent need to better understand the DL model predictions. In this study, we implemented a Convolutional Neural Network model to classify hair with and without scalp diseases based on online image datasets. Besides obtaining the standard performance metrics such as accuracy, precision and recall, the Local Interpretable Model-Agnostic Explanations (LIME) technique was used to interpret the model’s decisions. Our studies achieved a better accuracy of 96.63% compared to the previous studies but LIME interpretation of the results revealed that the high classification accuracy does not coincide with the model applicability or practicality.
ER  - 

TY  - CONF
TI  - Interpretation of Lesional Detection via Counterfactual Generation
T2  - 2021 IEEE International Conference on Image Processing (ICIP)
SP  - 96
EP  - 100
AU  - J. Kim
AU  - M. Kim
AU  - Y. M. Ro
PY  - 2021
KW  - Deep learning
KW  - Visualization
KW  - Image processing
KW  - Conferences
KW  - Data visualization
KW  - Medical diagnosis
KW  - Lesions
KW  - Deep learning
KW  - Explainable AI
KW  - Counterfactual generation
KW  - Medical image
DO  - 10.1109/ICIP42928.2021.9506282
JO  - 2021 IEEE International Conference on Image Processing (ICIP)
IS  - 
SN  - 2381-8549
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Image Processing (ICIP)
Y1  - 19-22 Sept. 2021
AB  - To interpret the decision of Deep Neural Networks (DNNs), explainable artificial intelligence research has been widely investigated. Especially, visualizing the attribution maps is known as one of the efficient ways to provide explanations for the trained networks. Applying existing visualization methods on medical images has significant issues in that the medical images commonly have inherent imbalanced data poses and scarcity. To tackle such issues and provide more accurate explanations in medical images, in this paper, we propose a new explainable framework, Counterfactual Generative Network (CGN). We embed counterfactual lesion prediction of DNNs to our explainable framework as prior conditions and guide to generate various counterfactual lesional images from normal input sources, or vice versa. By doing so, CGN can represent detailed attribution maps and generate corresponding normal images from leisonal inputs. Extensive experiments are conducted on the two chest X-ray datasets to verify the effectiveness of our method.
ER  - 

TY  - CONF
TI  - Explaining probabilistic Artificial Intelligence (AI) models by discretizing Deep Neural Networks
T2  - 2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)
SP  - 446
EP  - 448
AU  - R. Saleem
AU  - B. Yuan
AU  - F. Kurugollu
AU  - A. Anjum
PY  - 2020
KW  - Cloud computing
KW  - Artificial Intelligence
KW  - Deep Neural Networks
KW  - Partial differential equations
KW  - Discretization
DO  - 10.1109/UCC48980.2020.00070
JO  - 2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC)
Y1  - 7-10 Dec. 2020
AB  - Artificial Intelligence (AI) models can learn from data and make decisions without any human intervention. However, the deployment of such models is challenging and risky because we do not know how the internal decisionmaking is happening in these models. Especially, the high-risk decisions such as medical diagnosis or automated navigation demand explainability and verification of the decision making process in AI algorithms. This research paper aims to explain Artificial Intelligence (AI) models by discretizing the black-box process model of deep neural networks using partial differential equations. The PDEs based deterministic models would minimize the time and computational cost of the decision-making process and reduce the chances of uncertainty that make the prediction more trustworthy.
ER  - 

TY  - CONF
TI  - Actionable XAI for the Fuzzy Integral
T2  - 2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
SP  - 1
EP  - 8
AU  - B. Murray
AU  - D. T. Anderson
AU  - T. C. Havens
PY  - 2021
KW  - Conferences
KW  - Medical services
KW  - Linguistics
KW  - Agriculture
KW  - Security
KW  - Artificial intelligence
KW  - Task analysis
KW  - fuzzy integral
KW  - Choquet integral
KW  - XAI
KW  - explainable AI
KW  - artificial intelligence
KW  - linguistic summarization
DO  - 10.1109/FUZZ45933.2021.9494563
JO  - 2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
IS  - 
SN  - 1558-4739
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
Y1  - 11-14 July 2021
AB  - The adoption of artificial intelligence (AI) into domains that impact human life (healthcare, agriculture, security and defense, etc.) has led to an increased demand for explainable AI (XAI). Herein, we focus on an under represented piece of the XAI puzzle, information fusion. To date, a number of low-level XAI explanation methods have been proposed for the fuzzy integral (FI). However, these explanations are tailored to experts and its not always clear what to do with the information they return. In this article we review and categorize existing FI work according to recent XAI nomenclature. Second, we identify a set of initial actions that a user can take in response to these low-level statistical, graphical, local, and linguistic XAI explanations. Third, we investigate the design of an interactive user friendly XAI report. Two case studies, one synthetic and one real, show the results of following recommended actions to understand and improve tasks involving classification.
ER  - 

TY  - CONF
TI  - LISA : Enhance the explainability of medical images unifying current XAI techniques
T2  - 2022 IEEE 7th International conference for Convergence in Technology (I2CT)
SP  - 1
EP  - 9
AU  - S. H. P. Abeyagunasekera
AU  - Y. Perera
AU  - K. Chamara
AU  - U. Kaushalya
AU  - P. Sumathipala
AU  - O. Senaweera
PY  - 2022
KW  - Additives
KW  - Transfer learning
KW  - Neural networks
KW  - Mission critical systems
KW  - Debugging
KW  - Predictive models
KW  - Medical diagnosis
KW  - Explainable Artificial Intelligence
KW  - LIME
KW  - SHAP
KW  - Anchors
KW  - Integrated Gradients
KW  - XAI
KW  - CXR
KW  - Chest X-ray
KW  - Shapley Additive Explanations
KW  - Local Interpritable Model Agnostic Explanations
KW  - CNN
KW  - Unified Explanations
KW  - LISA
DO  - 10.1109/I2CT54291.2022.9824840
JO  - 2022 IEEE 7th International conference for Convergence in Technology (I2CT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE 7th International conference for Convergence in Technology (I2CT)
Y1  - 7-9 April 2022
AB  - This work proposed a unified approach to increase the explainability of the predictions made by Convolution Neural Networks (CNNs) on medical images using currently available Explainable Artificial Intelligent (XAI) techniques. This method in-cooperates multiple techniques such as LISA aka Local Interpretable Model Agnostic Explanations (LIME), integrated gradients, Anchors and Shapley Additive Explanations (SHAP) which is Shapley values-based approach to provide explanations for the predictions provided by Blackbox models. This unified method increases the confidence in the black-box model’s decision to be employed in crucial applications under the supervision of human specialists. In this work, a Chest X-ray (CXR) classification model for identifying Covid-19 patients is trained using transfer learning to illustrate the applicability of XAI techniques and the unified method (LISA) to explain model predictions. To derive predictions, an image-net based Inception V2 model is utilized as the transfer learning model.
ER  - 

TY  - CONF
TI  - Challenges facing AI and Big data for Resource-poor Healthcare System
T2  - 2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC)
SP  - 1426
EP  - 1433
AU  - A. Kaur
AU  - R. Garg
AU  - P. Gupta
PY  - 2021
KW  - Ethics
KW  - Costs
KW  - Medical services
KW  - Big Data
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - Faces
KW  - Health-care
KW  - Resource-poor
KW  - Big Data
KW  - Artificial Intelligence
KW  - AI
KW  - Developing Countries
KW  - Electronic Health Records
KW  - Healthcare Analytics
DO  - 10.1109/ICESC51422.2021.9532955
JO  - 2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC)
Y1  - 4-6 Aug. 2021
AB  - During the last decade, major advancements in the health-care system have developed by offering numerous benefits to the patients throughout the world but resource-poor countries are not benefited through the best practices of health-care due to the lack of educated health-care providers, infrastructure, financial and technical issues, etc. Health-care systems in resource-poor countries face many challenges including increased healthcare cost, patient safety, overtreatment and failure to adopt best practices for health-care. In such countries, massive data generate from various resources including medical imaging, patient record, pharmaceutical reports, and medical devices. The exponential growth in medical data and advancement in health-care technologies focus data analysts to come up with innovative solutions for improving health-care practices in poor countries. Big data analytics provide tools to collect manage and analyze structured and unstructured medical data to find useful insights. Complexity and volume of medical data also show that, Artificial intelligence (AI) has the ability to approximate conclusions without direct human input, which can be applied in the health-care system of resource-poor countries and is now being utilized to further develop health services in high-income countries. Numerous investigations show that, AI performs better than humans in certain health-care undertakings such as diagnosis of cancer, tumor, heart diseases, radiology, etc. Popular AI techniques include machine learning methods such as neural network, support vector machine, and deep learning for structured data as well as natural language processing for unstructured data. There is a variety of hurdles around the use of big data and AI in health-care includes regulation, permission, transparency, and accountability. Also, the collection of data from an individual-a prerequisite for big data analytics is a technical and ethical issue. There are a lot of challenges for AI and big data in health-care but efforts need to be made before these techniques can be deployed in ethical and safe way. In this chapter, we discuss the challenges that AI and big data techniques face in resource-poor health care system and how it can be used to improve health outcomes in resource-poor countries.
ER  - 

TY  - CONF
TI  - Causal Inference for Personalized Treatment Effect Estimation for given Machine Learning Models
T2  - 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)
SP  - 1289
EP  - 1295
AU  - J. Rust
AU  - S. Autexier
PY  - 2022
KW  - Training
KW  - Adaptation models
KW  - Uncertainty
KW  - Federated learning
KW  - Pipelines
KW  - Neural networks
KW  - Measurement uncertainty
KW  - causal ML
KW  - explainable artificial intelligence
KW  - federated learning
KW  - machine learning
KW  - AI in healthcare
KW  - treatment effect estimation
DO  - 10.1109/ICMLA55696.2022.00206
JO  - 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 21st IEEE International Conference on Machine Learning and Applications (ICMLA)
Y1  - 12-14 Dec. 2022
AB  - We propose a causal machine learning inference pipeline that combines a given predictive machine learning model with analytical estimations of average treatment effects. It enables to utilize any predictive model for causal inference, which makes it easy to adapt the approach to existing systems. By first estimating the average treatment effect of an intervention on predictors instead of the outcome variable, a causal relationship between an intervention and a wide range of variables is determined. Next, artificial samples are created that are evaluated using the given predictive model to link interventions and outcomes and also allows inferring measurements of uncertainty. Finally, simulations using again the given predictive model are performed to compute measurements of confidence and that allow to compare – according to the given predictive model – the effect of specific treatments. We furthermore demonstrate how this inference engine can be adapted to a privacy-preserving federated learning environment where training data is horizontally distributed across multiple datasets without compromising on our approach’s accuracy. The approach has been evaluated on a use case with a predictive model for the quality of life score of cancer patients, to determine medical interventions to improve their individual quality of life score.
ER  - 

TY  - CONF
TI  - The CRISP-ML Approach to Handling Causality and Interpretability Issues in Machine Learning
T2  - 2021 IEEE International Conference on Big Data (Big Data)
SP  - 2306
EP  - 2312
AU  - I. Kolyshkina
AU  - S. Simoff
PY  - 2021
KW  - Machine learning algorithms
KW  - Software algorithms
KW  - Machine learning
KW  - Big Data
KW  - Data models
KW  - Software
KW  - Stakeholders
KW  - interpretability
KW  - causality
KW  - machine learning
KW  - process methodology
KW  - CRISP-ML
DO  - 10.1109/BigData52589.2021.9671754
JO  - 2021 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Big Data (Big Data)
Y1  - 15-18 Dec. 2021
AB  - Interpretability in machine learning projects and one of its aspects - causal inference - have recently gained significant interest and focus. Due to the recent rapid appearance of frameworks, methods, algorithms and software most of which are in early stages of their development, it can be confusing for practitioners and researchers involved in a machine learning project to choose the best approach and set of techniques that would efficiently deliver valid insights while minimising the known risks of failure of data-related projects. CRISP-ML process methodology minimises this confusion by outlining a clear step-by-step process that explicitly treats of interpretability issues through every stage. The paper presents an update of CRISP-ML, which incorporates causality in a similar way and supports formalisation, design and implementation of specific instances of CRISP-ML process, subject to required levels of interpretability and causality of results. The approach is demonstrated on examples from the domains of credit risk, public health and healthcare.
ER  - 

TY  - CONF
TI  - An Explainable AI Model for Interpretable Lung Disease Classification
T2  - 2021 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS)
SP  - 98
EP  - 103
AU  - V. Pitroda
AU  - M. M. Fouda
AU  - Z. M. Fadlullah
PY  - 2021
KW  - Measurement
KW  - COVID-19
KW  - Pulmonary diseases
KW  - Lung
KW  - Bones
KW  - Convolutional neural networks
KW  - Artificial intelligence
KW  - Deep learning
KW  - explainable AI
KW  - Layer-wise Relevance Propagation
KW  - LIME
KW  - Deep Taylor Decomposition
KW  - Guided Backpropagation
KW  - medical diagnosis
KW  - chest X-ray
KW  - COVID-19
DO  - 10.1109/IoTaIS53735.2021.9628573
JO  - 2021 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS)
Y1  - 23-24 Nov. 2021
AB  - In this paper, we develop a framework for lung disease identification from chest X-ray images by differentiating the novel coronavirus disease (COVID-19) or other disease-induced lung opacity samples from normal cases. We perform image processing tasks, segmentation, and train a customized Convolutional Neural Network (CNN) that obtains reasonable performance in terms of classification accuracy. To address the black-box nature of this complex classification model, which emerged as a key barrier to applying such Artificial Intelligence (AI)-based methods for automating medical decisions raising skepticism among clinicians, we address the need to quantitatively interpret the performance of our adopted approach using a Layer-wise Relevance Propagation (LRP)-based method. We also used a pixel flipping-based, robust performance metric to evaluate the explainability of our adopted LRP method and compare its performance with other explainable methods, such as Local Interpretable Model Agnostic Explanation (LIME), Guided Backpropagation (GB), and Deep Taylor Decomposition (DTD).
ER  - 

TY  - JOUR
TI  - Blockchain for Privacy Preserving and Trustworthy Distributed Machine Learning in Multicentric Medical Imaging (C-DistriM)
T2  - IEEE Access
SP  - 183939
EP  - 183951
AU  - F. Zerka
AU  - V. Urovi
AU  - A. Vaidyanathan
AU  - S. Barakat
AU  - R. T. H. Leijenaar
AU  - S. Walsh
AU  - H. Gabrani-Juma
AU  - B. Miraglio
AU  - H. C. Woodruff
AU  - M. Dumontier
AU  - P. Lambin
PY  - 2020
KW  - Data models
KW  - Training
KW  - Machine learning
KW  - Servers
KW  - Biomedical imaging
KW  - Blockchain
KW  - data privacy
KW  - decentralized learning
KW  - distributed learning
DO  - 10.1109/ACCESS.2020.3029445
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 8
VL  - 8
JA  - IEEE Access
Y1  - 2020
AB  - The utility of Artificial Intelligence (AI) in healthcare strongly depends upon the quality of the data used to build models, and the confidence in the predictions they generate. Access to sufficient amounts of high-quality data to build accurate and reliable models remains problematic owing to substantive legal and ethical constraints in making clinically relevant research data available offsite. New technologies such as distributed learning offer a pathway forward, but unfortunately tend to suffer from a lack of transparency, which undermines trust in what data are used for the analysis. To address such issues, we hypothesized that, a novel distributed learning that combines sequential distributed learning with a blockchain-based platform, namely Chained Distributed Machine learning C-DistriM, would be feasible and would give a similar result as a standard centralized approach. C-DistriM enables health centers to dynamically participate in training distributed learning models. We demonstrate C-DistriM using the NSCLC-Radiomics open data to predict two-year lung-cancer survival. A comparison of the performance of this distributed solution, evaluated in six different scenarios, and the centralized approach, showed no statistically significant difference (AUCs between central and distributed models), all DeLong tests yielded  $p$ -val >0.05. This methodology removes the need to blindly trust the computation in one specific server on a distributed learning network. This fusion of blockchain and distributed learning serves as a proof-of-concept to increase transparency, trust, and ultimately accelerate the adoption of AI in multicentric studies. We conclude that our blockchain-based model for sequential training on distributed datasets is a feasible approach, provides equivalent performance to the centralized approach.
ER  - 

TY  - CONF
TI  - An Approach to identify Captioning Keywords in an Image using LIME
T2  - 2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)
SP  - 648
EP  - 651
AU  - S. Sahay
AU  - N. Omare
AU  - K. K. Shukla
PY  - 2021
KW  - Computational modeling
KW  - Education
KW  - Medical services
KW  - Machine learning
KW  - Predictive models
KW  - Task analysis
KW  - Intelligent systems
KW  - Explainable AI
KW  - Interpretability
KW  - Explainability
KW  - LIME
KW  - Image Captioning
DO  - 10.1109/ICCCIS51004.2021.9397159
JO  - 2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)
Y1  - 19-20 Feb. 2021
AB  - Machine Learning models are being increasingly deployed to tackle real-world problems in various domains like healthcare, crime and education among many others. However, most of the models are practically "black-boxes": although they may provide accurate results, they are unable to provide any conclusive reasoning for those results. In order for these decisions to be trusted, they must be explainable. Explainable AI, or XAI refers to methods and techniques in the application of AI such that the results of the solution are understandable by human experts. This paper focuses on the task of Image Captioning, and tries to employ XAI techniques such as LIME (Local Interpretable Model-Agnostic Explanations) to explain the predictions of complex image captioning models. It visually depicts the part of the image corresponding to a particular word in the caption, thereby justifying why the model predicted that word.
ER  - 

TY  - STD
TI  - IEEE Standard for Transparency of Autonomous Systems
T2  - IEEE Std 7001-2021
SP  - 1
EP  - 54
PY  - 2022
KW  - IEEE Standards
KW  - Autonomous systems
KW  - Artificial intelligence
KW  - Ethics
KW  - Robots
KW  - Intelligent transportation systems
KW  - autonomous systems
KW  - artificial intelligence
KW  - ethics
KW  - IEEE 7001
KW  - transparency
DO  - 10.1109/IEEESTD.2022.9726144
JO  - IEEE Std 7001-2021
IS  - 
SN  - 
VO  - 
VL  - 
JA  - IEEE Std 7001-2021
Y1  - 4 March 2022
AB  - Measurable, testable levels of transparency, so that autonomous systems can be objectively assessed, and levels of compliance determined, are described in this standard. (The PDF of this standard is available in the IEEE GET program at https://ieeexplore.ieee.org/browse/standards/get-program/page/series?id=93)
ER  - 

TY  - CHAP
TI  - 8 Artificial Intelligence and Machine Learning Approach for Development and Discovery of Drug
T2  - Artificial Intelligence for Health 4.0: Challenges and Applications
SP  - 211
EP  - 232
PY  - 2022
DO  - 
PB  - River Publishers
SN  - 9788770227834
UR  - http://ieeexplore.ieee.org/document/9933692
AB  - Healthcare is one of the major success stories of our times. Medical science has improved rapidly, raising life expectancy around the world. However, as longevity increases, healthcare systems face growing demands for their services, rising costs, and a workforce that is struggling to meet the needs of its patients. Healthcare is one of the most critical sectors in the broader landscape of big data because of its fundamental role in a productive, thriving society. Building on automation, artificial intelligence (AI) has the potential to revolutionize healthcare and help address some of the challenges set out above. The application of AI to healthcare data can literally be a matter of life and death. AI can assist doctors, nurses, and other healthcare workers in their daily work. AI in healthcare can enhance preventive care and quality of life, produce more accurate diagnoses and treatment plans, and lead to better patient outcomes overall. This book gives insights into the latest developments of applications of AI in biomedicine, including disease diagnostics, pharmaceutical processing, patient care and monitoring, biomedical information, and biomedical research. It also presents an outline of the recent breakthroughs in the application of AI in healthcare, describes a roadmap to building effective, reliable, and safe AI systems, and discusses the possible future direction of AI augmented healthcare systems. AI has countless applications in healthcare. Whether it&#x2019;s being used to discover links between genetic codes, to power surgical robots or even to maximize hospital efficiency; AI has been a boon to the healthcare industry.
ER  - 

TY  - CONF
TI  - Influence of Artificial Intelligence on Personalized Medical Predictions, Interventions and Quality of Life Issues
T2  - 2020 24th International Conference on System Theory, Control and Computing (ICSTCC)
SP  - 445
EP  - 450
AU  - M. Ivanovic
AU  - I. Balaz
PY  - 2020
KW  - Artificial intelligence
KW  - Medical services
KW  - Diseases
KW  - Predictive analytics
KW  - Predictive models
KW  - Cancer
KW  - Big Data
KW  - Artificial intelligence
KW  - Personalized services in medicine
KW  - Patients’ Quality of life issues
DO  - 10.1109/ICSTCC50638.2020.9259674
JO  - 2020 24th International Conference on System Theory, Control and Computing (ICSTCC)
IS  - 
SN  - 2372-1618
VO  - 
VL  - 
JA  - 2020 24th International Conference on System Theory, Control and Computing (ICSTCC)
Y1  - 8-10 Oct. 2020
AB  - The main intention of this paper is to give a brief discussion on modern approaches in gathering, processing, and using huge amounts of data in medicine and healthcare influenced by powerful techniques and approaches of Artificial Intelligence. Attention is put on several possibilities to improve patients' health status and quality of life issues. Characteristic examples and scenarios are presented inspired by contemporary activities within several big projects in medical domains and authors' long-lasting experiences in using Artificial Intelligence, Machine Learning, Agent Technologies in numerous applications including several medical areas.
ER  - 

TY  - CONF
TI  - Explainable Deep Learning Applied to Understanding Opioid Use Disorder and Its Risk Factors
T2  - 2019 IEEE International Conference on Big Data (Big Data)
SP  - 4883
EP  - 4888
AU  - T. E. Workman
AU  - Y. Shao
AU  - J. Kupersmith
AU  - F. Sandbrink
AU  - J. L. Goulet
AU  - N. M. Shaar
AU  - C. Spevak
AU  - C. Brandt
AU  - M. R. Blackman
AU  - Q. Zeng-Treitler
PY  - 2019
KW  - Machine learning
KW  - Logistics
KW  - Predictive models
KW  - Pain
KW  - Training
KW  - Correlation
KW  - Opioid Use Disorder
KW  - Explainable AI
KW  - Impact Scores
KW  - Deep Learning
DO  - 10.1109/BigData47090.2019.9006297
JO  - 2019 IEEE International Conference on Big Data (Big Data)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Big Data (Big Data)
Y1  - 9-12 Dec. 2019
AB  - Opioid Use Disorder is an international crisis, affecting many populations. Deep learning models can potentially predict opioid use disorder, but provide little insight to how predictions are derived. Impact scores, a new development in explainable artificial intelligence, measure how individual features affect deep learning outcomes. We modeled clinical visits to predict opioid use disorder, computed impact scores, and compared them to odds log ratios from logistic regression. Impact scores were generally comparable to odds log ratios, in providing insight to opioid abuse risk, but from a better-performing method than logistic regression.
ER  - 

TY  - CONF
TI  - Explaining Human Activities Instances Using Deep Learning Classifiers
T2  - 2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA)
SP  - 1
EP  - 10
AU  - L. Arrotta
AU  - G. Civitarese
AU  - M. Fiori
AU  - C. Bettini
PY  - 2022
KW  - Deep learning
KW  - Aggregates
KW  - Data visualization
KW  - Medical services
KW  - Data science
KW  - Behavioral sciences
KW  - Human activity recognition
KW  - eXplainable AI
KW  - Human Activity Recognition
KW  - Deep Learning
DO  - 10.1109/DSAA54385.2022.10032345
JO  - 2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA)
Y1  - 13-16 Oct. 2022
AB  - The recognition of human activities in sensorized smart-home environments enables a wide variety of healthcare applications, including the detection of early symptoms of cognitive decline. The most effective Human Activity Recognition (HAR) methods are based on supervised Deep Learning classifiers. Those models are usually considered as black boxes, and the rationale behind their decisions is difficult to understand for human beings. The recent advances in eXplainable Artificial Intelligence (XAI) offer promising tools to make HAR models more transparent. The state-of-the-art explainable HAR methods provide explanations for the output of classifiers that periodically predict the performed activity on short time windows (usually in the range of 15-60 seconds). However, non-technical users may be more interested in investigating explanations associated with complete activity instances (e.g., an instance of the cooking activity may last 30 minutes). Unfortunately, temporally extending the time window harms the recognition rate of HAR classifiers. In this paper, we propose DeXAR++: a novel method that generates explanations for human activity instances based on deep learning classifiers. The sensor data time windows used for classification are encoded as images. DeXAR++ aggregates the explanations generated by a computer-vision XAI approach on each time window to obtain a single explanation for approximated activity instances. Moreover, DeXAR++ includes a novel visualization approach particularly suitable for non-expert users. We evaluate DeXAR++ with both automatic and user-based evaluation methodologies on a public dataset of activities performed in smart-home environments, showing that our results outperform the ones obtained by state-of-the-art methods.
ER  - 

TY  - CONF
TI  - Metaverse assisted Telesurgery in Healthcare 5.0: An interplay of Blockchain and Explainable AI
T2  - 2022 International Conference on Computer, Information and Telecommunication Systems (CITS)
SP  - 1
EP  - 5
AU  - P. Bhattacharya
AU  - M. S. Obaidat
AU  - D. Savaliya
AU  - S. Sanghavi
AU  - S. Tanwar
AU  - B. Sadaun
PY  - 2022
KW  - Metaverse
KW  - Hospitals
KW  - Telemedicine
KW  - Medical services
KW  - Blockchains
KW  - Telecommunications
KW  - Stakeholders
KW  - Blockchain
KW  - Deep Learning
KW  - Explainable AI
KW  - Healthcare 5.0
KW  - Metaverse
KW  - Telesurgery
DO  - 10.1109/CITS55221.2022.9832978
JO  - 2022 International Conference on Computer, Information and Telecommunication Systems (CITS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Computer, Information and Telecommunication Systems (CITS)
Y1  - 13-15 July 2022
AB  - Smart healthcare has transitioned towards health-care 5.0, which allows ambient tracking of patients, emotive telemedicine, telesurgery, wellness monitoring, virtual clinics, and personalized care. Thus, the metaverse is a potential tool to leverage digital connectivity via improved healthcare experience in virtual environments. However, despite its potential benefits, patient’s sensitive information is captured, and digital avatars are created that interact with healthcare stakeholders for connected virtual care. As metaverse components are decentralized, blockchain (BC) is a potential solution to induce transparency and immutability in stored transactions on metaverse. For clinical decision support in BC-assisted metaverse enabled, Healthcare 5.0, accurate and interpretable diagnosis is critical. Thus, explainable AI (xAI) forms another critical component that provides trust in the healthcare informatics front. A dual solution of trusted informatics is possible via the interplay of BC and xAI in metaverse-enabled Healthcare 5.0. The article investigates the interplay through a proposed telesurgical scheme between patients, virtual hospitals, and doctors. Next, we discuss the potential challenges and present an experimental use-case of the benefits of our proposed architecture over traditional telesurgery systems.
ER  - 

TY  - CONF
TI  - Towards Interpretability and Personalization: A Predictive Framework for Clinical Time-series Analysis
T2  - 2021 IEEE International Conference on Data Mining (ICDM)
SP  - 340
EP  - 349
AU  - Y. Li
AU  - X. Zhang
AU  - B. Qian
AU  - Z. Gao
AU  - C. Guan
AU  - Y. Zheng
AU  - H. Zheng
AU  - F. Wu
AU  - C. Li
PY  - 2021
KW  - Precision medicine
KW  - Conferences
KW  - Time series analysis
KW  - Supervised learning
KW  - Semantics
KW  - Machine learning
KW  - Predictive models
KW  - Health informatics
KW  - Clinical time-series analysis
KW  - Deep learning
DO  - 10.1109/ICDM51629.2021.00045
JO  - 2021 IEEE International Conference on Data Mining (ICDM)
IS  - 
SN  - 2374-8486
VO  - 
VL  - 
JA  - 2021 IEEE International Conference on Data Mining (ICDM)
Y1  - 7-10 Dec. 2021
AB  - Clinical time-series is receiving long-term attention in data mining and machine learning communities and has boosted a variety of data-driven applications. Identifying similar patients or subgroups from clinical time-series is an essential step to design tailored treatments in clinical practice. However, most of the existing methods are either purely unsupervised that tend to neglect the patient outcome information or cannot generate personalized patient representation through supervised learning, thus may fail to identify ‘truly similar patients’ (i.e., patients who similar in both outcomes and individual outcome-related clinical variables). To tackle these limitations, we propose a novel predictive clinical time-series analysis framework. Specifically, our framework uses task-specific information to rule out the task-irrelevant factors in each patient data individually and generates the contribution scores that reveal the factors’ importance for the patient outcome. Then a patient representation construction method is proposed to generate task-related and personalized representations by combining remained factors and their contribution scores. At last, similarity measurement or cluster analysis can be conducted. We evaluate our framework on three real-world clinical time-series datasets, empirically demonstrate that our framework achieves improvements in prediction performance, similarity measurement, and clustering, thus potentially benefiting patient-similarity-based precision medicine applications.
ER  - 

TY  - CONF
TI  - Explaining Machine Learning Predictions: A Case Study
T2  - 2022 Trends in Electrical, Electronics, Computer Engineering Conference (TEECCON)
SP  - 72
EP  - 77
AU  - P. Dutta
AU  - N. B. Muppalaneni
PY  - 2022
KW  - Analytical models
KW  - Computational modeling
KW  - Sociology
KW  - Transportation
KW  - Machine learning
KW  - Predictive models
KW  - Market research
KW  - Artificial Intelligence
KW  - Diabetes Risk Prediction
KW  - Explanation
KW  - Machine Learning
DO  - 10.1109/TEECCON54414.2022.9854821
JO  - 2022 Trends in Electrical, Electronics, Computer Engineering Conference (TEECCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 Trends in Electrical, Electronics, Computer Engineering Conference (TEECCON)
Y1  - 26-27 May 2022
AB  - The growing trends and demands for Artificial Intelligence in various domains due to their excellent performance and generalization ability are known to all. These decisions affect the population in general as they usually deal with sensitive tasks in various fields such as healthcare, education, transportation, etc. Hence, understanding these learned representations would add more descriptive knowledge to better interpret the decisions with the ground truth. The European General Data Protection Regulation reserves the right to receive an explanation against a model producing an automated decision. Understanding the decisions would validate the model behavior, ensure trust, and deal with the risk associated with the model. Upon analyzing the relevant features, we can decide whether the model predictions could be trusted or not in the future. We can further try to reduce the misclassification rate by rectifying the features (of the misclassified instances) if needed. In this way, we can peek into the black-box and gain insight into a model’s prediction, thus understanding the learned representations. In pursuit of this objective, a common approach would be to devise an explanatory model that would explain the predictions made by a model and further analyze those predictions with the ground truth information. We initiated a case study on a diabetes risk prediction dataset by understanding local predictions made by five different Machine Learning models and trying to provide explanations for the misclassified instances.
ER  - 

TY  - CONF
TI  - Responsible Artificial Intelligence for Preterm Birth Prediction in Vulnerable Populations
T2  - 2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
SP  - 1
EP  - 6
AU  - G. Marvin
AU  - J. Nakatumba-Nabende
AU  - N. Hellen
AU  - M. G. R. Alam
PY  - 2022
KW  - Pediatrics
KW  - Technological innovation
KW  - Sociology
KW  - Buildings
KW  - Medical services
KW  - Fourth Industrial Revolution
KW  - Synchronization
KW  - Explainable Artificial Intelligence (XAI)
KW  - Preterm Birth
KW  - Predictive Healthcare
KW  - Maternal and Neonatal Health
KW  - Vulnerable Populations
DO  - 10.1109/CSDE56538.2022.10089301
JO  - 2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)
Y1  - 18-20 Dec. 2022
AB  - Newborn and child mortality prevention is one of the prioritized Sustainable Development Goals (SDG) targets of the World Health Organization (WHO) expected by 2030. This SDG Target 3.2 has prompted for a lot of solutions for the most affected regions like Sub-Saharan Africa, Central and Southern Asia in order to stop preventable deaths of children under the age of 5 years and newborns although available solutions are not yet sufficient to achieve the Goal. The 4th Industrial revolution has further advanced the need for a reliable interdisciplinary approach that leverages technological advancements in achieving the SDG Target 3.2. In this work, we present a trustworthy Artificial Intelligence (AI) solution that blends with the data driven emerging technologies to reduce this global burden by transparently and interpretably predicting preterm births for patients and physicians for predictive and preventive action towards lowering neonatal deaths and increasing child survival. This AI solution can globally improve maternal and child healthcare among nations the run curative healthcare systems. We used Random Forest and KNeighbors and obtained an accuracy of 100% and 78% with respectively with Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic (ADASYN) class balancing techniques. With interpretability of the random forest algorithm, we can responsibly improve AI technology adoption for maternal and child health and provide useful automated data driven insights to maternal healthcare management stakeholders and policy makers for a sustainable healthcare system in developing countries.
ER  - 

TY  - JOUR
TI  - The Role of Explainability in Assuring Safety of Machine Learning in Healthcare
T2  - IEEE Transactions on Emerging Topics in Computing
SP  - 1746
EP  - 1760
AU  - Y. Jia
AU  - J. McDermid
AU  - T. Lawton
AU  - I. Habli
PY  - 2022
KW  - Predictive models
KW  - Medical services
KW  - Hazards
KW  - Standards
KW  - Computational modeling
KW  - Data models
KW  - Training
KW  - Explainability
KW  - machine learning
KW  - safety assurance
DO  - 10.1109/TETC.2022.3171314
JO  - IEEE Transactions on Emerging Topics in Computing
IS  - 4
SN  - 2168-6750
VO  - 10
VL  - 10
JA  - IEEE Transactions on Emerging Topics in Computing
Y1  - 1 Oct.-Dec. 2022
AB  - Established approaches to assuring safety-critical systems and software are difficult to apply to systems employing ML where there is no clear, pre-defined specification against which to assess validity. This problem is exacerbated by the “opaque” nature of ML where the learnt model is not amenable to human scrutiny. Explainable AI (XAI) methods have been proposed to tackle this issue by producing human-interpretable representations of ML models which can help users to gain confidence and build trust in the ML system. However, little work explicitly investigates the role of explainability for safety assurance in the context of ML development. This paper identifies ways in which XAI methods can contribute to safety assurance of ML-based systems. It then uses a concrete ML-based clinical decision support system, concerning weaning of patients from mechanical ventilation, to demonstrate how XAI methods can be employed to produce evidence to support safety assurance. The results are also represented in a safety argument to show where, and in what way, XAI methods can contribute to a safety case. Overall, we conclude that XAI methods have a valuable role in safety assurance of ML-based systems in healthcare but that they are not sufficient in themselves to assure safety.
ER  - 

TY  - CONF
TI  - Deep Learning in Practice: Guidelines for Model Selection
T2  - 2019 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)
SP  - 64
EP  - 68
AU  - G. Ramachandran
AU  - V. Bhatia
PY  - 2019
KW  - Deep learning
KW  - Measurement
KW  - Industries
KW  - Accuracy
KW  - Computational modeling
KW  - Neural networks
KW  - Medical services
KW  - Numerical models
KW  - Standards
KW  - Guidelines
KW  - deep learning
KW  - artificial intelligence
KW  - neural networks
KW  - explainable AI
DO  - 10.1109/CCEM48484.2019.00014
JO  - 2019 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2019 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)
Y1  - 19-20 Sept. 2019
AB  - Deep Learning models are becoming the de facto standard in most image, text and speech understanding tasks. Model selection based only on numerical metrics such as accuracy and inference time is not sufficient especially in highly regulated industries like healthcare or autonomous driving where lives are at stake. In order to trust the model, interpreting the reasons behind the model's decisions is essential. We propose a three pronged approach in selecting models based on accuracy, inference time and on whether they learn the right features
ER  - 

TY  - CONF
TI  - A Review of Credit Card Fraud Detection Using Machine Learning Techniques
T2  - 2020 5th International Conference on Cloud Computing and Artificial Intelligence: Technologies and Applications (CloudTech)
SP  - 1
EP  - 5
AU  - N. Boutaher
AU  - A. Elomri
AU  - N. Abghour
AU  - K. Moussaid
AU  - M. Rida
PY  - 2020
KW  - Cloud computing
KW  - Online banking
KW  - Machine learning
KW  - Medical services
KW  - Learning (artificial intelligence)
KW  - Credit cards
KW  - Manufacturing
KW  - big data
KW  - machine learning techniques
KW  - credit card fraud detection
KW  - legitimate transaction
KW  - financial institutions
KW  - digitalization
KW  - e-commerce
DO  - 10.1109/CloudTech49835.2020.9365916
JO  - 2020 5th International Conference on Cloud Computing and Artificial Intelligence: Technologies and Applications (CloudTech)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2020 5th International Conference on Cloud Computing and Artificial Intelligence: Technologies and Applications (CloudTech)
Y1  - 24-26 Nov. 2020
AB  - Big Data technologies concern several critical areas such as Healthcare, Finance, Manufacturing, Transport, and E-Commerce. Hence, they play an indispensable role in the financial sector, especially within the banking services which are impacted by the digitalization of services and the evolvement of e-commerce transactions. Therefore, the emergence of the credit card use and the increasing number of fraudsters have generated different issues that concern the banking sector. Unfortunately, these issues obstruct the performance of Fraud Control Systems (Fraud Detection Systems & Fraud Prevention Systems) and abuse the transparency of online payments. Thus, financial institutions aim to secure credit card transactions and allow their customers to use e-banking services safely and efficiently. To reach this goal, they try to develop more relevant fraud detection techniques that can identify more fraudulent transactions and decrease frauds. The purpose of this article is to define the fundamental aspects of fraud detection, the current systems of fraud detection, the issues and challenges of frauds related to the banking sector, and the existing solutions based on machine learning techniques.
ER  - 

TY  - CHAP
TI  - Innovation on Machine Learning in Healthcare Services&#x2013;An Introduction
T2  - Machine Learning for Healthcare Applications
SP  - 1
EP  - 15
AU  - Parthasarathi Pattnayak
AU  - Om Prakash Jena
PY  - 2021
KW  - Medical services
KW  - Machine learning
KW  - Technological innovation
KW  - Particle measurements
KW  - Insurance
KW  - Atmospheric measurements
KW  - Hospitals
KW  - Costs
KW  - Propulsion
KW  - Organizations
DO  - 10.1002/9781119792611.ch1
PB  - Wiley
SN  - 9781119792604
UR  - http://ieeexplore.ieee.org/document/10953497
AB  - Summary <p>The healthcare offerings in evolved and developing international locations are seriously important. The use of machine gaining knowledge of strategies in healthcare enterprise has a crucial significance and increases swiftly. In the beyond few years, there has been widespread traits in how system gaining knowledge of can be utilized in diverse industries and research. The organizations in healthcare quarter need to take benefit of the system studying techniques to gain valuable statistics that could later be used to diagnose illnesses at a great deal in advance ranges. There are multiple and endless Machine learning application in healthcare industry. Some of the most common applications are cited in this section. Machine learning helps streamlining the administrative processes in the hospitals. It also helps mapping and treating the infectious diseases for the personalised medical treatment. Machine learning will affect physician and hospitals by playing a very dominant role in the clinical decision support. For example, it will help earlier identification of the diseases and customise treatment plan that will ensure an optimal outcome. Machine learning can be used to educate patients on several potential disease and their outcomes with different treatment option. As a result it can improve the efficiency hospital and health systems by reducing the cost of the healthcare. Machine learning in healthcare can be used to enhance health information management and the exchange of the health information with the aim of improving and thus, modernising the workflows, facilitating access to clinical data and improving the accuracy of the health information. Above all it brings efficiency and transparency to information process.</p>
ER  - 

TY  - JOUR
TI  - Harnessing the Power of Machine Learning in Dementia Informatics Research: Issues, Opportunities, and Challenges
T2  - IEEE Reviews in Biomedical Engineering
SP  - 113
EP  - 129
AU  - G. Tsang
AU  - X. Xie
AU  - S. -M. Zhou
PY  - 2020
KW  - Dementia
KW  - Neuroimaging
KW  - Machine learning
KW  - Support vector machines
KW  - Informatics
KW  - Prognostics and health management
KW  - Alzheimer's disease
KW  - cognitive assessment
KW  - deep learning
KW  - dementia
KW  - electronic medical records
KW  - health informatics
KW  - machine learning
KW  - neuroimaging
KW  - NLP
DO  - 10.1109/RBME.2019.2904488
JO  - IEEE Reviews in Biomedical Engineering
IS  - 
SN  - 1941-1189
VO  - 13
VL  - 13
JA  - IEEE Reviews in Biomedical Engineering
Y1  - 2020
AB  - Dementia is a chronic and degenerative condition affecting millions globally. The care of patients with dementia presents an ever-continuing challenge to healthcare systems in the 21st century. Medical and health sciences have generated unprecedented volumes of data related to health and wellbeing for patients with dementia due to advances in information technology, such as genetics, neuroimaging, cognitive assessment, free texts, routine electronic health records, etc. Making the best use of these diverse and strategic resources will lead to high-quality care of patients with dementia. As such, machine learning becomes a crucial factor in achieving this objective. The aim of this paper is to provide a state-of-the-art review of machine learning methods applied to health informatics for dementia care. We collate and review the existing scientific methodologies and identify the relevant issues and challenges when faced with big health data. Machine learning has demonstrated promising applications to neuroimaging data analysis for dementia care, while relatively less effort has been made to make use of integrated heterogeneous data via advanced machine learning approaches. We further indicate future potential and research directions in applying advanced machine learning, such as deep learning, to dementia informatics.
ER  - 

TY  - CONF
TI  - Interpretation of Artificial Intelligence Algorithms in the Prediction of Sepsis
T2  - 2019 Computing in Cardiology (CinC)
SP  - Page 1
EP  - Page 4
AU  - I. Murugesan
AU  - K. Murugesan
AU  - L. Balasubramanian
AU  - M. Arumugam
PY  - 2019
KW  - Prediction algorithms
KW  - Predictive models
KW  - Heart rate
KW  - Machine learning algorithms
KW  - Machine learning
KW  - Computational modeling
DO  - 10.22489/CinC.2019.332
JO  - 2019 Computing in Cardiology (CinC)
IS  - 
SN  - 2325-887X
VO  - 
VL  - 
JA  - 2019 Computing in Cardiology (CinC)
Y1  - 8-11 Sept. 2019
AB  - Despite the rise of Artificial Intelligence (AI) algorithms and their applications in various fields, their utilizations in high-risk fields like healthcare and finance is limited because of the lack of interpretability of their inner workings. Some algorithms are interpretable, but not accurate, whereas some produce accurate results and not decipherable. Research is underway to explore the possibilities to interrogate an AI system, and ask why it makes certain decisions. This paper aims to investigate the decision-making process by AI algorithms in the prediction of sepsis based on patients’ clinical records.We were ranked 59 in the PhysioNet/Computing in Cardiology Challenge 2019 and the utility score obtained on the full test set is 0.131, and our team name was ARUL.
ER  - 

TY  - CONF
TI  - ASCAPE: An open AI ecosystem to support the quality of life of cancer patients
T2  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
SP  - 301
EP  - 310
AU  - K. Lampropoulos
AU  - T. Kosmidis
AU  - S. Autexier
AU  - M. Savić
AU  - M. Athanatos
AU  - M. Kokkonidis
AU  - T. Koutsouri
AU  - A. Vizitiu
AU  - A. Valachis
AU  - M. Q. Padron
PY  - 2021
KW  - Hospitals
KW  - Conferences
KW  - Sociology
KW  - Ecosystems
KW  - Machine learning
KW  - Data models
KW  - Stakeholders
KW  - Quality-of-life
KW  - Artificial intelligence
KW  - intelligent systems
KW  - Machine Learning
KW  - Cancer Patients
DO  - 10.1109/ICHI52183.2021.00054
JO  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2021 IEEE 9th International Conference on Healthcare Informatics (ICHI)
Y1  - 9-12 Aug. 2021
AB  - The latest cancer statistics indicate a decrease in cancer-related mortality. However, due to the growing and ageing population, the absolute number of people living with cancer is set to keep increasing. This paper presents ASCAPE, an open AI infrastructure that takes advantage of the recent advances in Artificial Intelligence (Al) and Machine Learning (ML) to support cancer patients’ quality of life (QoL). With ASCAPE health stakeholders (e.g. hospitals) can locally process their private medical data and then share the produced knowledge (ML models) through the open AI infrastructure.
ER  - 

TY  - CONF
TI  - Building predictive models of healthcare costs with open healthcare data
T2  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
SP  - 1
EP  - 3
AU  - A. Ravishankar Rao
AU  - S. Garai
AU  - S. Dey
AU  - H. Peng
PY  - 2020
KW  - Medical services
KW  - Machine learning
KW  - Predictive models
KW  - Planning
KW  - Regression tree analysis
KW  - Open data
KW  - Medical diagnostic imaging
KW  - open health data
KW  - predictive models
KW  - cost prediction
DO  - 10.1109/ICHI48887.2020.9374348
JO  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
IS  - 
SN  - 2575-2634
VO  - 
VL  - 
JA  - 2020 IEEE International Conference on Healthcare Informatics (ICHI)
Y1  - 30 Nov.-3 Dec. 2020
AB  - Due to rapidly rising healthcare costs worldwide, there is significant interest in controlling them. An important aspect concerns price transparency, as preliminary efforts have demonstrated that patients will shop for lower costs, driving efficiency. This requires the data to be made available, and models that can predict healthcare costs for a wide range of patient demographics and conditions.We present an approach to this problem by developing a predictive model using machine-learning techniques. We analyzed de-identified patient data from New York State SPARCS (statewide planning and research cooperative system), consisting of 2.3 million records in 2016. We built models to predict costs from patient diagnoses and demographics. We investigated two model classes consisting of sparse regression and decision trees. We obtained the best performance by using a decision tree with depth 10. We obtained an R2 value of 0.76, which is better than the values reported in the literature for similar problems.
ER  - 

TY  - CONF
TI  - The Application of Deep Learning in Biomedical Informatics
T2  - 2018 International Conference on Robots & Intelligent System (ICRIS)
SP  - 391
EP  - 394
AU  - S. Wang
AU  - L. Fu
AU  - J. Yao
AU  - Y. Li
PY  - 2018
KW  - Conferences
KW  - Robots
KW  - Intelligent systems
KW  - Deep Learning
KW  - Healthcare
KW  - Biomedical informatics
DO  - 10.1109/ICRIS.2018.00104
JO  - 2018 International Conference on Robots & Intelligent System (ICRIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2018 International Conference on Robots & Intelligent System (ICRIS)
Y1  - 26-27 May 2018
AB  - The expansion of big data in biomedical and health field has driven the need of new effective analysis technology. Deep learning is a powerful machine learning method. With the contribution of rapid computational power improvement, it is becoming a promising technique to generate new knowledge, interpretation and gain insights from high-throughout, heterogeneous and complex biomedical data from different sources, such as medical imaging, clinical genomics, and electronic health records. This paper presents an overview of the application of deep learning approach in the biomedical informatics. First we introduce the development of artificial neural network and deep learning, then mainly focus on the researches applying deep learning in biomedical informatics field. We also discuss the challenges for future improvement, such as data quality and interpretability.
ER  - 

TY  - CONF
TI  - A Review of Interpretable Deep Learning for Neurological Disease Classification
T2  - 2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)
SP  - 900
EP  - 906
AU  - R. Katarya
AU  - P. Sharma
AU  - N. Soni
AU  - P. Rath
PY  - 2022
KW  - Deep learning
KW  - Neurological diseases
KW  - Communication systems
KW  - Medical services
KW  - Cognition
KW  - Deep Learning
KW  - Explainable Artificial Intelligence
KW  - Healthcare
KW  - Interpretability
KW  - Neurological diseases
DO  - 10.1109/ICACCS54159.2022.9785321
JO  - 2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)
IS  - 
SN  - 2575-7288
VO  - 1
VL  - 1
JA  - 2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)
Y1  - 25-26 March 2022
AB  - While deep learning models have been able to boast high accuracies for healthcare applications, their actual clinical use has been limited due to their black-box nature. To tackle this problem, interpretability or explainability methods are used to help provide reasoning behind the decisions of a deep learning model as well as to gain insight into their reasoning process. This is necessary due to the high-stakes nature of healthcare applications, in which every decision made must be well informed. In this paper, we showcase the various techniques that are used for interpretability in neurological disease classification. We discuss the advantages and drawbacks of various methods used to generate explanations and also discuss how to evaluate these explanations. Finally, we also discuss some of the flaws in evaluation techniques and the future directions for this field.
ER  - 

TY  - CONF
TI  - Machine Learning-Based Predictive Model for Surgical Site Infections: A Framework
T2  - 2021 National Computing Colleges Conference (NCCC)
SP  - 1
EP  - 6
AU  - S. Al-Ahmari
AU  - F. Nadeem
PY  - 2021
KW  - Machine learning algorithms
KW  - Hospitals
KW  - Surveillance
KW  - Surgery
KW  - Machine learning
KW  - Predictive models
KW  - Tools
KW  - data mining
KW  - machine learning
KW  - predictive model
KW  - surgical site infections
DO  - 10.1109/NCCC49330.2021.9428873
JO  - 2021 National Computing Colleges Conference (NCCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2021 National Computing Colleges Conference (NCCC)
Y1  - 27-28 March 2021
AB  - Surgical site infections impact hospital readmission rates, length of stay, and patient and hospital expense. The use of computational intelligence methods can help to predict the risk of SSIs and provide an early warning, enabling hospitals to prepare in advance to respond to these infections. The objective of this paper is to present a machine learning-based predictive model for surgical site infections. This paper also reviews the most recent machine learning-based models developed for the prediction of SSIs. When these predictive models are applied correctly and used effectively, they can be helpful for clinical surveillance teams. However, the implementation of these models and related tools requires quality data to be stored in electronic health records, which may not be available in all health information systems. The limitations of clinical data and the absence of labels adding several challenges with the implementation of the predictive models using machine learning; an imbalanced dataset is also a common issue that can influence the performance of the model, thus requiring an improved strategy to address this concern. Recently, the interpretability of predictive models has become important for hospital physicians to translate the models into real practices.
ER  - 

TY  - CONF
TI  - A Secure Healthcare System Design Framework using Blockchain Technology
T2  - 2019 21st International Conference on Advanced Communication Technology (ICACT)
SP  - 260
EP  - 264
AU  - S. Chakraborty
AU  - S. Aich
AU  - H. -C. Kim
PY  - 2019
KW  - Medical services
KW  - Blockchain
KW  - Sensors
KW  - Internet of Things
KW  - Stakeholders
KW  - Monitoring
KW  - Biomedical monitoring
KW  - Blockchain
KW  - Healthcare
KW  - IOT
KW  - Smart Healthcare
KW  - Wearables
KW  - Security. Privacy
DO  - 10.23919/ICACT.2019.8701983
JO  - 2019 21st International Conference on Advanced Communication Technology (ICACT)
IS  - 
SN  - 1738-9445
VO  - 
VL  - 
JA  - 2019 21st International Conference on Advanced Communication Technology (ICACT)
Y1  - 17-20 Feb. 2019
AB  - Blockchain, the technology of the future neutrally facilitated the financial transactions in cryptocurrencies by strictly eliminating the need for a governing authority or a management that was required to authorize the transactions based on trust and transparency. The Blockchain Network also follows the principle of absolute privacy and anonymity on the identification of the users associated in a transaction. Since the time of its inception, the Blockchain Technology has undergone research that has demonstrated some various kinds of methods to sort out the access control system of the conventional system. In recent years Blockchain has also shown optimum reliability in multiple sectors such as Smart Home, Healthcare, Banking, Information Storage Management, Security and etc. This work in terms is further concerned to the sector of Smart Healthcare, which has grown to a much affluence regarding the efficient technique of serving and dictating medical health care to the patients with the point of maintaining privacy of the patients' data and also the process of laying out real time accurate and trusted data to the medical practitioners. But in the scenario of Smart Healthcare, the primary concern arises in the fact of Privacy and Security of the data of the patients due to the interoperability of multiple stakeholders in the process. Also, there has been a fact of determining accurate and proper data to the doctors if the concerned subject is out of reach from the in hand medical service. Therefore, this Concern of privacy and also mitigation of the accurate data has been very much managed in the work by regulating, a monitoring and sensing paradigm with accordance to the IOT and the Blockchain as a transaction and access management system and also an appropriate medium for laying out accurate and trusted data for serving with deliberate medical care and benefits to the patients across.
ER  - 

TY  - CONF
TI  - Machine Learning on Stroke Risk Prediction Systems as Complementary Technology for Neurologists: A Critical Review
T2  - 2022 IEEE 14th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
SP  - 1
EP  - 6
AU  - C. Lucas
AU  - K. C. Padrique
AU  - M. Christa Lansangan
AU  - M. J. Isabel Gusi
AU  - M. Lubag
AU  - R. Concepcion
AU  - S. Lauguico
AU  - R. R. Vicerra
AU  - R. N. G. Naguib
PY  - 2022
KW  - Industries
KW  - Machine learning algorithms
KW  - Machine learning
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Market research
KW  - digital healthcare
KW  - graphical user interface
KW  - machine learning
KW  - stroke prediction algorithm
KW  - stroke risk
DO  - 10.1109/HNICEM57413.2022.10109527
JO  - 2022 IEEE 14th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
IS  - 
SN  - 2770-0682
VO  - 
VL  - 
JA  - 2022 IEEE 14th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM)
Y1  - 1-4 Dec. 2022
AB  - Stroke is considered the second-leading cause of death all over the world. Due to the risks entangled with stroke, the need to identity its early signs and symptoms is vital in preventing it. By recognizing the modifiable and non-modifiable risk factors related to stroke, numerous systems are developed through a machine learning (ML) approach. Ergo, utilizing stroke-risk prediction systems could prevent its occurrence and minimize fatality. This literature review focuses on assessing the current models and systems available. Numerous studies stick to utilizing traditional machine learning algorithms although the machine learning pipelines of the reviewed studies are mostly in question due to the lack of performing data pre-processing techniques. On the other hand, there are several studies which develop new machine learning algorithms based on traditional algorithms to perform more specific predictions. There has also been a perceived increase in studies wherein graphical user interface (GUI) is integrated in the process of presenting the results of a prediction system and this trend helps deliver the established systems to be used in the healthcare industry. Although that seems to be the end goal, implementation of machine learning algorithms still can be improved by setting guidelines and reporting standards to increase the acceptability and face validity of the algorithms’ predictions. Factor assessment is also key in addressing issues surrounding the transparency of machine learning by showing how risk factors have contributed to the prediction. Overall, further study and continued research on its reliability and dependability may aid in the forward trajectory of modern diagnosis.
ER  - 

TY  - CONF
TI  - Examining Effects of Schizophrenia on EEG with Explainable Deep Learning Models
T2  - 2022 IEEE 22nd International Conference on Bioinformatics and Bioengineering (BIBE)
SP  - 301
EP  - 304
AU  - C. A. Ellis
AU  - A. Sattiraju
AU  - R. Miller
AU  - V. Calhoun
PY  - 2022
KW  - Deep learning
KW  - Electrodes
KW  - Adaptation models
KW  - Machine learning algorithms
KW  - Mental disorders
KW  - Memory architecture
KW  - Medical services
KW  - schizophrenia
KW  - deep learning
KW  - diagnosis
KW  - explainable AI
DO  - 10.1109/BIBE55377.2022.00068
JO  - 2022 IEEE 22nd International Conference on Bioinformatics and Bioengineering (BIBE)
IS  - 
SN  - 2471-7819
VO  - 
VL  - 
JA  - 2022 IEEE 22nd International Conference on Bioinformatics and Bioengineering (BIBE)
Y1  - 7-9 Nov. 2022
AB  - Schizophrenia (SZ) is a mental disorder that affects millions of people globally. At this time, diagnosis of SZ is based upon symptoms, which can vary from patient to patient and create difficulty with diagnosis. To address this issue, researchers have begun to look for neurological biomarkers of SZ and develop methods for automated diagnosis. In recent years, several studies have applied deep learning to raw EEG for automated SZ diagnosis. However, the use of raw time-series data makes explainability more difficult than it is for traditional machine learning algorithms trained on manually engineered features. As such, none of these studies have sought to explain their models, which is problematic within a healthcare context where explainability is a critical component. In this study, we apply perturbation-based explainability approaches to gain insight into the spectral and spatial features learned by two distinct deep learning models trained on raw EEG for SZ diagnosis for the first time. We develop convolutional neural network (CNN) and CNN long short-term memory network (CNN-LSTM) architectures. Results show that both models prioritize the T8 and C3 electrodes and the δ- and y-bands, which agrees with previous literature and supports the overall utility of our models. This study represents a step forward in the implementation of deep learning models for clinical SZ diagnosis, and it is our hope that it will inspire the more widespread application of explainability methods for insight into deep learning models trained for SZ diagnosis in the future.
ER  - 

TY  - CONF
TI  - COVID-19 Mortality Prediction using Machine Learning: A Deep Forest Approach
T2  - 2022 International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics ( DISCOVER)
SP  - 245
EP  - 250
AU  - K. Chadaga
AU  - S. Prabhu
AU  - V. B. K
AU  - N. Sampathila
AU  - S. Umakanth
AU  - R. Chadaga
PY  - 2022
KW  - COVID-19
KW  - Machine learning algorithms
KW  - Correlation
KW  - Computer viruses
KW  - Very large scale integration
KW  - Regression analysis
KW  - Vaccines
KW  - COVID-19
KW  - Machine Learning
KW  - Mortality Prediction
KW  - Deep Forest
KW  - Healthcare
DO  - 10.1109/DISCOVER55800.2022.9974666
JO  - 2022 International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics ( DISCOVER)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2022 International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics ( DISCOVER)
Y1  - 14-15 Oct. 2022
AB  - COVID-19 is an extremely deadly disease which has wreaked havoc worldwide. Initially, the first case was reported in the wet markets of Wuhan, China in the early 2020’s. Though the mortality rate is low compared to other dangerous diseases, a lot of people have already succumbed to this virus. Vaccines have been successfully rolled out and it seems effective in preventing the severe symptoms of the coronavirus. However, a section of people (the elderly and people with existing comorbidities) still continue to die. It is extremely important to predict the patient vulnerability using machine learning since appropriate medicines and treatments can be given in time and precious lives can be saved. In this research, the deep forest classifier is utilized to predict the COVID-19 casualty status. This classifier requires extremely low hyperparameter tuning and can easily compete with the deep learning classifiers. This algorithm performed better than the traditional machine learning classifiers with an accuracy of 92%. The positive results obtained signifies the potential use of deep forest to prevent unwanted COVID-19 deaths by effectively deploying them in various medical facilities. Further, it can reduce the extreme burden already existing on healthcare systems caused by the novel coronavirus.
ER  - 

