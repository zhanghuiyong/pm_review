TY  - CONF
TI  - “Ethical Challenges and Solutions in AI-Powered Digital Health”
T2  - 2024 Global Digital Health Knowledge Exchange & Empowerment Conference (gDigiHealth.KEE)
SP  - 1
EP  - 8
AU  - Y. A. Ahmed
AU  - A. Osman
PY  - 2024
KW  - Ethics
KW  - Technological innovation
KW  - Data protection
KW  - Training data
KW  - Medical services
KW  - Electronic healthcare
KW  - Artificial intelligence
KW  - Protection
KW  - Public healthcare
KW  - Standards
KW  - Artificial intelligence
KW  - Healthcare
KW  - Ethics
KW  - Data Privacy
DO  - 10.1109/gDigiHealth.KEE62309.2024.10761737
JO  - 2024 Global Digital Health Knowledge Exchange & Empowerment Conference (gDigiHealth.KEE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Global Digital Health Knowledge Exchange & Empowerment Conference (gDigiHealth.KEE)
Y1  - 24-26 Sept. 2024
AB  - Addressing ethical issues in AI advancements is crucial for ensuring positive contributions to healthcare without compromising ethical standards and patient trust. This paper explores the ethical challenges around the use of Artificial Intelligence (AI) in digital health, focusing on data privacy, algorithmic bias, and transparency in autonomy. Therefore, the results show the existing issues of data vulnerability, including patient data leakage, stressing the importance of strong data protection mechanisms. Cases of bias in handling AI algorithms were pointed out primarily resulting in the inequality of health care provision, and hence the need to build better training data and techniques for bias elimination. The inability to reveal how and why an AI solution reached a particular conclusion undermines the patient's confidence in a given product or service, which is where the concepts of explainable AI and accountability come in. Ethical concerns regarding data privacy and trust erosion in medical practice are a growing concern. A qualitative study reveals significant gaps in existing literature on AI ethics in healthcare. the paper highlights key recommendations include enhanced regulatory frameworks, robust data protection measures, and increased transparency in AI algorithms to manage these ethical concerns. These solutions aim to ensure that the use of AI in healthcare is well-controlled, enhancing the healthcare system while respecting patient rights. Key funding and contributions from well-wishers have been instrumental in supporting this research, providing resources and insights essential for addressing these critical ethical challenges.
ER  - 

TY  - CONF
TI  - Exploring Ethical Dimensions of Employing Artificial Intelligence Algorithms in Healthcare Environments
T2  - 2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG)
SP  - 1
EP  - 9
AU  - B. A
AU  - J. M. Scaria
AU  - A. Trivedi
AU  - T. Sharma
AU  - A. M. Yadav
AU  - R. Nair
PY  - 2024
KW  - Training
KW  - Ethics
KW  - Data privacy
KW  - Accuracy
KW  - Medical services
KW  - Real-time systems
KW  - Safety
KW  - Predictive analytics
KW  - Monitoring
KW  - Testing
KW  - AI
KW  - Ethics
KW  - Healthcare
KW  - Machine learning
KW  - Patient data
KW  - Predictive analytics
KW  - Privacy
KW  - Scalability
KW  - Transparency
KW  - Trust
DO  - 10.1109/ICTBIG64922.2024.10911856
JO  - 2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG)
Y1  - 13-14 Dec. 2024
AB  - AI predictive analytics using patient data may dramatically enhance health outcomes in healthcare systems. The research uses prior patient data to predict health outcomes, focusing on societal problems including justice, accuracy, accessibility, data privacy, and equitable healthcare access. We carefully collect, clean, and select features from the data. Next, association analysis finds predictive characteristics. Machine learning, training, and testing optimize and regularize hyperparameters to ensure model performance. The assessment uses memory, accuracy, precision, and k-fold cross-validation to ensure reliability. Since continuous performance monitoring and feedback systems consider social concerns, real-time healthcare applications may employ this approach. The recommended strategy outperforms previous AI solutions in accuracy, bias reduction, data clarity, and rule compliance. The proposal also improves user trust, clinical efficacy, patient safety, and healthcare justice while tackling AI ethical issues. This in-depth examination of the approach demonstrates its potential to provide more equitable and effective healthcare solutions, making it a viable AI healthcare option. Finally, our research underscores the critical role of ethics in the design of AI-powered medical technologies, ensuring their effective operation and responsible utilization.
ER  - 

TY  - CONF
TI  - Blood Cell Identification Using Deep Transfer Learning and Explainable Artificial Intelligence Techniques
T2  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
SP  - 957
EP  - 962
AU  - G. M. Imdadul Alam
AU  - N. Tasnia
AU  - M. A. Hasan
AU  - R. Binte Rahman
PY  - 2024
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Microscopy
KW  - Transfer learning
KW  - Reliability
KW  - Blood
KW  - Monitoring
KW  - Medical diagnostic imaging
KW  - Residual neural networks
KW  - Deep Learning
KW  - Medical Image Analysis
KW  - Blood Cell
KW  - Explainable AI (XAI)
KW  - Grad-CAM
DO  - 10.1109/ICCIT64611.2024.11022117
JO  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 2474-9656
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
Y1  - 20-22 Dec. 2024
AB  - Blood cell identification is a critical task in medical diagnosis, particularly for detecting and monitoring various hematological conditions. Automation of blood cell detection offers a solution to these challenges. This study presents a comprehensive study on the application of deep learning for automated blood cell identification, specifically, the Inception V3 model, showcasing its potential to revolutionize diagnostic accuracy in medical imaging. By employing advanced pre-processing techniques, and optimal model design, the InceptionV3 model achieved impressive results with 98.8% test accuracy on a dataset of high-quality microscopic blood cell images. To enhance decision-making transparency, the study employs Explainable AI (XAI) approaches such as Grad-CAM and Grad-CAM++ to produce heatmaps that highlight significant regions influencing the model’s identification. The study incorporated a comparative investigation with other deep-learning models, including VGG16, VGG19, and RESNET50. The efficacy of blood cell identification is demonstrated by evaluation measures like precision, recall, F1 score, Jaccard similarity, MCC score, and overall accuracy. The combination of strong performance and transparency through XAI techniques makes it a valuable tool for precise and dependable diagnoses. The potential of integrating advanced deep learning and XAI methods to improve the reliability and clinical adoption of automated blood cell identification systems is highlighted in this study.
ER  - 

TY  - CONF
TI  - Adaptive AI Algorithms for Real-Time Monitoring and Intervention in Pediatric Healthcare Systems
T2  - 2024 5th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)
SP  - 864
EP  - 869
AU  - S. Gahane
AU  - P. Khode
AU  - P. Verma
PY  - 2024
KW  - Pediatrics
KW  - Accuracy
KW  - Target recognition
KW  - Heuristic algorithms
KW  - Medical services
KW  - Real-time systems
KW  - Decision trees
KW  - Convolutional neural networks
KW  - Artificial intelligence
KW  - Long short term memory
KW  - Pediatric Healthcare
KW  - AI
KW  - Real Time Monitoring
KW  - Clinical Decision Support System
DO  - 10.1109/ICDICI62993.2024.10810812
JO  - 2024 5th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 5th International Conference on Data Intelligence and Cognitive Informatics (ICDICI)
Y1  - 18-20 Nov. 2024
AB  - Implementing AI into pediatric health care has the potential to revolutionize patient care by providing accurate, individualized treatment plans. This paper discusses the step-by-step process of implementing an integrated AI system to meet the personalized care needs of children in the healthcare setting. The use of smart rules is made possible through the application of adaptive algorithms Longest Common Subsequence (LCS), Long Short-Term Memory networks (LSTM), Convolutional Neural Networks (CNN), Decision Tree and more. In this way, we plan to measure the extent to which the integrated AI system will assist in enhancing the diagnostic accuracy and management plans of patients when applied to different cases. Furthermore, the study explores the compatibility of the system with current EHR systems and healthcare information technology environments to ascertain its real-world implementation in the current clinical settings. The findings of this research should show that integrating the proposed hybrid AI system in real-time monitoring can help identify appropriate information for healthcare decision making and thus increase the quality of health care for children. The current study aims to address some of these gaps by presenting a systematic approach of implementing AI in pediatric care, which can help progress the field of healthcare by incorporating cutting-edge technologies for enhanced decision making.
ER  - 

TY  - CONF
TI  - Machine Learning in Healthcare: Decision Trees for Asthma Risk Prediction
T2  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
SP  - 1211
EP  - 1214
AU  - T. Soni
AU  - D. Gupta
AU  - M. Dutta
PY  - 2024
KW  - Analytical models
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Decision making
KW  - Medical services
KW  - Nearest neighbor methods
KW  - Predictive models
KW  - Decision trees
KW  - Random forests
KW  - Asthma
KW  - Asthma
KW  - Decision Tree
KW  - KNN
KW  - Random Forest
KW  - Machine Learning
DO  - 10.1109/ICSES63445.2024.10763341
JO  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
Y1  - 15-17 Oct. 2024
AB  - A chronic respiratory condition marked by hyperreactivity and inflammation of the airways., asthma presents serious health problems worldwide. Asthma prediction done early and precisely can result in better patient outcomes and care. The effectiveness of many machine learning algorithms in asthma prediction is investigated in this work., with an emphasis on a performance comparison of Decision Tree., K-Nearest Neighbours (KNN)., and Random Forest classifiers. Created prediction algorithms to detect people at risk of asthma using a large dataset including clinical., environmental., and genetic variables. Among KNN and Random Forest classifiers., the Decision Tree method outperforms them with the maximum prediction accuracy of 81 0/0. The Decision Tree model outperforms others because of its interpretability., which offers precise understanding of the decision-making process., and its capacity to manage complicated relationships between elements. The possibilities of Decision Tree models in asthma prediction are demonstrated by these results., which also emphasize the need of choosing suitable machine learning methods for efficient illness prediction. This work offers a potential method for early asthma diagnosis and customized therapeutic techniques., therefore supporting the continuous attempts to use machine learning in healthcare.
ER  - 

TY  - JOUR
TI  - Guaranteeing Correctness in Black-Box Machine Learning: A Fusion of Explainable AI and Formal Methods for Healthcare Decision-Making
T2  - IEEE Access
SP  - 90299
EP  - 90316
AU  - N. Khan
AU  - M. Nauman
AU  - A. S. Almadhor
AU  - N. Akhtar
AU  - A. Alghuried
AU  - A. Alhudhaif
PY  - 2024
KW  - Decision making
KW  - Closed box
KW  - Mathematical models
KW  - Explainable AI
KW  - Neurons
KW  - Predictive models
KW  - Brain modeling
KW  - Machine learning
KW  - Cancer detection
KW  - Prognostics and health management
KW  - Black-box machine learning
KW  - neural networks
KW  - interpretable machine learning
KW  - cancer prognosis
KW  - decision-making
KW  - formal methods
KW  - formal verification
KW  - colored petri nets
DO  - 10.1109/ACCESS.2024.3420415
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - In recent years, Explainable Artificial Intelligence (XAI) has attracted considerable attention from the research community, primarily focusing on elucidating the opaque decision-making processes inherent in complex black-box machine learning systems such as deep neural networks. This spike in interest originates from the widespread adoption of black-box models, particularly in critical domains like healthcare and fraud detection, highlighting the pressing need to understand and validate their decision-making mechanisms rigorously. In addition, prominent XAI techniques, including LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (Shapley Additive exPlanations), rely on heuristics and cannot guarantee the correctness of the explanations provided. This article systematically addresses this critical issue associated with machine learning and deep learning models, underscoring XAI’s pivotal role in promoting model transparency to enhance decision-making quality. Furthermore, this study advocates integrating Formal Methods to provide correctness guarantees for black-box internal decision-making. The proposed methodology unfolds in three pivotal stages: firstly, training black-box models using neural networks to generate synthetic datasets; secondly, employing LIME and SHAP techniques to interpret the models and visualize their internal decision-making processes; and finally, training decision trees on the synthetic datasets to implement Formal Methods for ensuring the correctness of the black-box model’s decision-making. To validate this proposed approach, experimentation was conducted on four widely recognized medical datasets, including the Wisconsin Breast Cancer and Thyroid Cancer (TC) datasets, which are available in the UCI Machine Learning Repository. Specifically, this research represents a significant contribution by pioneering a novel approach that seamlessly integrates XAI and Formal Methods, thereby furnishing correctness guarantees for internal decision-making processes within the healthcare domain.
ER  - 

TY  - CONF
TI  - A Review Paper on Innovative Deep Learning and Machine Learning Algorithms for Improved Cervical Cancer Identification
T2  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
SP  - 1095
EP  - 1100
AU  - P. N. Shah
AU  - R. A. H. Khan
PY  - 2024
KW  - Deep learning
KW  - Support vector machines
KW  - Accuracy
KW  - Recurrent neural networks
KW  - Reviews
KW  - Soft sensors
KW  - Transfer learning
KW  - Data models
KW  - Cervical cancer
KW  - Random forests
KW  - Cervical Cancer
KW  - Healthcare
KW  - Pap Smear Test
KW  - HPV Testing
KW  - SVM technique
KW  - KNN algorithm
KW  - CNN methodology
DO  - 10.1109/ICSES63445.2024.10763323
JO  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Sustainable Expert Systems (ICSES)
Y1  - 15-17 Oct. 2024
AB  - Cervical cancer is a major problem, especially in nations with low or middle incomes where access to cutting-edge healthcare and regular screening is constrained. Early detection and precise diagnosis are essential for effective therapy and higher survival rates. The identification of cervical cancer has shown outstanding and more accurate results with recent methods in machine learning (ML) and deep learning (DL). More accurate and efficient diagnostic tools are required since the use of conventional screening methods, like Pap smears and HPV tests, is restricted. The research looks into feature extraction techniques like Histogram of Oriented Gradients (HOG) and Gray Level Co-occurrence Matrix (GLCM) in addition to machine learning approaches like support vector machines (SVM), random forests, and k-nearest neighbors (k-NN). Convolution neural networks (CNNs) and transfer learning are addressed in order to achieve better performance on fewer datasets. Recurrent neural networks (RNNs) and generative adversarial networks (GANs), two cutting-edge deep learning techniques, are also studied. Some of the future directions include the development of individualized medical strategies, the application of ML and DL models to clinical practice, and the fusion of multiple communication data sources to increase diagnosis accuracy.
ER  - 

TY  - CONF
TI  - Liver Cancer Detection: A Review of Artificial Intelligence Techniques
T2  - 2024 4th International Conference on Innovative Sustainable Computational Technologies (CISCT)
SP  - 1
EP  - 6
AU  - K. V. Pawan
AU  - M. Singh
PY  - 2024
KW  - Deep learning
KW  - Liver cancer
KW  - Recurrent neural networks
KW  - Explainable AI
KW  - Medical services
KW  - Ultrasonography
KW  - Generative adversarial networks
KW  - Convolutional neural networks
KW  - Biomedical imaging
KW  - Tumors
KW  - Deep learning
KW  - Time-series Data
KW  - Medical Imaging
KW  - Explainable Artificial Intelligence (XAI)
KW  - Recurrent Neural Networks (RNNs)
KW  - Convolutional Neural Networks (CNNs)
KW  - Individualized Medicine
KW  - Generative Adversarial Networks (GANs)
DO  - 10.1109/CISCT62494.2024.11134175
JO  - 2024 4th International Conference on Innovative Sustainable Computational Technologies (CISCT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Innovative Sustainable Computational Technologies (CISCT)
Y1  - 27-28 Dec. 2024
AB  - One of the most innovative ways to identify liver cancer is using deep learning. In medical image analysis, Convolutional Neural Networks (CNNs) reliably detect potential tumors in CT, MRI, and ultrasonography. The potential for improved patient outcomes and early detection is great with this research. The use of recurrent neural networks (RNNs) to interpret time-series data, including blood tests, offers an alternate method. RNNs may help with early risk prediction and personalized monitoring plan development. The interpretability of AI models, data quality, and seamless integration into healthcare processes are still problems, though. Advances in data synthesis, explainable AI (XAI), and userfriendly interfaces are addressing these challenges. Future prospects appear bright thanks to emerging technologies like Generative Adversarial Networks (GANs) and customized medical approaches that leverage several data sources. Collaboration between researchers, healthcare providers, and technology companies is necessary to accelerate innovation and ensure that it is more broadly available.
ER  - 

TY  - CONF
TI  - Multi-Disease Diagnosis Using Medical Images
T2  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
SP  - 1
EP  - 6
AU  - A. G. K
AU  - A. R
AU  - D. R
AU  - E. Mohanraj
PY  - 2024
KW  - Deep learning
KW  - Computed tomography
KW  - Transfer learning
KW  - Lung
KW  - Data models
KW  - Robustness
KW  - Medical diagnosis
KW  - Disease diagnosis
KW  - Medical imaging
KW  - MRI
KW  - X-ray
KW  - CT scan
KW  - Deep learning
KW  - Convolutional neural networks
KW  - Cardiovascular disorders
KW  - Neurological disorders
KW  - Pulmonary disorders
KW  - VGG-16
DO  - 10.1109/AIMLA59606.2024.10531493
JO  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
Y1  - 15-16 March 2024
AB  - This work presents a novel approach for disease diagnosis using medical imaging modalities—MRI, X-ray, and CT scans—to classify heart, brain, and lung disorders. Leveraging "Deep learning" techniques, particularly "Convolutional Neural Networks" (CNNs), this work focuses on accurately identifying anomalies within these organs. A diverse dataset is prepared, encompassing varied cases of cardiovascular, neurological, and pulmonary conditions, enabling the model’s training and validation. The methodology involves collecting images, employing transfer learning, and implementing data augmentation techniques to enhance the model’s robustness. Evaluation metrics including accuracy, sensitivity, specificity, and AUC demonstrate the model’s efficacy in differentiating between disease categories and healthy instances. This research contributes to advancing healthcare by providing a tool for precise and timely diagnosis, potentially improving patient outcomes across these critical medical domains.
ER  - 

TY  - CONF
TI  - Enhancing PCOS Prediction Using Machine Learning and Explainable AI
T2  - 2024 International Conference on Intelligent Computing and Sustainable Innovations in Technology (IC-SIT)
SP  - 1
EP  - 5
AU  - B. J. Chelliah
AU  - S. K. Gahra
AU  - A. Senthilselvi
PY  - 2024
KW  - Machine learning algorithms
KW  - Frequency modulation
KW  - Explainable AI
KW  - Extreme learning machines
KW  - Computational modeling
KW  - Gaussian processes
KW  - Forestry
KW  - Medical services
KW  - Particle swarm optimization
KW  - Long short term memory
KW  - Extreme Learning Machine
KW  - Isolation Forest
KW  - Deep Belief Networks
KW  - Long Short-Term Memory
DO  - 10.1109/IC-SIT63503.2024.10862800
JO  - 2024 International Conference on Intelligent Computing and Sustainable Innovations in Technology (IC-SIT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Intelligent Computing and Sustainable Innovations in Technology (IC-SIT)
Y1  - 21-23 Nov. 2024
AB  - Polycystic Ovary Syndrome (PCOS) is a prevalent endocrine disorder among women of reproductive age, requiring timely diagnosis and intervention to manage associated health risks. This research explores the application of various machine learning (ML) techniques for detecting PCOS, providing a comparative analysis of nine advanced methodologies: Extreme Learning Machine (ELM), Isolation Forest, Factorization Machines (FM), Multivariate Gaussian Process (MGP), Non-negative Matrix Factorization (NMF), Gaussian Processes (GP), Deep Belief Networks (DBN), Particle Swarm Optimization (PSO), and Long Short-Term Memory (LSTM). The integration of Explainable AI techniques enhances the interpretability of these models, offering greater transparency in the decision-making process. The study leverages a dataset containing PCOS-related data to validate the proposed approach, evaluating the strengths and limitations of each ML model in terms of accuracy, interpretability, and diagnostic capability. This research aims to bridge the gap between advanced computational techniques and clinical applications, contributing to the growing field of ML-driven healthcare diagnostics. The findings highlight the potential of combining diverse machine learning frameworks to improve early detection and personalized treatment strategies for PCOS, ultimately improving patient outcomes.
ER  - 

TY  - CONF
TI  - Beyond the Black Box: Employing LIME and SHAP for Transparent Health Predictions with Machine Learning Models
T2  - 2024 International Conference on Advances in Computing, Communication, Electrical, and Smart Systems (iCACCESS)
SP  - 1
EP  - 6
AU  - K. Ashraf
AU  - S. Nawar
AU  - M. H. Hosen
AU  - M. T. Islam
AU  - M. N. Uddin
PY  - 2024
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Computational modeling
KW  - Static VAr compensators
KW  - Gaussian processes
KW  - Predictive models
KW  - Vectors
KW  - Machine Learning
KW  - Healthcare
KW  - Diagnosis
KW  - Prognosis
KW  - K-Nearest Neighbor
KW  - XAI
KW  - LIME
KW  - SHAP
KW  - Transparency
KW  - Accountability
DO  - 10.1109/iCACCESS61735.2024.10499522
JO  - 2024 International Conference on Advances in Computing, Communication, Electrical, and Smart Systems (iCACCESS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Advances in Computing, Communication, Electrical, and Smart Systems (iCACCESS)
Y1  - 8-9 March 2024
AB  - In the vast realm of healthcare, healthcare data gathered from patients is bountiful. With the continuous evolution and expansion of artificial intelligence, these healthcare data are a vital asset for us. Under the assistance of artificial intelligence, we can efficiently diagnose and prognose diseases to combat the increase in inaccurate prognosis and delayed diagnosis. In healthcare, diagnosis refers to identifying diseases or conditions in patients, while prognosis predicts the likely course and outcome of the medical conditions. To ease the diagnosis and prognosis, we explore the implementation of Machine Learning (ML) techniques and Simple Feedforward Neural Network. The machine learning models that are evaluated include Decision Tree (DT), Random Forest (RF), Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), and Gaussian Naive Bayes (GNB). After evaluation, the KNN model achieved the highest accuracy of 98.56%, along with F1-Score of 98.53%, Precision of 98.69%, and Recall Score of 98.52%. Later, we interpret the decision-making process of the machine learning algorithms by implementing Explainable Artificial Intelligence (XAI). LIME and SHAP, two types of XAI, are employed to explain and visualize the diagnosis capability and feature impact on the models.
ER  - 

TY  - CONF
TI  - Detecting Brain Cancer Using Explainable AI
T2  - 2024 7th International Conference on Signal Processing and Information Security (ICSPIS)
SP  - 1
EP  - 6
AU  - A. Rahman
AU  - S. S. Sohail
AU  - M. S. Alam
AU  - A. Sharma
AU  - W. Mansoor
PY  - 2024
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Information security
KW  - Medical services
KW  - Signal processing
KW  - Brain modeling
KW  - Hardware
KW  - Reliability
KW  - Biomedical imaging
KW  - Deep Learning
KW  - Artificial Intelligence
KW  - Explainable AI
KW  - Brain Tumor
KW  - Image Processing
KW  - LIME
DO  - 10.1109/ICSPIS63676.2024.10812596
JO  - 2024 7th International Conference on Signal Processing and Information Security (ICSPIS)
IS  - 
SN  - 2831-3844
VO  - 
VL  - 
JA  - 2024 7th International Conference on Signal Processing and Information Security (ICSPIS)
Y1  - 12-14 Nov. 2024
AB  - The accurate and early diagnosis of brain tumors that is one of the deadly cancers may save many humans. Proposed research focused on primary types of tumors detection (i.e. gliomas, meningiomas, and pituitary tumors). Further, the research is meticulous in fine-tuning the model’s parameters — including the loss function, optimizer, learning rate, epochs, and batch size — to optimize performance and achieve high accuracy in tumor detection. While the findings of this work mark a significant advancement in medical imaging and brain tumor detection, it acknowledges the prevailing limitations, such as the constraints imposed by hardware resources and the challenges of model generalization across diverse and more extensive datasets. Recognizing these limitations, the work sets the stage for future research, emphasizing the exploration of larger and more heterogeneous datasets, the adoption of more sophisticated deep learning models, with the aim of enhancing the precision, reliability, and clinical applicability of brain tumor detection methodologies. In essence, this research not only contributes to the field of medical imaging analysis but also represents a transformative step towards the realization of more accurate, timely, and personalized healthcare solutions, powered by the advancements in artificial intelligence and machine learning.
ER  - 

TY  - JOUR
TI  - Application of Example-Based Explainable Artificial Intelligence (XAI) for Analysis and Interpretation of Medical Imaging: A Systematic Review
T2  - IEEE Access
SP  - 26419
EP  - 26427
AU  - M. Fontes
AU  - J. D. S. De Almeida
AU  - A. Cunha
PY  - 2024
KW  - Explainable AI
KW  - Medical diagnostic imaging
KW  - Artificial intelligence
KW  - Systematics
KW  - Medical services
KW  - Reliability
KW  - Prototypes
KW  - Biomedical imaging
KW  - Explainable artificial intelligence
KW  - XAI
KW  - example-based
KW  - explanations by example
KW  - medical imaging
DO  - 10.1109/ACCESS.2024.3367606
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Explainable Artificial Intelligence (XAI) is an area of growing interest, particularly in medical imaging, where example-based techniques show great potential. This paper is a systematic review of recent example-based XAI techniques, a promising approach that remains relatively unexplored in clinical practice and medical image analysis. A selection and analysis of recent studies using example-based XAI techniques for interpreting medical images was carried out. Several approaches were examined, highlighting how each contributes to increasing accuracy, transparency, and usability in medical applications. These techniques were compared and discussed in detail, considering their advantages and limitations in the context of medical imaging, with a focus on improving the integration of these technologies into clinical practice and medical decision-making. The review also pointed out gaps in current research, suggesting directions for future investigations. The need to develop XAI methods that are not only technically efficient but also ethically responsible and adaptable to the needs of healthcare professionals was emphasised. Thus, the paper sought to establish a solid foundation for understanding and advancing example-based XAI techniques in medical imaging, promoting a more integrated and patient-centred approach to medicine.
ER  - 

TY  - CONF
TI  - Explainability in Artificial Intelligence: Techniques, Tools and Applications in Medicine and Bioinformatics
T2  - 2024 Fourth International Conference on Digital Data Processing (DDP)
SP  - 92
EP  - 97
AU  - F. R. Falvo
AU  - M. Cannataro
PY  - 2024
KW  - Measurement
KW  - Ethics
KW  - Technological innovation
KW  - Explainable AI
KW  - Scalability
KW  - Standardization
KW  - Genetics
KW  - Medical diagnosis
KW  - Bioinformatics
KW  - Medical diagnostic imaging
DO  - 10.1109/DDP64453.2024.00026
JO  - 2024 Fourth International Conference on Digital Data Processing (DDP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Fourth International Conference on Digital Data Processing (DDP)
Y1  - 30 Sept.-1 Oct. 2024
AB  - Explainability in artificial intelligence (AI) is essential to increase trust and transparency in AI systems:this study explores not only the techniques and tools developed to make AI models more understandable, but also provides an in-depth analysis of applications in critical fields such as medicine and bioinformatics. Compared to other works on the topic, this study stands out for its focus on the scalability and practical effectiveness of explainability techniques, such as LIME, SHAP, Grad-CAM and DeepLIFT, in specific contexts such as medical diagnostics and genetic analysis. Furthermore, the work discusses the open challenges in the field of XAI, proposing a methodological framework to evaluate and select the most appropriate techniques. The results highlight the need to develop interpretable and scalable models for the ethical and effective use of AI, promoting greater responsibility in the adoption of these technologies in critical fields. In conclusion, this study contributes with a global vision of current progress and future research directions, proposing concrete strategies to improve the practical adoption of XAI.
ER  - 

TY  - CONF
TI  - Integration of Generative Artificial Intelligence in Medicine - First Steps Toward an Ethical Risk Management Methodology
T2  - 2024 E-Health and Bioengineering Conference (EHB)
SP  - 1
EP  - 6
AU  - M. –. E. Iliuță
AU  - D. Trentesaux
AU  - E. Pop
AU  - M. –. A. Moisescu
PY  - 2024
KW  - Ethics
KW  - Pathology
KW  - Generative AI
KW  - Precision medicine
KW  - Decision making
KW  - Medical services
KW  - Solids
KW  - Medical diagnostic imaging
KW  - Standards
KW  - Optimization
KW  - Generative Artificial Intelligence
KW  - Ethical Aspects
KW  - Risks
KW  - Medicine
KW  - Rules and Practices
DO  - 10.1109/EHB64556.2024.10805663
JO  - 2024 E-Health and Bioengineering Conference (EHB)
IS  - 
SN  - 2575-5145
VO  - 
VL  - 
JA  - 2024 E-Health and Bioengineering Conference (EHB)
Y1  - 14-15 Nov. 2024
AB  - The integration of Generative Artificial Intelligence into clinical processes requires addressing ethical aspects and inherent risks, contributing not only to the automation of these processes but also to the ongoing development of personalized medicine. The generation of complex diagnostic and personalized treatment solutions must be carried out in accordance with deontological norms and ethical standards, aimed at ensuring the protection of patient autonomy and safety. The development of an architectural framework for the identification of pathologies and the provision of treatment scenarios must ensure both the efficiency of the solutions offered and their compliance with ethical standards, involving the creation of a set of rules and practices that encourage interdisciplinary collaboration between experts in technology, medicine, and law. The fundamental principles that facilitate the integration of emerging technologies into clinical processes promote patient autonomy, decision-making transparency, and clinical responsibility. (Abstract)
ER  - 

TY  - CONF
TI  - A Comparative Study of Machine Learning Classifiers and Ensemble Method for Breast Cancer Detection Using XAI Technique
T2  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
SP  - 1
EP  - 5
AU  - H. N
AU  - A. P. M
AU  - D. S
AU  - S. M
AU  - V. S. S
PY  - 2024
KW  - Support vector machines
KW  - Radio frequency
KW  - Performance evaluation
KW  - Accuracy
KW  - Neural networks
KW  - Medical services
KW  - Breast cancer
KW  - Ensemble learning
KW  - Random forests
KW  - Tumors
KW  - Breast Cancer Diagnosis
KW  - Explainable AI (XAI)
KW  - Random Forest Ensemble
KW  - SHAP and LIME Interpretability
KW  - Machine Learning in Healthcare
DO  - 10.1109/DELCON64804.2024.10866753
JO  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
Y1  - 21-23 Nov. 2024
AB  - Breast cancer remains a leading cause of mortality among women, necessitating accurate and explainable diagnostic tools. While machine learning (ML) models can enhance diagnostic accuracy, their interpretability is crucial for clinical adoption. A novel framework integrating Random Forest classifiers with ensemble techniques, SHapley Additive exPlanations (SHAP), and Local Interpretable ModelAgnostic Explanations (LIME) to provide both high performance and transparency in breast cancer diagnosis is proposed. The performance evaluation of Random Forest, Neural Networks, and feature selection-enhanced Random Forest models, with validation accuracy of 97%, 98%, and 96%, respectively. Our ensemble approach combining Random Forest (RF), Logistic Regression (LR), and SVM achieved 95% accuracy. SHAP highlights key features like menopause and tumor size, while LIME offers case-specific explanations to improve trust in AI-driven decisions. This framework delivers both robust accuracy and interpretability, making it a valuable tool for advancing ethical AI in healthcare.
ER  - 

TY  - CONF
TI  - Ethical Guidelines For Utilization of Artificial Intelligence In Healthcare: A Review
T2  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
SP  - 1
EP  - 6
AU  - V. Shete
AU  - A. Pethe
PY  - 2024
KW  - Ethics
KW  - Reviews
KW  - Databases
KW  - Medical services
KW  - Regulation
KW  - Artificial intelligence
KW  - General Data Protection Regulation
KW  - Standards
KW  - Monitoring
KW  - Guidelines
KW  - artificial intelligence
KW  - ethical standards
KW  - ethical guidelines
KW  - healthcare
KW  - general data protection regulation
DO  - 10.1109/IDICAIEI61867.2024.10842807
JO  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
Y1  - 29-30 Nov. 2024
AB  - The key issue is to develop guidelines for Artificial Intelligence (AI) data protection that respect individual rights and further the general welfare. Where possible, AI systems have to minimize data of a personal nature to the absolute minimum, anonymize such data, and encrypt it in maintaining data security in accordance with regulations such as the General Data Protection Regulation (GDPR). AI systems need to follow the levels defined by the European Commission to achieve proper transparency and explain ability for building confidence and enabling proper ethical control. The present review article aims to bring ethical standard for AI utilization upfront. Additionally, the present article focus on uncovering ethical guidelines for maintaining standards for AI use. A thorough search approach was used to find relevant reviews of the literature for the assessment. Phases of the strategy included scanning several databases, evaluating publications, and choosing the most relevant research for review. This review examined electronic databases including PubMed, Science Direct, EMBASE, and Google Scholar. These databases were chosen to provide comprehensive coverage of relevant content. Making use of studies and reviews published between 2000 and 2024.
ER  - 

TY  - CONF
TI  - Diabetic Retinopathy Detection using Deep Learning Framework and Explainable Artificial Intelligence Technique *
T2  - 2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
SP  - 411
EP  - 415
AU  - U. Posham
AU  - S. Bhattacharya
PY  - 2024
KW  - Deep learning
KW  - Measurement
KW  - Visualization
KW  - Diabetic retinopathy
KW  - Visual impairment
KW  - Sociology
KW  - Medical services
KW  - Diabetic retinopathy
KW  - Deep learning
KW  - CNN model
KW  - Detection
KW  - LIME Explainer
DO  - 10.1109/Confluence60223.2024.10463499
JO  - 2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
IS  - 
SN  - 2766-421X
VO  - 
VL  - 
JA  - 2024 14th International Conference on Cloud Computing, Data Science & Engineering (Confluence)
Y1  - 18-19 Jan. 2024
AB  - Diabetic retinopathy (DR) is a serious eye condition induced by diabetes that can result in eyesight. In order to overcome the vision impairment of diabetes mellitus patients, it is essential for early age prevention by medical practitioners. In the traditional approach, ophthalmologists conduct various screening tests to detect DR but fail to achieve accurate and conclusive diagnosis due to the associated time consuming processes. In order to eliminate such burden on the ophthalmologists, deep learning and machine learning techniques have evolved rapidly playing a significant role in the classification of diabetic and non diabetic patients. In this paper, the proposed framework implements a publicly available Benchmark APTOS 2019 Gaussian-filtered DR image dataset using a customized CNN model, yielding an enhanced accuracy of 98 %. In addition, the framework also employs a LIME explainer visualization model to provide enhanced transparency and interpretability to the generated predictions. This eliminates the “blackbox” -ed nature of the predicted results from the traditional ML process and enables healthcare providers to take clinical decisions with confidence providing visual representations of the significant features that contribute towards an outcome.
ER  - 

TY  - CONF
TI  - Interpretable Medical Image Diagnosis Methodology using Convolutional Neural Networks and Bayesian Networks
T2  - 2024 9th International Conference on Mathematics and Computers in Sciences and Industry (MCSI)
SP  - 128
EP  - 133
AU  - V. Zarikas
AU  - S. V. Georgakopoulos
PY  - 2024
KW  - Ethics
KW  - Privacy
KW  - Computational modeling
KW  - Scalability
KW  - Machine learning
KW  - Mathematics
KW  - Complexity theory
KW  - Medical diagnosis
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - AI medical diagnosis
KW  - Bayesian Networks
KW  - Interpretable AI
KW  - XAI
DO  - 10.1109/MCSI63438.2024.00029
JO  - 2024 9th International Conference on Mathematics and Computers in Sciences and Industry (MCSI)
IS  - 
SN  - 2996-2056
VO  - 
VL  - 
JA  - 2024 9th International Conference on Mathematics and Computers in Sciences and Industry (MCSI)
Y1  - 22-24 Aug. 2024
AB  - A novel general methodology for fully interpretable AI concerning medical images is analyzed. The various consecutive steps of the method are presented in an instructive way. The algorithm is quite general, covering most of the cases of medical images. Interpretable diagnosis based on medical images is particularly useful for demanding applications like medical ones. Results show an efficient performance.
ER  - 

TY  - CONF
TI  - Prediction Interpretations of Ensemble Models in Chronic Kidney Disease Using Explainable AI
T2  - NAECON 2024 - IEEE National Aerospace and Electronics Conference
SP  - 391
EP  - 397
AU  - K. M. T. Jawad
AU  - A. Verma
AU  - F. Amsaad
PY  - 2024
KW  - Analytical models
KW  - Explainable AI
KW  - Prevention and mitigation
KW  - Medical services
KW  - Predictive models
KW  - Aerospace electronics
KW  - Chronic kidney disease
KW  - CKD
KW  - Explainable AI
KW  - Ensemble Tree Models
DO  - 10.1109/NAECON61878.2024.10670652
JO  - NAECON 2024 - IEEE National Aerospace and Electronics Conference
IS  - 
SN  - 2379-2027
VO  - 
VL  - 
JA  - NAECON 2024 - IEEE National Aerospace and Electronics Conference
Y1  - 15-18 July 2024
AB  - Chronic Kidney Disease (CKD) is an irreversible disease affecting millons of people all around the world. To this date, no cure has been produced for CKD and it is financially very challenging to treat this disease. The irreversible nature of the disease makes it critical to be analyzed by Machine Learning models. Since it is a significant healthcare research domain, the decision results (of patients having or not having the disease) by the Machine Learning models should be explained to the patients and the clinical practitioners. The scope of this research is to apply ensemble Machine Learning models on prediction of CKD from a dataset of 400 subjects. In the prediction analysis part, this research focuses on addressing key features leading to the prediction. The Explainable AI (XAI) techniques are implemented to explain the clinical practitioners about necessary changes required in the features for contrasting class prediction.
ER  - 

TY  - CONF
TI  - Integration of Explainable Artificial Intelligence (XAI) in the Development of Disease Prediction and Medicine Recommendation System
T2  - 2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
SP  - 1
EP  - 5
AU  - J. Mathew
AU  - R. Chitra
AU  - C. Stephen
AU  - R. S. Koshy
PY  - 2024
KW  - Explainable AI
KW  - Decision making
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Robustness
KW  - Decision trees
KW  - explainable a
KW  - XA
KW  - machine learnin
KW  - decision tre
KW  - disease predictio
KW  - recommendation system
DO  - 10.1109/IATMSI60426.2024.10503250
JO  - 2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
IS  - 
SN  - 
VO  - 2
VL  - 2
JA  - 2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI)
Y1  - 14-16 March 2024
AB  - The contemporary healthcare landscape necessitates innovative solutions to improve transparency and understanding in medical decision-making. This paper proposes an advanced medicine recommendation system and a robust disease prediction model integrating explainable AI (XAI). Recognizing the prevalence of misinformation and the urgent need for user-friendly applications, the system empowers users to manage their health proactively. It can be extended beyond common diseases, encompassing rare diseases, and employs XAI algorithms, specifically SHAP and LIME, to enhance transparency. The system incorporates Random Forest Classifier and Decision Tree models, showcasing high accuracy and robustness. The explanation models contribute to user understanding, while performance metrics offer insights into model strengths and generalization abilities. Figures depict SHAP outputs for Decision Tree and Random Forest models, emphasizing transparency in medical predictions. This proposed system addresses critical healthcare challenges, fostering informed decision-making and user trust.
ER  - 

TY  - CONF
TI  - A Review on the Developments in the Field of AI-Based Gait Analysis
T2  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
SP  - 1
EP  - 5
AU  - V. Wankhede
AU  - P. Verma
AU  - S. Gahane
PY  - 2024
KW  - Patient monitoring
KW  - Musculoskeletal system
KW  - Reviews
KW  - Parkinson's disease
KW  - Medical treatment
KW  - Motion capture
KW  - Artificial intelligence
KW  - Wearable devices
KW  - Gait recognition
KW  - Wearable sensors
KW  - Gait
KW  - Machine Learning
KW  - Prediction
KW  - Healthcare
KW  - Artificial Intelligence
DO  - 10.1109/IDICAIEI61867.2024.10842709
JO  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)
Y1  - 29-30 Nov. 2024
AB  - The examination of human gait is essential for comprehending motor function, the evolution of diseases, and the effectiveness of rehabilitation in different clinical and research environments. Artificial Intelligence (AI) advancements have recently improved gait analysis methodologies, resulting in increased accuracy, efficiency, and therapeutic effectiveness. This review article compiles the existing literature on AI-based gait analysis. The review emphasizes the role of AI in gait analysis, namely via the use of vision-based approaches, wearable sensor technologies, markerless motion capture systems, and inertial sensor-based methodologies. AI has shown potential in identifying musculoskeletal problems including osteopenia and sarcopenia, forecasting the start of Parkinson's disease, assessing the effects of post-stroke rehabilitation, and diagnosing neurological conditions. In addition, the integration of AI with new technologies such as augmented reality (AR), smart sensors, and wearable devices presents intriguing opportunities to improve gait rehabilitation, enable individualized therapies, and facilitate remote patient monitoring. Although there have been significant breakthroughs, there are still obstacles in creating AI models that can be easily understood for clinical decision-making, establishing standardized procedures for collecting data, and verifying AI algorithms across various populations. This study underlines the transformational potential of AI-based gait analysis in customized healthcare, rehabilitation techniques, and clinical results. In order to fully harness the potential of AI in enhancing human health and well-being, it is imperative to do more research and foster multidisciplinary cooperation.
ER  - 

TY  - CONF
TI  - Addressing Challenges and Opportunities in Precision Medicine Using Deep Learning and Personalized Cancer Treatment
T2  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
SP  - 1
EP  - 5
AU  - G. S. P. Ghantasala
AU  - N. V. Kumari
AU  - R. Rajesh Sharma
AU  - P. Vidyullatha
AU  - A. Sungheetha
AU  - S. Bathool
PY  - 2024
KW  - Deep learning
KW  - Privacy
KW  - Accuracy
KW  - Cancer treatment
KW  - Genomics
KW  - Real-time systems
KW  - Bioinformatics
KW  - Medical diagnostic imaging
KW  - Cancer
KW  - Tumors
KW  - Tumor
KW  - Deep Learning
KW  - Personalized Medicine
KW  - Cancer Treatment
KW  - Prediction
KW  - Oncology
DO  - 10.1109/DELCON64804.2024.10866424
JO  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
Y1  - 21-23 Nov. 2024
AB  - The primary aim of personalized cancer treatment is to increase effectiveness and reduce side effects by tailoring therapies based on a patient's unique genetic and molecular characteristics. This research investigates how deep learning is transforming personalized cancer care through its ability to process and analyze complex, high-dimensional data. The study explores the use of deep learning techniques in predicting tumor classifications, genetic mutations, and treatment responses by integrating clinical data, genomic profiles, and medical imaging. It reviews recent advancements in deep learning applications for cancer, such as targeted therapy through genetic profiling, tumor identification from histopathology images, and optimization of treatment protocols. Additionally, the paper discusses challenges including model transparency, clinical integration, and data security. The study demonstrates how deep learning can enhance patient outcomes and outlines future opportunities for this evolving area of cancer care.
ER  - 

TY  - CONF
TI  - Risk Prediction of Heart Disease using Deep SHAP Techniques
T2  - 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT)
SP  - 332
EP  - 336
AU  - A. Saranya
AU  - S. Narayan
PY  - 2024
KW  - Deep learning
KW  - Heart
KW  - Visualization
KW  - Sensitivity
KW  - Explainable AI
KW  - Computational modeling
KW  - Medical services
KW  - Deep Learning
KW  - Explainable Artificial Intelligence
KW  - Deep SHAP
KW  - Health care
DO  - 10.1109/InCACCT61598.2024.10551212
JO  - 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT)
Y1  - 2-3 May 2024
AB  - The automatic identification o f h eart d iseases is a challenging problem in the medical field, a nd i t constitutes the primary cause of death. Heart failure prediction, one of the symptoms of cardiovascular disease, has gained importance among physicians. In addition to increasing feature ranking and clinical prediction, deep learning also helps with output interpretation for medical practitioners. Therefore, the idea behind Explainable Artificial Intelligence (XAI) is to address the problem of deep learning algorithms in the healthcare domain not being explainable, in this case, and give the user knowledge about the model’s inner working process in an understandable format. In our proposed work, convolutional neural networks are used to predict the disease, and the Deep SHAP model is used to visualize the model predictions with an accuracy of 0.90, sensitivity of 0.97, and F1-score of 0.86 for class 1. The results section includes metrics computed for two classes, such as recall, precision, and F1-score.
ER  - 

TY  - CONF
TI  - Attention-Based Explainable AI for Wearable Multivariate Data: A Case Study on Affect Status Prediction
T2  - 2024 IEEE 20th International Conference on Body Sensor Networks (BSN)
SP  - 1
EP  - 4
AU  - Y. Wang
AU  - Z. Yang
AU  - I. Azimi
AU  - A. M. Rahmani
AU  - P. Liljeberg
PY  - 2024
KW  - Performance evaluation
KW  - Accuracy
KW  - Explainable AI
KW  - Time series analysis
KW  - Medical services
KW  - Predictive models
KW  - Transformers
KW  - Biomedical monitoring
KW  - Wearable devices
KW  - Monitoring
KW  - Multivariate time series
KW  - Self attention
KW  - Graph attention network
KW  - xAI
KW  - Multi-modal sensing
KW  - Digital mental health
DO  - 10.1109/BSN63547.2024.10780702
JO  - 2024 IEEE 20th International Conference on Body Sensor Networks (BSN)
IS  - 
SN  - 2376-8894
VO  - 
VL  - 
JA  - 2024 IEEE 20th International Conference on Body Sensor Networks (BSN)
Y1  - 15-17 Oct. 2024
AB  - Wearable technology enables ubiquitous health monitoring where multivariate physiological and behavioral data can be captured over time. Such multivariate time series (MTS) data in healthcare applications needs technique to interpret the analysis results. However, existing deep learning models for MTS data analysis often lack interpretability, and current explainable AI (xAI) techniques fail to capture the temporal and inter-variable complexities inherent in MTS. This hinders the trust and integration of these AI-based systems in clinical decision-making. In this paper, we propose an attention-based xAI method to classify and interpret MTS data collected from wearable devices. Our approach leverages self-attention mechanisms and graph attention layers (GAT) to capture both temporal and inter-variable dependencies, providing interpretability at both the temporal and modality levels. We evaluate our method using a longitudinal affect status monitoring. The dataset was collected from 21 college students via wearable devices over one year. We train separate models for positive (PA) and negative affect (NA) prediction, and compare their performance with a Transformer-based method. Our method achieves robust classification performance, with 78.62% accuracy for PA and 76.30% for NA, while offering transparent explanations of its decisions. These findings highlight the potential of our xAI method for reliable and interpretable MTS classification in healthcare applications.
ER  - 

TY  - CONF
TI  - Explainable AI-Driven Handwriting Analysis for Early Alzheimer’s Disease Detection Using Machine Learning and Ensemble Models
T2  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
SP  - 2002
EP  - 2007
AU  - N. Samrat
AU  - M. Islam
AU  - S. Sharmi
AU  - R. Akter
AU  - A. Debnath
AU  - M. Raihan
AU  - A. Nag
AU  - A. K. Bairagi
PY  - 2024
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Medical treatment
KW  - Predictive models
KW  - Developing countries
KW  - Reliability
KW  - Alzheimer's disease
KW  - Information technology
KW  - Random forests
KW  - Alzheimer’s Disease
KW  - Handwriting Feature Analysis
KW  - Machine Learning
KW  - Ensemble
KW  - Explainable AI
KW  - LIME
KW  - ELI5
DO  - 10.1109/ICCIT64611.2024.11021728
JO  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 2474-9656
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
Y1  - 20-22 Dec. 2024
AB  - Alzheimer’s disease (AD) affects millions of people worldwide and is the primary cause of dementia. It is also one of the most common and deadly neurological conditions. Effective intervention requires an early diagnosis, particularly in underdeveloped nations like Bangladesh, where diagnostic resources and healthcare infrastructure are frequently limited. This research aims to use cutting-edge artificial intelligence (AI) and machine learning (ML) models to enhance early detection of AD. Random Forest achieved the highest accuracy of 91.00% among different ML and ensemble models that were trained on a comprehensive dataset that included neurological and cognitive factors. Explainable AI approaches like LIME and ELI5, which provide transparency by highlighting the significant features driving model predictions, were utilized to illustrate the decision-making process of the model further. These methods help clinicians comprehend and believe in AI-driven insights by simplifying sophisticated AI models. The research highlights the viability of AI-driven methods for early AD detection, particularly in resource-constrained settings, and illustrates how explainable AI can be integrated into healthcare settings. The findings support the usage of sophisticated AI models for the improvement of diagnosis accuracy and patient outcomes. To manage Alzheimer’s disease in clinical settings, this research offers a viable avenue for using AI-based predictive techniques.
ER  - 

TY  - CONF
TI  - The Intersection of Generative AI and Healthcare: Addressing Challenges to Enhance Patient Care
T2  - 2024 Seventh International Women in Data Science Conference at Prince Sultan University (WiDS PSU)
SP  - 134
EP  - 140
AU  - E. Albaroudi
AU  - T. Mansouri
AU  - A. Alameer
PY  - 2024
KW  - Privacy
KW  - Ethics
KW  - Technological innovation
KW  - Patient monitoring
KW  - Generative AI
KW  - Medical services
KW  - Stakeholders
KW  - Artificial Intelligence (AI)
KW  - Generative AI
KW  - Healthcare
KW  - Patient Care
KW  - Data Privacy
KW  - Errors
KW  - Injuries
KW  - Regulatory Compliance
DO  - 10.1109/WiDS-PSU61003.2024.00039
JO  - 2024 Seventh International Women in Data Science Conference at Prince Sultan University (WiDS PSU)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Seventh International Women in Data Science Conference at Prince Sultan University (WiDS PSU)
Y1  - 3-4 March 2024
AB  - This research analyses the evolving intersection of generative AI and healthcare. It explores the transformative potential of integrating generative AI in healthcare, particularly in process automation, patient care, patient monitoring, and diagnosis. However, the implementation of generative AI in healthcare faces challenges like medical privacy concerns, the possibility of errors and injuries, and regulatory compliance challenges. The research proposes solutions to the challenges of implementing generative AI to enhance patients’ health outcomes. The study demonstrates the importance of medical privacy and transparency in addressing privacy issues. Dealing with errors and injuries requires AI systems to be trained on complete data and quality oversight. Addressing regulatory compliance problems requires providers to be more proactive in engaging authorities and helping them understand the importance of creating AI-based laws in healthcare. The solutions allow researchers, providers, policymakers, and other stakeholders to fast-track the implementation of generative AI in healthcare while mitigating possible negative implications.
ER  - 

TY  - CONF
TI  - Explainability techniques for Artificial Intelligence models in medical diagnostic
T2  - 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
SP  - 6907
EP  - 6913
AU  - F. R. Falvo
AU  - M. Cannataro
PY  - 2024
KW  - Logistic regression
KW  - Computational modeling
KW  - Biological system modeling
KW  - Scalability
KW  - Machine learning
KW  - Predictive models
KW  - Diabetes
KW  - Medical diagnosis
KW  - Medical diagnostic imaging
KW  - Context modeling
KW  - Explainable Artificial Intelligence
KW  - Medical Diagnostics
KW  - Lime
KW  - Shape
DO  - 10.1109/BIBM62325.2024.10821826
JO  - 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
IS  - 
SN  - 2156-1133
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Y1  - 3-6 Dec. 2024
AB  - The integration of artificial intelligence (AI) techniques into clinical settings presents critical challenges due to the opacity of machine learning models, often referred to as "black boxes": this study explores the application of explainability techniques, specifically Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP), in the context of medical diagnostics. Using the "Diabetes Health Indicators Dataset", we applied Logistic Regression as a predictive model to identify key risk factors for diabetes and evaluate the ability of explainability techniques to improve transparency and interpretability. The results demonstrate that SHAP provides a detailed global and local understanding of feature importance, offering clinicians insights into key predictors such as HighBP, CholCheck, and GenHlth; LIME complements this by delivering intuitive explanations for individual predictions, enabling rapid and accessible interpretation. The combination of these techniques enhances trust in AI systems by providing both comprehensive insights and actionable explanations. Challenges related to computational complexity, scalability, and the integration of these methods into clinical workflows are also discussed, along with recommendations for future research aimed at developing scalable, interpretable AI models for ethical and responsible medical use.
ER  - 

TY  - CONF
TI  - Explainable FL-Based Framework for Diagnosis of Diabetic Retinopathy in Healthcare 4.0 Environment
T2  - 2024 10th International Conference on Signal Processing and Intelligent Systems (ICSPIS)
SP  - 85
EP  - 89
AU  - P. Patel
AU  - N. Jain
AU  - H. Patni
AU  - R. Fenil
AU  - R. Gupta
AU  - S. Tanwar
AU  - H. Shahinzadeh
PY  - 2024
KW  - Training
KW  - Diabetic retinopathy
KW  - Privacy
KW  - Visualization
KW  - Accuracy
KW  - Federated learning
KW  - Decision making
KW  - Medical services
KW  - Signal processing
KW  - Security
KW  - Explainable Federated Learning
KW  - Diabetic Retinopathy
KW  - Interpretability and Transparency in AI
KW  - Privacy Preservation
KW  - Image Processing
DO  - 10.1109/ICSPIS65223.2024.10931114
JO  - 2024 10th International Conference on Signal Processing and Intelligent Systems (ICSPIS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 10th International Conference on Signal Processing and Intelligent Systems (ICSPIS)
Y1  - 25-26 Dec. 2024
AB  - Ensuring interpretability and transparency in AIdriven decision-making is crucial, especially in healthcare. However, the resource-intensive nature of training complex AI models can often be a significant obstacle, particularly for smaller healthcare institutions and remote clinics. This is where Federated Learning (FL) and Explainable AI (XAI) come into play. This paper proposes a novel Explainable Federated Learning (EFL) framework for diagnosing Diabetic Retinopathy in a Healthcare 4.0 environment. Our approach combines the privacy-preserving benefits of FL with the enhanced transparency of XAI techniques, addressing the challenges of data privacy, resource optimization, and interpretability. By training lightweight local models on client devices and aggregating their weights to update a global model, our framework eliminates the need for high-resource centralized training, making it accessible to a wider range of healthcare providers. Furthermore, we integrate XAI methods, such as Local Interpretable Model-Agnostic Explanations (LIME), to provide visual explanations of the model's decision-making process. This enhances trust and transparency for medical professionals and patients while offering insights into the model's learning process. Our performance evaluation demonstrates the effectiveness of this EFL approach, with local models achieving high accuracy and low loss over multiple communication rounds and the global model exhibiting robust performance metrics, including accuracy, precision, recall, and F1-score. This framework can be readily extended to other healthcare applications where interpretability and privacy are paramount.
ER  - 

TY  - CHAP
TI  - 19 A novel blockchain-based artificial intelligence application for healthcare automation
T2  - Toward Artificial General Intelligence: Deep Learning, Neural Networks, Generative AI
SP  - 373
EP  - 386
AU  - Pramendra Kumar
AU  - Gunawan Widjaja
AU  - Nikhil S. Patankar
AU  - B. Suresh Kumar
PY  - 2024
KW  - Medical services
KW  - Blockchains
KW  - Artificial intelligence
KW  - Medical diagnostic imaging
KW  - Automation
KW  - Accuracy
KW  - Costs
KW  - Distributed ledger
KW  - Manuals
KW  - Machine learning algorithms
DO  - 
PB  - De Gruyter
SN  - 9783111324166
UR  - http://ieeexplore.ieee.org/document/10790396
AB  - The integration of blockchain and artificial intelligence (AI) technology can facilitate a comprehensive healthcare automation platform that can leverage the power of both AI and blockchain. This platform not only creates trust, enabling transparency of patient data sharing and record keeping, but also allows AI and machine learning algorithms to improve the efficiency and accuracy of healthcare technologies. It can provide healthcare data privacy, ease the access and sharing of medical records and other confidential health information, and enable fast and secure medical imaging transfers.
ER  - 

TY  - CONF
TI  - Multiple Machine Learning Models for Alzheimer’s disease detection for Mixed Data with Explainable AI
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
SP  - 1
EP  - 8
AU  - U. Rashmi
AU  - B. M. Beena
PY  - 2024
KW  - Proteins
KW  - Explainable AI
KW  - Computational modeling
KW  - Neurons
KW  - Medical services
KW  - Predictive models
KW  - Brain cells
KW  - Data models
KW  - Delays
KW  - Alzheimer's disease
KW  - Alzheimer’s Disease(AD)
KW  - Class balancing Techniques
KW  - Data Transformation
KW  - Classification models
KW  - Alzheimer’s Disease
KW  - Biomarkers
KW  - Mini Mental State Examinations Scores
KW  - Explainable AI
KW  - APOE Allele levels
DO  - 10.1109/ICCCNT61001.2024.10725637
JO  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
IS  - 
SN  - 2473-7674
VO  - 
VL  - 
JA  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
Y1  - 24-28 June 2024
AB  - Alzheimer’s disease involves a gradual decline in mental function, characterized by the accumulation of abnormal protein deposits around brain cells, resulting in irreversible neural cells death. Unfortunately, existing medications and procedures cannot regenerate these neurons. As the disease progresses symptoms of forgetfulness worsens, significantly impacting daily routines and lifestyle. The progression of Alzheimer’s disease is typically divided into three phases. The preclinical phase occurs when there are no noticeable symptoms, but early Alzheimer’s biomarkers are present. The prodromal phase, or mild cognitive impairment (MCI), is characterized by mild cognitive decline that does not significantly affect daily activities. Lastly, dementia represents the final phase, where moderate to severe cognitive impairment prevents independent living. Early prediction of Alzheimer’s onset is crucial to mitigate its progression. Machine learning (ML) and artificial intelligence (AI) models play a pivotal role in predicting the disease based on health and lifestyle parameters. These advanced technologies enable early detection, offering hope for interventions to delay or prevent further cognitive decline. The Machine learning model is built by developing 4 different models to get better base for disease prediction and after that the results are described using ExplainableAI tools such as SHAP and LIME as the healthcare domain needs explanation for all the results. Using this model we are getting an accuracy of around 94%-100%.
ER  - 

TY  - CONF
TI  - Methods and Applications of Artificial Intelligence In Mental Health Care
T2  - 2024 International Conference Automatics and Informatics (ICAI)
SP  - 614
EP  - 620
AU  - V. Atias
AU  - K. Atias
PY  - 2024
KW  - Support vector machines
KW  - Ethics
KW  - Solid modeling
KW  - Recurrent neural networks
KW  - Reviews
KW  - Mental health
KW  - Reliability
KW  - Artificial intelligence
KW  - Biomedical monitoring
KW  - Monitoring
KW  - artificial intelligence
KW  - mental health
KW  - well-being
KW  - machine learning
KW  - deep learning
DO  - 10.1109/ICAI63388.2024.10851672
JO  - 2024 International Conference Automatics and Informatics (ICAI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference Automatics and Informatics (ICAI)
Y1  - 10-12 Oct. 2024
AB  - Mental disorders affect one in eight individuals globally, but access to professional assistance and necessary information is often limited. Artificial intelligence (AI) technology has the potential to revolutionize mental health care. This paper aims to provide a theoretical overview of the recent application of AI in the field of mental health. A non-systematic review of studies from the last decade, focusing on recent findings, was conducted. The paper summarizes the commonly used machine learning techniques including Support vector machines, followed by Random forest, Naïve bayes, Logistic regression, and K-nearest neighbors for prediction and diagnosis, research, and classification. Among deep learning techniques, the most frequently applied are Convolutional neural network, Recurrent neural networks, and hybrid deep learning models for prediction, diagnosis, screening, and research. Natural language processing, wearable AI, and AI-based virtual reality are also applied for monitoring, treatment, prediction, and research. Most of the current attention is centered around depression, anxiety, and mental health problems in general. Although AI-based applications claim to improve wellbeing, limited research currently supports these claims. Furthermore, the paper explores the responsible implementation of AI in mental health care, considering technical, clinical, and ethical perspectives. The need for a regulatory framework for all AI solutions in mental health care is emphasized, and further research is called for to evaluate their reliability and risks. In conclusion AI application in mental health care is a delicate balancing act between optimism and caution.
ER  - 

TY  - JOUR
TI  - Characterizing the Contribution of Dependent Features in XAI Methods
T2  - IEEE Journal of Biomedical and Health Informatics
SP  - 6466
EP  - 6473
AU  - A. M. Salih
AU  - I. B. Galazzo
AU  - Z. Raisi-Estabragh
AU  - S. E. Petersen
AU  - G. Menegaz
AU  - P. Radeva
PY  - 2024
KW  - Explainable AI
KW  - Data models
KW  - Nuclear magnetic resonance
KW  - Biological system modeling
KW  - Noise measurement
KW  - Bioinformatics
KW  - Kernel
KW  - Dependency
KW  - proxy
KW  - XAI
DO  - 10.1109/JBHI.2024.3395289
JO  - IEEE Journal of Biomedical and Health Informatics
IS  - 11
SN  - 2168-2208
VO  - 28
VL  - 28
JA  - IEEE Journal of Biomedical and Health Informatics
Y1  - Nov. 2024
AB  - Explainable Artificial Intelligence (XAI) provides tools to help understanding how AI models work and reach a particular decision or outcome. It helps to increase the interpretability of models and makes them more trustworthy and transparent. In this context, many XAI methods have been proposed to make black-box and complex models more digestible from a human perspective. However, one of the main issues that XAI methods have to face especially when dealing with a high number of features is the presence of multicollinearity, which casts shadows on the robustness of the XAI outcomes, such as the ranking of informative features. Most of the current XAI methods either do not consider the collinearity or assume the features are independent which, in general, is not necessarily true. Here, we propose a simple, yet useful, proxy that modifies the outcome of any XAI feature ranking method allowing to account for the dependency among the features, and to reveal their impact on the outcome. The proposed method was applied to SHAP, as an example of XAI method which assume that the features are independent. For this purpose, several models were exploited for a well-known classification task (males versus females) using nine cardiac phenotypes extracted from cardiac magnetic resonance imaging as features. Principal component analysis and biological plausibility were employed to validate the proposed method. Our results showed that the proposed proxy could lead to a more robust list of informative features compared to the original SHAP in presence of collinearity.
ER  - 

TY  - CONF
TI  - Explainability of Brain Tumor Classification Based on Region
T2  - 2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS)
SP  - 1
EP  - 6
AU  - P. Narayankar
AU  - V. P. Baligar
PY  - 2024
KW  - Image analysis
KW  - Explainable AI
KW  - Decision making
KW  - Predictive models
KW  - Brain modeling
KW  - Convolutional neural networks
KW  - Artificial intelligence
KW  - Explainable Artificial Intelligence (XAI)
KW  - Artificial Intelligence AI
DO  - 10.1109/ICETCS61022.2024.10544289
JO  - 2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS)
Y1  - 22-23 April 2024
AB  - Medical image analysis plays a crucial role in modern healthcare, aiding clinicians in diagnosing and treating various medical conditions. With the advent of Artificial Intelligence (AI) and Machine Learning (ML), there has been a surge in the development of AI-powered algorithms for medical image analysis. Explainable Artificial Intelligence (XAI) techniques aim to provide interpretable and transparent insights into the decision-making process of AI models, enhancing their usability and trustworthiness in healthcare applications. We review the state-of-the-art XAI methods, including feature visualisation, attention mechanisms, and rule-based systems, and their application to medical image analysis. XAI techniques can pave the way for safer and more effective AI-driven medical solutions, ultimately benefiting healthcare providers and patients. As the healthcare industry continues to embrace AI, integrating XAI into medical image analysis is poised to revolutionize how diseases are detected, diagnosed, and treated. In our work, we are using the deep learning model to classify and the Explainable AI model, which explains the prediction of the model for Brain tumour disease using MRI images. We have used CNN for brain tumour disease classification. Out of 7043 images, we have taken 5722 for training and 1321 for testing and validating the disease. The model achieves 80% of accuracy. Explainable AI models like LIME, SHAP, Integrated Gradients, and Grad-CAM are used to interpret a model’s predictions on regions of interest inan image.
ER  - 

TY  - CONF
TI  - Pan-Cancer Classification System with Explainable AI Interpretation: A Feasibility Study
T2  - 2024 IEEE Congress on Evolutionary Computation (CEC)
SP  - 1
EP  - 6
AU  - Y. Mamatjan
PY  - 2024
KW  - Training
KW  - Accuracy
KW  - Explainable AI
KW  - Genomics
KW  - Brain modeling
KW  - Gene expression
KW  - Bioinformatics
KW  - Pan-Cancer Classification
KW  - Cancer Genomics
KW  - Molecular Diagnostics
KW  - Gene Expression
KW  - Methylome
KW  - Machine Learning
KW  - Random Forest
KW  - Model Interpretation
KW  - Explainable AI
DO  - 10.1109/CEC60901.2024.10611971
JO  - 2024 IEEE Congress on Evolutionary Computation (CEC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE Congress on Evolutionary Computation (CEC)
Y1  - 30 June-5 July 2024
AB  - Cancer genomics identifies all genes playing critical roles in carcinogenesis. The state-of-the-art cancer genomics profiling characterized many clinically and biologically relevant patterns that are not resolvable by morphology nor distinguishable under the microscope for cancer diagnosis. With that genomic information, doctors can develop an individualized treatment plan for cancer patients and provide precision medicine. However, several technical challenges (such as low tumor purity, batch effects and formalin-fixed, paraffin-embedded (FFPE) tissue restoration) potentially led to ambiguous diagnoses that needed to be solved in the clinical setting. The purpose of this study is to develop a robust tumor classification framework to improve cancer diagnosis and provide Explainable Artificial Intelligence (XAI) based interpretable results with increased transparency model interpretability of the classification. We utilized a large set of over six thousand tumor samples (DNA methylation and gene expression) from The Cancer Genome Atlas (TCGA). We implemented realistic variable selection by separating the training and test datasets and removed artificial and technical sources of variabilities to overcome batch effect issues while identifying the biological variation and making the prediction meaningful and robust. The Random Forest classifier produced about 95 and 96% accuracy for mRNA and methylation-based models respectively with minimum features of 50 methylation probes and gene expression signatures. We further developed an XAI strategy and applied it to a large brain cancer patient group to make an explainable patient-specific decision while tailoring the provided recommendations based on each patient's characteristics. This strategy demonstrates more accurate and practical molecular subtype classification with explainable AI for model interpretation.
ER  - 

TY  - CONF
TI  - A Novel Pharmacovigilance Strategy for Detecting Adverse Drug Reactions in Healthcare Using Machine Learning and Blockchain
T2  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
SP  - 763
EP  - 767
AU  - S. G. A
AU  - G. S
AU  - J. V. J
AU  - M. K
PY  - 2024
KW  - Drugs
KW  - Support vector machines
KW  - Machine learning algorithms
KW  - Medical services
KW  - Machine learning
KW  - Prediction algorithms
KW  - Blockchains
KW  - Safety
KW  - Classification algorithms
KW  - Long short term memory
KW  - AI
KW  - Pharmacovigilance
KW  - Reporting
KW  - Data Mining
KW  - Drug Problems
KW  - Predictive Analytics
KW  - Cloud Infrastructure
KW  - Adverse Events
KW  - Continuous Learning
KW  - Blockchain
KW  - Support Vector Machine
KW  - K-Means
KW  - LSTM
DO  - 10.1109/ICoICI62503.2024.10696339
JO  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
Y1  - 28-30 Aug. 2024
AB  - Pharmacovigilance plays a pivotal role in ensuring the safety and efficacy of pharmaceutical products post-market release. However, existing pharmacovigilance systems often face challenges in efficiently identifying and managing adverse drug reactions (ADRs), leading to potential risks for patient safety. This research proposes a novel data analysis and prediction strategy by integrating Artificial Intelligence (AI), specifically Machine Learning (ML) Algorithms such as Support Vector Machine (SVM), K-means and LSTM with Blockchain technology to enhance pharmacovigilance practices within healthcare systems. The integration of Machine Learning (ML) algorithms enables the automated detection predictive analytics, classification, and prioritization of ADR reports, thereby streamlining the pharmacovigilance process and facilitating timely intervention. Furthermore, blockchain technology provides a secure and immutable platform for storing and sharing pharmacovigilance data, ensuring data integrity, transparency, and traceability throughout the drug lifecycle. By synergistically integrating ML algorithm and blockchain into pharmacovigilance systems, healthcare stakeholders can enhance their ability to detect and respond to ADRs effectively, ultimately improving patient outcomes and drug safety. The prediction accuracy and efficiency has produced by the algorithm SVM is 85%, K-means is 90% and LSTM is 94%
ER  - 

TY  - CONF
TI  - Advancing Predictive Modeling in Healthcare A Data Science Approach Utilizing AI-Driven Algorithms
T2  - 2024 International Conference on Intelligent Computing and Emerging Communication Technologies (ICEC)
SP  - 1
EP  - 6
AU  - M. P. Putalpattu
AU  - K. Bhargavi
AU  - M. B. Mayani
AU  - P. Srinivas
AU  - A. Siddiqa
AU  - M. Kunkulagunta
PY  - 2024
KW  - Ethics
KW  - Technological innovation
KW  - Education
KW  - Surgery
KW  - Medical services
KW  - Streaming media
KW  - Safety
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Standards
KW  - Artificial intelligence (AI)
KW  - Healthcare system
KW  - Prediction analysis
DO  - 10.1109/ICEC59683.2024.10837024
JO  - 2024 International Conference on Intelligent Computing and Emerging Communication Technologies (ICEC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Intelligent Computing and Emerging Communication Technologies (ICEC)
Y1  - 23-25 Nov. 2024
AB  - Artificial intelligence (AI) is based on the premise that machines may learn from data collected from different sources to mimic human intellect in order to carry out tasks, identify patterns, or anticipate outcomes. Many areas of technology have made extensive use of AI and ML algorithms, including: autonomous vehicles, recommendation systems in e-commerce and social media, financial technology, question answering systems, and natural language processing. In a similar vein, AI is quietly revolutionising healthcare research. Half a century ago, there was a lot of interest in using a rule-based method for illness diagnosis and clinical decision assistance. When it comes to diagnosing diseases and developing individualized treatment programs, AI systems show remarkable accuracy in analyzing medical imagery. Streamlining operations using AI-powered solutions improves efficiency and the patient experience, while predictive analytics help find patients at high risk so they can get preventative treatments. By automating monotonous operations, particularly in the domains of surgery and rehabilitation, robots powered by artificial intelligence also improve healthcare delivery. Data quality, interpretability, bias, and regulatory frameworks are all important concerns that must be resolved before AI can be used responsibly. Ethical and successful AI integration into healthcare requires robust regulatory frameworks, education, safety validation, human-AI collaboration, and education.
ER  - 

TY  - JOUR
TI  - Big Data and AI Algorithms for Sustainable Development Goals: A Topic Modeling Analysis
T2  - IEEE Access
SP  - 188519
EP  - 188541
AU  - P. Nedungadi
AU  - S. Surendran
AU  - K. -Y. Tang
AU  - R. Raman
PY  - 2024
KW  - Artificial intelligence
KW  - Big Data
KW  - Sustainable development
KW  - Medical services
KW  - Data models
KW  - Technological innovation
KW  - Ethics
KW  - Market research
KW  - Climate change
KW  - Analytical models
KW  - Sustainable development goal
KW  - big data
KW  - artificial intelligence
KW  - healthcare
KW  - resilient energy
KW  - resilient infrastructure
KW  - industrial innovation
KW  - generative AI
DO  - 10.1109/ACCESS.2024.3516500
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - This study makes significant contributions to the field by examining the transformative role of big data and artificial intelligence (AI) in advancing Sustainable Development Goals (SDGs), particularly healthcare (SDG3), sustainable energy (SDG7), and industry and infrastructure (SDG9). Using BERTopic modeling, a machine learning technique, this research systematically analyzes literature from 2013 to 2024, providing an overview of AI and big data applications mapped to SDGs which is a first. This structured approach identifies key SDGs impacted by these technologies and highlights interdisciplinary methods that further enhance SDG outcomes. AI applications notably improve healthcare by advancing disease tracking, tailored treatments, and precision medicine, fostering universal healthcare and reducing noncommunicable disease mortality. In energy, AI-driven solutions optimize forecasting, grid management, and renewable integration, while in industry, they bolster infrastructure resilience through innovations like predictive maintenance and automated quality control within Industry 4.0 frameworks. The integration of automated text analysis and semantic context captures broad trends, contributing both methodologically and substantively at the intersection of AI and sustainability. Despite these advancements, the study underscores ethical concerns, including data privacy, security, and algorithmic biases. Interdisciplinary collaboration among healthcare professionals, engineers, environmental scientists, and AI experts is crucial to developing ethical, scalable AI solutions. The study suggests future research focus on AI transparency, scaling across diverse sectors, and integrating advanced techniques such as neurosymbolic AI and quantum neural networks to enhance system reliability. These insights offer practical implications, reinforcing the potential of AI and big data to address global challenges sustainably while calling for balanced attention to ethical and regulatory dimensions.
ER  - 

TY  - CONF
TI  - Explainable Machine Learning Models for Clinical Decision Support Systems
T2  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
SP  - 1
EP  - 6
AU  - A. Kumar
AU  - V. Veeraiah
AU  - T. N. Gongada
AU  - S. Ahamad
AU  - H. Khan
AU  - A. Gupta
PY  - 2024
KW  - Industries
KW  - Decision support systems
KW  - Data privacy
KW  - Analytical models
KW  - Technological innovation
KW  - Systematics
KW  - Computational modeling
KW  - Medical services
KW  - Transforms
KW  - Predictive models
KW  - Explainable Machine Learning
KW  - CDSS
KW  - Healthcare Industry
KW  - Decision Making
DO  - 10.1109/ICCCNT61001.2024.10725028
JO  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
IS  - 
SN  - 2473-7674
VO  - 
VL  - 
JA  - 2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)
Y1  - 24-28 June 2024
AB  - Explainable Machine Learning (ML) models are an essential component of Clinical Decision Support Systems (CDSS), since they provide the transparency and interpretability that are essential for efficient decision-making in the healthcare industry. With a particular emphasis on their capacity to provide comprehensible explanations in case of clinical predictions along with recommendations, this study investigates the development along with deployment of explainable ML models in (CDSS). We analyze a variety of strategies and methodologies that are used to improve interpretability of ML models in healthcare settings. This is accomplished by doing a substantial study of the relevant literature and case studies. Feature significance analysis, attention mechanisms, rule-based systems, and model-agnostic explanation approaches are some of the techniques that fall under this category. We also examine the difficulties and possibilities that arise when implementing explainable machine learning models in clinical settings that are based on the real world. These include issues around data privacy, compliance with legal requirements, and integration with preexisting processes in the healthcare industry. The purpose of this paper is to educate healthcare professionals, researchers, and policymakers about the potential of transparent and interpretable artificial intelligence models to improve patient outcomes, reduce medical errors, and enhance trust in AI-driven healthcare systems. This will be accomplished by elucidating the benefits and limitations of explainable ML in CDSS.
ER  - 

TY  - CONF
TI  - A Comprehensive Review of Existing Smart Summary Recommendation Model Enhanced Through RL and XAI Techniques
T2  - 2024 International Conference on Sustainable Communication Networks and Application (ICSCNA)
SP  - 301
EP  - 307
AU  - N. S M
AU  - S. S
AU  - A. Tomy
AU  - D. S
AU  - H. P S
AU  - J. H
PY  - 2024
KW  - Reviews
KW  - Explainable AI
KW  - Computational modeling
KW  - Green communications
KW  - Medical services
KW  - Reinforcement learning
KW  - Reliability
KW  - Recommender systems
KW  - Monitoring
KW  - Overfitting
KW  - Distributed Recommender System
KW  - Intensive Care unit
KW  - Reinforcement learning
KW  - Explainable AI
KW  - Clinical Decision-Making
DO  - 10.1109/ICSCNA63714.2024.10864185
JO  - 2024 International Conference on Sustainable Communication Networks and Application (ICSCNA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Sustainable Communication Networks and Application (ICSCNA)
Y1  - 11-13 Dec. 2024
AB  - This research work presents an innovative healthcare recommendation system designed to address the challenge of suboptimal feedback and medical errors. By integrating Reinforcement Learning (RL) and Explainable AI (XAI), the system aims to improve the accuracy and reliability of recommendations. The RL component enables the system to learn from real-world interactions, optimizing recommendations over time. XAI provides transparent explanations for model decisions, fostering trust and understanding among healthcare providers. This combination allows for personalized treatment plans, early disease detection, and efficient resource allocation. By addressing the limitations of traditional systems, this enhanced approach has the potential to revolutionize healthcare delivery, leading to improved patient outcomes and more efficient healthcare systems.
ER  - 

TY  - CONF
TI  - XAI and Computer Vision: Interpretable Deep Learning Approach with Explainable Artificial Intelligence for Brain Tumor Classification
T2  - 2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)
SP  - 970
EP  - 975
AU  - G. V. Moodely
AU  - S. M. I. H. Amiree
AU  - R. R N
AU  - S. Wanglen
AU  - S. Chauhan
AU  - S. K. Singh
PY  - 2024
KW  - Computer vision
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Decision making
KW  - Brain tumors
KW  - Medical services
KW  - Predictive models
KW  - Brain modeling
KW  - Convolutional neural networks
KW  - Separable ConvNet
KW  - ConvNet
KW  - XAI
KW  - LIME
KW  - Data Augmentation
KW  - Interpretability
DO  - 10.1109/GlobalAISummit62156.2024.10947779
JO  - 2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Artificial Intelligence and Emerging Technology (Global AI Summit)
Y1  - 4-6 Sept. 2024
AB  - Explainable Artificial Intelligence (XAI) plays a crucial role in the field of healthcare by enhancing transparency and decision-making processes in computer vision applications. In our research, we utilized two Convolutional Neural Network (CNN) models, SeparableConvNet and ConvNet, for the classification of brain tumors using computer vision techniques. These models were evaluated using a dataset consisting of 7023 human brain MRI scans collected from figshare, SARTAJ and Br35H datasets on Kaggle. The images were categorized into four groups: Gliomas, meningiomas, no tumor and pituitary. Both models underwent preprocessing steps including data normalization and augmentation to improve reliability and generalizability. The SeparableConvNet achieved a test accuracy of 96.64%, outperforming the ConvNet's test accuracy of 95.19%. To enhance interpretability and gain insights into the models' decision-making processes, “Local Interpretable Model-agnostic Explanations (LIME)” were employed. Our results highlight the effectiveness of CNN models in classifying brain tumors using computer vision and underscore the importance of XAI techniques in understanding these models' predictions.
ER  - 

TY  - CONF
TI  - Machine Learning in Medical Diagnosis and Treatment Planning
T2  - 2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)
SP  - 1554
EP  - 1559
AU  - M. Sharma
AU  - D. Singh
AU  - A. Tripathi
AU  - N. Ramajayam
AU  - P. Rawat
AU  - S. Malarvizhi
PY  - 2024
KW  - Ethics
KW  - Analytical models
KW  - Accuracy
KW  - Predictive models
KW  - Prediction algorithms
KW  - Planning
KW  - Medical diagnosis
KW  - Medical diagnostic imaging
KW  - Unsupervised learning
KW  - Diseases
KW  - Machine Learning
KW  - Medical Diagnosis
KW  - Treatment Planning
DO  - 10.1109/ICAC2N63387.2024.10894906
JO  - 2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)
Y1  - 16-17 Dec. 2024
AB  - Machine Learning (ML) is revolutionizing the field of medical diagnosis and treatment planning by enabling the analysis of vast amounts of medical data to identify patterns and make predictions with unprecedented accuracy. This paper investigates the application of ML algorithms in various aspects of healthcare, including disease diagnosis, prognosis, and the development of personalized treatment plans. Key ML techniques such as supervised learning, unsupervised learning, and deep learning are examined for their roles in interpreting complex medical data, ranging from imaging to genomic sequences. The research highlights how ML models can assist in early detection of diseases, predictive analytics for patient outcomes, and optimization of treatment strategies, leading to improved patient care and operational efficiencies. This model also discuss the integration of ML systems with electronic health records (EHRs) and the ethical considerations surrounding data privacy and algorithmic transparency. Through case studies and real-world implementations, the paper demonstrates the transformative impact of ML in reducing diagnostic errors, accelerating clinical workflows, and personalizing patient care. The findings underscore the potential of ML to enhance the precision and effectiveness of medical practice, fostering a future where data-driven insights significantly contribute to health outcomes.
ER  - 

TY  - CONF
TI  - ChatGPT and Large Language Models in Healthcare; a Bibliometrics Analysis and Review
T2  - 2024 IEEE 11th International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC)
SP  - 1
EP  - 6
AU  - A. Mosavi
AU  - F. Imre
AU  - V. T. Hung
PY  - 2024
KW  - Reviews
KW  - Large language models
KW  - Virtual assistants
KW  - Telemedicine
KW  - Surveillance
KW  - Bibliometrics
KW  - Medical services
KW  - healthcare
KW  - ChatGPT
KW  - large language models
KW  - artificial intelligence
KW  - deep learning
KW  - machine learning
KW  - LLMs
KW  - big data
KW  - data science
KW  - mathematics
KW  - applied artificial intelligence
KW  - XAI
KW  - natural-language programming
KW  - medical sciences
KW  - soft computing
KW  - applied machine learning
KW  - applied mathematics
KW  - applied informatics
KW  - generative artificial intelligence
KW  - generative AI
DO  - 10.1109/ICCC62278.2024.10582937
JO  - 2024 IEEE 11th International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 11th International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC)
Y1  - 4-6 April 2024
AB  - ChatGPT and similar large language models (LLMs) are becoming the essential tools in healthcare, offering diverse applications. This article presents a bibliometric analysis for studying the dimension and the role of ChatGPT and major large language models in healthcare. The LLMs’ impact, usage, and growth in medical literature is presented. Through exploring the Scopus database, this article uncovers the trends and contributions of LLMs. Our findings deliver insights into the evolution of the LLMs’ applications highlighting their significance and limitations in shaping future healthcare.
ER  - 

TY  - CONF
TI  - Enhancing Clinical Trust: The Role of AI Explainability in Transforming Healthcare
T2  - 2024 IEEE International Conference on Data Mining Workshops (ICDMW)
SP  - 543
EP  - 549
AU  - A. Pawlicka
AU  - M. Pawlicki
AU  - D. Jaroszewska-Choraś
AU  - R. Kozik
AU  - M. Choraś
PY  - 2024
KW  - Measurement
KW  - Ethics
KW  - Technological innovation
KW  - Generative AI
KW  - Explainable AI
KW  - Decision making
KW  - Medical services
KW  - Rhetoric
KW  - Artificial intelligence
KW  - Standards
KW  - AI
KW  - AI Explainability
KW  - Clinical AI
KW  - Generative AI
KW  - xAI
DO  - 10.1109/ICDMW65004.2024.00075
JO  - 2024 IEEE International Conference on Data Mining Workshops (ICDMW)
IS  - 
SN  - 2375-9259
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Data Mining Workshops (ICDMW)
Y1  - 9-9 Dec. 2024
AB  - In this paper, the tensions between technological progress and societal apprehension are explored, especially in the context of Generative AI and its clinical applications. The authors emphasize the critical role of Explainable AI in addressing these challenges. xAI aims to make AI decision-making processes more transparent, thereby fostering trust and aligning AI systems with human values. However, the lack of consensus on the desirable properties and evaluation metrics of xAI poses significant obstacles. The authors argue that by simplifying and refining existing metrics, the scientific community can enhance the transparency and accountability of AI in healthcare, ultimately ensuring that these technologies fulfill their promise of improving patient outcomes while adhering to ethical standards. The findings advocate for a collaborative approach to the responsible development of clinical AI, balancing innovation with the necessary safeguards.
ER  - 

TY  - CONF
TI  - A Novel Approach to Interpret Multiclass Skin Disease Using Explainable AI
T2  - 2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)
SP  - 761
EP  - 766
AU  - B. J. Chelliah
AU  - A. Senthilselvi
AU  - R. P. Pranav
AU  - S. Tilak
AU  - G. Arunprasad
AU  - S. S. Pandi
PY  - 2024
KW  - Training
KW  - Deep learning
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Predictive models
KW  - Skin
KW  - Data models
KW  - Diseases
KW  - Residual neural networks
KW  - Explainable AI
KW  - deep learning
KW  - Classification
KW  - Skin disease
KW  - Convolutional Neural Network
DO  - 10.1109/ICAICCIT64383.2024.10912321
JO  - 2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)
Y1  - 28-29 Nov. 2024
AB  - Skin diseases represent a major public health issue, where accurate diagnosis before treatment is crucial. Despite this, the complexity of multiclass skin disease classification compounded by the absence of large and comprehensive datasets, presents challenges in developing reliable diagnostic models. Current solutions are normally not explainable, resulting in healthcare professionals not trusting models. To address these challenges, a novel approach incorporating Explainable AI is proposed to improve both the interpretability and performance of skin disease classification. Using over 10,015 dermatoscopic images across 7 disease classes, Convolutional Neural Networks (CNNs), including XceptionNet and ResNet50, are applied after thorough research of existing works. The performance is then assessed through accuracy, precision, recall, and F1 score. The experimental results show that XceptionNet achieves superior accuracy with 82.2% compared to ResNet50 provided a better accuracy score of 78.2%. To enhance interpretability, Local Interpretable Model-agnostic Explanations (LIME) are incorporated as visual aids, which highlight the regions of the image that contribute most to the model’s predictions, making the predictions more comprehensible than using Black-box deep learning. This approach not only improves classification accuracy but also enhances trustworthiness through increased transparency. Future work will focus on image overlay in real-time video feed, improving data collections to make the models stronger, and designing a multiple-object occupancy deep learning model for applications with higher complexity.
ER  - 

TY  - CONF
TI  - Precision Medicine with Deep Learning: Enhancing Biomedical Applications with Superior Data Analysis and Customized Medical Solutions
T2  - 2024 International Conference on Smart Technologies for Sustainable Development Goals (ICSTSDG)
SP  - 1
EP  - 7
AU  - S. Rajagopalan
AU  - S. Kavitha
AU  - V. S. Pandi
AU  - L. P. J
AU  - M. Shanmuganatha
AU  - N. V. S. Natteshan
PY  - 2024
KW  - Deep learning
KW  - Drugs
KW  - Precision medicine
KW  - Biological system modeling
KW  - Medical services
KW  - Transforms
KW  - Biomarkers
KW  - Genetics
KW  - Bioinformatics
KW  - Medical diagnostic imaging
KW  - Genetic Information Analysis
KW  - Disease Biomarkers Identification
KW  - Drug Response Prediction
KW  - Medical AI Systems
KW  - Data Augmentation Techniques
KW  - Regularization Methods
DO  - 10.1109/ICSTSDG61998.2024.11026224
JO  - 2024 International Conference on Smart Technologies for Sustainable Development Goals (ICSTSDG)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Smart Technologies for Sustainable Development Goals (ICSTSDG)
Y1  - 6-8 Nov. 2024
AB  - The emergence of deep learning technologies has been a major boon to precision medicine, a novel strategy for patient care that adapts medical interventions to each person's unique traits. Deep learning's potential in precision medicine is the subject of this study, which investigates how to apply sophisticated neural network designs for better data processing and individualised medical solutions. The study starts by taking a look at where precision medicine is at the moment, drawing attention to the difficulties of analyzing complicated biomedical data and the importance of tailored treatment plans. Further exploration of deep learning principles follows, touching on different kinds of neural networks that excel at processing and understanding biomedical data, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs). Proving that deep learning can improve biomedical applications is a crucial part of the study. Among these applications of deep learning are the following: the prediction of medication responses, the study of genetic information, and the identification of illness biomarkers. Case cases are shown in the research where deep learning models have improved diagnostic accuracy and treatment efficacy by discovering previously invisible patterns in biomedical data. The paper also discusses the difficulties of using deep learning for precision medicine, including the significance of interpretability and confidence in AI systems for healthcare, the danger of overfitting models, and the necessity of big, high-quality datasets. In order to overcome these obstacles, the research delves into methodologies such as regularization methods, data augmentation approaches, and the building of explainable AI models. In addition, the study highlights how deep learning could help find novel medications and treatment methods. Deep learning has the potential to optimize clinical trial designs, find new therapeutic targets, and speed up drug development by evaluating massive volumes of biomedical data. Finally, the study highlights how deep learning can revolutionize precision medicine. Deep learning might completely transform healthcare by facilitating improved data analysis and the creation of personalized medical solutions. This, in turn, could lead to more efficient medical procedures and better patient results. Results from this study shed light on where deep learning applications in healthcare and biological research are headed and what they can do now.
ER  - 

TY  - JOUR
TI  - Deciphering Knee Osteoarthritis Diagnostic Features With Explainable Artificial Intelligence: A Systematic Review
T2  - IEEE Access
SP  - 109080
EP  - 109108
AU  - Y. Xin Teoh
AU  - A. Othmani
AU  - S. Li Goh
AU  - J. Usman
AU  - K. W. Lai
PY  - 2024
KW  - Artificial intelligence
KW  - Explainable AI
KW  - Data models
KW  - Predictive models
KW  - Medical diagnostic imaging
KW  - Osteoarthritis
KW  - Accuracy
KW  - Computer aided diagnosis
KW  - explainable artificial intelligence
KW  - explanation representation
KW  - knee osteoarthritis
KW  - radiology
DO  - 10.1109/ACCESS.2024.3439096
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Existing artificial intelligence (AI) models for diagnosing knee osteoarthritis (OA) have faced criticism for their lack of transparency and interpretability, despite achieving medical-expert-like performance. This opacity makes them challenging to trust in clinical practice. Recently, explainable artificial intelligence (XAI) has emerged as a specialized technique that can provide confidence in the model’s prediction by revealing how the prediction is derived, thus promoting the use of AI systems in healthcare. This paper presents the first survey of XAI techniques used for knee OA diagnosis. This survey identified 78 AI-based primary knee OA diagnostic test accuracy studies, of which 70 (89.7%) employed XAI. In 34 out of 70 (48.6%) of studies, XAI was utilized for the goal of visualization of predictions. Gradient-weighted class activation mapping (GradCAM) is the most common technique, being used in 24 out of 70 studies (34.3%), followed by SHapley Additive exPlanations (SHAP), being used in 9 out of 70 (12.9%) studies. All included studies analyzed the outcomes generated by XAI methods through qualitative analysis. However, only three studies utilized quantitative measures to evaluate the reliability of the XAI outcomes. We also observed that 64.3% of the studies utilized widely-circulated dataset, namely Osteoarthritis Initiative (OAI) extensively.The XAI techniques are discussed from two perspectives: data interpretability and model interpretability. Our paper provides an overview of XAI’s potential towards a more reliable knee OA diagnosis approach and helps to encourage its adoption in clinical practice.
ER  - 

TY  - CONF
TI  - Investigating Key Contributors to Hospital Appointment No-Shows Using Explainable AI
T2  - 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)
SP  - 1
EP  - 6
AU  - V. Yiye
AU  - O. Ugbomeh
AU  - C. P. Ezenkwu
AU  - E. Ibeke
AU  - V. Sharma
AU  - A. Alkhayyat
PY  - 2024
KW  - Measurement
KW  - Hypertension
KW  - Analytical models
KW  - Hospitals
KW  - Explainable AI
KW  - Predictive models
KW  - Multilayer perceptrons
KW  - Vectors
KW  - Resource management
KW  - History
KW  - Artificial Intelligence
KW  - AI
KW  - No-Shows
KW  - LIME
KW  - SHAP
KW  - XAI
KW  - Interpretable
KW  - Explainable
KW  - Machine Learning
KW  - Health Informatics
KW  - Health Analytics
KW  - Healthcare
KW  - Hospital
DO  - 10.1109/ICEECT61758.2024.10739123
JO  - 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)
IS  - 
SN  - 
VO  - 1
VL  - 1
JA  - 2024 International Conference on Electrical Electronics and Computing Technologies (ICEECT)
Y1  - 29-31 Aug. 2024
AB  - The healthcare sector has suffered from wastage of resources and poor service delivery due to the significant impact of appointment no-shows. To address this issue, this paper uses explainable artificial intelligence (XAI) to identify major predictors of no-show behaviours among patients. Six machine learning models were developed and evaluated on this task using Area Under the Precision-Recall Curve (AUC-PR) and F1-score as metrics. Our experiment demonstrates that Support Vector Classifier and Multilayer Perceptron perform the best, with both scoring the same AUC-PR of 0.56, but different F1-scores of 0.91 and 0.92, respectively. We analysed the interpretability of the models using Local Interpretable Model-agnostic Explanation (LIME) and SHapley Additive exPlanations (SHAP). The outcome of the analyses demonstrates that predictors such as the patients' history of missed appointments, the waiting time from scheduling time to the appointments, patients' age, and existing medical conditions such as diabetes and hypertension are essential flags for no-show behaviours. Following the insights gained from the analyses, this paper recommends interventions for addressing the issue of medical appointment no-shows.
ER  - 

TY  - CONF
TI  - A Fog-Driven Interpretable Human Motion Acknowledgement Structure with Edge Intelligence for Real-Time, Low-Latency Analytics
T2  - 2024 IEEE 2nd International Conference on Innovations in High Speed Communication and Signal Processing (IHCSP)
SP  - 1
EP  - 6
AU  - S. Mittal
AU  - J. Sohal
AU  - J. V
AU  - S. Nivarthi
AU  - N. N. Wasatkar
AU  - N. Kalidas
PY  - 2024
KW  - Deep learning
KW  - Cloud computing
KW  - Technological innovation
KW  - Explainable AI
KW  - Predictive models
KW  - Signal processing
KW  - Real-time systems
KW  - Human activity recognition
KW  - Internet of Things
KW  - Low latency communication
KW  - IoT
KW  - Human Activity Recognition
KW  - DL
KW  - XAI
KW  - IHAR
KW  - Sensors
KW  - ML
KW  - SHAP and LIME
DO  - 10.1109/IHCSP63227.2024.10960184
JO  - 2024 IEEE 2nd International Conference on Innovations in High Speed Communication and Signal Processing (IHCSP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 2nd International Conference on Innovations in High Speed Communication and Signal Processing (IHCSP)
Y1  - 6-8 Dec. 2024
AB  - Many healthcare applications, including fall detection and assisted living, are now making use of Human Activity Recognition, a field of active study. All over modern society, people are using these apps that cater to the Internet of Things. The need to make rapid, informed judgments is a major obstacle in many applications. As a possible answer to the latency problem, hierarchical edge-fog-cloud computing structures might be implemented. We next examine a fog-based activity detection system in our second piece of work, which involves collecting and transmitting patient sensor data to fog nodes. In the fog, these data are pre-processed, filtered, and evaluated to make dynamic judgments using Deep Learning. One component of the suggested system is a Deep Learning framework that has been brought up on the cloud. To guarantee that the Fog layer has intelligence, the trained Deep Learning framework is configured to operate on local fog nodes for the purpose of categorizing human activities. In this study, we provide an IHAR paradigm to bolster Human Activity Recognition by providing context for the findings. The use of Transparent Human Activity Recognition was highlighted by the successful model-level observations on the categorization. This study also compared the outcomes of other current Explainable AI reproductions, such LIME and SHAP, to find out which one performed improved when it came to Human Activity Recognition.
ER  - 

TY  - CONF
TI  - Dynamic Explainability in AI for Neurological Disorders: An Adaptive Model for Transparent Decision-Making in Alzheimer's Disease Diagnosis
T2  - 2024 IEEE 13th International Conference on Communication Systems and Network Technologies (CSNT)
SP  - 980
EP  - 986
AU  - A. Shukla
AU  - S. Upadhyay
AU  - P. R. Bachan
AU  - U. N. Bera
AU  - R. Kshirsagar
AU  - N. Nathani
PY  - 2024
KW  - Neurological diseases
KW  - Deep learning
KW  - Explainable AI
KW  - Transfer learning
KW  - Predictive models
KW  - Brain modeling
KW  - Real-time systems
KW  - Explainable AI
KW  - Alzheimer's Disease Diagnosis
KW  - Neural Network prediction
KW  - MobileNet V2
DO  - 10.1109/CSNT60213.2024.10546177
JO  - 2024 IEEE 13th International Conference on Communication Systems and Network Technologies (CSNT)
IS  - 
SN  - 2473-5655
VO  - 
VL  - 
JA  - 2024 IEEE 13th International Conference on Communication Systems and Network Technologies (CSNT)
Y1  - 6-7 April 2024
AB  - In this paper, we proposed a model that will solve the ‘X’ of the ‘Xai’ that is Explainable AI. The model is developed using deep learning and transfer learning algorithms using different methods to depict how the decisions and predictions of the Artificial Intelligence are made to be understandable for humans to interpret. The term deals with explaining how the models work and what all happens in each layer of neurons and the output is shown. The transparency in the process lets humans understand the way how the predictions are carried out, what are the parameters that the model is considering, what are the steps it takes to generate the final output. Here, we considered Alzheimer disease in the brain and brought out the results per layer of the model to comprehend the reason of the final result. This could made easy for humans to identify what are the errors, unknown biases and the number of possible paths the model can take in order to generate the more accurate output. This proposed model is able to identify and depict the processes going on while generating the result. The field tends to address the “black box” problem in the complex machine learning models. The analysis would be used by stakeholders to identify the real cause behind the interpretation of results. Considering these, there are many use cases for this new emerging field, some of them includes in the field of medical diagnosis, where the doctors would be able to identify the paradigm and produce the most accurate diagnosis that will cure the patient's disease, in the finance sector to identify the future trends, for the real time processing and many such fields.
ER  - 

TY  - CONF
TI  - Unveiling Genetic Disorders: Machine Learning and Deep Learning Approaches in Gene Expression Analysis
T2  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
SP  - 1315
EP  - 1320
AU  - K. Revathi
AU  - V. V. Karthikeyan
AU  - S. Priyanka
AU  - S. J. Prakash
PY  - 2024
KW  - Machine learning algorithms
KW  - Hospitals
KW  - Precision medicine
KW  - Genomics
KW  - Prediction algorithms
KW  - Breast cancer
KW  - Internet of Things
KW  - Gene expression
KW  - Prognostics and health management
KW  - Medical diagnostic imaging
KW  - Artificial Intelligence
KW  - Breast Cancer
KW  - Data-Preprocessing
KW  - Learning Algorithms
KW  - Health Care
DO  - 10.1109/ICoICI62503.2024.10696060
JO  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 Second International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI)
Y1  - 28-30 Aug. 2024
AB  - The human body is composed of cells, and each cell has a variety of components. Cancer is one of the most serious illnesses in the world and is a leading cause of India's death rate. Breast cancer is frequently diagnosed in women, and one in twenty-eight Indian women is likely to develop breast cancer during her lifetime. There is a need of early identification and accurate prognosis of breast cancer to improve the recovery rate of patients. Once identified, the stage of cancer is the information which shows the extent of cancer within a patient. A cancer staging system also performs an important role in the identification of most of the cancer subtypes. It interprets a patient's prognosis and the treatment options available. Cancer registries collect medical data related to cancer, such as the number of patients, cancer type, age, sex, recovery rate, mortality rate, etc. In order to conduct an analysis of cancer, cancer registries also require cancer stage information. Earlier, the anatomic stage based on anatomic factors was used to identify the cancer stage of breast cancer patients, but more recently, the prognostic stage has been used to identify the cancer stage of breast cancer patients. Clinical experts refer to the prognostic stage for cancer prognosis and for precise outcome prediction because it is based on anatomic and biologic factors. The results generated by ML algorithms will be made interpretable with the help of Explainable AI technology to identify Cancer Biomarkers from the Genomic data. From the obtained results, Precision Medicine will be recommended to the Cancer patients. The results will be validated in the Cancer hospitals on the actual patients being treated.
ER  - 

TY  - CONF
TI  - Predicting Mental Health Stress Levels Through Sleep Patterns using Ensemble Models and Explainable AI
T2  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
SP  - 1
EP  - 7
AU  - S. V. Monga
AU  - R. K. Chaudhary
AU  - D. Kumar
AU  - Jahanavi
AU  - V. Jain
PY  - 2024
KW  - Support vector machines
KW  - Ethics
KW  - Explainable AI
KW  - Mental health
KW  - Medical services
KW  - Predictive models
KW  - Nearest neighbor methods
KW  - Decision trees
KW  - Random forests
KW  - Monitoring
KW  - Explainable AI
KW  - Mental Health
KW  - Machine Learning
KW  - Stress Level Prediction
DO  - 10.1109/DELCON64804.2024.10866636
JO  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd Edition of IEEE Delhi Section Flagship Conference (DELCON)
Y1  - 21-23 Nov. 2024
AB  - This study investigates the potential of Explainable Artificial Intelligence (XAI) to enhance mental healthcare by improving diagnostic transparency and treatment precision. Using the “Human Stress Detection” dataset from Kaggle, we applied an ensemble of classifiers, including Random Forest, K-Nearest Neighbors, SVM, XGBoost, and Decision Trees, to assess stress levels based on physiological data. The Random Forest model achieved the highest accuracy of 98.41%. By leveraging XAI techniques like LIME and SHAP, the study demonstrates how XAI can improve the interpretability of mental health assessments, reduce biases, and promote more equitable and effective mental healthcare.
ER  - 

TY  - CONF
TI  - The Intersection of AI, ML, and Industry: A Review of Emerging Trends in Real-World Applications
T2  - 2024 4th International Conference on Advancement in Electronics & Communication Engineering (AECE)
SP  - 769
EP  - 773
AU  - A. Joshi
AU  - V. Kumar
AU  - N. Chauhan
AU  - A. Kumar
AU  - R. K. Singh
PY  - 2024
KW  - Industries
KW  - Productivity
KW  - Ethics
KW  - Reviews
KW  - Transportation
KW  - Medical services
KW  - Transforms
KW  - Market research
KW  - Manufacturing
KW  - Artificial intelligence
KW  - Artificial Intelligence
KW  - Machine Learning
KW  - Industry
KW  - Emerging Trends
KW  - Real-World Applications
KW  - Healthcare
KW  - Finance
KW  - Manufacturing
KW  - Retail
KW  - Transportation
DO  - 10.1109/AECE62803.2024.10911386
JO  - 2024 4th International Conference on Advancement in Electronics & Communication Engineering (AECE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Advancement in Electronics & Communication Engineering (AECE)
Y1  - 22-23 Nov. 2024
AB  - Machine learning (ML) and artificial intelligence (AI) are transforming a number of industries, spurring creativity, and improving productivity. This review article examines the new developments at the nexus of industry, artificial intelligence, and machine learning. It offers a thorough rundown of current developments and real-world uses. We demonstrate how artificial intelligence (AI) and machine learning (ML) are being incorporated into industrial processes to solve complex problems and enhance operational workflows by looking at important industries including healthcare, banking, manufacturing, retail, and transportation. We highlight the revolutionary power of these technologies, talk about the difficulties and moral issues, and suggest future paths for study and development through in-depth case studies and practical examples.
ER  - 

TY  - CONF
TI  - Deep Learning-Based Detection and Severity Classification of Alzheimer's Disease Stages Using a Custom CNN Model
T2  - 2024 International Conference on Augmented Reality, Intelligent Systems, and Industrial Automation (ARIIA)
SP  - 1
EP  - 5
AU  - P. Kaushik
AU  - V. Kukreja
AU  - A. Singh
PY  - 2024
KW  - Measurement
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Brain modeling
KW  - Feature extraction
KW  - Data models
KW  - Convolutional neural networks
KW  - Alzheimer's disease
KW  - Alzheimer’s Disease
KW  - Deep Learning
KW  - MRI Classification
KW  - Custom CNN
KW  - Early Diagnosis
DO  - 10.1109/ARIIA63345.2024.11051926
JO  - 2024 International Conference on Augmented Reality, Intelligent Systems, and Industrial Automation (ARIIA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Augmented Reality, Intelligent Systems, and Industrial Automation (ARIIA)
Y1  - 20-21 Dec. 2024
AB  - It is a neurodegenerative disorder wherein the cognitive function is drastically affected, particularly in aging populations. An early and definitive diagnosis ensures an appropriate treatment strategy that improves patient outcomes. The present study is directed toward presenting a Custom Convolutional Neural Network model, Custom CNN, to classify the stages of AD with high precision using MRI data. The dataset is made up of 6,400 MRI images divided among four classes: Non_Demented, Very_Mild_Demented, Mild_Demented, and Moderate_Demented. The preprocessing done on the data was resizing, normalization, and data augmentation to make the model robust. In the Custom CNN, several convolutional layers were included, dropout for regularization, and SoftMax for multi-class classification. Evaluation Metrics: Some of the evaluation metrics that have been used in assessing the performance of the model are: Accuracy, Precision, Recall, F1-score, confusion matrix. It reaches the general accuracy of 98% in being quite reliable in the various classifications, with some difficulties only in discriminating conditions that are very similar. These findings point toward great potential for deep learning models in supporting early AD detection. Future work will focus on improving feature extraction and model transparency through explainable AI, therefore contributing to the development of more accessible and effective diagnostic tools across different healthcare settings.
ER  - 

TY  - CHAP
TI  - 4 Practical Applications of XAI
T2  - Explainable Artificial Intelligence: A Practical Guide
SP  - 63
EP  - 82
AU  - Parikshit Narendra Mahalle
AU  - Yashwant Sudhakar Ingle
PY  - 2024
DO  - 
PB  - River Publishers
SN  - 9788770047128
UR  - http://ieeexplore.ieee.org/document/10659345
AB  - This book explores the growing focus on artificial intelligence (AI) systems in both industry and academia. It evaluates and justifies AI applications while enhancing trust in AI outcomes and aiding comprehension of AI feature development. Key topics include an overview of explainable AI, black-box model understanding, interpretability techniques, practical XAI applications, and future trends and challenges in XAI. Technical topics discussed in the book include: &#x2022; Explainable AI overview &#x2022; Understanding black-box models &#x2022; Techniques for model interpretability &#x2022; Practical applications of XAI &#x2022; Future trends and challenges in XAI.
ER  - 

TY  - CONF
TI  - Demystifying XAI: Understanding of Applications, Limitations, Challenges, and Future Perspectives
T2  - 2024 International Conference on Advances in Computing Research on Science Engineering and Technology (ACROSET)
SP  - 1
EP  - 8
AU  - P. Varshney
AU  - N. P. Singh Rathore
AU  - K. K. Sethi
AU  - S. S. Rajput
PY  - 2024
KW  - Surveys
KW  - Privacy
KW  - Explainable AI
KW  - Pain
KW  - Decision making
KW  - Medical services
KW  - Robustness
KW  - Security
KW  - General Data Protection Regulation
KW  - Usability
KW  - Explainable Artificial Intelligence
KW  - Deep Neural Network
KW  - Healthcare
KW  - Finance
KW  - Applications
DO  - 10.1109/ACROSET62108.2024.10743621
JO  - 2024 International Conference on Advances in Computing Research on Science Engineering and Technology (ACROSET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Advances in Computing Research on Science Engineering and Technology (ACROSET)
Y1  - 27-28 Sept. 2024
AB  - Artificial Intelligence (AI) appears to be omnipresent, but its usability is currently a pain point as seen with automation bias. The General Data Protection Regulation (GDPR) has started solving this problem. The premise behind Explainable AI (XAI) is described as making AI decisions interpretable, gaining user trust and robustness of applications in healthcare and finance. This paper gives a survey of recent XAI advancements, providing types of algorithms, and examples of tasks. Despite the concept of XAI, we still have the black-box issue of AI, and especially in critical areas like healthcare with sensitive information, we need to protect user privacy. It is proposed that XAI can expose the decision-making process of the AI, to make the AI decision-making more reliable and trustworthy. Evaluation metrics and potential future directions for XAI are discussed, with a guiding principle: AI needs to be ethical, trustworthy, and user-friendly.
ER  - 

TY  - CONF
TI  - Transforming Healthcare in the New AI Era through Integrated Personalized Healthcare
T2  - 2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)
SP  - 790
EP  - 794
AU  - N. Bisht
AU  - Z. S. A. Ali
AU  - N. A. Amir
AU  - E. S. Phalguna Krishna
AU  - M. Mudhafar
AU  - K. H. Shakir
AU  - A. H. Shnain
PY  - 2024
KW  - Privacy
KW  - Ethics
KW  - Biomedical equipment
KW  - Genomics
KW  - Medical services
KW  - Software
KW  - Artificial intelligence
KW  - Artificial intelligence (AI)
KW  - deep learning
KW  - machine learning
KW  - and personalised healthcare
DO  - 10.1109/ICACITE60783.2024.10617239
JO  - 2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)
Y1  - 14-15 May 2024
AB  - The rapid evolution of virtual era, along with wearable devices, genomic information, and superior imaging, is driving a tremendous shift inside the healthcare landscape in the direction of customized medication. This shift necessitates the software program of Artificial Intelligence (AI) to broaden focused healing solutions and make clinical predictions. However, the mixing of AI into healthcare will increase essential challenges concerning transparency, felony responsibility, and privacy. To address the ones issues efficaciously, the paper proposes numerous strategies, collectively with the development of translation structures to bridge the gap amongst AI outputs and medical applications, and the inclusion of $A I$-focused curricula in medical schooling. These obligations’ purpose to harness AI’s capability responsibly, ensuring that healthcare transport is every innovative and ethical. The overarching aim is to enable a customised healthcare environment in which AI tools beautify the accuracy and effectiveness of treatments tailor-made to character affected man or woman dreams.
ER  - 

TY  - CONF
TI  - Artificial Intelligence: Applications, Advances in Weather Forecasting, and Emerging Challenges
T2  - 2024 Congreso Internacional de Innovación y Tendencias en Ingeniería (CONIITI)
SP  - 1
EP  - 4
AU  - A. V. Osorio
AU  - F. Orlando Corredor
PY  - 2024
KW  - Training
KW  - Ethics
KW  - Accuracy
KW  - Weather forecasting
KW  - Transportation
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Trajectory
KW  - Artificial intelligence
KW  - Artificial Intelligence
KW  - AI applications
KW  - weather forecasting
DO  - 10.1109/CONIITI64189.2024.10854839
JO  - 2024 Congreso Internacional de Innovación y Tendencias en Ingeniería (CONIITI)
IS  - 
SN  - 2539-4320
VO  - 
VL  - 
JA  - 2024 Congreso Internacional de Innovación y Tendencias en Ingeniería (CONIITI)
Y1  - 2-4 Oct. 2024
AB  - Artificial Intelligence (AI) has evolved significantly since its inception, tracing its origins back to the mid-20th century with the ambition to simulate human intelligence. This paper provides a comprehensive overview of AI, beginning with its historical development. It explores AI’s applications across diverse industries, highlighting its transformative impact on healthcare, transportation, and entertainment. Specifically, it delves into AI’s role in revolutionizing weather forecasting by leveraging advanced algorithms to analyze meteorological data and predict weather patterns with unprecedented accuracy. Despite these advancements, the integration of AI presents notable challenges that must be addressed to improve AI performance.
ER  - 

TY  - CONF
TI  - An Interpretable Approach with Explainable AI for the Detection of Cardiovascular Disease
T2  - 2024 International Conference on Integrated Intelligence and Communication Systems (ICIICS)
SP  - 1
EP  - 6
AU  - V. Sapra
AU  - L. Sapra
PY  - 2024
KW  - Accuracy
KW  - Explainable AI
KW  - Pain
KW  - Decision making
KW  - Mortality
KW  - Predictive models
KW  - Prediction algorithms
KW  - Data models
KW  - Vectors
KW  - Cardiovascular diseases
KW  - random forest
KW  - cardiovascular disease
KW  - support vector machine
KW  - LIME
KW  - SHAP
DO  - 10.1109/ICIICS63763.2024.10860265
JO  - 2024 International Conference on Integrated Intelligence and Communication Systems (ICIICS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Integrated Intelligence and Communication Systems (ICIICS)
Y1  - 22-23 Nov. 2024
AB  - Cardiovascular disease (CVD) is one of the prominent contributors to global mortality. Early detection and precise diagnosis of the disease are critically required to reduce its impact. This paper proposes an interpretable approach using Explainable Artificial Intelligence (XAI) to detect cardiovascular disease. By combining different machine learning (ML) algorithms with XAI techniques, we aim to enhance the models' predictability and transparency. Further, we use SHAP (SHapley Additive exPlanations) to provide a human-understandable explanation of model predictions. The proposed approach emphasizes both the accuracy and interpretability of the model which enhances the model's performance and also solves the black-box problem associated with AI models. Our results demonstrate that the proposed models achieve competitive performance metrics, where ANN achieved the highest accuracy of 91%. This work highlights the importance of XAI to bridge the gap between ML models and clinical decision-making, fostering trust in AI-driven healthcare solutions for the early detection of cardiovascular disease.
ER  - 

TY  - CONF
TI  - Identification of EEG Features of Transcranial Electrical Stimulation (tES) Based on eXplainable Artificial Intelligence (XAI)
T2  - 2024 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)
SP  - 1153
EP  - 1158
AU  - P. Arpaia
AU  - L. Ammendola
AU  - M. Cropano
AU  - M. De Luca
AU  - A. D. Calce
AU  - L. Gargiulo
AU  - G. Lus
AU  - L. Maffei
AU  - D. Malangone
AU  - N. Moccaldi
AU  - S. Raimo
AU  - E. Signoriello
AU  - P. De Blasiis
PY  - 2024
KW  - Support vector machines
KW  - Training
KW  - Accuracy
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Precision medicine
KW  - Electrical stimulation
KW  - Feature extraction
KW  - Electroencephalography
KW  - Classification algorithms
KW  - Multiple Sclerosis
KW  - EEG device
KW  - tES
KW  - Theory of Mind
KW  - eXplainable AI
KW  - Machine Learning
DO  - 10.1109/MetroXRAINE62247.2024.10797224
JO  - 2024 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE)
Y1  - 21-23 Oct. 2024
AB  - Electroencephalographic (EEG) features of transcranial Electrical Stimulation (tES) effects in Multiple Sclerosis (MS) patients were identified. Machine Learning and eXplainable Artificial Intelligence (XAI) algorithms were the used methods for the EEG feature selection. Current tES-based treatments lack of adaptivity to their effects on individuals. Real-time modification of electrical stimulation parameters based on the trend of specific EEG features may represent a new perspective in terms of personalized medicine for MS patients. This preliminary study aimed to identify EEG features reflecting the effect of tES treatment. Ongoing analyses are exploring the correlation between the identified EEG features and the expected clinical outcomes. Five MS patients underwent non-pharmacological treatment combining Theory of Mind (ToM) training with tES or sham treatment. Pre-and post-treatment variation in EEG features were assessed both in Eyes Opened (EO) and Eyes Closed (EC) conditions. In particular, absolute and relative power across six frequency bands, and Posterior Dominant Rhythm (PDR) amplitude and frequency were explored. The Sequential Feature Selection (SFS) algorithm in combination with Support Vector Machine (SVM) classifier identified (i) difference of absolute powers in high beta band in T3 channel, (ii) difference of absolute powers in gamma band in T3 channel, and (iii) difference of absolute powers in gamma band in O2 channel in EO condition, and (i) difference of absolute powers in gamma band in T3 channel, (ii) difference of absolute powers in gamma band in C4 channel, and (iii) difference of PDR amplitude in O2 channel in EC condition as the most discriminating tES from sham treatment (67.5 % accuracy in EO and 83.33 % in EC conditions, respectively). The SHapley Additive exPlanations (SHAP) algorithm highlighted PDR amplitudes in O2 and O1 as most informative features.
ER  - 

TY  - CONF
TI  - Explainable AI for the Prediction and Estimation of Obesity Levels using Machine Learning Models
T2  - 2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)
SP  - 709
EP  - 713
AU  - J. Shen
AU  - S. Li
AU  - L. Xinglin
AU  - Y. Wang
AU  - H. N. Monday
AU  - G. U. Nneji
PY  - 2024
KW  - Support vector machines
KW  - Obesity
KW  - Ethics
KW  - Explainable AI
KW  - Decision making
KW  - Cause effect analysis
KW  - Estimation
KW  - Pressing
KW  - Predictive models
KW  - Public healthcare
KW  - Obesity
KW  - SHAP
KW  - Machine Learning
KW  - SVMSMOTE
KW  - explainability
KW  - SHAP value
DO  - 10.1109/EIECS63941.2024.10800143
JO  - 2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)
Y1  - 27-29 Sept. 2024
AB  - In recent years, the global obesity rate has surged, highlighting the urgent need to address this critical public health issue. This study proposes a predictive model for obesity levels based on physical condition and eating habits, utilizing advancements in machine learning and available datasets. The Obesity level dataset from the UCI repository was balanced using Support Vector Machines Synthetic Minority Oversampling Technique (SVM-SMOTE) and standardized. Twelve classifiers are employed with Shapley Additive exPlanations (SHAP) used to interpret the best-performing models and elucidate their decision-making processes. Often seen as black boxes, artificial intelligence models were demystified in this study by using SHAP values to achieve interpretability and causality. The AdaBoost model excelled, achieving an accuracy, F1-score and precision of 89.22%. Incorporating explainability enhances transparency and helps identify the root causes of obesity, providing valuable insights into this pressing health issue. This comprehensive analysis is crucial for the ethical adoption of artificial in healthcare, addressing transparency, causality, and interpretability in diagnosis. The proposed technique demonstrates excellent performance and offers a robust framework research and applications in public health.
ER  - 

TY  - CONF
TI  - Interpretability of Deep Learning Analysis Result of Intradialytic Hypotension Prediction Model with Recommendation Reports Utilizing Large Language Model
T2  - 2024 IEEE 6th Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)
SP  - 91
EP  - 95
AU  - Y. -c. Lin
AU  - J. -A. Wang
AU  - M. -h. Lee
AU  - H. -W. Hu
PY  - 2024
KW  - Deep learning
KW  - Analytical models
KW  - Biological system modeling
KW  - Ultraviolet sources
KW  - Retrieval augmented generation
KW  - Predictive models
KW  - Physiology
KW  - Data models
KW  - Random forests
KW  - Hypotension
KW  - machine learning
KW  - intradialytic hypotension
KW  - artificial intelligence
KW  - predictive analytics
KW  - ai-driven diagnostics
DO  - 10.1109/ECBIOS61468.2024.10885460
JO  - 2024 IEEE 6th Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 6th Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)
Y1  - 14-16 June 2024
AB  - Intradialytic hypotension (IDH) is a significant clinical problem during hemodialysis for patient safety and therapeutic effectiveness. To solve this issue, we used sophisticated deep learning algorithms to anticipate IDH to occur and improve dialysis treatment and care. We collected a dataset of 2,000 dialysis records from Annan Hospital and investigated numerous machine learning techniques, including multilayer perceptron (MLP), random forest, extreme random tree (Extra Tree), XGBoost, and optical gradient boosting (OGB). Machine (LightGBM) and Category Booster (CatBoost) improve the prediction ability of IDH occurrences. To accurately evaluate possible risk factors, we cleaned and prepared data, incorporating physiological indicators such as blood test data, and dialysis machine information. The model utilizes entropy and the Gini index to segment data optimally with locally interpretable model-agnostic explanations (LIME) for the forecast. The area under curve (AUC) of the receiver operating characteristic (ROC) was used to assess the model's predictive ability with models such as LightGBM, CatBoost, and Random Forest with Gini criteria. The model demonstrated a strong predictive ability (AUC = 0.83). In contrast, the ExtraTree model performed worse. Using the feature analysis result, critical predictive parameters such as preexisting systolic and diastolic blood pressure, effective blood flow, and ultrafiltration characteristics were identified. The natural language processing capabilities of ChatGPT-4 and Retrieval Augmented Generation (RAG) were used to synthesize meaningful clinical data.
ER  - 

TY  - CONF
TI  - Global Life Expectancy Prediction Using Machine Learning Ensemble Techniques
T2  - 2024 17th International Conference on Development in eSystem Engineering (DeSE)
SP  - 423
EP  - 427
AU  - D. U. Ozsahin
AU  - D. I. Emegano
AU  - L. R. David
AU  - A. J. Hussain
AU  - B. Uzun
AU  - I. Ozsahin
PY  - 2024
KW  - Radio frequency
KW  - Adaptation models
KW  - Machine learning algorithms
KW  - Biological system modeling
KW  - Predictive models
KW  - Boosting
KW  - Prediction algorithms
KW  - Planning
KW  - Resource management
KW  - Socioeconomics
KW  - ensemble
KW  - global life expectancy
KW  - health
KW  - ML use predictions
DO  - 10.1109/DeSE63988.2024.10912031
JO  - 2024 17th International Conference on Development in eSystem Engineering (DeSE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 17th International Conference on Development in eSystem Engineering (DeSE)
Y1  - 6-8 Nov. 2024
AB  - Global life expectancy (GLE) is the typical lifespan that an individual is expected to live on, determined statistically from data. In this study, it is predicted using a dataset of social, economic, health, and demographic variables from different nations to make robust predictive algorithms. This study focuses on using machine learning (ML) ensemble models to predict GLE. This involves using key factors from the dataset that influence GLE. In this study, we utilized ML ensemble models like Random Forest (RF), Light Gradient Boosting Machine (LGBM), Adaptive Boosting (AdaBoost), and eXtreme Gradient Boosting (XGB) for the prediction of GLE. The result showed that LGBM outperformed other models by scoring 1.1851, 1.8778, 0.9504, 0.9504, and 3.5259 as MAE, RMSE, R2, EVS and MSE respectively. The result can be used in healthcare policies, planning, budgeting, and allocations to enhance GLE. Also, we employed explainable artificial intelligence (XAI) for a universal interpretation of the model’s intricate performances. This study has the potential to contribute to the development of evidence-based policies and interventions to enhance global public health using precise GLE made through the application of ML ensemble techniques.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence (XAI) Approach to Heart Disease Prediction
T2  - 2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)
SP  - 1
EP  - 6
AU  - A. Sethi
AU  - S. Dharmavaram
AU  - S. K. Somasundaram
PY  - 2024
KW  - Accuracy
KW  - Explainable AI
KW  - Medical services
KW  - Organizations
KW  - Predictive models
KW  - User interfaces
KW  - Cardiovascular diseases
KW  - Explainable Artificial Intelligence (XAI)
KW  - Heart Disease
KW  - UCI Repository
KW  - Random Forest (RF)
KW  - LIME
KW  - SHAP
DO  - 10.1109/AIIoT58432.2024.10574635
JO  - 2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)
Y1  - 3-4 May 2024
AB  - Cardiovascular continue to stand as the leading cause of mortality globally. The World Health Organization (WHO) noted that in 2019, 32% of all fatalities were attributed to heart-related issues. In India, the Ministry of Health and Family Welfare reported that cardiovascular diseases accounted for 28.1% of the overall deaths. [1] Approximately 85% of individuals diagnosed with heart failure survive the first year after diagnosis, with rates decreasing to around 55% at two years, approximately 33% at five years, and about 35% at ten years. [2] Cardiovascular diseases, in 2019, claimed the lives of approximately 17.9 million people worldwide, making them the primary cause of death, constituting around 32% of all global deaths. [3] Underscoring the critical need for predictive models in early detection, our proposal integrates machine learning with Explainable AI (XAI) and a user interface (UI) tailored for medical professionals to address cardiovascular diseases (CVDs). We have developed a predictive model for heart disease with 96.07% accuracy, integrating factors like peak exercise ST segment slope, maximum heart rate, and exercise-induced angina. Our model prioritizes transparency, offering clear explanations for decisions to build trust in AI-driven healthcare. Emphasizing feature importance, it enables interpretable predictions, facilitating early detection and informed management. Our objective is two-fold: accurate risk identification and providing medical professionals with a user-friendly interface for transparent, reliable decision-making. This research advances healthcare by bridging AI with human understanding, enhancing outcomes in heart disease management.
ER  - 

TY  - CONF
TI  - Automated Fraud Detection in Financial Transactions using Machine Learning: An Ensemble Perspective
T2  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
SP  - 1
EP  - 6
AU  - Tamanna
AU  - S. Kamboj
AU  - L. Singh
AU  - T. Kaur
PY  - 2024
KW  - Machine learning algorithms
KW  - Reviews
KW  - Explainable AI
KW  - Finance
KW  - Medical services
KW  - Fraud
KW  - Stakeholders
KW  - Financial
KW  - Transaction
KW  - Leveraging
KW  - Detection
KW  - Frameworks
DO  - 10.1109/AIMLA59606.2024.10531422
JO  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Artificial Intelligence and Machine Learning Applications Theme: Healthcare and Internet of Things (AIMLA)
Y1  - 15-16 March 2024
AB  - Financial fraud is an increasing concern for both organizations and consumers as digital financial transactions become more prevalent. This presentation offers a comprehensive introduction to the computerized identification of fraudulent activity within financial transactions, examining the methods and technological advancements utilized to prevent such actions. The research explores the evolution of fraud detection systems, demonstrating how advanced automated frameworks leveraging machine learning (ML) and artificial intelligence (AI) algorithms including XGB and Random Forest have superseded more rudimentary human review processes. It illustrates the critical role data analytics performs in evaluating immense volumes of transactional data to identify potential markers of fraud. Additionally, it emphasizes the importance of collaboration between financial institutions, regulatory bodies, and information technology companies to develop robust and efficacious fraud defense systems. The study also focuses on the advantages automated fraud identification solutions hold over more conventional approaches, including improved accuracy, efficiency, and cost-effectiveness. The ongoing advancement of the automated fraud detection field is highlighted as vital, with technologies such as explainable AI simplifying the comprehension of decision-making workflows and promoting transparent communication between stakeholders.
ER  - 

TY  - JOUR
TI  - Advancing Ovarian Cancer Diagnosis Through Deep Learning and eXplainable AI: A Multiclassification Approach
T2  - IEEE Access
SP  - 116968
EP  - 116986
AU  - M. Radhakrishnan
AU  - N. Sampathila
AU  - H. Muralikrishna
AU  - K. S. Swathi
PY  - 2024
KW  - Ovarian cancer
KW  - Cancer
KW  - Accuracy
KW  - Computational modeling
KW  - Computer architecture
KW  - Task analysis
KW  - Explainable AI
KW  - Deep learning
KW  - Pathology
KW  - Deep learning
KW  - digital pathology
KW  - multiclassification
KW  - ovarian cancer
KW  - XAI
DO  - 10.1109/ACCESS.2024.3448219
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Ovarian cancer is a dangerous gynaecological malignancy, and the presence of many subtypes causes significant diagnostic difficulties. In general, the high accuracy of classification results in adequate prognosis and effectiveness of treatment. This work aims at the development of a Deep Learning (DL) approach for subtypes of ovarian cancer multiclassification, which tries to solve the problem of the creation of precise and reliable diagnostic methods. In the work, we have used and explored various DL models such as MobileNetV2, VGG19, ResNet18, ResNeXt, Xception, EfficientNet, and InceptionV3 to perform the classification task. Further, we used the state-of-the-art eXplainable Artificial Intelligence methods, including integrated gradient, saliency map, Grad-CAM, and DeepLift, to improve model interpretability. From our experiments, we inferred that the highest accuracy was achieved by InceptionV3, with a value of 97.96%. XAI techniques incorporated provide transparent insights into the model’s operations during the decision-making process, thus increasing the level of trust and clinical usability. The proposed DL approach, by leveraging InceptionV3 as its top performer, has convincingly demonstrated the potential of AI to revolutionize the diagnosis of ovarian cancer through a high level of accuracy in subtype classification. XAI techniques integrated allow transparency support for the model and further enable its clinical adoption. All of these developments have significant potential for improved patient outcomes within the scope of personalized medicine in ovarian cancer treatment.
ER  - 

TY  - CONF
TI  - Heart Disease Classification with XAI and Kernel SHAP
T2  - 2024 IEEE 3rd International Conference on Robotics, Automation, Artificial-Intelligence and Internet-of-Things (RAAICON)
SP  - 82
EP  - 85
AU  - N. Vasker
AU  - A. T. Nafisa
AU  - M. D. A. Rahman
AU  - M. Hasan
PY  - 2024
KW  - Heart
KW  - Logistic regression
KW  - Accuracy
KW  - Reviews
KW  - Decision making
KW  - Kernel
KW  - Usability
KW  - Robots
KW  - Diseases
KW  - Testing
KW  - XAI
KW  - Kernel SHAP
KW  - Logistic regression
KW  - Feature importance
KW  - Clinical decision-making
KW  - Heart disease diagnosis
KW  - Classification
DO  - 10.1109/RAAICON64172.2024.10928523
JO  - 2024 IEEE 3rd International Conference on Robotics, Automation, Artificial-Intelligence and Internet-of-Things (RAAICON)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 3rd International Conference on Robotics, Automation, Artificial-Intelligence and Internet-of-Things (RAAICON)
Y1  - 29-30 Nov. 2024
AB  - Heart disease is a major worldwide health concern, necessitating advances in precise and cost-effective diagnostics. While machine learning (ML) models have shown promise in this field, their opaque nature frequently impedes clinical use due to a lack of interpretability. This work investigates the effectiveness of Explainable AI (XAI) approaches, specifically Kernel SHAP, to increase the transparency and trustworthiness of machine learning models for heart disease categorization. We evaluated the interpretability and performance of a logistic regression model using a well-known heart disease dataset. Our logistic regression model has an accuracy of 85.2%, precision of 82.5%, recall of 84.7%, and F1-score of 83.6%. Kernel SHAP analysis gave useful insights, demonstrating that characteristics like ’ca’ (number of main arteries) and ’thal’ (thalassemia) have the high influence on predictions. These findings suggest that adding XAI approaches might significantly increase the confidence and acceptance of ML models in healthcare contexts, eventually leading to better patient outcomes.
ER  - 

TY  - CONF
TI  - An Explainable Ensemble Classifier for Infrared Breast Cancer Detection
T2  - 2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)
SP  - 1
EP  - 6
AU  - K. Raghavan
AU  - B. Sivaselvan
PY  - 2024
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Decision making
KW  - Learning (artificial intelligence)
KW  - Predictive models
KW  - Breast cancer
KW  - Explainable Artificial Intelligence
KW  - Explainability
KW  - Interpretable deep learning
KW  - Medical image analysis
KW  - Deep learning
DO  - 10.1109/AIIoT58432.2024.10574722
JO  - 2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)
Y1  - 3-4 May 2024
AB  - This study introduces an ensemble classifier that combines the benefits of three pre-trained neural networks: Xception, InceptionV3, and ResNet101. A squeeze-and-excite attention mechanism and Grad-CAM improve the classifier, enabling explainable AI. Using infrared images to detect breast cancer is complicated. Our approach addresses two key medical imaging issues: precise diagnosis and transparency in artificial intelligence decision-making. Using complementary network features and attention-integrated Grad-CAM to improve interpretability, our solution is comprehensive and practical. The ensemble classifier had 95.4% accuracy, demonstrating its ability to improve breast cancer detection. Explainable artificial intelligence (AI) techniques also give clinicians valuable insights into the model’s predictions, boosting trust and making AI tools more accessible to use in clinical settings. We used the Database For Mastology Research (DMR-IR) dataset to validate and implement our model in practice. This research signifies a notable advancement in the utilisation of AI in healthcare, providing a route towards diagnostic tools that are more precise, transparent, and user-friendly.
ER  - 

TY  - CHAP
TI  - Genome Data&#x2010;Based Explainable Recommender Systems
T2  - Genomics at the Nexus of AI, Computer Vision, and Machine Learning
SP  - 149
EP  - 168
AU  - V. Lakshmi Chetana
AU  - Hari Seetha
PY  - 2024
KW  - Genomics
KW  - Bioinformatics
KW  - Medical services
KW  - Explainable AI
KW  - Precision medicine
KW  - Diseases
KW  - Recommender systems
KW  - Computational modeling
KW  - Medical diagnostic imaging
KW  - Law
DO  - 10.1002/9781394268832.ch7
PB  - Wiley
SN  - 9781394268825
UR  - http://ieeexplore.ieee.org/document/10951833
AB  - Summary <p>Genomics and personalized medicine have revolutionized healthcare by allowing doctors to customize treatments to individual genetic profiles. Genomic recommender systems utilize advanced machine learning techniques to analyze genomic data and provide personalized recommendations for various applications in healthcare and genetics. These systems make recommendations and explain why specific treatments or medications are suggested, giving clarity to patients and building trust in the recommendation. Achieving transparency and interpretability in these systems is crucial to ensure that patients have confidence in their decisions based on their genetic data. This chapter comprehensively reviews existing research in this field, discusses its challenges, and highlights future directions for advancements in explainable genomic recommendation systems.</p>
ER  - 

TY  - CONF
TI  - Advanced Deep Learning Approaches for Personalized Medical Image Analysis: Revolutionizing Healthcare with Precision Imaging
T2  - 2024 International Conference on Recent Advances in Science and Engineering Technology (ICRASET)
SP  - 1
EP  - 5
AU  - S. Harne
AU  - M. Dhanamalar
AU  - D. Chanakya
AU  - K. Busa
AU  - P. Bramhe
AU  - R. Tiwari
PY  - 2024
KW  - Deep learning
KW  - Image analysis
KW  - Accuracy
KW  - Recurrent neural networks
KW  - Precision medicine
KW  - Medical services
KW  - Time measurement
KW  - Prognostics and health management
KW  - Medical diagnostic imaging
KW  - Diseases
KW  - Deep Learning
KW  - Convolutional Neural Networks
KW  - Recurrent Neural Networks
KW  - Medical Image Analysis
KW  - Model Interpretability
KW  - Federated Learning
DO  - 10.1109/ICRASET63057.2024.10895711
JO  - 2024 International Conference on Recent Advances in Science and Engineering Technology (ICRASET)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Recent Advances in Science and Engineering Technology (ICRASET)
Y1  - 21-22 Nov. 2024
AB  - Awareness of high precision deep learning in personalized medical image analysis can be regarded as a portend of a new healthcare paradigm shift by making a breakthrough in diagnostic accuracy, treatment effect, and the prognosis of diseases. This work is the description of the entire procedure that incorporates techniques that are perfectly for location trying to put into consideration the inherent challenges and opportunities in this profession. Applying enriched datasets with modern state-of-the-art deep learning architectures that include CNN’s (convolutionneural networking), RNN’s (recurrent neural networking), and hybrid models enables our approach to have higher performance across several tasks, among them is image segmentation, disease classification and lesion detection. A feasibility study in real clinical settings shows that these deep learning models improve the patient care pathway, ease the use for doctors, and integrate into the existing health care organizations very well. On top of that, performance evaluation indicators emphasize to what extent our models are scalable, resourced, and efficient which is in a turn ensures timely and accurate interpretation of medical imagery..The performance of our approach never stops with the improvement systems and the measure that make it better each time, and it would be perfect for future development of more enduring precision medicine. The use of basement advanced deep learning methodologies renders it possible to liberate the inner heights of the individualized medical image analysis, which in result translates into the whole world change of medical care delivery and the improvement of patient care worldwide.
ER  - 

TY  - CONF
TI  - An Explainable AI Data Pipeline for Multi-Level Survival Prediction of Breast Cancer Patients Using Electronic Medical Records and Social Determinants of Health Data
T2  - 2024 IEEE International Conference on Big Data (BigData)
SP  - 8661
EP  - 8664
AU  - S. Hashtarkhani
AU  - S. White-Means
AU  - S. Li
AU  - R. Rashid
AU  - F. Kumsa
AU  - C. Lemon
AU  - L. Chipman
AU  - J. Dapremont
AU  - B. White
AU  - A. Shaban-Nejad
PY  - 2024
KW  - Electric potential
KW  - Explainable AI
KW  - Hospitals
KW  - Pipelines
KW  - Urban areas
KW  - Data integration
KW  - Breast cancer
KW  - Socioeconomics
KW  - Electronic medical records
KW  - Tumors
KW  - electronic health record
KW  - explainable AI
KW  - breast cancer
KW  - data pipeline
DO  - 10.1109/BigData62323.2024.10825044
JO  - 2024 IEEE International Conference on Big Data (BigData)
IS  - 
SN  - 2573-2978
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Big Data (BigData)
Y1  - 15-18 Dec. 2024
AB  - This study introduces an innovative explainable AI (XAI) pipeline designed to predict breast cancer survival by integrating clinical, socioeconomic, and geographic data. Using data from 10,172 patients treated at hospitals in the Memphis, Tennessee metropolitan area, the pipeline identifies key survival determinants and reveals significant survival disparities affecting Black women. Advanced machine learning models combined with SHapley Additive exPlanations (SHAP) provide actionable and interpretable insights into the role of tumor stage, socioeconomic conditions, and access to preventive care. This framework facilitates personalized survival predictions and targeted equity-focused interventions, demonstrating the potential of multi-source data integration to address health inequities and improve patient outcomes.
ER  - 

TY  - CONF
TI  - The Evolution of Personalized Medicine in the Age of Big Data and Advanced Analytics
T2  - 2024 IEEE 2nd International Conference on Innovations in High Speed Communication and Signal Processing (IHCSP)
SP  - 1
EP  - 6
AU  - D. Dhabliya
AU  - K. R. Singh
AU  - A. Verma
AU  - R. Ahmad
AU  - S. C
AU  - A. Pattanaik
PY  - 2024
KW  - Ethics
KW  - Heuristic algorithms
KW  - Precision medicine
KW  - Signal processing algorithms
KW  - Big Data
KW  - Predictive models
KW  - Genetics
KW  - Prediction algorithms
KW  - Real-time systems
KW  - Optimization
KW  - Adaptive
KW  - Advanced Analytics
KW  - Big Data
KW  - Dynamic
KW  - Ethical Considerations
KW  - Framework
KW  - Genetic Variant Prioritization
KW  - Healthcare
KW  - Medicine
KW  - Optimized Treatment
KW  - Personalized
KW  - Precision
KW  - Real-time
KW  - Responsive
KW  - Transformative
DO  - 10.1109/IHCSP63227.2024.10959907
JO  - 2024 IEEE 2nd International Conference on Innovations in High Speed Communication and Signal Processing (IHCSP)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 2nd International Conference on Innovations in High Speed Communication and Signal Processing (IHCSP)
Y1  - 6-8 Dec. 2024
AB  - This study explores customized medicine using the customized Treatment Optimization Framework (PTOF). Advanced analytics and big data transform customized healthcare. The Genetic Variant Prioritization Algorithm (GVPA), Dynamic Treatment Response Prediction Model (DTRP), and Optimal Treatment Adjustment Algorithm (OTAA) make personalized medicine more dynamic and adaptable. PTOF is compared against GenoOpti, PharmaSys, BioDecide, RxPro, MedAlign, and OmniCare in our study. The results reveal that PTOF excels at data combination, prediction, and ethics. PTOF's capacity to adjust treatment regimens based on real-time projections raises the bar for specialist medicine and enhances patient care.
ER  - 

TY  - CONF
TI  - Behind the Scenes: An Explainable Artificial Intelligence (XAI) on the Service Classification of the 5G/B5G Network
T2  - 2024 3rd International Conference on Digital Transformation and Applications (ICDXA)
SP  - 167
EP  - 172
AU  - N. Allias
AU  - D. A. Kadir
AU  - A. M. Abdullahi
AU  - S. Ismail
PY  - 2024
KW  - Explainable AI
KW  - 5G mobile communication
KW  - Biological system modeling
KW  - Key performance indicator
KW  - Closed box
KW  - Quality of service
KW  - Handover
KW  - Explainable Artificial Intelligence (XAI)
KW  - Service Classification
KW  - 5G/B5G
KW  - Random Forest
DO  - 10.1109/ICDXA61007.2024.10470665
JO  - 2024 3rd International Conference on Digital Transformation and Applications (ICDXA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Digital Transformation and Applications (ICDXA)
Y1  - 29-30 Jan. 2024
AB  - Fifth generation and beyond (5G/B5G) is part of the next generation of mobile technology, which is expected to offer more capacity and faster speeds than the previous generation Long-Term Evolution (LTE) network. These features enable 5G/B5G to offer users a wide range of services, including smart cities, entertainment and multimedia, healthcare and mission-critical applications that benefit the entire economy and communities. However, due to growing demand, telecom service providers need to provide better Quality of service (QoS) on their networks to meet user expectations. Therefore, it is necessary to implement service classification that enables service providers to select the right network slices for each service to improve network QoS. Previous studies have predicted the classification of 5G/B5G services using traditional machine learning algorithms such as random forest and decision tree, achieving good accuracy. However, prediction based on black-box machine learning models was not transparent enough, so mobile service providers could not understand the reliability and interpretation of the results. Therefore, this article aims to model the system performance of 5G Key Performance Indicators (KPI) and extend the experiment to Explainable Artificial Intelligence (XAI) by using the Explainable AI to look behind the scenes of the chosen black-box models for service classification of 5G/B5G networks, as this is crucial for telecom service providers to make critical investment decisions around customer and employee experience. The results from the experiments show that the random forest has the highest confidence towards class Label 3 with the probability of 35 percent.
ER  - 

TY  - CONF
TI  - Healthcare Insurance Cost Classification Using Machine Learning's Linear Regression Approach
T2  - 2024 IEEE 9th International Conference for Convergence in Technology (I2CT)
SP  - 1
EP  - 6
AU  - K. S. Gill
AU  - R. Gupta
AU  - M. Kumar
AU  - R. Rawat
AU  - Y. Chanti
PY  - 2024
KW  - Training
KW  - Costs
KW  - Correlation
KW  - Linear regression
KW  - Insurance
KW  - Medical services
KW  - Machine learning
KW  - Artificial Intelligence
KW  - Deep Learning
KW  - Healthcare Insurance Cost Classification Analysis
KW  - Model Training
KW  - Linear Regression
DO  - 10.1109/I2CT61223.2024.10544081
JO  - 2024 IEEE 9th International Conference for Convergence in Technology (I2CT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 9th International Conference for Convergence in Technology (I2CT)
Y1  - 5-7 April 2024
AB  - Advanced analytical approaches are required for effective prediction and categorization of healthcare expenses because of the increasing complexity of these costs and the dynamic nature of the costs themselves. Within the scope of this research project, the classification of healthcare insurance costs is investigated via the use of machine learning, more especially linear regression. In order to identify correlations between these characteristics and to forecast insurance costs, the linear regression model makes use of a dataset that contains a wide variety of demographic, medical, and financial information. The ultimate goal of this study is to provide assistance to policymakers, insurers, and healthcare providers in optimising resource allocation and increasing cost-effectiveness within the healthcare sector. The increasing intricacies and ever-changing nature of healthcare expenses need sophisticated analytical techniques to provide precise forecasting and categorization. This work investigates the use of machine learning, particularly linear regression, for categorising healthcare insurance expenses. The linear regression model utilises a dataset that includes a wide range of demographic, medical, and financial characteristics. Its objective is to identify connections between these elements and make predictions about insurance costs. The study is centred on feature selection, model training, and performance assessment in order to improve the accuracy and interpretability of the predictive model. This research aims to use linear regression to give valuable insights into the determinants of healthcare insurance costs. The findings will be beneficial for policymakers, insurers, and healthcare providers as they can use this information to allocate resources more efficiently and enhance cost-effectiveness in the healthcare industry.
ER  - 

TY  - CONF
TI  - Cutting-Edge Image Recognition Leveraging Deep Learning and Machine Learning for Enhanced Accuracy
T2  - 2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)
SP  - 1
EP  - 6
AU  - A. Shrivastava
AU  - V. Kumar
AU  - J. P. Maurya
PY  - 2024
KW  - Deep learning
KW  - Training
KW  - Adaptation models
KW  - Visualization
KW  - Image recognition
KW  - Accuracy
KW  - Computational modeling
KW  - Visual systems
KW  - Data models
KW  - Reliability
KW  - Education Deep Neural Networks
KW  - Image Recognition
KW  - Computer Vision
KW  - Convolutional Neural Networks (CNNs)
KW  - Recurrent Neural Networks (RNNs)
DO  - 10.1109/ICAIQSA64000.2024.10882386
JO  - 2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA)
Y1  - 20-21 Dec. 2024
AB  - This paper investigates advanced techniques in image recognition and classification by integrating deep learning and machine learning approaches to achieve higher accuracy. Through the implementation of sophisticated training algorithms, the study demonstrates enhanced performance in recognizing and categorizing images across various data models. A major turning point in the development of image identification technology came in 2012 when deep neural networks were introduced. These networks surpassed earlier cutting-edge algorithms and completely changed the computer vision industry. This progress has brought us closer to achieving human-level accuracy in tasks such as identity verification. The role of large datasets like ImageNet is crucial, as they provide the foundation for the success of deep learning. With continuous research pushing the limits of picture identification and producing major advances in human knowledge, deep learning has a huge influence on business, society, and technology. Additional research in this area might lead to creative uses that revolutionize our relationship with our surroundings. Key topics discussed include data pre-processing, post-processing, model optimization, and accuracy enhancement. The findings highlight the potential of cutting-edge technologies to advance image classification and recognition in various sectors, such as medical imaging and visual analysis. The approach emphasizes scalability and adaptability, ensuring that models can be effectively applied to real-world scenarios. Future research will focus on refining these models to handle even more complex image datasets, further enhancing their practical utility and reliability.
ER  - 

TY  - CONF
TI  - Responsible AI Adoption in Healthcare: Opportunities and Challenges
T2  - 2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)
SP  - 1198
EP  - 1205
AU  - S. Chakraborty
AU  - T. Sarangi
AU  - S. Ghosh
AU  - B. Misra
AU  - N. Dey
PY  - 2024
KW  - Industries
KW  - Ethics
KW  - Privacy
KW  - Costs
KW  - Communication systems
KW  - Medical services
KW  - Organizations
KW  - Stakeholders
KW  - Artificial intelligence
KW  - Guidelines
KW  - responsible AI
KW  - smart healthcare
KW  - NLP
KW  - expert systems
KW  - machine learning
DO  - 10.1109/ICACCS60874.2024.10717182
JO  - 2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)
IS  - 
SN  - 2575-7288
VO  - 1
VL  - 1
JA  - 2024 10th International Conference on Advanced Computing and Communication Systems (ICACCS)
Y1  - 14-15 March 2024
AB  - An ethically and legally sound approach to creating and using artificial intelligence (AI) is known as responsible intelligence (AI). Responsible AI in healthcare can improve the field and enhance patient care, diagnosis, and treatment. According to these estimates, the adoption of AI in healthcare might significantly lower costs. However, ensuring ethical AI implementation presents difficulties that need considerable thought. To avoid harm and preserve trust, ethical concerns, including those related to privacy and bias, must be addressed. For accountability and transparency to be guaranteed, regulatory frameworks must change to reflect the changing reality of AI. For the purpose of designing and implementing responsible AI practices, stakeholder collaboration is essential. The World Health Organization has offered guidelines for the governance and ethics of AI in the healthcare industry. A primary area of research is how AI-related technologies and activities can be implemented responsibly. Dashboards and scorecards for responsible AI can offer information and accountability. The current paper presents an overview of responsible AI and its different techniques, challenges, opportunities, and applications in health care.
ER  - 

TY  - CONF
TI  - Decoding AI Complexity: SHAP Textual Explanations via LLM for Improved Model Transparency
T2  - 2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)
SP  - 197
EP  - 198
AU  - C. -C. Hsu
AU  - I. -Z. Wu
AU  - S. -M. Liu
PY  - 2024
KW  - Additives
KW  - Explainable AI
KW  - Large language models
KW  - Decision making
KW  - Buildings
KW  - Medical services
KW  - Decoding
KW  - Explainable AI
KW  - Generative AI
KW  - SHAP
KW  - LLM
DO  - 10.1109/ICCE-Taiwan62264.2024.10674465
JO  - 2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)
IS  - 
SN  - 2575-8284
VO  - 
VL  - 
JA  - 2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)
Y1  - 9-11 July 2024
AB  - With the continuous advancement of artificial intelligence (AI), particularly in widespread domains such as healthcare and environmental applications, there is an increasing demand for model interpretability. Understanding the decision-making process of models contributes to building trust in them. Hence, the development of Explainable AI (XAI) has become crucial. This study proposes an approach to generate text via a large language model (LLM) for interpretation to enhance the interpretability of SHAP (Shapley Additive exPlanations) plots. The goal is to make the interpretability of model decisions accessible even to non-IT experts through textual explanations.
ER  - 

TY  - CONF
TI  - An Explainable AI Driven Machine Learning Approach for Maternal Health Risk Analysis
T2  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
SP  - 1135
EP  - 1140
AU  - M. S. Hossen
AU  - P. Shaha
AU  - M. Saiduzzaman
AU  - M. Shovon
AU  - A. K. Akhi
AU  - M. S. Iqbal
PY  - 2024
KW  - Pregnancy
KW  - Accuracy
KW  - Explainable AI
KW  - Machine learning
KW  - Medical services
KW  - Risk analysis
KW  - Reliability
KW  - Information technology
KW  - Forecasting
KW  - Monitoring
KW  - Machine Learning
KW  - Maternal Health Risk
KW  - Explainable AI
KW  - SMOTE
KW  - Feature Selection
DO  - 10.1109/ICCIT64611.2024.11022383
JO  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 2474-9656
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
Y1  - 20-22 Dec. 2024
AB  - Maternal health during pregnancy is a severe issue, particularly in the rural areas of developing countries like Bangladesh, where a lack of access to healthcare and inadequate infrastructure increase risks. Maternal healthcare has a great deal of difficulty due to the absence of reliable tools for forecasting health concerns. Negative results frequently result from the traditional method's inability to diagnose and manage pregnancy- related problems correctly. While there are several ways to monitor maternal health conditions, machine learning has the potential to increase diagnosis accuracy, efficiency, and speed. In this research, by using several machine learning classifiers, we built a model that can analyze the maternal health risk during pregnancy. The Maternal Health Risk dataset from the UCI machine learning repository was used in this study. SMOTE was utilized to address the class imbalance data and generated an additional 99000 data. We assess the model before and after using SMOTE. Accuracy, precision, recall, and F1-Score were utilized to evaluate the model's performance. Extreme Gradient Boosting (XGBoost) is our standout performer, with an accuracy of 84% and 95% before and after using SMOTE, respectively. Additionally, Explainable AI was used to increase the model's readability. This study demonstrates the power of machine learning, which could revolutionize maternal health care by identifying maternal health risks early.
ER  - 

TY  - CONF
TI  - Explainable AI: A Way to Achieve Trustworthy AI
T2  - 2024 IEEE 10th Conference on Big Data Security on Cloud (BigDataSecurity)
SP  - 150
EP  - 155
AU  - Y. Li
AU  - Y. Xiao
AU  - Y. Gong
AU  - R. Zhang
AU  - Y. Huo
AU  - Y. Wu
PY  - 2024
KW  - Surveys
KW  - Explainable AI
KW  - Decision making
KW  - Finance
KW  - Closed box
KW  - Medical services
KW  - Big Data
KW  - Artificial Intelligence
KW  - Explainability
KW  - Machine Learning
KW  - Transparency
KW  - Intelligibility
DO  - 10.1109/BigDataSecurity62737.2024.00034
JO  - 2024 IEEE 10th Conference on Big Data Security on Cloud (BigDataSecurity)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 10th Conference on Big Data Security on Cloud (BigDataSecurity)
Y1  - 10-12 May 2024
AB  - AI is black-box and non-explainable, in other words, due to the complexity of the decision-making process of AI, people are unable to know why and how AI makes the decision. For these reasons, people will question and worry about AI’s decision-making. Against this background, researchers have proposed the concept of Explainable Artificial Intelligence (XAI). The aim of this paper is to help readers be able to better understand XAI through extensive surveys. In this paper, we discuss the concept of XAI, the motivation of the research, the current status of the research, and the difficulties in the research process. At the same time, we also discuss the specific methods to realize AI explainability by dividing AI explainability methods into two ways: transparent design and ex post facto interpretation. In addition, we select the three fields of healthcare, finance, and autonomous driving, discuss the application of XAI in these three fields, and select some representative results as examples through surveys. Finally, we summarize this article and present our outlook on the future development of XAI.
ER  - 

TY  - CONF
TI  - Machine Learning Algorithm for Maternal Health Risk Classification with SMOTE and Explainable AI
T2  - 2024 IEEE 9th International Conference for Convergence in Technology (I2CT)
SP  - 1
EP  - 6
AU  - B. U. Maheswari
AU  - A. Dixit
AU  - A. K. Karn
PY  - 2024
KW  - Pregnancy
KW  - Machine learning algorithms
KW  - Additives
KW  - Explainable AI
KW  - Prediction algorithms
KW  - Blood pressure
KW  - Obstetrics
KW  - Obstetric healthcare
KW  - Machine learning
KW  - SMOTE
KW  - Explainable AI
DO  - 10.1109/I2CT61223.2024.10543709
JO  - 2024 IEEE 9th International Conference for Convergence in Technology (I2CT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE 9th International Conference for Convergence in Technology (I2CT)
Y1  - 5-7 April 2024
AB  - Targeted obstetric care refers to being specific with the caretaking of a pregnant woman based on her symptoms and risk levels. It is a crucial task because pregnancy-related complications can be life-threatening. Pregnant women's body undergo a variety of changes, which may be mental, emotional, or physical. It is possible to analyze these changes to assess the health complications. These include blood pressure and blood sugar readings, which can be examined to determine a pregnant woman's health risk level. Machine Learning (ML) algorithms are employed to determine maternity health risks based on a woman’s health statistics. In this work, several ML algorithms, have been applied to the maternal health risk dataset to predict the mother’s risk level. Moreover, parameterized tuning has been applied to improve the performances of these conventional algorithms. The performance of these algorithms is studied before and after applying the Synthetic Minority Over-sampling Technique (SMOTE) technique to balance the data. The results exhibit that after SMOTE enhancement, the XGBoost upon hyperparameter tuning gives an accuracy of 88.89% in predicting mothers’ risk level. Hence, XGBoost can be deployed in the field of providing targeted obstetric care by accurately predicting health risk levels. This work also embeds the Explainable AI (XAI) using SHapley Additive exPlanations (SHAP) and LIME (Local Interpretable Model-agnostic Explanations) to provide trust in the ML model to gynecologists. Various plots are generated using LIME, SHAP, and ELI5 to showcase the ML model’s interpretability on global and local levels.
ER  - 

TY  - CONF
TI  - Survey of Ankylosing Spondylitis for Biomedical Imaging with Deep Neural Networks
T2  - 2024 International Conference on Signal Processing, Computation, Electronics, Power and Telecommunication (IConSCEPT)
SP  - 1
EP  - 7
AU  - K. S. Prasad
AU  - S. Jana
PY  - 2024
KW  - Deep learning
KW  - Ethics
KW  - Accuracy
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Signal processing algorithms
KW  - Feature extraction
KW  - Ankylosing Spondylitis (AS)
KW  - Predictive models
KW  - Diagnostic imaging
KW  - Black box
KW  - Features
KW  - Supervised learning
KW  - neural networks
DO  - 10.1109/IConSCEPT61884.2024.10627872
JO  - 2024 International Conference on Signal Processing, Computation, Electronics, Power and Telecommunication (IConSCEPT)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Signal Processing, Computation, Electronics, Power and Telecommunication (IConSCEPT)
Y1  - 4-5 July 2024
AB  - Ankylosing Spondylitis (AS) is a chronic inflammat ory disease that affects the axial spine and peripheral joints, manifesting through symptoms such as spinal stiffness, back pain, and joint involvement. Medical imaging, particularly X-rays and MRIs, is essential for diagnosing AS and monitoring its progression. Recent advancements in artificial intelligence (AI), especially deep learning techniques, have significantly enhanced AS diagnosis by enabling the automated analysis of medical images to identify subtle patterns and markers indicative of the disease. This paper introduces an innovative framework that integrates deep learning with Explainable AI (XAI) techniques to improve the diagnostic process for AS. Our deep learning model, trained on an extensive dataset of MRI scans, demonstrates high accuracy and robustness in detecting AS. Nevertheless, the inherent complexity of deep learning models, often described as “black box” models, raises concerns about their interpretability and transparency in clinical decision-making. To address these challenges, we incorporate XAI methods such as Grad-CAM and LIME, which provide visual and feature-based explanations of the model's predictions. These techniques enhance the interpretability of the model, allowing clinicians to understand and trust AI-driven decisions. The novelty of our approach lies in the seamless integration of XAI methods with deep learning for AS diagnosis, which not only improves diagnostic accuracy but also ensures compliance with ethical and legal standards in healthcare. Continued research and collaboration between AI experts and healthcare professionals are crucial to fully realizing the potential of deep learning and XAI in AS diagnosis and to ultimately enhance patient care.
ER  - 

TY  - CONF
TI  - Explainable Artificial Intelligence Models for Birth Weight Prediction Based on Maternal Parameters in Ethiopia
T2  - 2024 IEEE 5th International Conference on Electro-Computing Technologies for Humanity (NIGERCON)
SP  - 1
EP  - 5
AU  - G. M. Setegn
AU  - A. Agegne Engda
AU  - W. K. Worku
AU  - B. Endalamaw Dejene
AU  - A. O. Salau
AU  - N. Wereta Asnake
PY  - 2024
KW  - Surveys
KW  - Pediatrics
KW  - Obesity
KW  - Accuracy
KW  - Explainable AI
KW  - Predictive models
KW  - Boosting
KW  - Prediction algorithms
KW  - Random forests
KW  - Standards
KW  - Birth Weight
KW  - SHAP
KW  - LIME
KW  - Eli5
KW  - Machine Learning
KW  - Explainable Artificial Intelligence
DO  - 10.1109/NIGERCON62786.2024.10927388
JO  - 2024 IEEE 5th International Conference on Electro-Computing Technologies for Humanity (NIGERCON)
IS  - 
SN  - 2377-2697
VO  - 
VL  - 
JA  - 2024 IEEE 5th International Conference on Electro-Computing Technologies for Humanity (NIGERCON)
Y1  - 26-28 Nov. 2024
AB  - Abnormal birth can have negative impacts on both mothers and infants, leading to complex deliveries and potential long-term health issues such as obesity during childhood and adolescence. This study aimed to develop explainable artificial intelligence models for the prediction of birth weight based on maternal parameters in Ethiopia. The data set was collected from the Ethiopian Demographic Health Survey (EDHS). Homogenous ensemble machine learning algorithms with class decomposition (one versus one and one versus the rest) and without class decomposition were employed after step-forward feature selection techniques. The model was further explained using the LIME, Eli5, and SHAP XAI tools to achieve model-specific interpretability. The performance of the models was evaluated and compared using the standard metrics of accuracy, precision, recall, and F1 score. The overall accuracy of the random forest, bagging, cat boosting, and extra tree with one-versus-rest methods was 95.0%, 95.3%, 95.8%, and 97.4%, respectively. The experimental results showed that the extra tree with one versus the rest outperformed with 97.4% accuracy compared to others, and the researchers decided to use the extra tree further using the XAI tool. The presented XAI techniques provide useful information to understand abnormal birth weight risk and predict results for family members and healthcare providers. Based on the XAI results, it was clear that the greatest risk factors for birth weight prediction were age, sex of the child, birth order, number of households, residency, education level, and religion.
ER  - 

TY  - CONF
TI  - Predictive Analytics for Cardiovascular Health: A Machine Learning Approach
T2  - 2024 International Conference on Advances in Modern Age Technologies for Health and Engineering Science (AMATHE)
SP  - 1
EP  - 6
AU  - A. Khanna
AU  - P. Singh
AU  - Inzimam-Ul-Hassan
AU  - I. Kaur
PY  - 2024
KW  - Heart
KW  - Training
KW  - Machine learning
KW  - Medical services
KW  - Predictive models
KW  - Prediction algorithms
KW  - Data models
KW  - Heart disease prediction
KW  - Machine learning
KW  - XGBoost
KW  - Healthcare analytics
KW  - Predictive modeling
DO  - 10.1109/AMATHE61652.2024.10582101
JO  - 2024 International Conference on Advances in Modern Age Technologies for Health and Engineering Science (AMATHE)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Advances in Modern Age Technologies for Health and Engineering Science (AMATHE)
Y1  - 16-17 May 2024
AB  - Heart disease is still a major worldwide health concern, and early diagnosis and intervention depend on precise and fast prognostic techniques. The goal of this project is to use machine learning methods to meet this urgent demand. The basis for our analysis in this study is a heterogeneous dataset that includes patient demographics, medical history, and results of diagnostic tests. We use a carefully thought-out approach that includes preprocessing the data, choosing four different machine learning models training, evaluating, and deploying the models. This research embodies the intersection of data science and healthcare, offering a promising path toward more effective heart disease management and improved patient outcomes.
ER  - 

TY  - JOUR
TI  - A Secure and Interpretable AI for Smart Healthcare System: A Case Study on Epilepsy Diagnosis Using EEG Signals
T2  - IEEE Journal of Biomedical and Health Informatics
SP  - 3236
EP  - 3247
AU  - I. Ahmad
AU  - M. Zhu
AU  - G. Li
AU  - D. Javeed
AU  - P. Kumar
AU  - S. Chen
PY  - 2024
KW  - Electroencephalography
KW  - Feature extraction
KW  - Discrete wavelet transforms
KW  - Biological system modeling
KW  - Data security
KW  - Epilepsy
KW  - Explainable AI
KW  - Smart healthcare
KW  - Decision making
KW  - Biomedical signal
KW  - data security
KW  - epileptic seizures
KW  - explainable artificial intelligence
KW  - smart healthcare
DO  - 10.1109/JBHI.2024.3366341
JO  - IEEE Journal of Biomedical and Health Informatics
IS  - 6
SN  - 2168-2208
VO  - 28
VL  - 28
JA  - IEEE Journal of Biomedical and Health Informatics
Y1  - June 2024
AB  - The efficient patient-independent and interpretable framework for electroencephalogram (EEG) epileptic seizure detection (ESD) has informative challenges due to the complex pattern of EEG nature. Automated detection of ES is crucial, while Explainable Artificial Intelligence (XAI) is urgently needed to justify the model detection of epileptic seizures in clinical applications. Therefore, this study implements an XAI-based computer-aided ES detection system (XAI-CAESDs), comprising three major modules, including of feature engineering module, a seizure detection module, and an explainable decision-making process module in a smart healthcare system. To ensure the privacy and security of biomedical EEG data, the blockchain is employed. Initially, the Butterworth filter eliminates various artifacts, and the Dual-Tree Complex Wavelet Transform (DTCWT) decomposes EEG signals, extracting real and imaginary eigenvalue features using frequency domain (FD), time domain (TD) linear feature, and Fractal Dimension (FD) of non-linear features. The best features are selected by using Correlation Coefficients (CC) and Distance Correlation (DC). The selected features are fed into the Stacking Ensemble Classifiers (SEC) for EEG ES detection. Further, the Shapley Additive Explanations (SHAP) method of XAI is implemented to facilitate the interpretation of predictions made by the proposed approach, enabling medical experts to make accurate and understandable decisions. The proposed Stacking Ensemble Classifiers (SEC) in XAI-CAESDs have demonstrated 2% best average accuracy, recall, specificity, and F1-score using the University of California, Irvine, Bonn University, and Boston Children's Hospital-MIT EEG data sets. The proposed framework enhances decision-making and the diagnosis process using biomedical EEG signals and ensures data security in smart healthcare systems.
ER  - 

TY  - CONF
TI  - GRACE: Gradient-based XAI Scheme for Channel Estimation in Wireless Communications
T2  - 2024 IEEE International Mediterranean Conference on Communications and Networking (MeditCom)
SP  - 572
EP  - 577
AU  - A. K. Gizzini
AU  - Y. Medjahdi
AU  - M. B. Mabrouk
PY  - 2024
KW  - Backpropagation
KW  - 6G mobile communication
KW  - Explainable AI
KW  - Filtering
KW  - Bit error rate
KW  - Closed box
KW  - Channel estimation
KW  - 6G
KW  - AI
KW  - XAI
KW  - channel estimation
KW  - input filtering
KW  - perturbation-based
KW  - gradient-based
DO  - 10.1109/MeditCom61057.2024.10621232
JO  - 2024 IEEE International Mediterranean Conference on Communications and Networking (MeditCom)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE International Mediterranean Conference on Communications and Networking (MeditCom)
Y1  - 8-11 July 2024
AB  - The support of artificial intelligence (AI) is a key element in future 6G networks, where the concept of native AI will be introduced. Moreover, AI is widely employed in different critical applications such as autonomous driving and medical diagnosis. In such applications, using AI as black-box models is risky and challenging since the logic behind the decision-making methodology of the AI model is unclear. Consequently, it is crucial to understand and trust the decisions taken by these models to ensure their safe and efficient deployment. Tackling this issue can be achieved by developing explainable AI (XAI) schemes that aim to explain the logic behind the black-box model behavior. One solution to achieve this is to identify the relevant model inputs that are contributing the most towards its decisions. In this context, this paper proposes a gradient-based XAI scheme for channel estimation denoted as GRACE that aims to classify the model inputs using gradient backpropagation. Unlike perturbation-based input filtering, the proposed GRACE filtering strategy is based on the internal model architecture. Hence it provides a more efficient and trustworthy input classification. Performance evaluation in terms of bit error rate (BER) shows that identifying the relevant model inputs by the proposed GRACE leads to a better BER performance in comparison to the recently proposed perturbation-based scheme.
ER  - 

TY  - CONF
TI  - Building Trust in AI-Driven Decision Making for Cyber-Physical Systems (CPS): A Comprehensive Review
T2  - 2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)
SP  - 1
EP  - 8
AU  - R. U. Mhapsekar
AU  - M. I. Umrani
AU  - M. Faizan
AU  - O. Ali
AU  - L. Abraham
PY  - 2024
KW  - Explainable AI
KW  - Reviews
KW  - Decision making
KW  - Buildings
KW  - Medical services
KW  - Cyber-physical systems
KW  - Soil
KW  - Agriculture
KW  - Security
KW  - Reliability
KW  - Artificial Intelligence
KW  - Cyber-Physical Systems
KW  - XAI
KW  - trustworthy AI
DO  - 10.1109/ETFA61755.2024.10710855
JO  - 2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)
IS  - 
SN  - 1946-0759
VO  - 
VL  - 
JA  - 2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)
Y1  - 10-13 Sept. 2024
AB  - Recent advancements in technology have led to the emergence of Cyber-Physical Systems (CPS), which seamlessly integrate the cyber and physical domains in various sectors such as agriculture, autonomous systems, and healthcare. This integration presents opportunities for enhanced efficiency and automation through the utilization of Artificial Intelligence (AI) and Machine Learning (ML). However, the complexity of CPS brings forth challenges related to transparency, bias, and trust in AI-enabled decision-making processes. This research explores the significance of AI and ML in enabling CPS in these domains and addresses the challenges associated with interpreting and trusting AI systems within CPS. Specifically, the role of Explainable AI (XAI) in enhancing trustworthiness and reliability in AI-enabled decision-making processes is discussed. Key challenges such as transparency, security, and privacy are identified, along with the necessity of building trust through transparency, accountability, and ethical considerations.
ER  - 

TY  - CONF
TI  - Gingival Biotype Assessment in Intraoral Photographs Using Deep Learning
T2  - 2024 3rd International Conference on Health Big Data and Intelligent Healthcare (ICHIH)
SP  - 147
EP  - 152
AU  - L. Liu
AU  - S. Quan
AU  - E. Tian
AU  - J. Li
PY  - 2024
KW  - Deep learning
KW  - Training
KW  - Visualization
KW  - Accuracy
KW  - Biological system modeling
KW  - Teeth
KW  - Sensitivity and specificity
KW  - Clinical diagnosis
KW  - Artificial intelligence
KW  - Probes
KW  - gingival biotype
KW  - artificial intelligence
KW  - deep learning
DO  - 10.1109/ICHIH63459.2024.11064875
JO  - 2024 3rd International Conference on Health Big Data and Intelligent Healthcare (ICHIH)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 3rd International Conference on Health Big Data and Intelligent Healthcare (ICHIH)
Y1  - 13-15 Dec. 2024
AB  - This study aimed to construct a novel artificial intelligence (AI) model to assessment gingival biotype and investigate the accuracy of it based on intraoral photographs. To our knowledge, this is the first attempt to use deep learning algorithms to determine gingival biotype. A total of 1600 subjects fulfilling selection criteria were included. The gingival biotype of the maxillary anterior teeth was determined by the probe transparency method and novel artificial intelligent models. Accuracy, sensitivity, specificity, and F1 score were measured for gingival biotype detection and classification. This research demonstrates that deep learning models can determine gingival biotype in oral photographs with high accuracy, sensitivity and specificity. The successful application of deep learning in the field will undoubtedly assist the clinical diagnosis and treatment planning of dentists.
ER  - 

TY  - CONF
TI  - A Survey of Cardiovascular Disease Identification and Prediction Using Machine Learning and Deep Learning Methods
T2  - 2024 International Conference on Emerging Research in Computational Science (ICERCS)
SP  - 1
EP  - 5
AU  - R. S. Ravi
AU  - K. Vanitha
AU  - A. T.K.
PY  - 2024
KW  - Surveys
KW  - Support vector machines
KW  - Deep learning
KW  - Collaboration
KW  - Medical services
KW  - Market research
KW  - Data models
KW  - Convolutional neural networks
KW  - Cardiovascular diseases
KW  - Monitoring
KW  - Cardiovascular diseases
KW  - ML
KW  - DL
KW  - predictive modelling
KW  - healthcare technology
KW  - diagnostic tools
KW  - performance metrics
KW  - clinical validation
KW  - data privacy
KW  - wearable technology
KW  - big data
KW  - health monitoring systems
KW  - model interpretability
KW  - research trends
DO  - 10.1109/ICERCS63125.2024.10895641
JO  - 2024 International Conference on Emerging Research in Computational Science (ICERCS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Emerging Research in Computational Science (ICERCS)
Y1  - 12-14 Dec. 2024
AB  - The majority of mortality and morbidity in a variety of populations are caused by cardiovascular disease (CVD), resulting in a significant global health concern. The identification of disease through CT, MRI and radiology analysing through image screening to identify the exact region of blocks, strokes, plages or irregularity of vital signs, there is an increasing urgency to establish more effective identification and prediction methods that can facilitate timely interventions and improve patient outcomes. The application of modern technologies such as machine learning (ML) and deep learning (DL) has garnered considerable attention in the healthcare industry as a solution to the limitations of conventional diagnostic techniques. These data-driven techniques offer novel solutions for analysing complex medical data, enabling healthcare professionals to enhance diagnostic accuracy and predictive capabilities for cardiovascular conditions. This study aims to identify CVD by providing a comprehensive analysis of various ML and DL methods. Based on the datasets and performance indicators, techniques such as Support Vector Machines (SVM), Artificial Neural Networks (ANN), Random Forests (RF), and Convolutional Neural Networks (CNN) are evaluated for training and validation. Furthermore, we investigate the emerging trends within this rapidly evolving field, including the role of big data, wearable technology, and real-time health monitoring systems. Significant progress has been produced, presenting challenges such as data privacy concerns, sample interpretation, and the necessity for robust clinical validation. As such, this paper also outlines future directions for research aimed at overcoming these obstacles while improving the integration of ML and DL into clinical practice. This survey contributes to a more profound experience of how ML and DL can revolutionize CVD’s early detection and management by delineating the current landscape and potential avenues for further exploration.
ER  - 

TY  - CONF
TI  - Explainable AI-Driven Improved Disease Prediction through Symptom Analysis with Custom BERT
T2  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
SP  - 2327
EP  - 2332
AU  - M. M. Islam
AU  - M. Tanbeer Jubaer
AU  - A. Y. Srizon
AU  - M. Ali Hossain
AU  - M. F. Faruk
AU  - S. M. Mahedy Hasan
AU  - A. F. M. Minhazur Rahman
AU  - N. Tasnim Esha
PY  - 2024
KW  - Training
KW  - Analytical models
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Computer architecture
KW  - Predictive models
KW  - Data models
KW  - Diseases
KW  - Overfitting
KW  - Disease Prediction
KW  - BERT
KW  - Explainable AI
KW  - Symptom2Disease
KW  - Medical Diagnosis
KW  - Text Classification
KW  - Model Interpretability
KW  - Overfitting Mitigation
DO  - 10.1109/ICCIT64611.2024.11022559
JO  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
IS  - 
SN  - 2474-9656
VO  - 
VL  - 
JA  - 2024 27th International Conference on Computer and Information Technology (ICCIT)
Y1  - 20-22 Dec. 2024
AB  - This study proposes a custom BERT-based hybrid model aimed at enhancing the accuracy and explainability of disease prediction based on patient symptom descriptions. Utilizing the Symptom2Disease dataset, which includes 1,200 samples across 24 disease categories, the model processes textual symptom data and maps it to corresponding diseases. Key contributions of this work include the integration of Explainable AI (XAI) techniques to provide transparency in the model’s decision-making process and the employment of dropout to mitigate overfitting, ensuring that the model generalizes well on unseen data. The proposed model builds upon BERT’s pre-trained architecture by incorporating additional layers that refine the embeddings and enhance classification accuracy. Through rigorous experimentation, the model achieved an impressive 99% accuracy, outperforming existing models such as DistilBERT and MCN-BERT in both accuracy and training efficiency. The inclusion of XAI allows for visualizations of word-level contributions, offering insights into how specific symptoms influence disease predictions. Furthermore, a comparative analysis with previous studies demonstrates that the proposed model not only performs with higher accuracy but also achieves faster training times, making it both computationally efficient and effective for practical medical applications. The paper highlights the potential for applying hybrid architectures in healthcare, improving disease prediction through advanced natural language processing techniques.
ER  - 

TY  - JOUR
TI  - Advancing eHealth in Society 5.0: A Fuzzy Logic and Blockchain-Enhanced Framework for Integrating IoMT, Edge, and Cloud With AI
T2  - IEEE Access
SP  - 195710
EP  - 195730
AU  - J. Dutta
AU  - D. Puthal
PY  - 2024
KW  - Medical services
KW  - Artificial intelligence
KW  - 6G mobile communication
KW  - Security
KW  - Blockchains
KW  - Electronic healthcare
KW  - Real-time systems
KW  - Accuracy
KW  - Standards
KW  - Reliability
KW  - Fuzzy
KW  - blockchain
KW  - XAI
KW  - human-centric AI
KW  - eHealth
KW  - Society 5.0
DO  - 10.1109/ACCESS.2024.3520799
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Society 5.0 envisions a human-centered society where advanced technologies seamlessly integrate to enhance quality of life, particularly in healthcare. To advance eHealth within this vision, we present a comprehensive framework that integrates the Internet of Medical Things (IoMT), edge computing, and cloud services with Explainable Artificial Intelligence (XAI) and blockchain technology, customized for the 6G era. We introduce the Health Prediction using Cloud Edge 2.0 (HPCE 2.0) algorithm, which employs fuzzy logic to effectively combine historical Electronic Health Records (EHRs) with real-time IoMT data, providing precise and personalized health severity level predictions. To ensure data integrity and security, we integrate the Proof of Authentication 2.0 (PoAh 2.0) consensus mechanism within a blockchain-enhanced IoMT-Edge-Cloud framework. A case study on predicting cardiac arrest in elderly patients demonstrates the practical effectiveness of our framework. Utilizing XAI models such as LIME and SHAP, we provide both local and global explanations for AI predictions, enhancing transparency and trust in healthcare applications; counterfactual explanations offer actionable insights for patients to proactively manage health risks. Security assessments confirm efficient block formation and verification times, validating the system’s scalability and compliance with stringent security standards. This work sets a new standard in digital healthcare by aligning technological advancements with ethical considerations, fostering a human-centered approach consistent with Society 5.0’s vision. Harnessing the capabilities of emerging 6G networks, our framework paves the way for more responsive, secure, and interpretable AI-driven healthcare solutions.
ER  - 

TY  - JOUR
TI  - Explainable AI for Intrusion Detection Systems: LIME and SHAP Applicability on Multi-Layer Perceptron
T2  - IEEE Access
SP  - 30164
EP  - 30175
AU  - D. Gaspar
AU  - P. Silva
AU  - C. Silva
PY  - 2024
KW  - Intrusion detection
KW  - Explainable AI
KW  - Analytical models
KW  - Perturbation methods
KW  - Surveys
KW  - Internet of Things
KW  - Computer security
KW  - Machine learning
KW  - Artificial intelligence
KW  - Multilayer perceptrons
KW  - Artificial intelligence
KW  - explainability
KW  - intrusion detection system
KW  - local interpretable model-agnostic explanations
KW  - machine learning
KW  - shapley additive explanations
DO  - 10.1109/ACCESS.2024.3368377
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - Machine learning-based systems have presented increasing learning performance, in a wide variety of tasks. However, the problem with some state-of-the-art models is their lack of transparency, trustworthiness, and explainability. To address this problem, eXplainable Artificial Intelligence (XAI) appeared. It is a research field that aims to make black-box models more understandable to humans. The research on this topic has increased in recent years, and many methods, such as LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations) have been proposed. Machine learning-based Intrusion Detection Systems (IDS) are one of the many application domains of XAI. However, most of the works about model interpretation focus on other fields, like computer vision, natural language processing, biology, healthcare, etc. This poses a challenge for cybersecurity professionals tasked with analyzing IDS results, thereby impeding their capacity to make informed decisions. In an attempt to address this problem, we have selected two XAI methods, LIME, and SHAP. Using the methods, we have retrieved explanations for the results of a black-box model, part of an IDS solution that performs intrusion detection on IoT devices, increasing its interpretability. In order to validate the explanations, we carried out a perturbation analysis where we tried to obtain a different classification based on the features present in the explanations. With the explanations and the perturbation analysis we were able to draw conclusions about the negative impact of particular features on the model results when present in the input data, making it easier for cybersecurity experts when analyzing the model results and it serves as an aid to the continuous improvement the model. The perturbations also serve as a comparison of performance between LIME and SHAP. To evaluate the degree of interpretability increase, and the explanations provided by each XAI method of the model and directly compare the XAI methods, we have performed a survey analysis.
ER  - 

TY  - CONF
TI  - An Approach for XAI Visualizations for Explainability of Alzheimer’s Detection
T2  - 2024 2nd International Conference on Networking, Embedded and Wireless Systems (ICNEWS)
SP  - 1
EP  - 6
AU  - S. Khanapur
AU  - C. B. Bharadwaj
AU  - R. Bhardwaj
AU  - J. S. Nayak
PY  - 2024
KW  - Wireless communication
KW  - Analytical models
KW  - Image analysis
KW  - Explainable AI
KW  - Soft sensors
KW  - Transforms
KW  - Predictive models
KW  - Data models
KW  - Alzheimer's disease
KW  - Medical diagnostic imaging
KW  - Explainable Artificial Intelligence
KW  - Analysis of Medical Images
KW  - Interpretability
DO  - 10.1109/ICNEWS60873.2024.10731000
JO  - 2024 2nd International Conference on Networking, Embedded and Wireless Systems (ICNEWS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 2nd International Conference on Networking, Embedded and Wireless Systems (ICNEWS)
Y1  - 22-23 Aug. 2024
AB  - In the field of medical image analysis, the integration of machine learning (ML) models has significantly enhanced the process of identifying and diagnosing Alzheimer’s disease. However, the opacity of these models, often termed as "black boxes," raises concerns regarding their interpretability and reliability in clinical settings. Explainable artificial intelligence (XAI) techniques have emerged as crucial tools to address these challenges by clarifying the reasoning behind model outputs. This paper offers an extensive review of literature concerning the employment of Explainable Artificial Intelligence (XAI) methodologies, such as SHAP, LIME, GradCAM, DeepLIFT, Saliency Maps, and LRP to augment the comprehensibility of ML models applied in the detection of Alzheimer’s disease.
ER  - 

TY  - CONF
TI  - An Explainable Artificial Intelligence Strategy for Transparent Deep Learning in the Classification of Eye Diseases
T2  - 2024 IEEE International Conference on Computing, Applications and Systems (COMPAS)
SP  - 1
EP  - 6
AU  - E. H. Shipra
AU  - M. Sazzadur Rahman
PY  - 2024
KW  - Training
KW  - Deep learning
KW  - Accuracy
KW  - Explainable AI
KW  - Computational modeling
KW  - Decision making
KW  - Predictive models
KW  - Prediction algorithms
KW  - Ophthalmology
KW  - Convolutional neural networks
KW  - Explainable Artificial Intelligence
KW  - Eye Disease
KW  - Fundus
KW  - Local Interpretable Model-agnostic Explanations
DO  - 10.1109/COMPAS60761.2024.10797058
JO  - 2024 IEEE International Conference on Computing, Applications and Systems (COMPAS)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 IEEE International Conference on Computing, Applications and Systems (COMPAS)
Y1  - 25-26 Sept. 2024
AB  - Significant global health issues are posed by eye illnesses, especially in poor nations with inadequate financial and technological resources. Recent advances in pattern recognition for medical diagnosis have increased accuracy, but consistent health data collection and Artificial Intelligence integration are critical for Machine Learning model reliability. Explainable AI has made expert-level evaluation critical in sensitive areas. This study uses a hybrid approach to provide accurate categorization of eye diseases by utilizing deep learning assisted by Explainable Artificial Intelligence. The key contributions include using a Sequential Convolutional Neural Network to categorize eye disease images, significantly boosting accuracy with pre-trained CNN models, and employing the Local Interpretable Model-Agnostic Explanations method for clear result interpretations. The study compares two experimental models, a black-box CNN model and a glass-box model using XAI algorithm LIME. While pre-trained models like Inception V3 and Xception showed training accuracies of 81.5% and 85.5% and validation accuracies of 78.8% and 73.8%, respectively, the sequence model obtained training accuracy of 80.48% and validation accuracy of 76.34%. When combined with LIME, these pre-trained models showed increased interpretability providing thorough insights into disease prediction. Future research can be done on improving the effectiveness of AI-driven ophthalmic decision-making by comparing pre-trained models and XAI algorithms and experimenting with different CNN architectures.
ER  - 

TY  - CONF
TI  - AI in Bioinformatics and Genomics
T2  - 2024 International Conference on Intelligent & Innovative Practices in Engineering & Management (IIPEM)
SP  - 1
EP  - 8
AU  - S. Mukherjee
AU  - K. Thapliyal
AU  - A. Maurya
AU  - N. Chaudhary
AU  - S. Sudeshna
PY  - 2024
KW  - Biotechnology
KW  - Accuracy
KW  - Reviews
KW  - Systems biology
KW  - Genomics
KW  - Prediction algorithms
KW  - Biology
KW  - Reflection
KW  - Bioinformatics
KW  - Artificial intelligence
KW  - AI
KW  - Bioinformatics
KW  - Genomics
KW  - complex biological data
KW  - accelerating advancements
KW  - biotechnology
DO  - 10.1109/IIPEM62726.2024.10925681
JO  - 2024 International Conference on Intelligent & Innovative Practices in Engineering & Management (IIPEM)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Intelligent & Innovative Practices in Engineering & Management (IIPEM)
Y1  - 25-25 Nov. 2024
AB  - Artificial intelligence (AI) has revolutionized various fields, including bioinformatics and genomics, by offering powerful tools and techniques to analyse and interpret complex biological data. This research paper explores the significant role of AI in bioinformatics and genomics, highlighting its applications, advancements, challenges, and future directions. Through a comprehensive review of relevant literature, case studies, and examples, and aims the power of AI transformation to provide insights on the study of biological systems, accelerating advancements in personalized medicine and biotechnology and enhancing our understanding of genomics. These areas have risen from the needs of biologists to utilize and help interpret the vast amounts of data that are constantly being gathered in genomic research. Artificial intelligence (AI) has increasingly gained attention in bioinformatics research and computational molecular biology has become common for the researchers to apply the off-shelf systems with the availability of different types of AI algorithms. At present researchers are facing difficulties in choosing the best method that could be applied to a specific data set, with various intelligent methods available in the literature. The researchers need to apply machine learning and AI with different tools and techniques which present the data in an annotated with context, estimates of accuracy, proper predictions and explanation. comprehensible fashion. This article aims to review the use of AI in the areas of bioinformatics and computational molecular biology (DNA sequencing). The AI in functional genomics, and its application in this paper defines the importance of applicability and interpretability including ethical, legal and economic issues.
ER  - 

TY  - CONF
TI  - Algorithmic Approaches, Practical Implementations and Future Research Directions in Machine Learning
T2  - 2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)
SP  - 121
EP  - 126
AU  - V. Sharma
AU  - D. Sharma
AU  - S. Kumar Punia
PY  - 2024
KW  - Industries
KW  - Machine learning algorithms
KW  - Explainable AI
KW  - Reviews
KW  - Force
KW  - Transportation
KW  - Finance
KW  - Medical services
KW  - Learning (artificial intelligence)
KW  - Semisupervised learning
KW  - Machine Learning
KW  - Algorithmic Approaches
KW  - Practical Implementations
KW  - Semi-Supervised Learning
KW  - Explainable AI
KW  - Robustness
KW  - Privacy-Preserving Learning
KW  - Quantum Machine Learning
KW  - Multimodal AI
KW  - Ethical AI
KW  - Cross-Domain Collaboration
KW  - Future Research Directions
DO  - 10.1109/ICAC2N63387.2024.10895837
JO  - 2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)
Y1  - 16-17 Dec. 2024
AB  - Machine learning has become a disruptive force that is advancing technology and changing industries. With an emphasis on algorithmic techniques, real-world applications, and important future research avenues, this study examines the rapidly changing field of machine learning. It explores the fundamentals of supervised, unsupervised, and semi-supervised learning algorithms and highlights how they are used in a variety of fields, including autonomous systems, healthcare, and finance. The report highlights the potential of several important future research directions, such as explainable AI, robust and privacy-preserving learning, quantum machine learning, and multimodal AI, to overcome present constraints and open up hitherto unheard-of possibilities. These multidisciplinary research avenues emphasize the value of interdisciplinary cooperation in addressing difficult problems and guaranteeing the creation of morally sound and significant AI systems. In order to help academics and practitioners who want to progress the subject, this review attempts to give a thorough grasp of the present situation and potential future direction of machine learning.
ER  - 

TY  - CONF
TI  - Improving Cardiac Disease Prognosis Through Machine Learning: An Extensive Examination of Algorithms and Predictive Models
T2  - 2024 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
SP  - 1
EP  - 6
AU  - N. Patel
AU  - R. Vaghela
PY  - 2024
KW  - Support vector machines
KW  - Heart
KW  - Logistic regression
KW  - Machine learning algorithms
KW  - Cardiac disease
KW  - Machine learning
KW  - Predictive models
KW  - Artificial Intelligence
KW  - Machine learning
KW  - Heart Ailment
KW  - Healthcare
KW  - Prognosis
KW  - SVM
DO  - 10.1109/ASSIC60049.2024.10507998
JO  - 2024 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
IS  - 
SN  - 
VO  - 
VL  - 
JA  - 2024 International Conference on Advancements in Smart, Secure and Intelligent Computing (ASSIC)
Y1  - 27-29 Jan. 2024
AB  - The heart plays a vital role as the central organ in the human body, responsible for supplying oxygen and nutrients to all other organs through the blood circulation. However, any malfunction in its functioning can lead to severe consequences. One of the examples is cardiac arrest. The early prediction of cardiac issues poses a significant challenge in the medical field. By harnessing various machine learning techniques, it becomes feasible to make predictions based on additional health-related data, thereby facilitating the identification and prevention of heart-related problems. With the abundance of patient data available, these predictive models can assist in forecasting future disease outbreaks. Machine learning algorithms, such as Decision Trees, Logistic Regression, K-Nearest Neighbors, Support Vector Machines, and others, have been extensively utilized in the prognosis of cardiac diseases, enabling more accurate predictions.
ER  - 

TY  - JOUR
TI  - Decoding Gait Signatures: Exploring Individual Patterns in Pathological Gait Using Explainable AI
T2  - IEEE Access
SP  - 192603
EP  - 192617
AU  - D. Slijepcevic
AU  - F. Horst
AU  - M. Leonard Simak
AU  - W. Immanuel Schöllhorn
AU  - B. Horsak
AU  - M. Zeppelzauer
PY  - 2024
KW  - Three-dimensional displays
KW  - Pathology
KW  - Reliability
KW  - Support vector machines
KW  - Training
KW  - Gain measurement
KW  - Footwear
KW  - Explainable AI
KW  - Accuracy
KW  - Legged locomotion
KW  - Biomechanics
KW  - gait analysis
KW  - gait recognition
KW  - ground reaction forces
KW  - interpretability
KW  - layer-wise relevance propagation
KW  - machine learning
KW  - personalized medicine
KW  - precision medicine
DO  - 10.1109/ACCESS.2024.3513893
JO  - IEEE Access
IS  - 
SN  - 2169-3536
VO  - 12
VL  - 12
JA  - IEEE Access
Y1  - 2024
AB  - This study explores the application of machine learning (ML) to derive and analyze individual gait patterns (i.e., gait signatures) from ground reaction force data. This study leverages three datasets containing 2,092 individuals, including 1,283 cases with pathological gait, and addresses three key objectives: (1) Demonstrating the uniqueness of gait signatures in a large-scale dataset with heterogeneity introduced by patient data and various conditions. (2) Characterizing gait signatures using explainable artificial intelligence (XAI) to highlight specific features that contribute to their uniqueness. (3) Evaluating the reliability of gait signatures and their characterizations across different numbers of individuals and training samples per individual. The results show that ML can accurately differentiate unique gait patterns across healthy individuals and patients with pathological gait patterns, highlighting the importance of considering individual gait signatures in clinical gait analysis. The high reliability of a person’s unique gait signature may provide the basis for more personalized treatment decisions and rehabilitation programs, with XAI methods providing valuable insights into the key features that characterize individual gait. These results indicate that even more refined and personalized approaches are possible, extending beyond the conventional categories of pathology, age, and sex. This study provides a foundation for exploring the practical impact of gait signatures on rehabilitation, clinical diagnosis, and personalized treatment strategies.
ER  - 

TY  - CONF
TI  - Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh
T2  - 2024 6th International Conference on Sustainable Technologies for Industry 5.0 (STI)
SP  - 1
EP  - 6
AU  - S. Sarker
PY  - 2024
KW  - Deep learning
KW  - Visualization
KW  - Accuracy
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Computational modeling
KW  - Brain tumors
KW  - Brain modeling
KW  - Rendering (computer graphics)
KW  - Data models
KW  - Brain tumor classification
KW  - Explainable AI (XAI)
KW  - Transfer Learning(TL)
KW  - Grad-CAM
KW  - Grad-CAM++
KW  - VG-GNNet
KW  - DenseNet
DO  - 10.1109/STI64222.2024.10951092
JO  - 2024 6th International Conference on Sustainable Technologies for Industry 5.0 (STI)
IS  - 
SN  - 2996-170X
VO  - 
VL  - 
JA  - 2024 6th International Conference on Sustainable Technologies for Industry 5.0 (STI)
Y1  - 14-15 Dec. 2024
AB  - Brain tumors, regardless of being benign or ma-lignant, pose considerable health risks, with malignant tumors being more perilous due to their swift and uncontrolled prolifer-ation, resulting in malignancy. Timely identification is crucial for enhancing patient outcomes, particularly in nations such as Bangladesh, where healthcare infrastructure is constrained. Manual MRI analysis is arduous and susceptible to inaccuracies, rendering it inefficient for prompt diagnosis. This research sought to tackle these problems by creating an automated brain tumor classification system utilizing MRI data obtained from many hospitals in Bangladesh. Advanced deep learning models, including VGGI6, VGGI9, and ResNet50, were utilized to classify glioma, meningioma, and various brain cancers. Explainable AI (XAI) methodologies, such as Grad-CAM and Grad-CAM++, were employed to improve model interpretability by emphasizing the critical areas in MRI scans that influenced the categorization. VGG16 achieved the most accuracy, attaining 99.17%. The integration of XAI enhanced the system's transparency and stability, rendering it more appropriate for clinical application in resource-limited environments such as Bangladesh. This study highlights the capability of deep learning models, in conjunction with explainable artificial intelligence (XAI), to enhance brain tumor detection and identification in areas with restricted access to advanced medical technologies.
ER  - 

TY  - CHAP
TI  - Building Robust Medical Algorithms
T2  - AI Doctor: The Rise of Artificial Intelligence in Healthcare - A Guide for Users, Buyers, Builders, and Investors
SP  - 27
EP  - 65
AU  - Ronald M. Razmi
PY  - 2024
KW  - Artificial intelligence
KW  - Biomedical imaging
KW  - Medical services
KW  - Data models
KW  - Training
KW  - Soft sensors
KW  - Buildings
KW  - Large language models
KW  - Diseases
KW  - Computed tomography
DO  - 10.1002/9781394240197.ch2
PB  - Wiley
SN  - 9781394240180
UR  - http://ieeexplore.ieee.org/document/10951980
AB  - Summary <p>The best datasets have the perfect combination of quantity and quality, as well as enough diversity to be fully representative of the different types of patients that the algorithm will be used for. Obtaining high&#x2010;quality data for the training and validation of Artificial intelligence (AI) models is challenging. If the people want to aggregate data from different sources so that they can train and use AI algorithms in healthcare, data standardization will be critical. Federated learning is an up&#x2010;and&#x2010;coming approach to AI training that aims to protect the privacy of sensitive user data by ensuring that it never leaves their device. Art generated by generative AI can be impossible to distinguish from a painting by humans. Synthetic data is one way of dealing with the issue of creating datasets for algorithm training.</p>
ER  - 

TY  - CONF
TI  - Exploring Radiologists' Reluctance Towards Machine Learning Models and Explainable AI in Brain Tumor Detection
T2  - 2024 6th International Conference on Advancements in Computing (ICAC)
SP  - 25
EP  - 30
AU  - B. Goonatillaka
AU  - P. Haddela
PY  - 2024
KW  - Training
KW  - Accuracy
KW  - Three-dimensional displays
KW  - Explainable AI
KW  - Magnetic resonance imaging
KW  - Computational modeling
KW  - Brain tumors
KW  - Medical services
KW  - Predictive models
KW  - Brain modeling
KW  - Explainable AI
KW  - Brain Tumor Detection
KW  - MRI Imaging
KW  - ML model Interpretability
KW  - Medical Data Analysis
DO  - 10.1109/ICAC64487.2024.10851023
JO  - 2024 6th International Conference on Advancements in Computing (ICAC)
IS  - 
SN  - 2837-5424
VO  - 
VL  - 
JA  - 2024 6th International Conference on Advancements in Computing (ICAC)
Y1  - 12-13 Dec. 2024
AB  - Machine Learning (ML) models have made significant advancements in medical sciences, yet their application in clinical practice is hindered by concerns over transparency and accountability. Explainable Artificial Intelligence (XAI) emerges as a solution to these challenges, offering interpretable insights into ML model predictions for clinicians. This research investigates the extent to which existing XAI techniques support decision-making in brain tumor detection for radiologists. A highly accurate ML model based on the VGG16 architecture was developed for detecting brain tumors from MRI images, achieving training and testing accuracy of 99.46% and 96.28%, respectively. A series of experiments were conducted integrating various XAI techniques. The study reveals, despite high model accuracy, false positives can mislead clinical decisions, especially when datasets contain multiple MRI sequences with conflicting diagnostic information. The findings, validated by expert radiologists, underscore the importance of using 3D imaging or at least consecutive 2D slices from the same sequence to improve diagnostic accuracy. The paper presents the experimental details highlighting the critical gap between 2D MRI slice-based diagnosis and the need for comprehensive 3D analysis. The research concludes with recommendations for improved dataset design and the integration of XAI to enhance the reliability and accountability of AI -driven healthcare diagnostic systems.
ER  - 

